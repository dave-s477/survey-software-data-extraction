<html><body link="black" alink="black" vlink="black" hlink="black">
<b>PMC1239908</b><br>
A method for finding single-nucleotide polymorphisms with allele frequencies in sequences of deep coverage<br>
<br>
<br>
Background<br>
Information concerning the allele frequencies of single-nucleotide polymorphisms (SNPs) is needed to select an optimal subset of common SNPs for use in association studies [1]. One approach to finding common SNPs with allele frequencies is to generate DNA sequences from a sufficient number of samples in a population. This approach requires that computational methods have an ability to handle thousands of sequences from the same genome location (sequences of deep coverage). In this paper, we describe a computational method for finding common SNPs with allele frequencies in sequences of deep coverage. We present results from the method on human expressed sequence tags (ESTs) of deep coverage, which are currently a major source of DNA sequences of deep coverage. The method is also expected to be useful for finding common mutations in sequences of deep coverage produced in a cancer genome project [2].<br>
The <software>PolyBayes</software> program is widely used to find SNPs in redundant DNA sequences [3,4]. It first constructs a multiple sequence alignment based on pairwise alignments of each sequence with a high-quality genomic sequence called an anchor. Then it identifies and removes paralogous sequences that have a high number of observed differences with the anchor sequence. Next it computes an SNP probability score for each column of the multiple sequence alignment based on a rigorous Bayesian formula. The formula uses the prior probabilities of all the nucleotide permutations for the column, which are estimated from the quality scores of the bases on the column.<br>
We enhance the <software>PolyBayes</software> program in several aspects to handle single-pass sequences (query sequences) of deep coverage. First, all the paralogous regions of the finished human genome sequence are included as anchor sequences. Each query sequence is assigned to the corresponding anchor sequence that is different from each of the remaining anchor sequences at some positions but is identical to the query sequence at most of the positions. This approach separates paralogous sequences by making use of the positions where paralogous sequences differ but sequences from the same genome location agree.<br>
Second, pairwise alignments of corresponding query and anchor sequences are used to construct profiles, one per anchor sequence. At each position of an anchor sequence, its profile contains the numbers and types of high-quality query bases that are aligned to the position of the anchor sequence. Candidate SNPs are produced based on the profiles, instead of multiple sequence alignments for the following reason. As the number of single-pass sequences in a multiple sequence alignment increases, the number of gap columns in the alignment increases but the number of identity columns in the alignment does not increase. Thus, it is difficult to construct an accurate multiple sequence alignment for single-pass sequences of deep coverage.<br>
Third, because the pairwise alignment of corresponding query and anchor sequences may contain regions of low similarity due to sequencing errors or contaminants, the highly similar regions of the alignment are found by a dynamic programming algorithm. Only the highly similar regions are used in generation of the profile.<br>
Our computer program named <software>PolyFreq</software> was compared with <software>PolyBayes</software> on eighteen data sets of human EST sequences of deep coverage. Results from <software>PolyFreq</software> and <software>PolyBayes</software> indicate that <software>PolyFreq</software> ran to completion and used almost all input sequences in computation of the allele frequencies of SNPs for every data set.<br>
<br>
Results<br>
The method for finding SNPs with allele frequencies was implemented as a computer program. The source code of the program is freely available for academic use [5, see Additional file 1]. The program takes as input a set of high-quality anchor sequences and a set of query sequences with quality scores. The set of anchor sequences includes all the paralogous regions of the genome for the set of query sequences. The anchor and query sequences are from the same species.<br>
The program reports candidate SNPs in the anchor sequences. For each candidate SNP, the program reports its position in the anchor sequence, its local context in the anchor sequence, and base types with a frequency greater than a cutoff. The frequency of a base type is also given in a rational form with the number of query bases of the type as the enumerator and the total number of query bases as the denominator.<br>
To evaluate <software>PolyFreq</software>, eighteen data sets of human EST sequences of deep coverage were constructed as follows. Eighteen clusters of human EST sequences, each containing at least 1,000 EST sequences with trace data, were randomly selected from the April, 2005 release of the <database>UniGene</database> database [6]. The eighteen <database>UniGene</database> clusters also contain EST sequences without trace data. For each of the eighteen <database>UniGene</database> clusters, an EST data set was obtained by selecting all EST sequences with trace data from the cluster. The set of quality score sequences for each of the eighteen data sets was produced from the trace data with <software>Phred</software> [7]. The quality score q of a base is obtained by the formula q = -10 log p, where p is the estimated error probability of the base [8]. For example, a quality score of 20 corresponds to an error probability of 0.01. The EST sequences in each of the eighteen sets were produced from 71 to 118 cDNA libraries derived from diverse human tissues and cell lines [9]. Each of the eighteen data sets of full-length EST sequences without any masking was used as a query set.<br>
For each query set, its set of anchor sequences was obtained by comparing the query sequences with the finished genome sequence at the <software>BLAT</software> web server [10]. By using stringent settings for <software>BLAT</software>, a set of two human anchor sequences was produced for each of three query data sets, and a set of one human anchor sequence was produced for each of the remaining query data sets. Each set of anchor sequences was screened for repeats with <software>RepeatMasker</software> [11].<br>
The <software>PolyFreq</software> program was run on each pair of query and anchor sets. The <software>PolyFreq</software> program ran successfully to completion for each of the eighteen data sets. The following values were used for the parameters of the program: 50, minimum depth of coverage for each candidate SNP; 0.1%, minimum minor allele frequency; 5 bp, minimum perfect block length; 20, minimum base quality score in the perfect block; 90%, minimum percent identity for query-anchor alignments; 97%, minimum percent identity for the highly similar regions of query-anchor alignments.<br>
Although <software>PolyBayes</software> was not designed to deal with data sets of deep coverage, we tested <software>PolyBayes</software> on the eighteen data sets of deep coverage to see how <software>PolyBayes</software> would behave on the data sets. Because <software>PolyBayes</software> takes only one anchor sequence, the corresponding anchor sequence was selected and given to <software>PolyBayes</software> for each data set. On eight of the eighteen data sets, <software>PolyBayes</software> ran successfully to completion. On the remaining data sets, <software>PolyBayes</software> terminated abnormally without producing any output file after running for a few hours. The default values for all the parameters but the SNP probability output cutoff of <software>PolyBayes</software> were used; <software>PolyBayes</software> terminated abnormally more frequently under other parameter values. A value of 0.75 for the SNP probability output cutoff was used to produce a lower number of false positives than the default value of 0.5.<br>
The abnormal termination of <software>PolyBayes</software> might be related to the deep coverage of the data set and full-length EST sequences with low-quality ends or contaminants. Thus, for each set of full-length EST sequences, a set of trimmed EST sequences was produced by removing the ends of every sequence that are not highly similar to the corresponding anchor sequence. For each data set, the number of trimmed sequences was close to the number of full-length sequences. The <software>PolyBayes</software> program was also run on each set of trimmed sequences. It ran to completion for thirteen out of the eighteen data sets.<br>
All the tests were performed on a Dell workstation with two 3.0-Ghz processors and 4 Gb of main memory. <software>PolyFreq</software> took less than one hour on every data set, whereas <software>PolyBayes</software> was two to ten times slower than <software>PolyFreq</software> on every data set. The memory requirements of <software>PolyFreq</software> and <software>PolyBayes</software> on the data sets were similar and were about 30 to 40 times the input size.<br>
The <software>PolyFreq</software> and <software>PolyBayes</software> programs were compared on every data set for which <software>PolyBayes</software> ran successfully to completion on either the set of full-length sequences or the set of trimmed sequences. For each data set, results produced by <software>PolyFreq</software> on the set of full-length sequences were included in the comparison, whereas results produced by <software>PolyBayes</software> on both sets of full-length and trimmed sequences were included. The SNPs from the <database>dbSNP</database> database [12] that were mapped by the following method to the anchor sequences were used as true SNPs for the comparison. Each SNP from <database>dbSNP</database> is specified by a local sequence context. For each data set of EST sequences with a <database>RefSeq</database> sequence [13], each SNP from <database>dbSNP</database> that occurs in the <database>RefSeq</database> sequence was determined by finding the exact occurrence of its sequence context in the <database>RefSeq</database> sequence. Each SNP from <database>dbSNP</database> in the <database>RefSeq</database> sequence was mapped to a corresponding anchor sequence by using a spliced alignment of the <database>RefSeq</database> and anchor sequences. Because four data sets had no <database>RefSeq</database> sequence, no SNPs from <database>dbSNP</database> were mapped to the anchor sequences for the data sets.<br>
For each program on every data set with a <database>RefSeq</database> sequence, the number of true positives, the number of false positives, and the number of false negatives were computed. The number of true negatives was not collected because of its large value. Also reported were the number of sequences in the data set and the number of sequences that were used by the program to compute candidate SNPs. The comparison results are shown in Table 1.<br>
The results in Table 1 indicate that <software>PolyFreq</software> could handle the data sets of full-length reads with problem regions and with very deep coverage. <software>PolyFreq</software> used 1,997 to 8,280 sequences on the five data sets for which <software>PolyBayes</software> terminated abnormally. On the data sets for which <software>PolyBayes</software> ran to completion, <software>PolyFreq</software> was similar to <software>PolyBayes</software> in the number of true positives and the number of false negatives, and is better than <software>PolyBayes</software> in the number of false positives. <software>PolyBayes</software> used significantly fewer sequences than <software>PolyFreq</software> on some of the data sets. Note that the ability to use as many sequences as possible is necessary for accurate computation of the allele frequencies of SNPs.<br>
<br>
Discussion<br>
We originally developed a method for assembling sequences of deep coverage. The method constructs multiple sequence alignments of large width for contigs. The method has to deal with a large number of gap columns in the multiple sequence alignment. We later agreed with one of the reviewers that it is not necessary to construct multiple sequence alignments for analysis of sequences of deep coverage. The reviewer also suggested that we focus on SNP analysis in sequences of deep coverage. Those suggestions motivated us to develop the method reported in this paper.<br>
The <software>PolyFreq</software> program keeps <software>PolyBayes</software>' feature of performing comparisons between query and anchor sequences, instead of performing comparisons among query sequences. In addition, <software>PolyFreq</software> constructs profiles by using the highly similar regions of pairwise alignments of corresponding query and anchor sequences, instead of multiple alignments of query and anchor sequences. Thus, the efficiency and accuracy of <software>PolyFreq</software> are not significantly affected by query sequences of deep coverage. On the contrary, <software>PolyFreq</software> can compute the allele frequencies of SNPs more accurately in query sequences of deep coverage.<br>
As sequencing costs are significantly reduced in the future, single-pass sequences from hundreds to thousands of individuals will be produced. Those sequences will be of deep coverage. Our current work suggests that it is possible to analyze sequences of deep coverage by using pairwise alignments of the sequences with the finished genome sequence, instead of multiple sequence alignments.<br>
<br>
Methods<br>
We first present the major steps of our method for finding common SNPs with allele frequencies in a set of query sequences and a set of anchor sequences. Then we describe each step in detail. The method consists of the following major steps:<br>
1. Compute an alignment of anchor sequences for each pair of anchor sequences.<br>
2. Compute an alignment of query and anchor sequences for each pair of similar query and anchor sequences.<br>
3. For each query sequence, find the corresponding anchor sequence that is different from each of the remaining anchor sequences at some positions but is identical to the query sequence at most of the positions.<br>
4. Find the highly similar regions of their alignment for each pair of corresponding query and anchor sequences.<br>
5. For each anchor sequence, use the highly similar regions of every alignment involving the anchor sequence to construct a profile for the anchor sequence. At each position of the anchor sequence, its profile contains the numbers and types of high-quality query bases that are aligned to the position of the anchor sequence.<br>
6. Report each candidate SNP with major and minor allele frequencies if its minor allele frequency is greater than a cutoff.<br>
In step 1, for each pair of anchor sequences, an alignment of the sequences in given orientation as well as an alignment of the sequences in opposite orientation is constructed with <software>GAP3</software>, a global alignment program specially designed for genomic sequences with long different regions between similar regions [14]. One of the two alignments with a larger score is saved for the pair of sequences. The alignments saved in this step are to be used in step 3 for finding the corresponding anchor sequence for each query sequence.<br>
In step 2, pairs of similar query and anchor sequences are found with <software>DDS2</software>, which produces a high-scoring chain of segment pairs (ungapped alignment fragments) between the two sequences in the pair [15]. For each pair of similar query and anchor sequences, an alignment of the sequences in the pair is constructed with <software>GAP22</software>, an improved version of the <software>GAP2</software> program [16] for quickly computing an alignment in a small area of the dynamic programming matrix, which is determined based on the chain of segment pairs. If the percent identity of the alignment is greater than a cutoff, then the alignment is saved for the pair of sequences.<br>
In step 3, for each query sequence that is highly similar to two or more anchor sequences, the corresponding anchor sequence for the query sequence is selected among the anchor sequences through pairwise comparisons. Initially, one anchor sequence is taken as the current leader. Then the rest are compared with the current leader one at a time. Consider the comparison between the current leader and the current challenger. The winner between the two anchor sequences is produced by using the alignment of the two anchor sequences and their alignments with the query sequence. A common match occurs at a position of the query sequence, a position of the current leader, and a position of the current challenger if the three positions are pairwise aligned on each of the three alignments and contain the same base. The winner between the two anchor sequences is the one with a larger number of uncommon matches in its alignment with the query sequence. The winner becomes the current leader. After all the pairwise comparisons, the final leader is the corresponding anchor sequence for the query sequence.<br>
In step 4, for each pair of corresponding query and anchor sequences, the highly similar regions of the alignment of the two sequences are identified in linear time with <software>LCP</software>, a program for finding regions of a sequence that meet a content requirement [17]. Each of the highly similar regions found by <software>LCP</software> has a percent identity greater than or equal to a cutoff p and is strictly optimal. The score of a region of the alignment is the sum of scores of every base match and every base difference in the region, where the score of every base match is 1 - p and the score of every base difference is - p. A region is optimal if its score is not less than the score of any other region that overlaps with it. An optimal region is strictly optimal if it is not completely contained in any optimal region other than itself.<br>
In step 5, only substitutions in the highly similar regions of every alignment of corresponding query and anchor sequences are used to construct a profile for the anchor sequence because the remaining regions of the alignment have a high rate of difference, which is likely due to sequencing errors or contaminants in the query sequence. Additional requirements are introduced below because a long highly similar region may still contain a packet of sequencing errors in the middle. A sufficiently long section in a highly similar region of an alignment is a perfect block if the section consists only of exact base matches and the quality score of each query base in the section is greater than or equal to a cutoff [18]. A substitution in a highly similar region of an alignment is acceptable if it is immediately flanked on each side by a perfect block. Acceptable and unacceptable substitutions are illustrated in Figure 1A.<br>
For each anchor sequence, its profile contains four counts at each position: one count for each query base type. For example, the base A count at the anchor position is the number of acceptable substitutions at the anchor position and at a query sequence position with base A, in a highly similar region of an alignment of the anchor sequence with the query sequence. A count for a query base type at the anchor position is 0 if there is no acceptable substitution at the anchor position and at any query sequence position with the query base type. The frequency of each of the four counts is the count divided by the sum of the four counts if the sum is positive.<br>
In step 6, each profile is scanned for candidate SNPs. A candidate SNP occurs at an anchor sequence position if the sum of the four counts for the position is greater than or equal to a cutoff and at least two of the four counts have a frequency greater than a cutoff. All candidate SNPs with allele frequencies are reported along with a local anchor sequence region for each candidate SNP. A candidate SNP with allele frequencies from one of the examples in Table 1 is shown in Figure 1B.<br>
<br>
Author's contributions<br>
XH designed the strategy for solving the problem and provided guidance to JW. JW worked out the details of the strategy, developed the program, and produced results on data sets with the program. XH wrote the paper and JW formatted it in Word. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2717951</b><br>
Unsupervised assessment of microarray data quality using a Gaussian mixture model<br>
<br>
<br>
Background<br>
Recently, the MicroArray Quality Control (MAQC) consortium found that most microarray platforms will generate reproducible data when used correctly by experienced researchers [1]. Despite this positive result, it has been suggested that 20% or more of the data available in public microarray data repositories may be of questionable quality [2]. For this reason, discriminating between high and low quality microarray data is of the highest importance, and several recent publications have dealt with this problem; detailed reviews are provided by Wilkes et al. [3] and Eads et al. [4].<br>
Several approaches have emphasized the importance of measuring, either directly or indirectly, the integrity of the RNA samples used in the experiment (e.g. [5-7]). Other research has focused on spatial artifacts: problems that typically arise during hybridization due to bubbling, scratches and edge effects [8,9].<br>
In the case of Affymetrix GeneChips, which we will use to demonstrate our method, there are standard benchmark tests provided by the manufacturer [10]. A standard complementary approach is to use the <software>R</software> statistical software, along with the <database>BioConductor</database> [11] "<package>affy</package>" [12] and "<package>affyPLM</package>" [13] packages, to produce a series of diagnostic plots for the assessment of GeneChip quality (see additional file 1: Fig S3, S4). A review of the quality control features available in <database>BioConductor</database> can be found in [14], and a variety of software packages are now available to assist in the automation of this process [15-19].<br>
In general, the goal of these approaches is to identify chips that are outliers ? either in relation to other chips in the same experiment or the entire theoretical population of similar chips. Often, it is assumed that a rational decision regarding data quality is made only after considering several quasi-orthogonal dimensions of quality. Chips are typically rejected only after a preponderance of the evidence indicates poor quality; a slightly unusual score on a single metric is frequently ignored, while a number of moderately or highly unusual scores on a variety of quality metrics is often grounds for exclusion of a particular chip from further analysis. However, there are no universal, robust thresholds available for the identification of outliers according to the various quality variables. Instead, decisions are necessarily made using historical data, either implicitly or explicitly.<br>
Therefore, recent efforts have focused on providing a "holistic", accurate, and automatic interpretation of diagnostic plots and quality metrics. Burgoon et al. [20] describe a custom, in-house protocol for assessing data quality of two-color spotted cDNA arrays. The authors advocate an integrated "Quality Assurance Plan" which attempts to integrate quality control at every level of the experimental procedure.<br>
Another example is the <software>RACE</software> system [15,16]. This system utilizes various statistics extracted from the <database>BioConductor</database> diagnostic plots, along with a random forest classifier, to automatically identify low quality data. However, like the quality assurance protocol described by Burgoon et al., the <software>RACE</software> system relies on a large expert-annotated data set. For this reason, it is difficult to keep the system up-to-date in the face of rapidly changing technology, with new chip types continually being introduced into the market. A further challenge is to adapt such a system to similar, but slightly different, types of data such as Affymetrix SNP arrays, exon arrays, or arrays produced by other manufacturers such as Illumina and Agilent.<br>
In this paper we investigate a method for unsupervised classification that was designed with these considerations in mind. First, we describe how to frame the interpretation of microarray quality indicators as an unsupervised classification problem using a Gaussian mixture model. We show how the model parameters can be estimated using the Expectation-Maximization (EM) algorithm [21], and how they can be used to construct a Na?ve Bayes classifier for identifying low quality data.<br>
Previous work has demonstrated that na?ve Bayes classifiers perform well with labeled training sets in the supervised version of the problem discussed in this paper [15]. The combination of Na?ve Bayes together with EM has been used with considerable success in other problem domains, including text classification [22]. Gaussian mixture models have been applied to automatic quality assessment of phone signal clarity [23] and mass spectrometry data [24], and in other stages of the microarray processing pipe-line, including identification of differentially expressed genes [25], assessment of the concordance between sets of similar microarray data sets [26], and even quality control at the spot detection and image fluorescence analysis level [27]. However, this is the first research we are aware of that employs this estimation approach, in conjunction with a na?ve Bayes classifier, for the purpose of array-level quality control of microarray data.<br>
In the following sections, we describe the datasets used in this research, and explain the implementation of both the supervised and unsupervised versions of the quality classifier. We demonstrate that the performance of the unsupervised classifier is comparable to a supervised classifier constructed from expert-labeled data. We also apply the algorithm to Affymetrix exon array data, and compare the observed quality indicator distributions with those obtained from 3' expression arrays.<br>
<br>
Methods<br>
Datasets<br>
Our first dataset is a set of 603 Affymetrix raw intensity microarray data files, from 32 distinct experiments downloaded from the NCBI <database>GEO</database> database [28]. A variety of Affymetrix GeneChip 3' Expression array types are represented in the dataset, including: ath1121501 (Arabidopsis, 248 chips; <database>GEO</database> accession numbers: GSE5770, GSE5759, GSE911 [29], GSE2538 [30], GSE3350 [31], GSE3416 [32], GSE5534, GSE5535, GSE5530, GSE5529, GSE5522, GSE5520, GSE1491 [33], GSE2169, GSE2473), hgu133a (human, 72 chips; GSE1420 [34], GSE1922), hgu95av2 (human, 51 chips; GSE1563 [35]), hgu95d (human, 22 chips; GSE1007 [36]), hgu95e (human, 21 chips; GSE1007), mgu74a (mouse, 60 chips; GSE76, GSE1912 [37]), mgu74av2 (mouse, 29 chips; GSE1947 [38], GSE1419 [39,40]), moe430a (mouse, 10 chips; GSE1873 [41]), mouse4302 (mouse, 20 chips; GSE5338 [42], GSE1871 [43]), rae230a (rat, 26 chips; GSE1918, GSE2470), and rgu34a (rat, 44 chips; GSE5789 [44], GSE1567 [45], GSE471 [46]). These experiments cover many of the species commonly analyzed using the GeneChip platform, and were selected to represent a variety of tissue types and experimental treatments.<br>
The <database>BioConductor</database> rma() function was used to perform probeset summarization, background subtraction and quantile normalization, with each raw intensity (.CEL) file preprocessed together with the other chips from the same <database>GEO</database> experiment. A variety of quality control indicators, listed in Table 1, were then computed for each chip. A list of all the .CEL files and their <database>GEO</database> identifiers, along with quality control feature scores and expert annotations, can be found in additional file 2. Also included in the file are descriptions explaining how each of the 29 quality control feature scores is computed from the raw expression data.<br>
The second dataset consists of all of the exon array .CEL files available in the <database>GEO</database> database at the time of this analysis (540 .CEL files). Fourteen different experiments are represented (GSE10599 [47], GSE10666 [48], GSE11150 [49], GSE11344 [50], GSE11967 [51], GSE12064 [52], GSE6976 [53], GSE7760 [54], GSE7761 [55], GSE8945 [56], GSE9342, GSE9372 [57], GSE9385 [58], GSE9566 [59]). The dataset includes examples of the Mouse Exon 1.0 ST array and several versions of the Human Exon 1.0 ST array. This dataset was processed using two different methods. First, the same set of quality indicators described above for the 3' expression dataset was prepared using the <database>BioConductor</database> packages in <software>R</software>. The "aroma" <fileFormat>.cdf</fileFormat> annotation files [60] were used to read in expression values for the core probes on the arrays. In addition, this second dataset was also processed using the <software>Affymetrix Expression Console</software> software. Only the "core" probesets were considered and the software was used to perform "gene-level" probeset summarization, background subtraction and quantile normalization using the "RMA sketch" option in the software. Several alternative quality indicators were then computed (Table 2). A list of the <fileFormat>.CEL</fileFormat> files and their <database>GEO</database> identifiers and also the various quality control feature scores is included in additional file 3. Detailed descriptions of the <software>Affymetrix Expression Console</software> quality control features can be found in [61].<br>
<br>
Expert Annotation<br>
A domain expert analyzed the 3' expression dataset (dataset 1) and assigned quality scores according to a procedure which is based on experience gained during almost three years of bioinformatics support within the Lausanne DNA Array Facility (DAFL). This quality control procedure is described in [15]. Briefly, the chip scan images and the distributions of the log scale raw PM intensities are visualized. Smaller discrepancies between chips are common and can often be removed by normalization. Remaining discrepancies usually indicate low quality data, possibly caused by problems in the amplification or labelling step. The general 5' to 3' probe intensity gradient averaged over all probe sets on a chip is also examined. The slope and shape of the resulting intensity curves depend on the RNA sample source, the amplification method, and the array type. In general, the specific shape of the curves is less important for the quality check than their agreement across the experiment. Pseudo-images representing the spatial distribution of residuals and weights derived from the probeset summarization model are very important diagnostics. Small artifacts are not critical when using robust analysis methods; however, extended anomalies are taken as an indication of low quality. In addition, box plot representations of the Normalized Un-scaled Standard Error (NUSE) from the probe level model fit and the Relative Log Expression (RLE) between each chip and a median chip are examined. These plots are used to identify problematic chips showing an overall deviation of gene expression levels from the majority of all measured chips. A chip may be judged as having poor quality if it is an apparent outlier in the experiment-wide comparison of several quality measures. Each array was given a quality score of 0, 1 or 2, with 0 being "acceptable quality" (519 chips), 1 being "suspicious quality" (56 chips) and 2 being "unacceptable quality" (28 chips). For the purposes of classification, chips with scores of 1 or 2 were combined into the composite "low quality" class.<br>
<br>
Supervised Na?ve Bayes Classifier<br>
Previous research has demonstrated that quality assessment of microarray data can be successfully automated with the use of a supervised classifier [15,20]. The goal of supervised classification is to utilize an annotated training dataset to learn a function that can be used to correctly classify unlabeled instances. In the case of microarray quality assessment, the training dataset consists of the quality control features computed for each chip, combined with the quality annotation for each chip.<br>
By making the simplifying assumption that all features are conditionally independent, na?ve Bayes classifiers attempt to directly model the probability that a particular data point belongs to each class. Given the class label, each feature is assumed to follow an independent, univariate distribution. These distributions are, of course, unknown, but the maximum likelihood parameter estimates can be determined from a labeled training set. Then, for each unlabeled instance, Bayes' rule can be applied to compute the conditional probability that the instance belongs to each of the possible classes. Because we had prior success performing classification on a similar data set using Na?ve Bayes with Gaussian feature distributions [15], we again chose to model the features using independent normal distributions. However, the approach could easily be adapted to use alternative distributions, for example, Student's t-distribution or the skew-normal distribution.<br>
Under this framework, the probability that an unlabeled instance belongs to the low quality class is estimated as follows:<br>
(1)<br>
where c ? {0,1} signifies the class label, with 0 denoting "high quality" and 1 denoting "low quality,"  is a length p vector of features describing the unlabeled instance., and  is the Gaussian density for the ith feature, among low quality chips.<br>
The marginal probability of observing a low quality chip, Pr{c = 1}, can be estimated from the proportion of low quality chips in the training set. Furthermore, the marginal density for a particular combination of feature values, , independent of the class label, is equal to:<br>
(2)<br>
For the purposes of classification, this algorithm assigns class 1 to an unlabeled instance , if Pr(c = 1|} &gt; t, where t is a threshold parameter, ordinarily set to 0.5 in order to approximate the Bayes optimal decision rule. By varying this parameter, it is also possible to construct ROC curves which display the tradeoff between sensitivity and specificity for various decision thresholds.<br>
<br>
Unsupervised Na?ve Bayes Classifier<br>
The standard (supervised) approach to constructing a na?ve Bayes classifier employs maximum likelihood estimation to infer the distribution parameters of each classification feature from an expert-annotated training set. It is, however, also possible to construct an "unsupervised" na?ve Bayes classifier by using an unannotated dataset as input. In this case, the EM algorithm is used to infer the feature distributions, assuming an appropriate Gaussian mixture model, as described in the following section.<br>
<br>
Gaussian Mixture Model and the EM Algorithm<br>
The na?ve Bayes classification model described above requires parameter estimates for the quality control metrics, conditional on each quality class. In the absence of annotated data, however, the quality classes of the unannotated training instances are additional unknowns that must be estimated along with the distributional parameters. We model the unannotated dataset using a Gaussian Mixture Model, under the assumption that microarray data can be reasonably classified into the dichotomy of "high quality" and "low quality" chips, and that the unlabeled training set contains examples of each.<br>
Given a large set of microarray data files, the first step is to compute values for each of the various quality control features. Then, for each feature, we assume that the observed distribution of scores is generated by an underlying Gaussian mixture model with two components: 1) chips having high quality and 2) chips having low quality. Given the mixture component, c ? {0,1}, each feature is assumed to follow a Normal  distribution. However, in the case of an unlabeled dataset, the true mixture component is unknown. We further assume that, marginally, the class label for each instance is a simple Bernoulli random variable with probability ? of indicating a low quality chip. Under this model, the (log) likelihood of the dataset is:<br>
(3)<br>
where:<br>
? x is an N ? p matrix containing the p feature values for the N items in the dataset, with  denoting the length p feature vector for the ith data point.<br>
? ? is a 2 ? p parameter matrix containing, in each column, ?0 and ?1 for the pth feature;  is the length p parameter vector for the jth Gaussian mixture component (j ? {0,1}).<br>
? ?2 is a 2 ? p parameter matrix containing, in each column,  and  for the pth feature;  is the length p parameter vector for the jthGaussian mixture component.<br>
?  is a length N vector containing the (unknown) class labels for each of the N data points.<br>
?  is a length 2 probability vector containing the probability that a randomly chosen data point belongs to each class.<br>
The likelihood function in equation 3 can be maximized using the EM algorithm [21]. The EM algorithm is a well-known method for maximizing mixture model likelihood functions by iteratively performing two steps:<br>
? E Step: Estimate the unknown class labels, based on the current estimates for the other parameters.<br>
? M Step: Given current class labels, compute the maximum likelihood estimators for the parameters ?, ?2, and .<br>
To implement the EM algorithm, we introduce an additional N ? 2 matrix, w, which contains, for each data point, i, the current guesses for p( = 0) and p( = 1). After initializing all parameters and the weight matrix, w, to random values, the EM algorithm proceeds as follows:<br>
M step: For j ? {0,1}, k ? {1 ... p}<br>
(4)<br>
(5)<br>
(6)<br>
E Step: For i ? {1 ... N}, j ? {0,1}<br>
(7)<br>
where normpdf(x, ?, ?2) denotes the probability density of a normal distribution evaluated at x. Because the algorithm can possibly converge to local optima, it is prudent to run the algorithm several times after random restarts. Additionally, each  was constrained to be &gt; = .001 to avoid convergence to a trivial solution. Further details concerning this implementation of the EM algorithm and the associated Gaussian mixture model can be found in [62]. Once estimates have been obtained for ?, ?2 and , any unlabeled instance can be classified according to these mixture components using na?ve Bayes, according to equation 1 (or equivalently, equation 7, in the case of the original unlabeled dataset). Since our assumption is that low quality chips are outliers with respect to these quality features, we use the mixture component corresponding to the smallest value from  to identify the low quality class.<br>
<br>
Feature Selection<br>
In order to achieve optimal classification performance, it is important to select an appropriate subset of the classification features. Ideally, this subset should include independent features that are each individually predictive of the class label.<br>
To measure the ability of each feature to predict the correct class label in a training set (where "correct" label is defined as either the expert annotation in the supervised case, or the estimated w matrix in the unsupervised case), we first constructed an N ? p score matrix, S, where each cell Sij contains a distance measuring the discrepancy between the true and predicted class for data point , given the jth feature and the parameter estimates for that feature:<br>
(8)<br>
Then for each feature, j, these scores were totaled across all N data points<br>
(9)<br>
Finally, the p scores were sorted in ascending order, to rank the features by their ability to predict the correct class label. Denote the rank of feature j according to the value of this score as S[j].<br>
To identify correlations among the quality control features, we next computed the p ? p Pearson correlation matrix. Let ?jk denote the correlation between features j and k, and ?[j]k represent the rank of the correlation of feature j with feature k among all other features correlated with k, with features ranked in order of descending correlation. To select a subset of n features, we used the following forward selection algorithm:<br>
? First, select the single feature that is most predictive of the class labels, i.e. the feature with S[j] = 1.<br>
? Then, sequentially, for the remaining n-1 features, select the feature j to satisfy:<br>
(10)<br>
where F denotes the set of previously selected features. The constants c1 and c2 in this expression are weighting factors that can be modified to control the tradeoff between selection for independent features and features that are highly correlated with the class label. We used 0.5 for each.<br>
<br>
<br>
Results and discussion<br>
Parameter Estimates<br>
3' Expression Arrays<br>
We applied the unsupervised mixture model described above to the 3' expression array data (hiding the expert quality labels). For nearly all of the 29 quality control features considered, the unsupervised EM parameter estimates very closely approximate the corresponding supervised MLE estimates, a result which indicates that the unsupervised approach was able to discover patterns in the data that are in agreement with the expert annotations. Additional file 4 contains the mixture model parameter estimates for , ?0, ?1,  and  for each of the quality control features. These estimates were obtained by applying the EM algorithm to the entire unlabeled dataset. For comparison, the table also includes the maximum likelihood estimates obtained using the expert-annotated class labels. Figure 1 shows some representative examples. Plots of this nature reveal that, in most cases, the EM and (supervised) MLE estimates exhibit only minor differences, generally with magnitudes analogous to the discrepancies shown in Figures 1a?d.<br>
The EM estimates appear to be reasonable in all cases, given the original intent of each quality metric. For example, given the normalized (log-scale) expression values, the RLE metric measures the distribution of the quantity  for each chip, where  is the log expression measurement for probeset g, on chip i, and mg is the median expression of probeset g across all arrays. In general, since it is ordinarily assumed that the majority of genes are not differentially expressed across chips, the quantity Mgi is expected to be distributed with median 0. In addition, chips that more frequently have extreme expression values will have a large inter-quartile range for this statistic. Figure 1b indicates that, as expected, low quality chips were indeed more likely to have a large inter-quartile range for the RLE statistic.<br>
Parameter estimates for the other metrics also agree with our expectations. For example, the estimates for metrics relating to probe-level model weights and residuals reflect the expectation that low quality chips should have larger residuals and more down-weighted probesets (Figure 1c, d). Similarly, the estimates indicate that low quality chips are more likely to have RNA degradation plots that are different from other chips in the same experiment. The low quality chips also tend to have both mean raw and mean normalized intensities that are either significantly higher or lower than other chips in the same experiment.<br>
<br>
Exon Arrays<br>
The Affymetrix exon array platform is different from the 3' expression array platform in several important ways [63]. For example, the 3' expression array targeting the human genome (Hgu133) has, on average, 1 probeset pair for each well-annotated gene; each probeset consists of 11 individual 25-mer probes, which primarily target the 3' region of the gene. In contrast, the Human Exon 1.0 ST array has 1 probeset for each exon for each gene in the target genome. Each probeset contains, in general, 4 (rather than 11) 25-mer probes. Unlike 3' expression arrays, exon arrays lack mismatch probes. Instead, the background expression level for each probe is estimated by averaging the intensities of approximately 1000 surrogate genomic and anti-genomic background probes having the same GC content as the target probe. Because most genes consist of several exons, the median number of probes per gene is increased on the exon array from 11 on the 3' array to between 30?40 [64]. However, genes with fewer exons are covered by fewer probes. In fact, there are a few thousand well-annotated single exon genes covered by only 4 probes [63]. Furthermore, the feature size on the exon arrays has been reduced from 11 ? 11 microns on the HGU133 array to 5 ? 5 microns on the Human Exon 1.0 ST array (about 1/5 the area). This change may increase the expression variance, at least at the probeset level [63]. Exon arrays also utilize a different hybridization protocol which uses sense-strand labeled targets, and results in DNA-DNA hybridizations rather than the DNA-RNA hybridizations used with traditional 3' arrays [65]. These differences suggest that the distributions of key quality control indicators may differ between the two platforms.<br>
For the exon arrays, the resulting probability estimate for low quality chips was .397 ? nearly twice what was obtained for the 3' arrays. This is reflected in Figure 2 as the larger areas under the red curves for exon arrays compared to 3' arrays, and as the smaller areas under the green curves for exon arrays compared to 3' arrays. For the majority of the indicators, the estimated distributions were qualitatively similar to those estimated for the 3' arrays (Figure 2a, c, d). One interesting difference is that in the exon arrays, the low quality chips appear to be more likely to have median raw intensity values that are lower than other chips in the same experiment (Figure 2b), whereas for the 3' arrays, both abnormally high and low median raw intensities appear to be indicative of bad chips.<br>
To check the robustness of our estimates, we also analyzed a separate set of quality control indicators (Table 2) computed using the <software>Affymetrix Expression Console</software> software. In agreement with the estimate obtained using the first set of quality metrics, the inferred probability for low quality chips was .394 using the <software>Expression Console</software> quality indicators. At a qualitative level, the estimates for the <software>Expression Console</software> quality indicators generally agreed with our expectations. For example, Figure 3a shows that, as expected, lower quality chips tend to have larger residuals when fitting the RMA probe-level summarization model. Similarly, Figures 3b and 3c show that low quality chips are more likely to have higher variability in the RLE metric. Interestingly, the SCORE.pos.vs.neg.auc metric, which measures the area under an ROC curve discriminating between positive and negative controls, did not indicate a major difference between high and low quality chips. This seems to be in conflict with the recommendation by Affymetrix that this is potentially one of the most useful quality control indicators for exon arrays [61]. This observation could reflect the fact that labs detecting unusual values for this metric may have been more likely to exclude the corresponding chips from further analysis.<br>
<br>
<br>
Classifier Performance Evaluation<br>
3' Expression Arrays<br>
After obtaining parameter estimates for various quality control features for the 3' expression arrays, we next sought to compare the performance of the unsupervised and supervised classifiers. A 10-fold cross-validation procedure was used to compare the performance of na?ve Bayes classifiers constructed using distribution parameters estimated using either the standard maximum likelihood method or, alternatively, the unsupervised mixture model approach. For each of 10 iterations, 9/10 of the 603 data instances were used as a training set, for both parameter estimation and also the selection of 5 classification features. For classifiers built using supervised MLE estimation ("MLE + Na?ve Bayes"), the expert generated labels were used to distinguish between high and low quality chips in the training set. For the unsupervised classifier ("EM + Na?ve Bayes"), the expert labels in the training set were ignored and the EM algorithm was used to estimate parameters of a Gaussian mixture model. The remaining unused 10th of the data was used to assess the performance of the classifier, using the expert labels as the standard of truth. The performance of the two algorithms was nearly identical. The confusion matrices (additional file 1: Table S1) show the classification results for the two algorithms using a classification threshold of 0.5. The accuracy of the MLE + Na?ve Bayes method was .907 with a false positive rate of .058, while the accuracy of the EM + Na?ve Bayes method was .910 with a false positive rate of .079. An ROC curve, constructed by varying the classification threshold, is shown in Figure 4. The area under the ROC curve (AUC) was .9455 for the unsupervised method and .9402 for the supervised method. Although this performance is good, it is possible that these results could be improved even more by identifying and using alternative (other than normal) distributions to model one or more of the classification features.<br>
In many real world scenarios the amount of unlabeled data available greatly exceeds the amount of expert-labeled data. To test the performance of the two classifiers under these conditions, we performed additional 10-fold cross-validation experiments similar to the previous test. However, in this case, the supervised MLE + Na?ve Bayes classifier was trained using random subsets of instances from each labeled training fold, while the EM + Na?ve Bayes classifier was constructed using the entire unlabeled training fold. Subsets containing 10, 20, 30, 60, 75, and 100 instances were used to train the supervised classifier. The ROC curves in Figure 4 indicate that the EM + Na?ve Bayes classifier appears to have an advantage when the amount of unlabeled training data available greatly exceeds the amount of expert-labeled data. For example, the unsupervised method clearly outperforms the supervised method when 30 or fewer labeled instances were available. Table S2 (available in additional file 1) contains the resulting confusion matrix for the case in which 30 labeled training instances were used, with a classification threshold of 0.5.<br>
<br>
3' Exon Arrays<br>
To demonstrate the general applicability of our method, we constructed unsupervised classifiers using the two sets of quality control variables and the entire unlabeled training set. These classifiers were then used to predict classification labels for each data point. Figure S6 (in additional file 1) shows a Venn diagram comparing the classification results for classifiers constructed using the <database>BioConductor</database> quality features and the <software>Expression Console</software> quality features. In most, but not all, cases, the classifiers agree on the characterization of each chip with regard to quality. In addition, both classifiers agree that approximately 39% of the data is low quality. Additional file 3 contains the classification labels obtained using unsupervised classifiers constructed using each set of quality variables.<br>
<br>
<br>
Simulation Results<br>
The agreement between the quality control feature distribution parameters estimated using the supervised maximum likelihood method and the estimates obtained with the unsupervised Gaussian mixture model suggests that our domain expert has uncovered a plausible dichotomy of chips within our dataset. To further confirm that the chips classified as having low quality were indeed more likely to negatively impact tests for differential expression, we performed a simple simulation. The procedure involved adding an offset to the observed expression measurements for a subset of the probesets on a set of "treatment" arrays, and then comparing these arrays with a set of unmodified "control" arrays sampled from the same experiment (details not shown). Among those chips designated by the expert as low quality, the majority (approximately 70%) impaired the ability to detect simulated differential expression when included in an analysis, compared to only about 10% of the chips classified as having high quality.<br>
<br>
<br>
Conclusion<br>
In this paper we have illustrated the efficacy of an unsupervised classification approach to assessing microarray data quality. Our method uses unlabeled training data to identify apparent distinctions between "good" and "bad" quality chips within the dataset. The method then integrates measurements obtained across a variety of quality dimensions into a single composite quality score which can be used to accurately identify low quality data.<br>
Our method is flexible and can be easily adapted to accommodate alternate quality statistics and platforms. Because this technique requires only unannotated training data, it is easy to keep the resulting classifier up-to-date as technology evolves, and the adaptable nature of the system makes arbitrary, universal quality score thresholds unnecessary. Moreover, since a na?ve Bayes classification approach involves the estimation of the underlying, univariate distributions for each of the classification parameters, this method allows for intuitive explanations that offer an advantage over other "black box" classification systems [66,67]. For example, under this framework, it is possible to infer which diagnostic plots and features are most relevant for the classification of a particular chip. These plots can then be presented to the user in order to explain the classification. A quality control method that incorporates an interpretation of standard diagnostic plots is an extension of a familiar process already used by many labs, and good diagnostic plots can provide powerful and convincing evidence of data quality artifacts.<br>
An important caveat for this, and any quality control methodology, is that the decision about what to do with the detected low quality chip(s) is dependent on the experimental design, the number of low quality chips detected, and the magnitude of the defects encountered. In many cases, low quality chips still contain valuable information, and in some cases the most effective strategy may be to simply down-weight these chips rather than discarding them entirely [68].<br>
Nevertheless, with the availability of a variety of rapidly growing public repositories for microarray data, the continual appearance of new microarray chip types, and the increasing usage of genomics data by research organizations worldwide, the development of robust and flexible methods for microarray quality assessment is now more important than ever. An advantage of the approach described in this paper is that, once a classifier has been constructed, the run-time required to automatically classify new instances is minimal. This makes the method ideal for use as a component of a batch processing system, such as a screening tool for use with public databases, or as a step in a meta-analysis pipeline.<br>
<br>
Availability and requirements<br>
? Project name: Unsupervised Assessment of Microarray Data Quality Using a Gaussian Mixture Model.<br>
? Availability: A <software>Matlab</software> implementation of these algorithms and the corresponding analyses is available in additional file 5.<br>
? Operating system: Implemented and tested under Windows XP.<br>
? Programming language: Matlab 7.0.1.15, service pack 1.<br>
? Other requirements: <software>Matlab Statistics Toolbox</software> version 6.1.<br>
? License: Brian E. Howard. Free for non-commercial use.<br>
? Any restrictions to use by non-academics: Contact corresponding author.<br>
<br>
Authors' contributions<br>
BEH implemented the method and analyzed the data. BEH, BS, and SH conceived of the method and study design, and collaborated to prepare the manuscript. All authors approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1904380</b><br>
Slowness: An Objective for Spike-Timing?Dependent Plasticity?<br>
Our nervous system can efficiently recognize objects in spite of changes in contextual variables such as perspective or lighting conditions. Several lines of research have proposed that this ability for invariant recognition is learned by exploiting the fact that object identities typically vary more slowly in time than contextual variables or noise. Here, we study the question of how this ?temporal stability? or ?slowness? approach can be implemented within the limits of biologically realistic spike-based learning rules. We first show that slow feature analysis, an algorithm that is based on slowness, can be implemented in linear continuous model neurons by means of a modified Hebbian learning rule. This approach provides a link to the trace rule, which is another implementation of slowness learning. Then, we show analytically that for linear Poisson neurons, slowness learning can be implemented by spike-timing?dependent plasticity (STDP) with a specific learning window. By studying the learning dynamics of STDP, we show that for functional interpretations of STDP, it is not the learning window alone that is relevant but rather the convolution of the learning window with the postsynaptic potential. We then derive STDP learning windows that implement slow feature analysis and the ?trace rule.? The resulting learning windows are compatible with physiological data both in shape and timescale. Moreover, our analysis shows that the learning window can be split into two functionally different components that are sensitive to reversible and irreversible aspects of the input statistics, respectively. The theory indicates that irreversible input statistics are not in favor of stable weight distributions but may generate oscillatory weight dynamics. Our analysis offers a novel interpretation for the functional role of STDP in physiological neurons.<br>
<br>
Introduction<br>
The ability to recognize objects in spite of possible changes in position, lighting conditions, or perspective is doubtlessly an advantage in everyday life. However, our brain usually performs this task with such astonishing ease that we are seldom aware of the complexity this recognition problem comprises. On the level of primary sensory signals (e.g., light that stimulates a single retinal receptor), even small changes in the position of the object to be recognized may lead to vastly different stimuli. Our brain thus has to somehow identify rather different stimuli as representations of the same underlying cause, i.e., it has to develop an internal representation that is invariant to irrelevant changes of the stimulus. The work presented here is motivated by the question of how such invariant representations could be established.<br>
Because of the limited amount of information in the genome as well as the apparent flexibility of the neural development in different environments, it seems unlikely that the information needed to form invariant representations is already there at the beginning of individual development. Some information must be gathered from the sensory input experienced during interaction with the environment; it has to be learned. As this learning process is likely to be at least partially unsupervised, the brain requires a heuristics as to what stimuli should be classified as being the same.<br>
One possible indicator for stimuli to represent the same object is temporal proximity. A scene that the eye views is very unlikely to change completely from one moment to the next. Rather, there is a good chance that an object that can be seen now will also be present at the next instant of time. This implies that invariant representations should remain stable over time, that is, they should vary slowly. Inverting this reasoning, a sensory system that adapts to its sensory input in order to extract slowly varying aspects may succeed in learning invariant representations. This ?slowness? or ?temporal stability? principle is the basis of a whole class of learning algorithms [1?7]. Most applications of this approach have focused on models of the visual system, in particular on the self-organized formation of complex cell receptive fields in the primary visual cortex [8,9].<br>
For clarity, we will focus on one of these algorithms, slow feature analysis (SFA; [10]); a close link to the so-called ?trace rule? will arise naturally. The goal of SFA is the following: given a multidimensional input signal x(t) and a finite-dimensional function space F, find the input?output function g1(x) in F that generates the most slowly varying output signal y1(t) = g1(x(t)). It is important to note that the function g1(x) is required to be an instantaneous function of the input signal. Otherwise, slow output signals could be generated by low-pass filtering the input signal. As the goal of the slowness principle is to detect slowly varying features of the input signals, a mere low-pass filter would certainly generate slow output signals, but it would not serve the purpose.<br>
As a measure of slowness, or rather ?fastness,? SFA uses the variance of the time derivative,<br>
				, which is the objective function to be minimized. Here, ???t denotes temporal averaging. For mathematical convenience and to avoid the trivial constant response, y1(t) = const, a zero-mean, and unit variance constraint are imposed. Furthermore, it is possible to find a second function g2(x) extracting y2(t) = g2(x(t)) that again minimizes the given objective under the constraint of being uncorrelated with y1(t), a third one uncorrelated with both y1(t) and y2(t), and so on, thereby generating a set of slow features of the input ordered by the degree of slowness. However, in this paper, we will consider just one single output unit.<br>
			<br>
SFA has been applied to the learning of translation, rotation, and other invariances in a model of the visual system [10], and it has been shown that when applied to image sequences generated from static natural images, SFA learns functions that reproduce a wide range of features of complex cells in primary visual cortex [8]. Iteration of the same principle in a hierarchical model in combination with a sparseness objective has been used to model the self-organized formation of spatial representations resembling place cells as found in the hippocampal formation of rodents [11] (see [12] for related work).<br>
These findings suggest that on an abstract level SFA reflects certain aspects of cortical information processing. However, SFA as a technical algorithm is biologically rather implausible. There is in particular one step in its canonical formulation that seems especially odd compared with what neurons are normally thought to do. In this step the eigenvector that corresponds to the smallest eigenvalue of the covariance matrix of the time derivative of some multidimensional signal is extracted. The aim of this paper is to show how this kind of computation can be realized in a spiking model neuron.<br>
In the following, we will first consider a continuous model neuron and demonstrate that a modified Hebbian learning rule enables the neuron to learn the slowest (in the sense of SFA) linear combination of its inputs. Apart from providing the basis for the analysis of the spiking model, this section reveals a mathematical link between SFA and the trace learning rule, another implementation of the slowness principle. We then examine if these findings also hold for a spiking model neuron, and find that for a linear Poisson neuron, spike-timing?dependent plasticity (STDP) can be interpreted as an implementation of the slowness principle.<br>
<br>
Results<br>
Continuous Model Neuron<br>
Linear model neuron and basic assumptions.<br>
First, consider a linear continuous model neuron with an input?output function given by<br>
						with <br>
						 indicating the input signals, wi the weights, and aout the output signal. For mathematical convenience, let <br>
						and aout (t) be defined on the interval t ? [??, ?] but differ from zero only on [0,T], which could be the lifetime of the system. We assume that the input is approximately whitened on any sufficiently large interval [ta,tb] ? [0,T] (i.e., each input signal has approximately zero mean and unit variance and is uncorrelated with other input signals):<br>
						<br>
						<br>
						This can be achieved by a normalization and decorrelation step of the units projecting to the considered unit. Furthermore, we assume that the output is normalized to unit variance, which for whitened input means that the weight vector is normalized to length 1. In an online learning rule, this could be implemented by either an activity-dependent or a weight-dependent normalization term. Thus, for the output signal we have:<br>
						<br>
						In the following, we will often consider filtered signals. Therefore, we introduce abbreviations for the convolution f?g and the cross-correlation f * g of two functions f(t) and g(t):<br>
						<br>
						For convenience, we will often use windowed signals, indicated by a hat<br>
						which allows us to replace the integration of a signal s(t) over [ta,tb] by an integration of ?(t) over [??, ?]. We assume that the interval [ta,tb] is long compared to the width of the filters. In this case, effects from the integration boundaries are negligible, and we have<br>
						Similar considerations hold for the cross-correlation (Equation 8).<br>
					<br>
Since convolution and cross-correlation are conveniently treated in Fourier space, we repeat the definition of the Fourier transform  and the power spectrum Ps(?) of a signal s(t).<br>
						<br>
						<br>
					<br>
Throughout the paper, we make the assumption that input signals (and hence also the output signals) do not have significant power above some reasonable frequency ?max.<br>
<br>
Reformulation of the slowness objective.<br>
SFA is based on the minimization of the second moment of the time derivative, <br>
						. Even though there are neurons with transient responses to changes in the input, we believe it would be more plausible if we could derive an SFA-learning rule that does not depend on the time derivative, because it might be difficult to extract, especially for spiking neurons. It is indeed possible to replace the time derivative by a low-pass filtering as follows:<br>
						<br>
						<br>
						<br>
						<br>
						<br>
						<br>
						<br>
						<br>
						Thus, SFA can be achieved either by minimizing the variance of the time derivative of the output signal or by maximizing the variance of the appropriately filtered output signal. Figure 1 provides an intuition for this alternative. The filter fSFA is obviously a low-pass filter, as one would expect, with a <br>
						 power spectrum below the limiting frequency ?max. Because the phases are not determined, further assumptions are required to fully determine an SFA filter. However, we will proceed without defining a concrete filter, since it is not required for the considerations below.<br>
					<br>
<br>
Hebbian learning on filtered signals.<br>
It is known that standard Hebbian learning under the constraint of a unit weight vector applied to a linear unit maximizes the variance of the output signal. We have seen in the previous section that SFA can be reformulated as a maximization problem for the variance of the low-pass filtered output signal. To achieve this, we simply apply Hebbian learning to the filtered input and output signals, instead of to the original signals.<br>
Consider a hypothetical unit that receives low-pass filtered inputs and, therefore, because of the linearity of the unit and the filtering, generates a low-pass filtered output<br>
						where fSFA is the kernel of the linear filter applied. It is obvious that a filtered Hebbian learning rule<br>
						with fin: = fout: = fSFA maximizes the objective in Equation 21.<br>
					<br>
Remember that the input is white (i.e., the <br>
						 are uncorrelated and have unit variance), and the weight vector is normalized to norm one by some additional normalization rule, so that we know that the output signal aout has the same variance no matter what the direction of the weight vector is. Thus, the filtered Hebbian plasticity rule (together with the normalization rule not specified here) optimizes slowness (Equation 13) under the constraint of unit variance (Equation 6). Figure 2 illustrates this learning scheme. It also underlines the necessity for a clear distinction between processing and learning. Although the slowness principle does not allow low-pass filtering as a means of generating slow signals during processing, the learning rule may well make use of low-pass filtered signals to detect slowly varying features in the input signal. This distinction will become particularly important for the Poisson model neuron below, as it incorporates an excitatory postsynaptic potential (EPSP) that acts as a low-pass filter during processing. An implementation of the slowness principle in such a system must avoid the system exploiting the EPSP as a means of generating slow signals.<br>
					<br>
<br>
Alternative filtering procedures.<br>
If learning is slow, the total weight change over a time interval [ta,tb] in a synapse can be written as<br>
						<br>
						<br>
						<br>
						<br>
						<br>
						<br>
					<br>
Thus, one can either convolve input and output signal with filters fin and fout, respectively, the input signal with fout * fin, or the output signal with fin * fout. Note that [fin * fout](t) = [fout * fin](?t). One can actually use any pair of filters fin and fout as long as fin * fout fulfills the condition<br>
						<br>
					<br>
<br>
Relation to other learning rules.<br>
Hebbian learning on low-pass filtered signals is the basis of several other models for unsupervised learning of invariances [1,4,6]. These models essentially subject the output signal to an exponential temporal filter f(t) = ?(t)exp(??t) and then use Hebbian learning to associate it with the input signal. Here, ?(t) denotes the Heaviside step function, which is 0 for t &lt; 0 and 1 for t ? 0. This learning rule has been named the ?trace rule.? The considerations in the last section provide a link between this approach and ours. We simply have to replace fin with a ?-function and fout with f(t). Equation 29 then takes the form<br>
						since the output signal <br>
						 is a linear function of the input (see  Equation 1). In the previously mentioned applications of the trace rule, the statistics of the input signals were always reversible, so we will assume that all correlation functions <br>
						 are symmetric in time. This implies that only the symmetric component of f(t) is relevant for learning:<br>
						It is easy to show that the learning rule in Equation 31 can be interpreted as a gradient ascent on the following objective function:<br>
						<br>
						By comparison with Equation 19, it becomes clear that the trace rule implements a very similar objective as our model. The only difference is that the power spectrum in Equation 20 is replaced by the Fourier transform of the filter fsym. Note that in order to be able to interpret ? as an objective function, it should be real-valued. The replacement of f with fsym ensures that <br>
						 is real-valued and symmetric, so ? is real-valued as well. The Fourier transform of fsym is given by<br>
						This shows that the only difference between the trace rule and our model lies in the choice of the power spectrum for the low-pass filter. While we are using a parabolic power spectrum with a cutoff (Equation 20), the trace rule uses a power spectrum with the shape of a Cauchy function (Equation 35).<br>
					<br>
From this perspective, one can interpret SFA as a quadratic approximation of the trace rule. To what extent this approximation is valid depends on the power spectra of the input signals. If most of the input power is concentrated at low frequencies, where the power spectrum resembles a parabola, the learning rules can be expected to learn very similar weight vectors. In fact, any Hebbian learning rule that leads to an objective function of the shape of Equation 19 with a low-pass filtering spectrum in the place of <br>
						 essentially implements the slowness principle, as among signals with the same variance, it will favor slower ones.<br>
					<br>
<br>
<br>
Spiking Model Neuron<br>
Real neurons do not transmit information via a continuous stream of analog values like the model neuron considered in the previous section, but rather emit action potentials that carry information by means of their rate and probably also by their exact timing, a fact we will not consider here. How can the model developed so far be mapped onto this scenario?<br>
The linear Poisson neuron.<br>
Again, we restrict our analysis to a simple case by modeling the spike-train signals by inhomogeneous Poisson processes. Note that at this point, we restrict our analysis to a rate code, thus neglecting possible coding paradigms that rely on precise timing of spikes.<br>
To generate the input spike trains, we first add sufficiently large constants <br>
						 to the continuous and zero-mean signals <br>
						 to turn them into strictly positive signals that can be interpreted as rates<br>
						<br>
					<br>
The constants <br>
						 represent mean firing rates, which are modulated by the input signals <br>
						. From the input rates <br>
						, we then derive inhomogeneous Poisson spike trains <br>
						 drawn from ensembles <br>
						 such that<br>
						where <br>
						 denotes the average over the ensemble <br>
						.<br>
					<br>
The output rate is modeled as a weighted sum over the input spike trains convolved with an EPSP ?(t) plus a baseline firing rate r0, which ensures that the output firing rate remains positive. This is necessary as we allow inhibitory synapses (i.e., negative weights).<br>
						<br>
					<br>
Note that in this scheme, the EPSP reflects the change in the postsynaptic firing probability due to a presynaptic spike rather than a change in the membrane potential. Ideally, it includes all delay effects in neuronal transmission.<br>
The output of this spiking neuron is yet another inhomogeneous Poisson spike train Sout(t) drawn from an ensemble Eout, given a realization of the input spike trains <br>
						 such that<br>
						<br>
					<br>
It should be noted that not only is the output spike train Sout(t) stochastic in this model, but also the underlying output rate m(t), which is a function of the stochastic variables <br>
						 and generally differs for each realization of the input. This is the reason why the input and output spike trains are not statistically independent. However, due to the linearity of the model neuron, the output rate is still simply<br>
						<br>
						<br>
						<br>
						<br>
						and the joint firing rate is<br>
						<br>
						<br>
					<br>
The first term would result also from a rate model, while the second term captures the statistical dependencies between input and output spike trains mediated by the synaptic weights wi and the EPSP ?(t).<br>
<br>
STDP can perform SFA.<br>
In this section, we will demonstrate that in an ensemble-averaged sense it is possible to generate the same weight distribution as in the continuous model by means of an STDP rule with a specific learning window.<br>
Synaptic plasticity that depends on the temporal order of pre- and postsynaptic spikes has been found in a number of neuronal systems [14?18], and has raised a lot of interest among modelers [19,20] (for a review, see [21]). Typically, synapses undergo long-term potentiation (LTP) if a presynaptic spike precedes a postsynaptic spike within a timescale of tens of milliseconds and long-term depression (LTD) for the opposite temporal order. Assuming that the change in synaptic efficacy occurs on a slower timescale than the typical interspike interval, the STDP weight dynamics can be modeled as<br>
						<br>
					<br>
Here, <br>
						 denotes the spike times of the presynaptic spikes at synapse i and <br>
						 denotes the postsynaptic spike times. W(t) is the learning window that determines if and to what extent the synapse is potentiated or depressed by a single spike pair. The convention is such that negative arguments t in W(t) correspond to the situation where the presynaptic spike precedes the postsynaptic spike. <br>
						 and mout are the numbers of pre- and postsynaptic spikes occurring in the time interval [ta, tb] under consideration. ? is a small positive learning rate. Note that due to the presence of this learning rate, the absolute scale of the learning window W is not important for our analysis.<br>
					<br>
We circumvent the well-known stability problem of STDP by applying an explicit weight normalization (<br>
						) instead of weight-dependent learning rates as used elsewhere [22?24]. Such a normalization procedure could be implemented by means of a homeostatic mechanism targeting the output firing rate (e.g., by synaptic scaling; for reviews, see [25,26]).<br>
					<br>
Modeling the spike trains as sums of delta pulses (i.e., <br>
						), the learning rule in Equation 47 can be rewritten as<br>
						<br>
						<br>
					<br>
Taking the ensemble average allows us to retrieve the rates that underlie the spike trains and thus the signals <br>
						 and <br>
						 of the continuous model:<br>
						<br>
						<br>
						<br>
					<br>
Expanding the products in Equation 52 gives rise to a number of terms, among which only one depends on both the input and the output signal <br>
						 and <br>
						. Because each input signal has a vanishing mean, terms containing just one input signal lead to negligible contributions. The remaining terms depend only on the mean firing rates <br>
						 and <br>
						:<br>
						<br>
					<br>
A generalized version of Equation 53 that incorporates non-Hebbian plasticity (i.e., terms that depend on the pre/postsynaptic signals only) has been derived and discussed by Kempter et al. [27]. Regarding the effects of the input signals on learning, the decisive term is the first one. The other two are rather unspecific in that they do not depend on the properties of the input and output signals <br>
						 and <br>
						.<br>
					<br>
The second term alone would generate a competition between the weights: synapses that experience a higher mean input firing rate <br>
						 grow more rapidly than those with smaller input firing rates. If we assume that the input neurons fire with the same mean firing rate, all weights grow with the same rate, so the direction of the weight vector remains unchanged. Thus, due to the explicit weight normalization, this term has no effect on the weight dynamics and can be neglected.<br>
					<br>
If the integral over the learning window is positive, the third term in Equation 53 favors a weight vector that is proportional to the vector of the mean firing rates of the input neurons. It thus stabilizes the homogeneous weight distribution and opposes the effect of the first term, which captures correlations in the input signals. Note that this is only true if the integral over the learning window is positive; otherwise, this term introduces a competition between the weights [24,27]. One possible interpretation is that the neuron has a ?default state? in which all synapses are equally strong and that correlations in the input need to surpass a certain threshold in order to be imprinted in the synaptic connections. Interestingly, this threshold is determined by the integral over the learning window, which implies that neurons that balance LTP and LTD should be more sensitive to input correlations.<br>
An alternative possibility is that the neuron possesses a mechanism of canceling the effects of this term. From a computational perspective this would be sensible, as the mean firing rates <br>
						 and <br>
						 do not carry information about the input, neither in rate nor in a timing code. If we conceive neurons as information encoders aiming at adapting to the structure of their input, this term is thus more hindrance than help. Assuming that the neuron compensates for this term, the dynamics of the synaptic weights are governed exclusively by the correlations in the input signals as reflected by the first term. In the following, we will restrict our considerations to this term and omit the others.<br>
					<br>
Rearranging the temporal integrations, we can rewrite Equation 53 for the weight updates as<br>
						<br>
					<br>
The first conclusion we can draw from this reformulation is that for the dynamics of the learning process the convolution of the learning window with the EPSP and not the learning window alone is relevant. As discussed below, this might have important consequences for functional interpretations of the shape of the learning window.<br>
Second, by comparison with Equation 29, it is obvious that in order to learn the same weight distribution as in the continuous model, the learning window has to fulfill the condition that<br>
						<br>
						<br>
					<br>
Here, W0 is the convolution of W with ? and is equal to the learning window in the limit of an infinitely short, ?-shaped EPSP. As the power spectrum <br>
						 is of course real, W0 is symmetric in time. Note that the width of W0 scales inversely with the width of the power spectrum <br>
						, which in turn is proportional to ?max. Once the power spectrum <br>
						 and the EPSP is given, Equation 56 uniquely determines the learning window W. Because it is W0 rather than W that determines the learning dynamics, we will refer to W0 as the ?effective learning window.?<br>
					<br>
<br>
Learning windows.<br>
According to the last section, we require special learning windows to learn the slow directions in the input. This of course raises the question of which window shapes are favorable, and in particular if these are in agreement with physiological findings.<br>
Given the shape of the EPSP and the power spectrum <br>
						, the learning window is uniquely determined by Equation 56. Remember that the only parameter in the power spectrum <br>
						 is the frequency ?max, above which the power spectrum of the input data was assumed to vanish. For simplicity, we model the EPSP as a single exponential with a time constant ?:<br>
						<br>
					<br>
For this particular EPSP shape, the learning window can be calculated analytically by inverting the Fourier transform in Equation 56. The result can be written as<br>
						W0 is symmetric, so its derivative is antisymmetric. Thus, the learning window is a linear combination of a symmetric and an antisymmetric component. As the width of W0 scales with the inverse of ?max, its temporal derivative scales with ?max. Accordingly, the symmetry of the learning window is governed by an interplay of the duration ? of the EPSP and the maximal input frequency ?max. For ? ? 1 / ?max the learning window is dominated by W0 and thus symmetric, whereas for ? ? 1 / ?max, the temporal derivative of W0 is dominant, so the learning window is antisymmetric.<br>
					<br>
We have assumed that the input signals have negligible power above the maximal input frequency ?max. Thus, the temporal structure of the input signals can only provide a lower bound for ?max. On the other hand, exceedingly high values for ?max lead to very narrow learning windows, thereby sharpening the coincidence detection and reducing the speed of learning. Moreover, it may be metabolically costly to implement physiological processes that are faster than necessary. Thus, it appears sensible to choose ?max such that 1 / ?max reflects the fastest timescale in the input signals. Accordingly, the symmetry of the learning window is governed by the relation between the length of the EPSP and the fastest timescale in the input data. If the EPSP is short enough to resolve the fastest input components, the learning window is symmetric. If the EPSP is too long to fully resolve the temporal structure of the input (i.e., it acts as a low-pass filter), the learning window will tend to be antisymmetric.<br>
We choose a value of ?max = 1 / (40 ms). The argument for this choice is that within a rate code, the cells that project to the neuron under consideration can hardly convey signals that vary on a faster timescale than the duration of their EPSP. It is thus reasonable to choose the time constant of the EPSP and the inverse of the cutoff frequency to have the same order of magnitude. Typical durations of cortical EPSPs are of the order of tens of milliseconds (see [28] for further references and a critical discussion), so 40 ms seems a reasonable value.<br>
Figure 3 illustrates the connection between <br>
						, W0, the learning window, and the EPSP. It also shows the learning windows for three different durations of the EPSP, while keeping ?max = 1 / (40 ms). The oscillatory and slowly decaying tails of W(t) are due to the sharp cutoff of the power spectrum <br>
						 at |?| = ?max and become less pronounced if <br>
						 is smoothened out.<br>
					<br>
As negative time arguments in W(t) correspond to the case in which the presynaptic spike (and thus the onset of the resulting EPSP) precedes the postsynaptic spike, the shape of the theoretically derived learning window for physiologically plausible values of ? and ?max (? = 1 / ?max = 40 ms; middle row in Figure 3) predicts potentiation of the synapse when a postsynaptic spike is preceded by the onset of an EPSP and depression of the synapse when this temporal order is reversed. This behavior is in agreement with experimental data from neocortex and hippocampus in rats as well as from the optic tectum in Xenopus [14?18]. To further illustrate this agreement, Figure 4 compares the data as published by Bi and Poo [16] with the learning window resulting from a smoothened power spectrum with the shape of a Cauchy function (Equation 35) instead of <br>
						. As demonstrated above, this corresponds to implementing the slowness principle in form of the trace rule. Interestingly, the resulting learning window has the double-exponential shape that is regularly used in models of STDP (e.g., [24,29,30]). As the absolute scale of the learning window is not determined in our analysis, it was adjusted to facilitate the comparison with the experimental data.<br>
					<br>
<br>
Interpretation of the learning windows.<br>
The last section leaves a central question open: why are these learning windows optimal for slowness learning and why does the EPSP play such an important role for the shape of the learning window?<br>
Let us first discuss the case of the symmetric learning window, that is, the situation in which the EPSP is shorter than the fastest timescale in the input signal. Then, the convolution with the EPSP has practically no effect on the temporal structure of the signal and the output firing rate can be regarded as an instantaneous function of the input rates. We can thus neglect the EPSP altogether. The learning mechanism can then be understood as follows: assume at a given time t the postsynaptic firing rate rout is high and causes a postsynaptic spike. Then, the finite width of the learning window leads to potentiation not only of those synapses that participated in initiating the spike but also of those that transmit a spike within a certain time window around the time of the postsynaptic spike. As this leads to an increase of the firing rate within this time window, the learning mechanism tends to equilibrate the firing rates for neighboring times and thus favors temporally slow output signals.<br>
If the duration of the EPSP is longer than the fastest timescale in the input signal, the output firing rate is no longer an instantaneous function of the input signals but generated by low-pass filtering the signal aout with the EPSP. This affects learning, because the objective of the continuous model is to optimize the slowness of aout, whose temporal structure is now ?obscured? by the EPSP. In order to optimize the objective, the system thus has to develop a deconvolution mechanism to reconstruct aout. From this point of view, the learning window has to perform two tasks simultaneously. It has to first perform the deconvolution and then enforce slowness on the resulting signal. This is most easily illustrated by means of the condition in Equation 55. The convolution of the learning window with the EPSP generates the effective learning window W0 that is independent of the EPSP and which coincides with the learning window for infinitely short EPSPs. Intuitively, we could solve Equation 55 by choosing a learning window that consists of the ?inverse? of the EPSP and the EPSP-free learning window W0. An intuitive example is the limiting case of an infinitely long EPSP. The EPSP then corresponds to a Heaviside function and performs an integration, which can be inverted by taking the derivative. Thus, the learning window for long EPSPs is the temporal derivative of the learning window for short EPSPs. The dependence of the required learning window on the shape of the EPSP is thus caused by the need of the learning window to ?invert? the EPSP.<br>
These considerations shed a different light on the shape of physiologically measured learning windows. The antisymmetry of the learning window may not act as a physiological implementation of a causality detector after all, but rather as a mechanism for compensating intrinsic low-pass filters in neuronal processing such as the EPSP. For functional interpretations of STDP, it may be more sensible to consider the convolution of the learning window with the EPSP than the learning window alone.<br>
It should be noted that, according to our learning rule, the weights adapt in order to make a hypothetical instantaneous output signal aout optimally slow. This does not necessarily imply that the output firing rate rout, which is generated by low-pass filtering aout with the EPSP, is optimally slow. In principle, the system could generate more slowly varying signals by exploiting the temporal structure of the EPSP. However, the motivation for the slowness principle is the idea that the system learns to detect invariances in the input signal, and that from this perspective the goal of creating a slowly varying output signal is not an end in itself but a means to learn invariances. Thus, the low-pass filtering effect of the EPSP should not be exploited but ignored or compensated.<br>
<br>
General learning windows and EPSPs.<br>
Although the asymmetry in LTP/LTD induction observed by Bi and Poo [16] has also been observed in other studies, the decay times for the LTP and the LTD branches of the learning window appear to be different in other preparations [18]. One may thus ask how robust our interpretation is with respect to the detailed shape of the learning window. To address this question, we start with some general learning window W and EPSP ? and ask under which conditions the effective learning window W0 = W ? ? prefers slowly varying features in the input.<br>
As a starting point, we use the dynamics of the weights in Equation 54 as generated by the input statistics. Using <br>
						 and defining the correlation functions <br>
						 yields<br>
						The dynamics thus follows a linear difference equation with a dynamic matrix Aij whose properties are determined by the correlation function Cij(t) and the effective learning window W0(t). One important question is whether the weights approach a stable fixed-point state or oscillate. In this context, the symmetry properties of Aij and thus those of Cij are crucial. The correlation functions obey the relation<br>
						which couples their spatial symmetry (i.e., the symmetry with respect to the indices i and j) to their temporal symmetry. For instance, if the input statistics are reversible, i.e., for Cij(t) = Cij(?t), Cij is symmetric in the indices and so is Aij. If the input statistics were ?perfectly irreversible,? i.e., Cij(t) = ? Cij(?t), Cij and Aij would be antisymmetric. This motivates the splitting of the correlation functions Cij into a temporally symmetric and an antisymmetric component: Cij = Cij+ + Cij? with Cij?(t) = ?Cij?(?t). In a similar fashion, we split the effective learning window W0 = W0+ + W0?. For symmetry reasons, the dynamical matrix Aij can then be separated into two components<br>
						<br>
					<br>
Because of the symmetry relation in Equation 60, Aij+ is symmetric in i and j, while Aij? is antisymmetric. This shows that the effective learning window W0 can be split into two functionally different components. The symmetric component picks up the reversible aspects of the input statistics while the antisymmetric component detects irreversibilities, e.g., possible causal relations within the input data. It is this antisymmetric component of the learning window that has previously been interpreted as a means for sequence learning and predictive coding [19,31]. Note that the associated weight update ?jAij?wj is always orthogonal to the weight itself. Thus, irreversibilities in the input data in combination with an antisymmetric learning window work against the development of a stable weight distribution, even if the input statistics are stationary. In particular, weight oscillations on the timescale of learning may occur. For instance, in networks with recurrent connections that learn according to STDP, previous studies have shown that the network tends to develop a state of distributed synchrony [32] that resembles synfire chains. These activity patterns display a pronounced causal structure, so it would be interesting to check if the synaptic weights that emerge in such a network are stable or show oscillations. It is likely that in this context the model constraints on the weights play an important role. If the weights are limited by hard boundaries as in [32], they tend to saturate, thereby avoiding oscillatory solutions. In the case of softer weight constraints, e.g., in models of STDP with multiplicative weight-dependence, oscillations may occur.<br>
If W0 is symmetric or if the input statistics are reversible, Cij? = 0, the dynamical matrix Aij = Aij+ is symmetric. As already seen for the case of the continuous model neuron, the learning dynamics can then be interpreted as a gradient ascent on the objective function<br>
						<br>
					<br>
As discussed earlier, this objective function can be interpreted as an implementation of the slowness principle if W0+(?) is a low-pass filter, i.e., it has a global maximum at zero frequency. This indicates that at least for reversible input statistics the preference of STDP for slow signals may be rather insensitive to details of the learning window.<br>
<br>
<br>
<br>
Discussion<br>
Neurons in the central nervous system display a wide range of invariances in their response behavior, examples of which are phase invariance in complex cells in the early visual system [33], head direction invariance in hippocampal place cells [34], or more complex invariances in neurons associated with face recognition [35]. If these invariances are learned, the associated learning rule must somehow reflect a heuristics as to which sensory stimuli are supposed to be categorized as being the same. Objects in our environment are unlikely to change completely from one moment to the next but rather undergo typical transformations. Intuitively, responses of neurons with invariances to these transformations should thus vary more slowly than others. The slowness principle uses this intuition and conjectures that neurons learn these invariances by favoring slowly varying output signals without exploiting low-pass filtering.<br>
SFA [10] is one implementation of the slowness principle in that it minimizes the mean square of the temporal derivative of the output signal for a given set of training data. SFA has been used to model a wide range of physiologically observed properties of complex cells in primary visual cortex [8] as well as translation, rotation, and other invariances in the visual system [10]. In combination with a sparse coding objective, SFA has also been used to describe the self-organized formation of place cells in the hippocampal formation [11].<br>
The algorithm that underlies SFA is rather technical, and it has not yet been examined whether it is feasible to implement SFA within the limitations of neuronal circuitry. In this paper we approach this question analytically and demonstrate that such an implementation is possible in both continuous and spiking model neurons.<br>
In the first part of the paper, we show that for linear continuous model neurons, the slowest direction in the input signal can be learned by means of Hebbian learning on low-pass filtered versions of the input and the output signal. The power spectrum of the low-pass filter required for implementing SFA can be derived from the learning objective and has the shape of an upside-down parabola.<br>
The idea of using low-pass filtered signals for invariance learning is a feature that our model has in common with several others [1,4,6]. By means of the continuous model neuron, we have discussed the relation of our model to these ?trace rules? and have shown that they bear strong similarities.<br>
The second part of the paper discusses the modifications that have to be made to adjust the learning rule for a Poisson neuron. We find that in an ensemble-averaged sense it is possible to reproduce the behavior of the continuous model neuron by means of spike-timing?dependent plasticity (STDP). Our study suggests that the outcome of STDP learning is not governed by the learning window alone but rather by the convolution of the learning window with the EPSP, which is of relevance for functional interpretations of STDP.<br>
The learning window that realizes SFA can be calculated analytically. Its shape is determined by the interplay of the duration of the EPSP and the maximal input frequency ?max, above which the input signals are assumed to have negligible power. If ?max is small, i.e., if the EPSP is sufficiently short to temporally resolve the most quickly varying components of the input data, the learning window is symmetric, whereas for large ?max or long EPSPs, it is antisymmetric. Interestingly, physiologically plausible parameters lead to a learning window whose shape and width is in agreement with experimental findings. Based on this result, we propose a new functional interpretation of the STDP learning window as an implementation of the slowness principle that compensates for neuronal low-pass filters such as the EPSP.<br>
An important question in this context is on which timescales is this interpretation valid. It is conceivable that for signals that vary on a timescale of less than a hundred milliseconds, a learning window with a width of tens of milliseconds can distinguish slower from faster signals. STDP could thus be sufficient to establish invariant representations in early sensory processing, e.g., visual receptive fields that become invariant to microsaccades inducing small translations. Although it is unlikely that STDP alone can distinguish between signals that vary on behavioral timescales of hundreds of milliseconds or even seconds, this may not be problematic, because it is probably not sensible to order all aspects of the stimuli according to how quickly they vary. Rather, one should distinguish input components that vary so quickly that they are unlikely to be behaviorally relevant from those that vary on behavioral timescales. From this perspective, the intrinsic timescale of the learning rule should be such that its discriminative power is best on a timescale where this transition occurs. It is conceivable that this transition timescale lies on the order of several tens of milliseconds. The learning of high level invariances that correspond to behavioral timescales will probably require additional mechanisms with corresponding intrinsic timescales, e.g., sustained firing in response to a stimulus [36].<br>
For general learning windows and EPSPs, the convolution of the learning window with the EPSP can be split into a symmetric component and an antisymmetric component. The symmetric component picks up reversible aspects of the input statistics while the antisymmetric component detects irreversible aspects. Previous functional interpretations of STDP have mostly concentrated on the antisymmetric component, which has been interpreted, e.g., as a mechanism for sequence learning or predictive coding [19,31] or for reducing recurrent connectivity in favor of feed-forward structures [30,32]. Other studies have neglected the phase structure of the learning window altogether and concentrated on its power spectrum, proposing that timing-dependent plasticity performs Hebbian learning on an optimal estimate of the input signals in the presence of noise [37,38]. Note that these interpretations are not necessarily contradictory to ours, because the slowness interpretation relies on the symmetric component of the learning window only and thus on the reversible aspect of the input statistics. These considerations indicate that depending on the temporal structure of the input, STDP may have different functional roles.<br>
A different approach to unsupervised learning of invariances with a biologically realistic model neuron has been taken by K?rding and K?nig [39]. In their model, bursts of backpropagating spikes gate synaptic plasticity by providing sufficient amounts of dendritic depolarization. These bursts are assumed to be triggered by lateral connections that evoke calcium spikes in the apical dendrites of cortical pyramidal cells.<br>
Of course the model presented here is not a complete implementation of SFA. We have only considered the central step of SFA, the extraction of the most slowly varying direction from a set of whitened input signals. To implement the full algorithm, additional steps are necessary: a nonlinear expansion of the input space, the whitening of the expanded input signals, and a means of normalizing the weights. When traversing the dendritic arborizations of a postsynaptic neuron, axons often make more than one synaptic contact. As different input channels may be subjected to different nonlinearities in the dendritic tree (cf. [40]), the postsynaptic neuron may have access to several nonlinearly transformed versions of the same presynaptic signals. Conceptually, this resembles a nonlinear expansion of the input signals. However, it is not obvious how these signals could be whitened within the dendrite. On the network level, however, whitening could be achieved by adaptive recurrent inhibition between the neurons [41]. This mechanism may also be suitable for extracting several slow uncorrelated signals as required in the original formulation of SFA [10] instead of just one. We assumed an explicit weight normalization in the description of our model. However, one could also use a modified learning rule that implicitly normalizes the weight vector as long as it extracts the signal with the largest variance. A possible biological mechanism is synaptic scaling [25], which is believed to multiplicatively rescale all synaptic weights according to postsynaptic activity, similar to Oja's rule [26,42]. Thus, it appears that most of the mechanisms necessary for an implementation of the full SFA algorithm are available, but that it is not yet clear how to combine them in a biologically plausible way.<br>
Another critical point in the analytical derivation for the spiking model is the replacement of the temporal by the ensemble average, as this allows recovery of the rates that underlie the Poisson processes. The validity of the analytical results thus requires some kind of ergodicity in the training data, a condition which of course needs to be justified for the specific input data at hand.<br>
It is still open whether the results presented here can be reproduced with more realistic model neurons. The spiking model neuron used here was simplified in that it had a linear relationship between input and output firing rate. In many real neurons, highly nonlinear behavior was observed. Interestingly, Hebbian learning for nonlinear rate-based neurons has previously been associated with the detection of higher-order moments of the input statistics [43], thereby providing a mechanism for extracting statistically independent components of the input signal. Because for sparse input statistics independent component analysis is closely related to sparse coding [44], it is tempting to speculate that within a rate picture, temporally nonlocal plasticity with a nonlinear input?output relation implements a combination of sparseness and slowness. Learning paradigms that combine these two objectives are thus an interesting field for further studies [11,45].<br>
Another nonlinearity that we have neglected is the frequency- and weight-dependence of STDP [16,46]. Additional work will be needed to examine how these interfere with the proposed functional role of STDP. Furthermore, modeling the spiking mechanism of a neuron by an inhomogeneous Poisson process is also a severe simplification that ignores basic phenomena of spike generation in biological neurons such as refractoriness and thresholding. It is not clear how these characteristics would change the learning rule that leads to an implementation of the slowness principle. It seems to be a very difficult task to answer these questions analytically. Simulations will be necessary to verify the results derived here and to analyze which changes appear and which adaptations must be made in a more realistic model of neural information processing.<br>
In summary, the analytical considerations presented here show that (i) slowness can be equivalently achieved by minimizing the variance of the time derivative signal or by maximizing the variance of the low-pass filtered signal, the latter of which can be achieved by standard Hebbian learning on the low-pass filtered input and output signals; (ii) the difference between SFA and the trace learning rule lies in the exact shape of the effective low-pass filter?for most practical purposes the results are probably equivalent; (iii) for a spiking Poisson model neuron with an STDP learning rule, it is not the learning window that governs the weight dynamics but the convolution of the learning window with the EPSP; (iv) the STDP learning window that implements the slowness objective is in good agreement with learning windows found experimentally. With these results, we have reduced the gap between slowness as an abstract learning principle and biologically plausible STDP learning rules, and we offer a completely new interpretation of the standard STDP learning window.<br>
<br>
Methods<br>
The methods employed in this paper rely on standard mathematical techniques as commonly used in the theory of synaptic plasticity (see, e.g., [47]).<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2265486</b><br>
The Signaling Petri Net-Based Simulator: A Non-Parametric Strategy for Characterizing the Dynamics of Cell-Specific Signaling Networks<br>
Reconstructing cellular signaling networks and understanding how they work are major endeavors in cell biology. The scale and complexity of these networks, however, render their analysis using experimental biology approaches alone very challenging. As a result, computational methods have been developed and combined with experimental biology approaches, producing powerful tools for the analysis of these networks. These computational methods mostly fall on either end of a spectrum of model parameterization. On one end is a class of structural network analysis methods; these typically use the network connectivity alone to generate hypotheses about global properties. On the other end is a class of dynamic network analysis methods; these use, in addition to the connectivity, kinetic parameters of the biochemical reactions to predict the network's dynamic behavior. These predictions provide detailed insights into the properties that determine aspects of the network's structure and behavior. However, the difficulty of obtaining numerical values of kinetic parameters is widely recognized to limit the applicability of this latter class of methods.<br>
<br>
Introduction<br>
Signaling networks are complex, interdependent cascades of signals that process extracellular stimuli, received at the plasma membrane of a cell, and funnel them to the nucleus, where they enter the gene regulatory system. These signaling networks underlie how cells communicate with one another, and how they make decisions about their phenotypic changes, such as division, differentiation, and death. Further, malfunction of these networks may alter phenotypic changes that cells are supposed to undergo under normal conditions, and potentially lead to devastating consequences on the organism. For example, altered cellular signaling networks can give rise to the oncogenic properties of cancer cells [1],[2], increase a person's susceptibility to heart disease [3], and have been shown to be responsible for many other devastating diseases such as congenital abnormalities, metabolic disorders and immunological abnormalities [1],[4].<br>
In light of the crucial role signaling networks play in the proper functioning of cells and biological systems as a whole, and given the grave consequences their alterations may have on the behavior of cells, elucidating the connections in the networks, and understanding how they operate, are two central questions in cell biology. However, unlike the ?pathway view? of signaling as linear cascades, signaling networks are highly interconnected, involve cross-talk among several pathways, and contain feedback and feed-forward loops [5]. Figure 1 illustrates this issue in a network of signaling cascades, which is stimulated by EGF and contains several players in cancer pathways. For example, multiple paths lead from EGFR to mTOR-Raptor, resulting in feed-forward loops. Some of these paths activate mTOR-Raptor, while others inhibit it. Further, the network contains two feedback loops, one from p70S6K to EGFR and another from MAPK1,2 to EGFR.<br>
These and other complexities make it very difficult to analyze signaling networks by experimental biology approaches alone. As a result, computational methods have been developed and combined with experimental biology approaches, producing powerful tools for the analysis of these networks [6]. These computational methods produce hypotheses that guide the experimental design, leading to more informative experiments, while experimental results help refine the computational models, resulting in more accurate predictive tools.<br>
In a recent survey, Papin et al. classified existing computational methods into two categories: structural and dynamic network analysis [6]. Structural network analysis is mainly based on the network's connectivity, which is typically readily available from numerous public signaling network databases (e.g., [7]?[9]), and makes inferences about global network properties as well as individual protein functions. This category can be further refined into two sub-categories, both of which are solely based on connectivity information, yet differ in the type of answers they provide. For example, the methods described in [10]?[13] infer ?static? properties of the network, such as numbers of paths, reachability results, etc. In a series of papers, Palsson and co-workers [6], [14]?[16] introduced extreme pathway analysis techniques, which are more appropriate for metabolic networks, yet have been applied to signaling networks to characterize various properties of networks, such as redundancy and cross-talk. Similar analyses have also been formalized and conducted using the principles of S- and T-invariants in Petri Nets (e.g., [17]?[20]).<br>
Methods for dynamic network analysis use, in addition to the network connectivity, the kinetic parameters of the biochemical reactions. The goal of these methods is to model the actual kinetics of the network and obtain through simulation the actual quantities of proteins involved in signal transduction. One of the most widely used techniques in this category is systems of ordinary differential equations (ODEs) (e.g., [21]?[25]). Within such a system, each reaction is modeled by a series of equations connecting reactant concentrations to product concentrations through differential relationships involving reaction rate constants. Given the difficulty of obtaining the numerical values of kinetic parameters [19],[26] and standardization of the parameters and models [27], the applicability of these methods is limited in practice to small-scale networks [6],[28].<br>
Petri Nets have also been used for simulating the dynamics of signaling networks [29]?[31]. While such approaches somewhat relax the necessity for biologically exact kinetic parameters, current Petri Net-based approaches still require the selection of weights and/or probability distributions for individual interactions in the model. As a result, selecting the values for Petri Net parameters presents challenges similar to those encountered in ODE modeling.<br>
Structural network analysis assumes mainly connectivity information about the model, and provides insights into global, static properties of the network. Dynamic analysis in general assumes numerical values of the kinetic parameters, and provides predictions of network dynamics by quantifying the change in concentration and activity-level (the concentration of the active form of a given protein) of the individual proteins and complexes in the network. To obtain a more detailed analysis one must either solve parameter optimization problems for a large number of molecules and interactions or conversely experimentally derive these values.<br>
Given the difficulty of obtaining numerical values of kinetic parameters [19],[26] and the implications this has on the applicability of dynamic analysis methods [6], it is imperative to develop innovative approaches that combine the attractive low requirements of structural network analysis techniques with the detailed answers provided by dynamic analysis techniques?specifically the response of individual proteins to signals which travel through the network.<br>
Several recent efforts in this direction have produced encouraging results. An approach using a boolean network simulation method, based on work in the area of gene regulatory networks, successfully used only signaling network connectivity information to predict the speed of signal transduction through a stomata signaling network [32]. The use of piecewise linear systems of ODEs have also had success in analyzing some of the dynamics of gene regulatory and signaling networks without using exact kinetic parameters (e.g., [33]?[35]). The obstacle to extending the method in [32] to model individual protein responses to signal transduction is the boolean model used to discretize the signal as it propagates. In a boolean model, the signal is either present or absent at each node in the network. Such two-state models of signal transduction simplify the underlying biochemistry to the point where it is difficult to model changes in protein concentration more precisely than present or absent. Modeling such gradients of concentration changes and the effects of those changes may be important to predicting individual protein responses, motivating our effort to devise more fine-grained ways to model and simulate the dynamics of signaling networks. The challenges to using linear-piecewise ODEs to model a signaling network center around the issue of identifying all the ODEs required to model the underlying network as well as scalability issues involved in simulating large systems of ODEs.<br>
In this paper, we extend the synchronized Petri net model and firing policy such that the resulting framework models cellular signaling processes. We call this extension the signaling Petri net (SPN). By coupling this with a novel strategy for Petri net execution and sampling, we obtain a method capable of characterizing some dynamics of signaling networks while using only connectivity information about these networks.<br>
To validate our method, we studied the MAPK1,2 and AKT network shown in Figure 1 in two breast cancer cell lines. This network was chosen because the EGFR receptor and its downstream signaling network play a very important role in development, differentiation, and oncogenic transformation. Two very important signaling molecules within the cell are MAPK and AKT, both of which can be activated by EGFR, and contains several potential regulatory paths between them. We constructed a model network of EGF regulation of MAPK and AKT which includes several feedback and feed-forward loops all of which were constructed based on experimental findings from different laboratories around the world [36]?[43]. We analyzed, both experimentally and computationally, the change in activity-level of several proteins in response to targeted manipulation of TSC2 and mTOR-Raptor. Using the model network, the predictions from our method agreed with experimental results in over 90% of the cases, and in those where they did not agree, our method correctly identified discrepancies that could be traced back to incompleteness in the network connectivity model.<br>
<br>
Materials and Methods<br>
Our approach combines elements of the boolean network simulator in [18] with a synchronized Petri net model [44]. In [18], Li et al. present a non-parametric approach that accurately predicts the speed of signal propagation through a network. However, as their method assumes a binary model of activation?every protein is either active (true) or inactive (false)?modeling a range of activity-levels is difficult. Petri nets, while able to model concentrations using tokens, require parameters describing the kinetic characteristics of the network, which are typically difficult to obtain.<br>
Our method models signal flow as the pattern of token accumulation and dissipation within places (proteins) over time in the Petri net. Transitions in the network represent directed protein interactions; each transition models the effect of a source protein on a target protein. Through transition firings, the source can influence the number of tokens assigned to the target, called the token-count, modeling the way that signals propagate through protein interactions in cellular signaling networks.<br>
In order to overcome the issue of modeling reaction rates in the network, signaling dynamics are simulated by executing the signaling Petri net (SPN) for a set number of steps (called a run) multiple times, each time beginning at the same initial marking. For each run, the individual signaling rates are simulated via generation of random orders of transition firings (interaction occurrences). When the results of a large enough number of runs are averaged together, we find that the series of token-counts correlate with experimentally measured changes in the activity-levels of individual proteins in the underlying signaling network. In essence, the tokenized activity-levels computed by our method should be taken as abstract quantities whose changes over time correlate to changes that occur in the amounts of active proteins present in the cell. It is worth noting that some of the most widely used experimental techniques for protein quantification?western blots and microarrays?also yield results that are treated as indications, but not exact measurements, of protein activity-levels within the cell. Thus in some respects, the predictions returned by our SPN-based simulator can be interpreted like the results of a western blot or microarray experiment looking at changes relative to ?control?.<br>
The key insight behind our approach is the assumption that, while all network parameters determine the actual signal propagation to some extent, the network connectivity is the most significant single determinant. While this is clearly a gross simplification, several researchers have observed that the connectivity of a biological network dictates, to a great extent, the network's dynamics [18], [45]?[47]. Some have conjectured that biological network connectivities have evolved to have a stabilizing effect on the overall network behavior, making the network more resilient to local fluctuations in other network parameters such as reaction rates and protein binding affinities [45],[47]. Here we present the signaling Petri net (SPN) model and the signaling Petri net-based simulator whose designs collectively utilize this assumption and couple it with a Petri net tokenization scheme that quantifies the changes in protein activity-levels that occur as signals propagate through the network. In the following sections, we describe the synchronized Petri net, how we extended it to create the signaling Petri net, and a novel strategy for executing the signaling Petri net to simulate signaling network dynamics.<br>
Petri Nets<br>
A Petri net is a graph that consists of two types of nodes, places, and transitions [44]. Edges in the graph, called arcs, are directed and connect places to transitions or transitions to places. Thus, the Petri net is a bipartite graph. Formally, a Petri net is a 4-tuple Q?=??P,T,I,O? where<br>
In order to simulate a dynamic process, a number of tokens is assigned to each place in order to indicate the presence of some quantitative property. This assignment of tokens to places encodes the state of the system and is called a marking, denoted m. A marked Petri net, R?=??Q,m0?, is a Petri net with a marking m0, called the initial marking. For the remainder of this paper, the term Petri net (PN) refers to a marked Petri net.<br>
Changes in the state of the system are simulated by executing the Petri net?evaluating the effect of transitions on the marking of the network. These changes in marking are induced by sequential firing one or more transitions. When a transition fires, it removes a token from each place connected to it by input arcs and adds a token to each place connected to it by output arcs. The number of tokens removed from inputs and added to outputs can be specified by weighting the input arcs. However, as our extension does not use this weighting property, we do not consider this very common PN formulation here.<br>
A transition can only fire when it is enabled, meaning that each of its input places has at least one token in the current marking. If a transition t, when fired on a marking m1, produces marking m2, then we write m1|t?m2.<br>
This notation can be extended to represent the effect of firing a series of transitions. A firing sequence, ??=?(t1,t2,?,tj) is a sequence of transitions. The sequence's cumulative effect on the system's state is denoted m0|??mf where m0 is the initial marking and mf is the marking produced by the firing of the sequence of transitions in the order specified in ?. In this paper, we write  to indicate the marking produced by the first g transitions in ?. Therefore, in the above example, .<br>
For a more complete introduction to types of Petri nets and their properties, we refer the reader to [44].<br>
Synchronized Petri nets<br>
Synchronized Petri nets model systems in which the firing of a transition is triggered by a specific event that occurs in the environment. The marked Petri net is extended to include a set of these events and a mapping function that assigns an event to each transition. When transition t's assigned event occurs, transition t is fired. Formally, a synchronized Petri net is a 3-tuple ?R,E,Sync?, where [44]:<br>
R is a marked Petri net,<br>
E?=?{e1,e2,?,es} is a set of events, and<br>
Sync:T?E?{e} maps each transition in the Petri net to an event. Event e is the always occurring event. Any transition associated with e is always immediately fired upon becoming enabled.<br>
When executing a synchronized Petri net, transition t is fired when its associated event e?=?Sync(t) occurs. The order in which events are generated depends upon the environment which generates them. Just as in the marked Petri net, when a transition fires, it removes one token from each place connected by input arcs and gives one token to each place connected by output arcs.<br>
As will be discussed in the next sections, we extend the synchronized Petri net paradigm to model the dynamics of a signaling network. To our knowledge, ours is the first use of the synchronized Petri net to model biochemical systems. In principle it is well suited to signaling networks since places represent proteins, tokens represent concentrations, and transitions represent directed protein interactions. A model of signaling event occurrence can be used to generate events and fire transitions, providing a way of simulating the signaling network's behavior. These and other design details will be discussed in the next section.<br>
<br>
<br>
The Signaling Petri Net-Based Simulator<br>
A high-level sketch of our simulator is given is Figure 2. Details and rationale for specific design decisions will be discussed in subsequent sections.<br>
During the simulation, the input signaling Petri net is executed multiple times on a firing sequence constructed by the signaling event generator. The signaling event generator imposes an ordering on transition firing such that it creates a two-time scale simulation. The smaller time scale is discretized as the firing of a single transition. This unit is referred to as the firing time scale. Firing steps are nested within a larger time scale, called time blocks, in which each transition is fired exactly once. Thus, there are |T| firings per block. Since the simulation is run for the specified number of time blocks, B, there are B|T| firing steps in the simulation.<br>
The time structure for an example simulation is illustrated in Figure 3. This dual-time approach is necessitated by the rate parameter sampling strategy we employ. Since the rate parameters are not known, our method executes many simulation runs (Step 2 in Figure 2) in order to sample the space of possible rate parameters. The markings returned by these runs are then averaged (Step 3 in Figure 2). The only requirement placed on the different rate parameter values is that all events occur within the same larger time frame?the time block. Therefore, within every time block all edges are evaluated once, though not necessarily in the same order.<br>
This idea of evaluating random event orderings within a two-time scale system has appeared before in the domain of transcriptional networks [48]. In that study, Chaves et al. employed a two-time scale formulation of network updates similar in concept to the one we describe here. In their work, they assumed a boolean model of regulation and characterized the effect of different relative rates of transcription within the same network on the final steady state reached. In contrast, our method is designed to operate on tokenized models of signaling networks with the ultimate intent of predicting the activity-level changes of proteins in the underlying signaling network over time.<br>
In the next sections, we discuss in greater detail the core design decisions underlying our method: the signaling Petri net, transition firing, signaling network event generator, constructing the initial marking for the model, and sampling signaling rates. We then discuss how our strategy can be used to predict the outcome of perturbation experiments.<br>
<br>
The Signaling Petri Net<br>
The goal of our method is to predict the signal flow through a cell-specific network under specific experimental conditions. As a result, the signaling Petri net model must characterize the connectivity of the signaling network, the connectivity-level network properties that are unique to the cell type and experimental conditions under which the network is being studied, and the signaling processes of activation and inhibition.<br>
The signaling Petri net is a synchronized Petri net with: 1) a specific way of modeling activating and inhibiting interactions using places, transitions, and arcs; 2) a one-to-one correspondence between events and transitions such that every transition is associated with a unique event; 3) modified rules regarding how many tokens are moved in response to a transition firing; and 4) a signaling network event generator.<br>
Places correspond to the activated forms of signaling proteins. The number of tokens assigned to place p in marking ms, ms(p), abstractly represents the amount of active protein p present in that network state. Signaling interactions are modeled using transitions and their connected input and output arcs. Each transition, t, is associated with a unique signaling event, e, such that when e occurs, transition t fires. Figure 4 shows the equivalent signaling Petri net for a signaling network.<br>
Formally, a signaling Petri net is a 3-tuple S?=??R,E,Sync?, where:<br>
R is a marked Petri net,<br>
E is a set of signaling events such that |E|?=?|T| and there is no always occurring event, and<br>
Sync:T?E is a one-to-one mapping which assigns each transition a unique signaling event.<br>
The initial marking of a signaling Petri net, m0, represents the state of rest from which the network is starting and being simulated. Proteins whose concentrations are known to be high can be given a large number of tokens, and those whose concentrations are known to be low can be assigned few or zero tokens. Attention to the initial marking is central to modeling cell-specific networks. In many cell lines, specific proteins are known to contain mutations that render them perpetually active or inactive [49]. Furthermore, experimental studies frequently involve the targeted manipulation of various proteins within the network. Both of these phenomena induce state changes in certain proteins at various time points that must be modeled. The way in which these are modeled will be discussed when the simulator design is explained.<br>
<br>
Transition Firing<br>
When a signaling interaction A?B (A activates B) or A?B (A inhibits B) occurs, it has the effect of changing the state of the system by modifying the activity-level of A and/or B. Thus, in the SPN used to model this network, the associated transition, t, will fire at time ? and produce marking m?+1 from m?. The way in which m?+1 is computed from m? depends on the set of input and output arcs attached to the transition as well as the number of tokens moved by the transition.<br>
The combination of input and output arcs connected to a transition is determined exclusively by the type of interaction and the transition firing model. However, different topologies, combinations of input and output arcs, are needed to model the different biochemical processes that mediate protein-protein interactions in a signaling network. Here we examine four of the most common biochemical processes, identify the corresponding topological motifs, and ultimately devise a modeling policy best suited for non-parametric simulation of signal flow.<br>
In post-translational modification (PTM), a protein mediates the addition or removal of a phospho group at a specific phosphorylation site on another protein. In GTP/ATP binding, a protein triggers the exchange of GDP (ADP) from GTP (ATP) on another protein. In a recruitment process, a protein mediates the relocalization of another protein to a different part of the cell. Finally, in a complexing process, a protein binds to another protein to create a complex, which can then participate in other reactions. In the first two processes, the mediating protein usually acts as an enzyme that participates in the reaction but is not consumed by the reaction. In the latter two processes, the participating protein often becomes unavailable to other reactions, transiently while the protein recruitment is taking place and for longer durations when complexing occurs. To model these two cases, we identified the two different token-passing policies implemented by the different topological motifs depicted in Figure 5.<br>
Token consumption<br>
In this policy, u?v consumes tokens in u in order to generate new tokens for v. In order to model this, pu is connected to transition t1 through an arc and pv is connected to t1 through an output arc. When t1 fires, some number of tokens in pu are moved into pv. Similarly, u?v consumes tokens in u in order to consume tokens in v. This is modeled by connecting pu to t2 with an input arc and pv to t2 with an input arc. When t2 fires, some number of tokens are removed from both pu and pv. This policy models a recruitment or complexing event in which u binds to another molecule, thereby creating a molecule of type v. A molecule of type u has been consumed in order to generate or deactivate a molecule of type v.<br>
<br>
Token conservation<br>
In this policy, u?v generates new tokens for v while conserving those in u. In order to model this, pu is connected to transition t3 through a read arc. Node pv is connected to t3 through an output arc. When t3 fires, some number of tokens in pu is read (but not removed) and copied into pv. Similarly, u?v consumes tokens in v while conserving those in u. This is modeled by connecting pu to t4 with a read arc and pv to t4 with an input arc. When t4 fires, some number of tokens in pu are read and removed from pv. Enzymes will often behave in this way: inducing a change in a molecule (v) without themselves undergoing any change. A molecule of u has induced a change in a different molecule of type v without itself changing state.<br>
Ideally, for each interaction in the network, the associated transition could be embedded in the topology corresponding to the interaction's underlying biochemical mechanism. However, connectivity-level knowledge of the network does not provide this information for each interaction. In the absence of these details, we use one token-passing policy for all interactions in the network. We implemented and tested both the consuming and conserving policies and found that token conservation provides significantly more accurate results when compared to experimentally derived data. This is not surprising, as post-translational modification and GTP/ATP binding events are responsible for many activation state changes in signaling networks [1], [50]?[52]. It is worth noting that our approach does not restrict the net structure to token conserving topologies. Thus, it is possible to use the token consumption topologies where such processes are known to occur. However, as our focus in this paper is designing a purely non-parametric simulation method, we consider the use of information regarding the biological mechanism of signaling as a potential way to further improve the accuracy of our method's predictions and identify this as a direction for future work.<br>
The transition topologies, as described above, do not designate how the number of tokens added to or removed from pv is determined. However, we know that in biochemical signaling networks concentration has an effect on the strength of a signaling event [53]?[55]. Specifically, the higher u's concentration, the stronger its effect on v?the more tokens that pu has, the more tokens of pv should be affected (generated or consumed).<br>
However, because of the stochastic nature of the underlying biochemistry, it would be inaccurate to assume that all active u molecules will always participate in an interaction with v. In order to accommodate this observation, when transition t fires, we randomly select the number of pu's tokens to be involved in the subsequent evaluation of the transition, which we call a signaling event. Note that, according to our choice of topology, pu can always be identified as the node connected to the transition by a read arc. In this paper, we assume a uniform distribution for selecting the number of tokens involved in a given signaling event, but acknowledge that other distributions may be more appropriate under certain circumstances and identify this as a topic deserving further consideration.<br>
Let ms(x) denote the number of tokens in node x at time s. For an interaction (u,v), under the token conservation policy detailed above, u's token-count remains unchanged after the firing of t, whereas v's token-count is updated based on the following formula:where random(p,q) is a random integer drawn from a uniform distribution over the range [p,q].<br>
If we employ the policy of token passing with consumption, then after ms(v) has been computed based on the formula above, ms(u) is updated as:<br>
<br>
<br>
Signaling Network Event Generator<br>
The SPN topology and transition token-number selection policy alone do not specify the speed with which individual signaling interactions occur. However, such rates must be accounted for when simulating a signaling network. ODEs characteristically model such details as reaction rate constants; parameterized Petri nets specify these in a variety of ways including transition firing rates and firing probabilities [17],[30]. In synchronized Petri nets, the environment controls the generation of events. Thus, the signaling network event generator is responsible for controlling the timing and ordering of signaling events. However, as our objective is a non-parametric simulation method, our approach must either estimate these parameters or operate without explicit knowledge of them.<br>
Estimating reaction rates using only connectivity is currently beyond the predictive or inferential capabilities of computers. While there has been some work in the area of predicting reaction rates, all results of which we are aware require knowledge about the mechanism of signaling (e.g., [56]). As a result, without enriching the SPN model, it is doubtful that rate parameters can be accurately estimated.<br>
For this reason, the signaling network event generator operates without explicit knowledge of the rate parameters. To compensate for this ?missing? knowledge, we make use of an observation of signaling networks discussed earlier: a network's connectivity determines its dynamics. Several studies have found that the connectivity of biochemical networks desensitizes them to small fluctuations in the kinetic biochemical parameters [45]?[47]. Understood within the context of evolution ? a stochastic process that tweaks signaling network parameters across generations ? this is a highly desirable property as it ensures that an offspring remains viable despite fluctuations in the exact tuning of its cellular machinery. If this property holds, then small fluctuations in the rate parameters should have a marginal effect on the overall propagation of signal through the network. We can consider these small effects to be noise obscuring the underlying dynamics of the network connectivity. By taking many samples of the network dynamics under a variety of reaction rate assignments and then averaging these dynamics, we simultaneously reduce the noise introduced by any one rate assignment and strengthen the underlying dynamic characteristics of the network's connectivity.<br>
However, since reaction rate constants can vary by several orders of magnitude?from 10?10 to 103, the task of correctly selecting parameters close to the true parameters is non-trivial. In fact, without having some estimate of the actual rate parameters, it is unclear as to how to measure closeness at all. Clearly, these are among the issues that make parameter estimation so difficult for ODE and Petri net approaches. Since our comparisons will be relative and not absolute, we take a relative approach to modeling rate parameters. The space of possible rate values is the space of possible signaling event orderings.<br>
This idea is illustrated in Figure 3A. Protein A affects the activity of protein D through two separate pathways. Assuming that A is active to begin with, the relative speed of these two pathways determines the final activity of D. If the pathway through C is faster than the pathway B?D, then D will be active. However, if the pathway speeds are reversed, then D will remain inactive. The overall outcome of this network can be represented without any use of numeric reaction rates by representing the reaction rates as an ordering over all the edges in the network. We can extend this idea to the SPN by observing that there exists a unique event for each signaling edge in the signaling network.<br>
This sampling strategy is the motivation for the dual-time framework depicted in Figure 3B and implemented by the signaling network event generator shown in Figure 6. Time blocks are the larger time intervals during which every signaling event occurs exactly once. Since every transition in the SPN is associated with a unique event, each transition will fire exactly once in each time block. Transition firings are the smaller time units that impose a strict sequential order on the occurrence of signaling events. While this strict sequentiality of firing models relative reaction rates, it also discretizes the effect of signaling events. Though this is consistent with the definition of transition firing in discrete time Petri nets (only one transition is evaluated at a given point in time) [44], in biological signaling networks there is no such serial evaluation constraint. However, our validation with experimental data suggests that this discretization approximation does not affect the overall validity of the simulation results.<br>
<br>
Defining the Initial State<br>
As mentioned previously, the initial state of the SPN is the initial marking, m0. As the SPN provides no explicit information on how this marking should be built, we propose three ways to construct the initial state: zero, basal, or experimentally derived. In a zero initial state, the simulator initializes all proteins to have zero tokens. The basal initial state is a random distribution of activation levels intended to model the cell when no impulses due directly to external stimuli are propagating through the signaling network. Though a basal network is considered at rest, in general it will not have a zero marking since signal flows are known to occur even in unstimulated signaling networks through autocrine and paracrine secretions by the cells. The experimentally derived initial state is based on knowledge about the activity levels of various proteins just prior to the addition of the external stimuli.<br>
When accurate experimental data is available such as results from microarrays or western blots, the experimentally derived initial state may be the most accurate. A challenge in using experimental data, however, is determining how best to assign numbers of tokens based on the experimentally observed activity levels.<br>
In the absence of reliable experimental data, the basal initial state seems more accurate than the zero initial state. However, it presents the challenge of properly selecting the basal activity-levels to assign to each protein in the model network. In [18], a basal initial state was constructed by activating a small number of randomly selected proteins in the signaling network. However, the work in [18] was done using a boolean model. Translating this approach into a tokenized model creates the additional complexity of determining how many tokens each basally active protein should receive. The correct values are likely to depend on the specific signaling network and experimental conditions.<br>
We performed preliminary tests to compare the effect of using different basal versus zero markings on the outcome of the simulator. We found that the basal and zero states produced indistinguishable predictions so long as less than 30% of the proteins were activated and a small number of tokens (&lt;5) were used when constructing the basal marking. This is not as surprising as it may seem at first. Inhibitory edges will quickly consume a small number of tokens scattered throughout the network, effectively returning much of the network to the zero state before a stimulation event can propagate through.<br>
Furthermore, while validating our method, we also compared the predictions produced by SPNs based on a zero initial state and experimentally derived initial state. These, too, did not produce noticeably different final results for similar reasons as discussed above. Details of these comparisons will be discussed further in the Results and Discussion sections.<br>
However, since all three initial state construction strategies yield qualitatively identical predictions, using zero initial states has the advantage of invoking the fewest unnecessary assumptions about the network (as in the case of the basal initial state) and requiring the least experimental data (as in the case of the experimentally derived state). Nonetheless, in our implementation of the tool, we allow for using any one of these three initial state construction strategies.<br>
<br>
Modeling Cell-Specific Signaling Networks<br>
Whereas consensus signaling networks typically represent the connectivity in normal cells, many experiments are conducted on abnormal cells in which oncogenic mutations, gene knockouts, and pharmacological inhibitors have altered the behavior of various signaling nodes in the network. In an SPN, these alterations to the signaling network can be modeled by adding/removing transitions (and associated input/output arcs) and explicitly setting the token count for various proteins in the initial state.<br>
The two network alterations which are commonly induced by oncogenic mutations, gene knockouts, or pharmacological inhibitors are constitutively high or low protein activity-levels, meaning that a protein is either unable to be inhibited or unable to be activated. The simulator allows for proteins to be specified as either fixed High or Low. Here we explain how these are modeled by changes to the SPN.<br>
If protein u is fixed high, then this protein cannot be inhibited. Thus, all transitions that remove tokens from pu are removed from the SPN. The fact that u is high, however, also suggests that it maintains a higher activity level in general. Therefore, in the initial state, m0(pu)?=?H, where H is a non-zero number of tokens. Since all inhibiting transitions have been removed from the SPN, throughout any execution, place pu will always have at least H tokens.<br>
In experiments, we have observed that the choice of the value of H does not change the relative outcome of the simulations. While H will affect the actual number of tokens present in a given place as well as the number of time blocks required to observe certain activity-level changes, the relative changes in activity-level (number of tokens) among different proteins (places) does not change. As a result, one is free to select any reasonable value of H (for our experiments, we used H?=?10) as long as this H is held constant across all simulations whose results will be compared.<br>
If protein u is fixed low, then this protein cannot be activated. Thus, all transitions that add tokens to pu are removed from the SPN. The fact that u is low, however, also suggests that it maintains a constantly low activity level in general. Therefore, in the initial state, m0(pu)?=?L, where L is a small number of tokens (in our simulations we use L?=?0). Since pu is only inhibited, we observed that all constitutively low proteins quickly had their marking reduced to zero.<br>
Unlike the value of H, extra caution must be taken when selecting values for representing L. A value of L that is too large can destabilize the early propagation of signal through the network. In our experiments, we obtained best results for values of L very close to or equal to zero (L?2). Beyond this, the final results obtained depended on other values in the network, the strength of the signal, and the duration of the simulation.<br>
<br>
Simulating a Signaling Network<br>
Figure 7 provides more detailed versions of the simulation algorithm outlined in Figure 2. Steps 1 and 2 of the Simulate procedure constructs the initial marking and net topology to incorporate perpetually high proteins, H, and perpetually low proteins, L. In this paper, proteins that are assigned high activity-levels receive an initial token count of 10 in order to model a higher-than-average initial activity-level. As discussed earlier, using other values of H scale the activity-levels of all the proteins in the network, but will not qualitatively change their relative activities.<br>
The loop in Step 3 runs r individual simulation runs. Each run receives a different event ordering, ?e, thereby implementing the interaction rate sampling strategy. The time block/step structure is contained within the ordering ?e (see Figure 6C). As a result, the SPN execution step simulates the events by firing their associated transition. Only those markings that correspond to time block boundaries are sampled.<br>
After Simulate finishes collecting the time block markings from all the runs, Step 4 computes the average markings for each time block and Step 5 returns these averages.<br>
<br>
Simulating a Perturbation Experiment<br>
We tested the accuracy and performance of our method by simulating the effect of two different targeted manipulations to a well-known signaling network. We compared these predictions to experimental results produced by performing the actual manipulations on two separate cancer cell lines.<br>
The perturbations we considered in this study altered the constitutive activity-level of various proteins in the network (as opposed to affecting specific signaling interactions). Therefore, we modeled the perturbations as changes in the high and low proteins?Hc and Lc for the control (unperturbed) network and Hp and Lp for the perturbed network.<br>
A variant of the Simulate method was required to quantify how a perturbation changed the protein token-counts for each time block. Figure 8 shows the algorithm we used. In the procedure DifferentialSimulate, the input S provides the consensus SPN. Inputs Hc and Lc specify the control high and low proteins, the inputs Hp and Lp specify the perturbed high and low proteins. After Steps 1?5 construct two separate SPNs for the control and perturbed conditions, the loop in Step 6 performs r independent simulations over the control and perturbed models. Step 6d computes the difference between the markings at the end of each time block in the perturbed and control networks. The marking difference  yields the marking  where  for each v?P. Following the loop, the marking differences are averaged to obtain the time series (?1,?2,?,?B) where ?b(v) is the average change in the token-count for protein v at time block b.<br>
For values of |?b|&gt;0 for a given molecule v, we can conclude that the perturbation caused a change in the activity-level of v at time block b only if the difference observed is statistically significant. We use a t-test to determine whether this change is statistically significant for protein v at time block b. Computing the t-test for two distributions (control and perturbation) requires knowledge of the mean (?c,b and ?p,b) as well as the variance  for both distributions. In order to obtain these parameters for the control network, a large number, X, of independent simulations is run. Simulation i provides a single series of markings, . The mean is then computed:The variance is computed similarly:The parameters ?p,b,v and  for the perturbed network are computed as described above by substituting the perturbed network for the control network. Using these parameters, the t-value for molecule v at time block b can be computed from the formulaThe statistical significance of the difference can then be obtained by comparing the t-value to the desired critical value.<br>
Note that the DifferentialSimulate procedure and the associated significance test can predict the effect not only of perturbations, but also of any two different experimental (or cellular) conditions imposed on the same signaling network. As a result, in addition to perturbation experiments, our method can also be used to study the effects of other phenomena that induce changes in the propagation of signal through a signaling network.<br>
<br>
<br>
<br>
Cell-Specific Signaling Network Models<br>
Figure 1 shows the signaling network we analyzed. We obtained the core connectivity from a published literature survey on the EGFR network [57]. We added to this several other well-established interactions taken from literature [36]?[43]. The response of this network to various perturbations was measured and simulated in two separate breast cancer cell lines: MDA231 and BT549. The core signaling Petri net used, SEGFR, is captured by the following signaling proteins and interactions: places (the set P): vEGFR, vSRC, vRac, vMEKK4, vMEK4, vJNK, vMEKK6, vMEK6, vSTAT, vGrb2, vShc, vSOS, vRB, vELK, vBAD, vNFKB, vRAS, vGAB1, vPIP3, vPI3K, vPDK1, vPTEN, vc-Raf, vAKT,vLKB1, vMEK, vGSK3?, vAMPK, vTSC2, vMAPK1,2, vRSK, vRheb, vmTOR-Raptor, v4EBP1, vp70S6K, vp38, and vpS6.<br>
Protein interaction network motifs (the combination of arcs and transitions): vEGFR?vGrb2, vGrb2?vShc, vShc?vSOS, vSOS?vRAS, vGrb2?vGAB1, vGAB1?vPI3K, vEGFR?vSRC, vSRC?vSTAT, vPI3K?vPIP3, vPIP3?vPDK1, vRAS?vc-Raf, vPDK1?vAKT, vRAS?vRac, vRac?vMEKK4, vMEKK4?vMEK4, vMEK4?vJNK, vJNK?vSTAT, vRac?vMEKK6, vMEKK6?vMEK6, vMEK6?vp38, vp38?vSTAT, vPDK1?vp70S6K, vPTEN?vAKT, vAKT?vc-Raf, vAKT?vGSK3?, vAKT?vTSC2, vAKT?vAMPK, vAKT?vBAD, vAKT?vNFKB, vAKT?vp70S6K, vLKB1?vAMPK, vMEK?vMAPK1,2, vMAPK1,2?vRB, vMAPK1,2?vELK, vMAPK1,2?vSTAT, vGSK3??vTSC2, vAMPK?vTSC2, vMAPK1,2?vEGFR, vMAPK1,2?vTSC2, vMAPK1,2?vp70S6K, vMAPK1,2?vRSK, vRSK?vTSC2, vTSC2?vRheb, vRheb?vmTOR-Raptor, vAKT?vmTOR-Raptor, vmTOR-Raptor?v4EBP1, vmTOR-Raptor?vp70S6K, vp70S6K?vEGFR, vSRC?vSRC, vRac?vRac, vMEKK4?vMEKK4, vMEK4?vMEK4, vJNK?vJNK, vMEKK6?vMEKK6, vMEK6?vMEK6, vSTAT?vSTAT, vGrb2?vGrb2, vShc?vShc, vSOS?vSOS, vRAS?vRAS, vc-Raf?vc-Raf, vMEK?vMEK, vMAPK1,2?vMAPK1,2, vRB?vRB, vELK?vELK, vRSK?vRSK, vGAB1?vGAB1, vPIP3?vPIP3, vp38?vp38, vPI3K?vPI3K, vPDK1?vPDK1, vAKT?vAKT, vBAD?vBAD, vNFKB?vNFKB, vAMPK?vAMPK, vmTOR-Raptor?vmTOR-Raptor, vp70S6K?vp70S6K, vpS6?vpS6, v4EBP1?v4EBP1.<br>
Notice that the last several edges are self-inhibitory loops (e.g., vRas?vRas). These loops are used to model regulatory mechanisms that are not present in the model network.<br>
For molecules that do not have specific inhibitory edges modeled in the network, we use the self-inhibitory loop to prevent exponential increase in the token counts and to model inhibitory mechanisms beyond the scope of the network. For example, consider the molecule Ras in the network shown in Figure 1. In the model, this protein is not inhibited. However, biologically we know that Ras has intrinsic GTPase function which inactivate itself. In order to model this, we introduce a self-inhibitory loop.<br>
The differences between the two cell-specific networks are captured by following activity assignments to various proteins in the SPN. In the MDA231 cell line, HMB?=?{vRas, vEGF} and LMB?=??. In the BT549 cell line, HBT?=?{vEGF} and LBT?=?{vPTEN}.<br>
Of the two perturbations we considered, one significantly knocked down the activity-level of TSC2 and the other knocked down mTOR-Raptor. While the core SPN still modeled these networks, separate perturbed activity-assignments were required for each cell line-perturbation pairing: LMB-TSC2?=?LMB?{vTSC2}, LMB-mTOR?=?LMB?{vmTOR-Raptor}, LBT-TSC2?=?LBT?{vTSC2} and LBT-mTOR?=?LBT?{vmTOR-Raptor}.<br>
<br>
Setup for Perturbation Experiments<br>
Cell culture and stimulation<br>
Human MDA-MB-231 (MDA231) and BT549 breast cancer cells were routinely maintained in RPMI supplemented with 10% FBS. For signaling experiments, logarithmically growing cells were serum-starved for 16 hours and then subjected to treatments by epidermal growth factor (EGF) (20 ng/mL) (Cell Signaling Technology, Beverly, Massachusetts) for 30 minutes. Controls were incubated for corresponding times with DMSO. To knock down TSC2, cells were treated with short interfering RNA (siRNA) (Dharmacon, Lafayette, Colorado) for 72 hours prior to EGF stimulation. Control cells were transfected with non-targeting (N/T) siRNA (Dharmacon, Lafayette, Colorado) prior to EGF treatment.<br>
<br>
Antibodies<br>
The following antibodies were used for immunoblotting: anti-phospho-p44/42 MAPK, anti-phospho-GSK3? (S21/S9); anti-phospho-AKT(ser473); anti-phospho-TSC2(T1462); anti-phospho-mTOR(S2448); anti-phospho-P70S6K(T389) (Cell Signaling Technology, Boston, Massachusetts); and anti-?-Actin (Sigma-Aldrich, St. Louis, Missouri).<br>
<br>
SDS-PAGE and immunoblotting<br>
Cells were lysed by incubation on ice for 15 minutes in a sample lysis buffer (50 mM Hepes, 150 mM NaCl, 1 mM EGTA, 10 mM Sodium Pyrophosphate, pH 7.4, 100 nM NaF, 1.5 mM MgCl2, 10% glycerol, 1% Triton X-100 plus protease inhibitors; aprotinin, bestatin, leupeptin, E-64, and pepstatin A). Cell lysates were centrifuged at 15,000 g for 20 minutes at 4?C. The supernatant was frozen and stored at ?20?C. Protein concentrations were determined using a protein-assay system (BCA, Bio-Rad, Hercules, California), with BSA as a standard. For immunoblotting, proteins (25 ?g) were separated by SDS-PAGE and transferred to Hybond-C membrane (GE Healthcare, Piscataway, New Jersey). Blots were blocked for 60 minutes and incubated with primary antibodies overnight, followed by goat anti-mouse IgG-HRP (1?30,000; Cell Signaling Technology, Boston, Massachusetts) or goat anti-rabbit IgG-HRP (1?10,000; Cell Signaling Technology) for 1 hour. Secondary antibodies were detected by enhanced chemiluminescence (ECL) reagent (GE Healthcare, Piscataway, New Jersey). All experiments were repeated a minimum of three independent times.<br>
<br>
Setup for perturbation simulations<br>
To select the block duration parameter, B, we compared the experimentally derived fold change of AKT in the MDA231 cell line to the AKT fold changes predicted for B?=?10, 20, 50, 100, and 1000. We found B?=?20 to be the best fit and used this value for all simulations in this study.<br>
We also experimented with input parameter r, the numbers of individual simulation runs averaged per simulation. We tried a range extending from r?=?100 to r?=?1000. We found that no observable changes occurred in trends for r?400. Therefore, r?=?400 was used for all simulations in this study.<br>
We considered both the zero and experimentally derived initial states as the initial markings for the TSC inhibition simulations. The experimental states for both cell lines were derived from western blots produced from cells that were incubated in DMSO and serum-starved for 16 hours. Unsampled molecules were assigned a marking of zero. The number of tokens assigned to each sampled molecule was directly proportional to the darkness of the line on the western blot. This assignment was done by hand, though devising automated and standardized methods for the construction of experimentally derived initial states is an important direction for future work. Since most of the molecules in the network were not sampled, only mTOR-Raptor, TSC2, GSK3?, p70S6K, AKT, and MAPK were given non-zero markings. The initial markings used are shown in Table 1.<br>
Since experimental results for the mTOR-Raptor inhibition were obtained from literature, we did not have experimental results for construction of experimentally derived initial states. Therefore, we used the zero initial states for the mTOR-Raptor inhibition simulations.<br>
<br>
<br>
<br>
Results<br>
In order to evaluate the accuracy of our simulation method, we tested its predictions of the effect of targeted manipulations on two cell-specific versions of the signaling network depicted in Figure 1. In each cell line, a TSC2-specific siRNA was applied and the concentration of several key proteins in the EGFR network were sampled 30 minutes after stimulation with EGF. This was repeated in the absence of the TSC2 siRNA in order to obtain the concentration in the control network. We also collected a corpus of literature detailing the response of signaling proteins activity-levels to the inhibition of mTOR-Raptor using Rapamyacin [43],[58]. Predictions were generated by our simulator for the TSC2 and mTOR-Raptor perturbations in both cell lines.<br>
Simulation<br>
To simulate a perturbation, we used two networks both based on the signaling network shown in Figure 1: the control network for the cell line and the perturbed network for the cell line. The control networks for the cell lines were different because it was important to model the cell-specific mutations. In the case of the BT549 cell line, there is a mutation that leads to the loss of PTEN, which makes AKT always active. In the MDA231 cell line, there is a mutation in Ras, which makes it always active. As shown in the formulation of the model, these are modeled using fixed activity assignments in the simulator.<br>
The TSC2 (mTOR-Raptor) perturbed network for a cell line was created by taking the control network and fixing the activity-level of TSC2 (mTOR-Raptor) to zero for the duration of the simulation, effectively simulating the pharmacological inhibition of the protein. For each cell-line/perturbation pair, we ran the simulator on the control and perturbed networks using the DifferentialSimulate procedure in Figure 8 which computed the change in token-counts induced by the perturbation for all proteins in the model. These change plots are shown in Figure 9 for TSC2 and in Figure 10 for mTOR-Raptor. We ran the simulations using both experimentally derived initial states as well as zero initial states. The initial state used did not change the overall trends observed in the simulations.<br>
Using the t-test described in the Methods section, we also computed the statistical significance of the final time block (b?=?20) for each molecule considered. For each molecule considered, 400 runs, 20 time blocks, and 50 samples were used. With the exception of GSK3? which did not show a significant response to the perturbation, the changes of all other proteins sampled were beyond the 0.05 significance level (see Table 2). The statistical insignificance of the change in GSK3? is not surprising since, as shown in Figure 1, GSK3? is solely activated by LKB, a molecule fixed high in both cell lines. Thus, we should not expect either perturbation to have a significant effect on the activity of GSK3?, which is what the t-value indicates.<br>
<br>
Experimental Results<br>
After the TSC2 perturbation was applied to a cell line, the protein concentrations were collected using western blots. Details are given in the Materials and Methods section. The western blot results are shown in Figure 9.<br>
<br>
<br>
Discussion<br>
As can be seen in Table 3, our method correctly predicted the relative protein activity-level changes induced by the TSC2 perturbation in both cell lines, for most molecules sampled. Notice that no change (?) was reported for the predicted response of MAPK to the TSC2 perturbation despite the fact that a small change did occur in its marking during the simulation (see Figure 9) and the t-value for the change is significant (see Table 2). At first, interpreting this value as no change may seem misleading. However, one of the significant challenges in experimental perturbation experiments is separating true system responses from the background noise created by experimental variables that cannot be precisely controlled (among them cell population sizes, variability in microarray antibody binding effectiveness, and limited sensitivity of hardware and software used to quantify experimental results). As a result, a common practice is to only consider those substantial changes that are well beyond the background noise level. Our interpretation of the small predicted change in MAPK as no change reflects the fact that such small changes would not be detectable in microarray or western blot results. Thus, though such a small fluctuation might have occurred in the real data, it would not have been detected by the biologists and most likely would appear in the experimental data to have not changed.<br>
Similar reasoning guided our decision to characterize the simulation (and experimental) results as either up (?), down (?), or no change (?) in general. Since the amount of protein registered in a microarray or western blot is not always a reliable indicator of the exact amount of protein (or protein form) being measured, biologists are often reluctant to report degrees of increases or decreases?preferring binary observations such as up or down which are less subject to influence by extraneous experimental conditions. It is true that our simulation method produces precisely quantified increases or decreases which can be taken to indicate degrees of change in response to perturbations. However, as experimental techniques cannot reliably measure degrees of increase or decrease, we judged the binary (up or down) characterization to be a more reliable way of validating our method. Certainly, our method provides additional information of degrees of change and we consider studying the accuracy of these degrees to be an important area for future work.<br>
Our method also correctly predicted the activity-level change of AKT in response to mTOR-Raptor inhibition as reported by a number of studies [43],[58]. Further, our method predicted that, when mTOR-Raptor is inhibited, the level of p70S6K in the MDA231 cell line decreased, which also had been observed experimentally [59].<br>
The only incorrect prediction made by our method was the activity-level change of p70S6K in the BT549 cell line. However, BT549 cells contain an RB mutation [49] which could alter p70S6K phosphorylation [60]. It is a strength of our simulator that the discrepancy between our method's predictions and the experimental results identified a section of the model in which additional connectivity has been found which might account for the difference observed.<br>
The predictions made by our simulator would be exceedingly difficult to derive by visual or manual inspection. Table 4 shows the number of paths between several pairs of compounds within the network. Where there is more than one path connecting two molecules, feed forward and feed backward loops are present. Attempting to determine, by hand, how these different loops will interact with one another is, by itself, a difficult endeavor even when not considering the additional task of deriving the rest of the network dynamics simultaneously. For the larger networks that are now becoming available, computational analysis becomes even more crucial to obtaining insights into the dynamic behavior of the network.<br>
Despite the complexity of the network dynamics, it was straightforward to find and integrate the connectivity information used to build it. Most of the information sources [36]?[43] established the existence of various pathways and provided few or no biochemical or kinetic details. As a result, the literature we used would have provided little assistance is building a parameterized Petri net or ODE model. Due to the proliferation of curated signaling network repositories and searchable literature archives, connectivity information is relatively abundant which makes the ad hoc assembly of networks a relatively straightforward endeavor. This further underscores the advantage of using our method over ODEs or parameterized Petri nets to quickly model and characterize some of the dynamics of a signaling network.<br>
For simulations that will be compared to experimental results, the time parameter must be selected carefully. The time parameter, B, indicates how many time blocks our method will simulate. The time block is an abstract unit of time. Therefore, before comparing experimental results and predictions, it is necessary to determine how many seconds, minutes, or hours correspond to a time block. This can be done by comparing a prediction of the simulator with the experimentally measured activity-level of one or two proteins at several time points in order to determine what time blocks correspond to the different sampled time points. In the present study, we calibrated our time blocks only once for two cell lines and six experimental conditions (two cell lines, with/without TSC2, with/without mTOR-Raptor). To select the time parameter we used the experimentally measured activity changes in two proteins at two time points. In contrast to other predictive dynamic analysis tools which require multiple time points and multiple protein samples in order to calibrate simulation and model parameters, our method has relatively low time and resource investment.<br>
Besides the time parameter, the other component of our simulations which involved experimentally obtained knowledge was the initial states. The experimentally derived initial states require that some experimental data be available providing information on the initial concentrations of individual signaling proteins in the network prior to stimulation. However, in the network that we considered here, the overall behavior of the network and of individual signaling proteins was resilient to changes in the initial states used. Zero and experimentally derived both produced the same overall change predictions. Thus, while experimentally derived initial states may be important for the simulation of some networks, it may well be the case that many networks (such as the one we considered in this paper) can be simulated without this knowledge?further reducing the experimental work that must be done prior to simulation.<br>
The fact that our simulator produced accurate predictions for a variety of experimental conditions using the one core network model and set of simulation parameters also distinguishes our method from other predictive approaches. The only aspects of the model that were modified during the simulations were activity-levels reflecting the immediate effects of either the underlying tumor mutations (Ras and PTEN) or the perturbations (mTOR-Raptor and TSC2 targeted manipulation). In contrast, the accuracy of ODEs and Petri nets predictions are known to be sensitive to small changes to the model. For comparative studies such as the one conducted in this paper, an ODE or parameterized Petri net model might need to be re-constructed with different parameters for each experimental condition of interest. As a result, while it is possible to obtain our simulation results using these models, it remains beyond the capabilities of any existing ODE or parameterized Petri net system to provide insights into the effects of experimental conditions on the dynamic behavior of a signaling network with so little initial time and resource investment.<br>
Though our method's predictions will not be as accurate as the results returned by a correctly parameterized ODE, biologists using our method can derive information about a network's dynamic behavior without having to conduct extensive experimentation and computationally expensive parameter estimation. This novel capability offers scientists the exciting prospect of being able to test hypotheses regarding signal propagation in silico. As a result, by using our method researchers can evaluate a wide array of network responses in order to determine the most promising experiments before even entering the laboratory.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2787923</b><br>
Distributed Dynamical Computation in Neural Circuits with Propagating Coherent Activity Patterns<br>
Activity in neural circuits is spatiotemporally organized. Its spatial organization consists of multiple, localized coherent patterns, or patchy clusters. These patterns propagate across the circuits over time. This type of collective behavior has ubiquitously been observed, both in spontaneous activity and evoked responses; its function, however, has remained unclear. We construct a spatially extended, spiking neural circuit that generates emergent spatiotemporal activity patterns, thereby capturing some of the complexities of the patterns observed empirically. We elucidate what kind of fundamental function these patterns can serve by showing how they process information. As self-sustained objects, localized coherent patterns can signal information by propagating across the neural circuit. Computational operations occur when these emergent patterns interact, or collide with each other. The ongoing behaviors of these patterns naturally embody both distributed, parallel computation and cascaded logical operations. Such distributed computations enable the system to work in an inherently flexible and efficient way. Our work leads us to propose that propagating coherent activity patterns are the underlying primitives with which neural circuits carry out distributed dynamical computation.<br>
<br>
Introduction<br>
To understand brain function, it is essential to study the collective electrical activity of neural circuits [1]. This activity typically exhibits intriguing spatiotemporally organized patterns: they are commonly observed in multi-unit electrophysiological recording, EEG local field potential recording, MEG, optical imaging and fMRI imaging, both in spontaneous activity [2]?[5] and evoked responses [6]?[21]. In space, these patterns often take the form of localized patches or clusters of activity [2]?[16]. Recordings over large populations of neurons have shown that several of such localized patterns can occur simultaneously across cortical regions [2]?[16]. Over time, these patterns often do not remain at specific locations. As self-sustained entities, they propagate or move about in space [4]?[8], [10]?[16]. In doing so, they interact with each other, resulting in dynamical collective behavior. Here we will consider what kind of functional role this behavior may have.<br>
Propagating coherent patterns have been registered in the experimental literature as ?spreading? or ?drifting? activity [4]?[8] or as ?traveling waves? [13]?[24]. The simultaneous presence of several of these patterns has been observed in the spontaneous activity of cat visual cortex [4],[5; see 25 for a corresponding model study], in evoked response patterns in turtle olfactory bulb [14], and visual cortex of various species [9],[15], as well as in sensorimotor cortex of behaving mice [7]. When several localized, moving patterns occur together, they are likely to interact. Indeed, interactions have been shown to occur in rat somatosensory cortex [13]. To describe the collective activity in olfactory, visual, auditory and somatosensory cortices of behaving rabbits, the term ?interacting wave packets? was explicitly used [11],[12], which nicely captures the relevance of propagations and interactions of these patterns.<br>
Despite the ubiquity of these patterns and their interactions, their fundamental functional role has remained unknown. Although some authors have speculated on the role of propagating waves [26], the functional implications of other aspects such as the simultaneous presence of multiple propagating patterns or their interactions have remained completely unclear. Current theoretical frameworks describe neural activity either in computational or dynamical systems perspectives. Conventional computational theory is based on the manipulation and representation of static symbols [27]. This perspective contradicts the temporal variability of brain activity, which calls for a dynamical systems approach. When dynamical systems theories are applied to neuroscience, the prevailing concept is that of stable low-dimensional attractors [28]. This notion, although it has provided many important insights, is less suitable to capture the functional role of brain activity in its actual spatiotemporal complexity.<br>
We need to resolve the restrictions of conventional computation and standard dynamical systems theories, in order to describe neural activity and understand its fundamental function. This study is based on the consideration that neural circuits are spatially-extended, pattern-forming systems, containing large numbers of simple neurons with spatially restricted connectivity [29],[30],[31]. In spatially extended physical systems composed of large numbers of simple interacting elements, such as reaction-diffusion systems and fluidic systems, localized propagating coherent patterns are a common feature known under different names, including wave packets, spots, breathers and soliton waves, amongst others [32],[33],[34]. They are an emergent, collective property of these systems.<br>
Using these systems as analogy, we construct a simple, spatially extended neural circuit model to represent the gross architecture within the cerebral cortex. As an emergent, collective property of the system, the circuit exhibits dynamical activity patterns, reproducing some of the complexities observed in empirical studies. In particular, the circuit provides simultaneous propagation of multiple locally coherent patterns and their interactions. By revealing how their ongoing collective behavior can naturally embody computation, we demonstrate what fundamental function these patterns can serve.<br>
Propagating coherent spiking patterns can support several essential aspects of a computational processing. As self-sustained objects, these patterns can signal information by propagating across neural circuits. Information processing, or computation, occurs when they interact or, specifically, collide with each other. Collectively, these patterns perform distributed, parallel and cascaded computational operations, thereby enabling neural systems to work in an efficient and flexible way. We shall call this distributed dynamical computation, which is proposed as a framework for understanding spatiotemporal propagating activity patterns in neural circuits. This understanding links their dynamics with a form of non-conventional, abstract computation.<br>
<br>
Results<br>
<br>
Qualitative characterization of spatiotemporal patterns<br>
Significant correlations exist between neural activities recorded at different levels, from spikes and field potentials to fMRI [4],[35]. We focus this study on the most basic of these levels: neuron spiking behavior. Given that the myriad details of neuronal anatomy and its function are only partially known, rather than modeling neurons in great detail, we use a simple model (see Methods section) in an effort to capture some of the features of real neurons and neural circuits crucial for dynamical spatiotemporal pattern formation.<br>
Starting from random initial conditions and after initial transients, the neural circuit generates collective activity, in which localized pattern structures with temporal regularities are clearly in evidence (see Methods section for more details including coherent and incoherent spiking patterns). By varying the excitatory coupling strengths  and inhibitory coupling strengths  (See Methods section) within the range considered, we distinguish three types of patterns. The patterns are qualitatively characterized by the following phenomena, respectively: (1) localized incoherent spiking patterns occur, which slightly move around; the motions are constrained within local areas without any long-range movements; (2) several spatially localized coherent patterns move about; the movements are long-range across space and over time; there are many interactions or collisions between them, resulting in complex, dynamical collective behaviors; (3) several localized coherent patterns occur, showing regular motions without complicated interactions.<br>
Fig. 1 shows the instantaneous activity patterns of each type and Fig. 2 maps out where these three types of patterns occur in the parameter space. Among these patterns, those of Type 2 appear the most intriguing ones, showing the greatest space-time complexity in their overall behavior. Several coherent patterns originate at apparently random locations, and propagate in all possible directions. Each time a pattern sweeps through a given region, the direction varies. Sometimes patterns are spontaneously annihilated, but most of time they persist, traveling over long distances as self-sustained objects. These distances for the most part exceed the coupling ranges and can cover the whole space of the model circuit. The patterns interact when they meet; this includes non-destructive interactions, in which the moving patterns modulate each other's states, as well as destructive ones, in which one or both of the patterns are annihilated.<br>
Patterns of Type 2 qualitatively reflect many of the features observed in empirical studies, particularly the distributed properties of multiple activity patterns [4]?[7], [11]?[16], their propagations [4],[5],[11],[12],[14],[15], and their interactions [11],[12],[13]. In addition, the movements have a seemingly random feature. Several experimental studies, for instance in cortical local field potentials of rabbits, have pointed out that localized coherent structures called ?wave packets? originate from random locations and propagate in variable directions [11],[12]. Propagating coherent activity patterns termed ?traveling waves? have the likewise variability of moving speeds and directions in the collective activity of monkey and cat visual cortex and in that of rat hippocampus [22],[23].<br>
<br>
Quantitative properties of dynamical spatiotemporal patterns<br>
Since the collective behavior of Type 2 reflects some key features of activity patterns observed in real neural circuits, we shall mainly focus on these patterns and investigate their quantitative properties. As propagations are the most obvious dynamical feature of these patterns, a convenient quantitative measure is their velocities. As shown in Fig. 1B, collective activity at any moment is sustained by the clusters of neurons. We label each of them with a letter. We calculate the center-of-mass position  of jth cluster at time moment t, , , where  and  are the x and y position of ith neuron of the cluster, and  is the total number of neurons in the cluster. We define  as the size of the cluster. For Pattern a in Fig. 1B, its center-of-mass is positioned at (5.1, 74.2). Based on center-of-mass positions, a 2-dimensional velocity with components in x and y directions is: . The magnitude of velocity is speed : . We consider the velocity with  ms. Table 1 gives the velocities of the localized patterns shown in Fig. 1B. To get further quantitative characteristics of these patterns, we calculated the distributions of their sizes and their speeds, which are shown in Fig. 3. It is interesting to note that the distributions are qualitatively similar to that of traveling waves in rat hippocampus [24] and cat visual cortex [22]. In addition, to understand how these patterns change as the parameters change when the system is in the regime of Type 2 patterns, we have calculated the change of the mean values of the speeds and sizes as a function of system parameters. The results are shown in Fig. S1 and Fig. S2 in the supporting information.<br>
Another significant property of Type 2 patterns involves the complex dynamics of their collective propagating behavior, which can be quantified by mean-squared-displacement (MSD). Firstly, for each localized activity pattern, its traveling trajectory is obtained by feeding its center-of-mass positions into an algorithm developed for tracking them over time. Based on the trajectories of all moving patterns, the MSD as a function of time increment  is:(1)<br>
The bracket  represents averaging over time t and across trajectories. On a given trajectory,  and  are its positions at time moments t and .<br>
We calculated the MSD for Types 2 and 3 patterns, since they involve long-range propagations across the circuit. Fig. 4 (red dots) shows the log-log plot of the MSD as a function of  for Type 2 patterns. As the plot shows, the MSD function appears to follow a straight line, suggesting that it is a power function of the time increment. To verify this observation, we used the maximum-likelihood method [36], and obtained the result that the best fit is a power function,  with an exponent . With different system parameters, the exponent for the patterns of Type 2 is in the range . Type 3 patterns, shown in black dots in Fig. 4, have a MSD with exponent , which characterizes regular movements along straight lines. For comparison, we also show in blue dots a normal random diffusion process (Brownian motion), for which the scaling exponent is . A MSD as a power function of  and with an exponent larger than 1 and smaller than 2 indicates that the collective propagating behavior is neither a fully random motion nor a regular motion, instead it is in-between these two extremes. In fact, the collective behavior of Type 2 patterns is a kind of non-normal diffusion process, known as anomalous super-diffusion [37],[38].<br>
The fact that the behavior of Type 2 patterns belongs to the class of non-normal diffusion process is quite informative; it indicates that there are long-range spatiotemporal correlations for the propagating patterns [37],[38]. In neuroscience, nontrivial spatial and temporal correlations have been very well documented by analyzing brain activity from several different perspectives, including the temporal fluctuations of power of brain oscillations [39],[40], the distribution of size of neural avalanches [41], and the intervals of synchronized activity [42]. Our present study provides an alternative measure of nontrivial spatiotemporal correlations, regarding specifically the characteristics of propagating wave patterns that have been ubiquitously observed in brain activity.<br>
The above described quantitative measures reveal the main features of Type 2 patterns. These measures can therefore be used to obtain a parameterization scheme for the present model, in order to show the existence of the dynamical patterns in the parameter space, as was done in obtaining Fig. 2. Our explorations of the model with much larger numbers, such as , of neurons have suggested that Type 2 patterns are quite common. For instance, if , a model with  neurons shows Type 2 patterns when .<br>
<br>
Dynamical computation by propagating coherent activity patterns<br>
Having characterized the complexity of the propagating patterns, we are now ready to approach the question of their fundamental function. This question can now be specified: how do the dynamical patterns enable the system to do computation? To develop answers to this question, we use the methodology of examining how general-purpose computations can be embedded in autonomous dynamical processes without setting up specific computational tasks. Note that because of its conceptual simplicity and convenience, the methodology of revealing general-purpose computation based on the autonomous dynamics of a system has played an important role in developing a computational theory of the brain [43] and investigating the general computational capabilities of dynamical systems [44].<br>
For these purposes, let us consider that at any time there are several spatially localized coherent patterns. These can be labeled by letters as demonstrated in Fig. 1B. From among several moving patterns, we select two of them, Pattern a and Pattern b, without loss of generality, at an arbitrary time moment after an initial transient, . We then focus on the way they propagate and interact. The detailed space-time behavior of the two coherent patterns can be viewed in Video S1 in the supporting information, which we recapitulate in Fig. 5A. At time moment , Pattern a is centered at position P1 (57.8, 50.4), and Pattern b at position Q1 (75.1, 32.6). The two patterns propagate along their own paths, represented as black lines in Fig. 5A until around time moment , when Pattern a is at P2 (54.0, 41.1), and Pattern b is at Q2 (64.4, 31.6), where they collide with each other. After that, their states and therefore their momentums are changed compared to those before their collision. Here ?collision? is used to describe a co-current change in two structures' momentums due to their physical proximity, even though the objects do not actually touch each other. Because of the lateral inhibitory coupling, each moving pattern gets inhibitory effects from another one when they propagate to approach each other, hence resulting in the repulsive interactions as shown in Fig.5A. After the collision at time moment , there are coherent structures centered at positions P3 (47.3, 39.5) and Q3 (59.0, 25.9).<br>
In order to examine how abstract computational operations can be embedded in the ongoing interactions between propagating coherent activity patterns, a simple manipulation can be helpful. For instance, consider the situation before the collision, at a time moment such as t1?=?620503 ms or at any other time along the incoming path. If we eliminate all the spikes belonging to Pattern b, there will be no encounter at the rendezvous point Q2 (64.4, 31.6) at time moment 620507 ms; Pattern a will continue, unaffected by Pattern b, to move in the same direction. Thus it will follow the path indicated by the dashed black line in Fig. 5A to pass through P4 (51.6, 27.9) at time moment . Similarly, if all spikes in Pattern a are eliminated at t1, Pattern b will continue its original path along the dashed red line and pass through Q4 (54.8, 30.1) at . These results demonstrate that two moving spiking patterns effectively modulate each other when they meet at the right time and at the right place.<br>
To reveal how the collective behaviors of these patterns can support the essential aspects of a computational processing, which include signal (or ?information?) transmission and signal (or ?information?) processing [45], their dynamics is considered at a more abstract, computational level of analysis. Firstly, we perform the following abstraction: the presence of a localized coherent activity pattern at a particular position within the circuit signifies ?1?, whereas its absence signifies ?0?. Hence, based on the abstraction, a localized coherent pattern that is propagating can represent a bit of information (or ?signal?). It is in the spirit of McCulloch and Pitts' classical study to abstract from neuronal activities to binary values in order to develop a computation theory of the brain [43]. In [43], neural activities are excitatory and inhibitory synaptic inputs of threshold neurons along fixed lines, while here the relevant activities are the coherent patterns at the level of spiking neural circuits and are emergent properties unconstrained by fixed lines. Because the presence of a localized pattern is abstracted or interpreted to represent a bit of information ?1?, through its dynamical behavior, i.e., its propagation, the bit of information (or the signal ?1? ) can then be transferred along its propagating path from one part of the circuit to another. For an illustration, when the Pattern a propagates along its path from P1 (57.8, 50.4) to P2 (54.0, 41.1) shown in Fig. 5A, a bit of ?1? signal is transferred along this path from position P1 to P2, and clearly the speed of the information transfer is the propagating speed of the pattern. Thus, the propagating behavior of the coherent patterns is a primary mechanism for transferring information over long space-time distance. Interestingly, the functional role of the propagating behavior makes its trajectories analogous to real physical wires used to transfer electrical pulses in electrical circuits.<br>
Thus far, we have introduced the emergent localized coherent patterns with their role of representing signals, and elucidated that their propagation can support an essential function of a computational processing, which is signal transmission. We shall now turn to signal or information processing. The processing can be embedded in the dynamical interactions between these patterns: the patterns implement logical functions and the locations of their possible encounters act as logic gates. This principle is illustrated schematically in Fig. 5B, in which moving spiking patterns are represented by filled circles for an illustrative purpose.<br>
In Fig. 5B, signals A and B represent, respectively, the presence or absence at  of localized activity patterns at positions P1 and Q1 shown in Fig. 5A. The interaction logic gate at P2, Q2 can carry out the following operations: A AND B or, equivalently, B AND A, A AND NOT B, B AND NOT A. At time moment , , there will be a coherent pattern at position P3 if there was one at P1 and another at Q1. No other patterns are involved in these particular interactions. Hence, signal C at P3 is ?1? if and only if A is ?1? and B is ?1?. So we have: C?=?A AND B. Likewise, signal D at Q3 is ?1? when both B and A are ?1? (D?=?B AND A), realizing the same AND function. Similarly, there will be a pattern at P4 if and only if there was one at P1 and none at Q1. The signal E at position P4 is: E?=?A AND NOT B. Signal F at position is Q4: F?=?B AND NOT A, which implements the same AND NOT function. Owing to its AND and NOT capabilities, the interaction gate is a universal logic primitive. By abstractly depicting the localized coherent patterns as bits of information, signal processing or computation can be accomplished in terms of logical functions. Hence, the dynamics of the propagating patterns, that is, their collisions, can support another essential function of a computational processing, which is signal processing. The logical operations performed here are reminiscent of those done with collisions in the billiard ball model [45],[46],[47], which has played a very important role in linking basic physical laws with computation theory.<br>
Considering the number of neurons active within a cluster as its ?mass? and using its propagation velocity, we can calculate whether the collision preserves momentum. The velocity of Pattern a before collision is ?=?(?1.26, ?1.69), and that after collision is ?=?(?2, ?0.4). The size or the mass of Pattern a is 12, that is, . For Pattern b, its velocity before collision is ?=?(?2.45, ?0.9) and that after collision is ?=?(?1.35, ?2.15). The mass of this pattern is 13, . The x-component of total momentum before collision and after collision is  , respectively. Apparently , thus there is no conservation of momentum; the interaction is a kind of inelastic collision; and so it is not reversible. This is generally the case for interactions in our model. In this respect, our model differs from the billiard ball model [46],[47],[45], in which interactions are typically reversible.<br>
<br>
Cascaded computational operations<br>
The elementary logical operations demonstrated above can essentially be interpreted as computational building blocks in neural circuits. To show this, we need to demonstrate that they are cascadable, that is, the output of one logical operation is able to be used as an input to another one [48]. In other words, these operations must support compositionality. For the elementary computation as shown in Fig. 5B, both input and output signals are propagating activity patterns. Indeed, output of one operation can later be used as input for another one. During the ongoing evolution of the activity patterns, as shown in Fig. 6A and the corresponding Video S2 in the supporting information, at  an interaction happens when Pattern a is at P1 (14.3, 41.4), and Pattern b is at Q1 (32.0, 44.0). The outcome of this interaction is carried through space by propagating patterns, one of which at  is located at P2 (15.9, 24.2), where it interacts with another one, Pattern d at Q2 (31.7, 21.9), which comes from a different part of the circuit. Note that only those patterns relevant for illustrating the cascaded operation are shown in Fig. 6A. The example shows how populations of neurons collectively route signals through the circuit in a manner that naturally embeds cascaded operations.<br>
The cascaded logical operations are shown in Fig. 6B in an abstract form. Particularly, we can get the composed outputs, such as A AND B AND D. The occurrence of the computation represented by collision of the yellow and blue filled circles at Positions P2 (15.9, 24.2) and Q2 (31.7, 21.9) at time moment  is enabled by the computation that has previously occurred at time moment . During the cascaded operations, propagation is essential to make local information available at larger spatial scales and to assemble signals that are distributed over space and time.<br>
<br>
Distributed parallel computational processing<br>
At the computational level of analysis, the pattern dynamics supports two fundamental activities of a computational processing: signal transmission and signal processing. We now turn to the functional implications of multiple, propagating patterns that are distributed over the different parts of the circuit. Indeed, the simultaneous presence of these patterns and their ongoing behavior can provide the needed substrate for distributed parallel signal transferring and processing. Fig. 7A (Video S3 in the supporting information), shows that, Pattern g and Pattern h collide with each other at time moment t?=?862039 ms, when g is at (30.7, 62.7) and h is at (41.0, 55.1). Meanwhile in the other parts of the circuit the Pattern k and Pattern l collide, when k is at (41.6, 33.2) and l is at (45.2, 19.9). This example illustrates that several interactions can occur in parallel at the different parts of the circuit. The parallel interactions embody the parallel logical operations shown in Fig. 7B. They involve the Patterns g?h and k?l, which are physically distributed over the circuit. Furthermore, another localized pattern such as Pattern m, located at a different position, is moving along its own path. This one, as we have shown above, has the function to transfer a binary signal ?1? along its path. Thus, propagating patterns co-occurring can at any time either transfer signals or process signals, resulting in a computational processing carried out in a distributed parallel way.<br>
It is important to emphasize that the distributed computing scheme in the spatially-extended spiking neural circuit exhibits the typical features of the parallel distributed processing paradigm proposed for the brain: a set of large number of neurons, recurrent connections without central controllers, and patterns of activation distributed across neurons [49]. In our case, however, these patterns consist of spiking activity collectively propagating. Distributed computational processing is supported by the co-occurrence of several of these patterns; each of these patterns either propagates along, or collides with others. Based on this consideration, the number of co-occurring patterns, to some degree of approximation, can be used to estimate the system's distributed parallel processing capacity. Some factors, such as the range of coupling would delimit the maximal number of coherent structures operating simultaneously in the system. When the system is in the Type 2 regime, generally more patterns propagating simultaneously will result in more collisions. For our current model, we calculated the number of co-occurring localized coherent patterns, which is denoted as  at each time moment to characterize the complexity of the distributed computational processing. The result is shown in Fig. 8. Note that at any time moment considerable numbers of patterns are involved in carrying out the different aspects of a computational processing: signal transmission (propagations without collisions) or signal processing (collisions).<br>
<br>
The effects of external perturbations<br>
We have studied how general-purpose computation is implemented based on ongoing, autonomous dynamics of the propagating patterns. A question that naturally arises is: how does the system deal with external perturbations? To answer this question, firstly, we add external perturbations to one of ongoing propagating patterns. We then follow the evolution of both the perturbed and the unperturbed systems in order to capture the spreading of the perturbations. For instance, for Pattern a shown in Fig. 9A, in the original system, the pattern moves along the black line and it interacts with Pattern b when it reaches the position (34.4, 35.9). After external perturbations are added to Pattern a, as shown in Fig. 9A, the propagating trajectory of the pattern (red line in Fig. 9A) is slightly shifted in comparison with the original propagating path. This consequently results in the situation that Patterns a and b collide at a slightly different time moment at a slightly different position, compared to the original event. This shift leads to a different outgoing propagating trajectory (the trajectory after collision) for Pattern b. It is important to notice that before the collision, there are no changes to the propagating path of Pattern b; it is just the collision between the perturbed propagating pattern with an co-occurring one that results in the changes in the outgoing path of Pattern b. Instead of interrupting ongoing activity patterns, external perturbations modulate the propagating trajectories and interactions.<br>
To characterize the effects of external perturbations, we calculated the distance between the outgoing trajectories when there are external perturbations and the corresponding original outgoing ones. The distance is defined as: , where  is the center-of-mass position of a pattern of the original system and  is that of the corresponding pattern of the perturbed system. , is total time length considered for an outgoing trajectory after a collision. In the current study, T?=?8 ms. In order to obtain the statistics of the modulation, external perturbations are added to many different interacting pairs of propagating patterns before their collisions. Fig. 9B shows the distribution of the distance. We have also calculated the dynamics of the perturbed propagating trajectories and found that they also can be quantified as an anomalous super-diffusion process. This indicates that the statistical properties of the collective motions of the propagating patterns are preserved under external perturbations. Let us note that experimental studies have found that, in the visual cortex of freely viewing ferrets, stimulus-evoked activity reflects the modulation and triggering of intrinsic circuit dynamics by sensory signals, with a preservation of collective correlations of neural firing rates [50].<br>
<br>
<br>
<br>
Discussion<br>
The importance of spatiotemporal dynamical patterns in the brain has been proposed in [29], with an emphasis on spatial modes and their coupling. Here, we have focused on propagating coherent activity patterns, which are ubiquitous in the brain. These dynamical patterns are neither random nor stable; rather they are characterized by rich dynamical behaviors. We have used a simple, stereotypical spiking neural circuit to generate spatially localized propagating patterns. The patterns capture some of the key features of real pattern complexities: a distribution of multiple localized activity patterns, their propagations and their mutual interactions. To understand their fundamental functional role, we propose the notion of distributed dynamical computation. Localized propagating patterns are the underling primitives of dynamical computation; over time they transfer information across space and process information through their interactions. Collisions distributed over different locations and occurring at different time moments can be connected to each other by propagating patterns. This mechanism enables elementary computations to occur in a cascaded fashion, resulting in more complex computations. In addition, several interactions distributed across different locations can occur simultaneously, resulting in parallel processing.<br>
Dynamical computation emerges on the basis of activity in neural circuits; they enable and sustain propagating localized patterns and their interactions. In this framework, the propagation and processing of signals are fluid; signals do not rely on the fixed physical lines of neural circuits to guide their propagation trajectories. The computations are not confined to specific anatomical sites; rather they occur wherever moving patterns collide with each other. With respect to real-brain architecture, this is clearly a simplification. We may consider the neural architecture as biasing the trajectories of propagating patterns to various extents. Nevertheless, a certain independence of fixed connectivity structure must be at the basis of how flexibility in brain functions is achieved.<br>
Propagating coherent activity patterns implement logical operations in a manner reminiscent of the collisions in the classical billiard ball model [45],[46],[47]. In this model, however, collisions are elastic and reversible, whereas in our model they are inelastic and therefore irreversible. This allows the exchange of information between the interacting patterns. The corresponding computations are equally irreversible and therefore context-dependent. An additional essential difference with the billiard ball model is that computation in our model is naturally embedded in the ongoing behavior of a circuit.<br>
Computation based on the propagations and interactions of coherent spiking patterns in neural systems is definitely a non-conventional form of computation. Conventional computation requires information to be represented and manipulated in the form of static symbols [27]. As the longstanding debate between computationalists and dynamicists [51] has pointed out, static symbols are less suitable to describe the temporal variability in the way the brain executes its functions and how it achieves flexibility. Dynamical computation can capture the spatiotemporal characteristics of brain activity patterns and provide them with an underlying computational interpretation. By synthesizing dynamics and computation, the present approach offers a starting point for a comprehensive understanding of the working mechanisms of the brain.<br>
The collective propagation of activity patterns through a substrate of neurons can be portrayed as spatiotemporal spike chains. Our current emphasis on propagating patterns bears a similarity to the paradigm of synfire chains [52],[53], in which sequential spike chains play a central role. They are obtained by setting up feed-forward networks, designed to support wave-like spikes propagation through them. These networks perform information processing by synchronizing different spike chains [52],[53]. In our model, spatiotemporal spike chains are an emergent property of recurrent networks [54]. Rather than synchrony, their nonlinear pattern-forming capacities and transient interactions are the essential mechanisms for dynamical computation.<br>
In the current study we have mainly focused on general-purpose computation based on ongoing, autonomous dynamics of neural circuits. We have also found that external perturbations can modulate the ongoing patterns, which include their propagations and interactions. Hence, propagating activity patterns could enable neural systems carry out some specific computations when actual sensory inputs are given. Indeed, propagating coherent patterns such as propagating waves have been found in evoked activity [7],[9],[11],[12],[14],[55]. Furthermore, during whole computing processes based on propagating coherent patterns, internal synaptic modifications and external feedbacks from other parts of the brain can be used to shape or control dynamical wave pattern to generate specific propagating patterns as required outputs or behavior sequences. The effect from feedback activity is analogous to use feedback signals to control waves patterns in spatially-extended non-equilibrium physical systems [56].<br>
Instead of focusing on multiple, stationary patch patterns [57],[58] or single propagating wave pattern as in the most studies about neural fields [58],[59], the current model generates dynamical spiking activity patterns that can capture some of the complexities of empirically observed patterns. Therefore, the current study provides specific, experimentally testable predictions. In particular, the collective behavior of interacting, propagating coherent patterns belongs to the class of anomalous super-diffusion. As a process with underlying long-range coherence, collective anomalous super-diffusion is an important indicator of complicated, nontrivial interactions between propagating patterns. Its presence can be tested experimentally in a straightforward way. First qualitative indications that this process may occur in real neural circuits are the seeming randomness of the points of origin of neural activity patterns and the variability of their propagating directions [11],[22],[23]. More conclusive evidence can be obtained through calculating the MSD of the collective motions in the same way as for the current model data.<br>
In the current dynamical computational framework, propagating coherent activity patterns are the fundamental primitives for signaling information and for processing information through their interactions. Indeed, at the level of neural circuits, signaling information by propagating coherent patterns has been clearly and very well documented as an important component of the function of the cortex [18]?[21]. Interactions between multiple active patterns, however, have merely been registered in experimental studies without considering their importance [11],[12],[13]. Our current work shows in an abstract, principled way how these interactions could play a key role in dynamical computation. For instance, in the visual cortex of ferrets, top-down influences have been found to be evident in terms of localized wave patterns [17], which could have collisions with wave patterns evoked by external visual inputs; such collisions might reflect ?attention guided? processing of visual stimuli. It is, therefore, of crucial importance to study interactions between different propagating wave patterns experimentally and sow how they relate to the functions of the cortex.<br>
<br>
Methods<br>
<br>
A spiking neural circuit model<br>
Our model represents biological neurons by integrate-and-fire spiking neurons that are uniformly distributed across a two-dimensional grid. The free dynamics of each neuron is:(2)where t is time, V is the membrane potential of the neuron,  is the time scale of membrane potential change, , and  is constant external stimulation. Each neuron thus is an intrinsic oscillator. When the membrane potential reaches the threshold value  the neuron releases a spike, after which its membrane potential is reset and the neuron remains quiet for a refractory period . Each neuron receives excitatory and inhibitory inputs from other neurons. We include a delay time  into the interactions between neurons. For simplicity, we chose ; our results do not depend sensitively on these values. Considering the values of delay time and refractory time, we let the model evolve in time steps of . Eq. (2) can be integrated in  to obtain the membrane potential for a single neuron:(3)Let , so Eq. (3) can be written as: . The whole spiking neural circuit is:(4a)(4b)where  are indices of neurons, . N is the total number of neurons,   is the Heaviside step function:  for , and  for  In the simple model, not all the details of real spiking behavior can be reproduced. To get spikes, the threshold   is used. The reason for using a threshold value is that for real neurons, after their membrane potentials reach certain threshold values, neuronal electrical activities are manifest as short electrical pulses (spikes) [60].  and  is the corresponding excitatory and inhibitory coupling strength from the th and th neurons to the ith neuron. A similar model has been used to study the statistical properties of interspike intervals [61]. In our study, a ?Mexican-hat? coupling scheme is used:(5)where  is the Euclidean distance between two neurons on a two-dimensional grid where the neurons occupy integer coordinates, and . Connections between neurons are confined to , and periodic boundary conditions are used in the study. Some experimental studies have suggested that inhibitory connections are more spatially restricted than excitatory ones [62], and others have found that the opposite is true [63]. Based on these coupling parameters, in our model the range of excitatory connections is spatially more restricted than inhibitory ones.<br>
For any ith neuron, from Eq. (5) we can obtain that when , , the input is excitatory and the neurons within this distance range are denoted as  in Eq. (4a). The total excitatory synaptic input to the ith neuron is ; Otherwise when , , the input is inhibitory, and corresponding neurons in this range are denoted as  in Eq. (4a). The total inhibitory synaptic inputs to the ith neuron is , . The excitatory coupling strength  as used in Eq. (4) is ; the inhibitory coupling strength is , . We used . The parameter  was varied within the range , and  was varied within the range  to obtain different spatiotemporal activity patterns. The network has been simulated with random initial conditions. Some initial time steps have been discarded until there is no significant change in the variability of interspike intervals of individual neurons.<br>
<br>
Coherent and incoherent patterns<br>
The patterns consist of the collective spiking activity of populations of neurons with clear spatial structures, i.e., spatially localized structures in our case. To quantify the spatial localization property, we used in our study the following criteria: (1) for a cluster of neurons that are firing, the distance between any two neurons within the cluster is smaller than , ; (2) the distance between the center-of-mass position of this cluster of neurons and that of any others is larger than , . These criteria allow us to detect clustered patchy activity patterns that are spatially localized in terms of their spatial separation with others. Furthermore, we can discriminate coherent patterns and incoherent patterns, according to the statistics of their internal geometrical organizations at the level of individual spikes. The statistics can be obtained as follows. For a neuron within a given localized spiking pattern, we firstly draw a line to link its coordinate point where the neuron is located and the center-of-mass position point of the whole group, and then calculate the angle of the line related to the x-axis of the two-dimensional grid. We can get the angles for all neurons within the pattern. Then the angles are sorted into an ascending numerical order. For example, a pattern that has j neurons in total, after sorting, the series of the sorted angles is , and the series of corresponding neurons is : . Then we calculate the distances between two successive neurons in this series, for instance the distance between the  and the  neuron is . Finally, we can obtain the standard derivation of these distances, which is . From the procedure used to get the standard derivation, it is apparent that this is a measure of the variability regarding how individual spikes are geometrically organized around the corresponding center-of-mass position; a smaller value means that the spikes are spatially more organized around its center-of-mass position. In our study we used the criterion that, a pattern is coherent when its , otherwise it is incoherent.<br>
<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

</body></html>
