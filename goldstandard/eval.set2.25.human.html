<html><body link="black" alink="black" vlink="black" hlink="black">
<b>PMC373441</b><br>
<software>GOTree Machine</software> (<software>GOTM</software>): a web-based platform for interpreting sets of interesting genes using Gene Ontology hierarchies<br>
<br>
<br>
Background<br>
Microarray and proteome technologies are producing sets of genes and proteins that are differentially regulated under varying conditions. Other studies such as quantitative trait analysis, large-scale mutagenesis studies, and other large-scale genetic studies are also producing sets of interesting genes. The number of genes in the gene sets may be large. The functional data that can be associated with each gene is quite complex. However, the in-depth knowledge of gene function possessed by individual biologists is limited to relatively narrow research fields. Searching for patterns and evaluating the functional significance of those patterns from large groups of genes constitutes a big challenge for biologists. Most resources that are available for retrieving functional information are displayed in a one-gene-at-a-time format. Bioinformatics tools are needed for assisting the functional profiling of large sets of genes.<br>
Gene nomenclature has been used frequently to describe gene products [1]. While the goal for gene nomenclature is to create a unique designation for gene names, gene name is often not unique even within a species. Trying to attach significant biological information to the name can be problematic. In fact, many revisions in nomenclature have occurred as the knowledge of the function of the gene product has developed [2]. The information about gene function is primarily contained in the articles indexed in the <database>Medline</database> database. In this form, it is readable by scientists but not easily interpreted by computers on a large scale. Tools based on literature profiling have been developed by a few groups to assist biologists in the interpretation of sets of interesting genes [3-5]. However, these methods depend on the identification of gene-reference relationships and have problems such as ambiguous gene names and symbols, context of categories etc. [3].<br>
The use of ontological methods to structure biological knowledge is an active area of research and development [2]. Ontologies provide a mechanism for capturing a community's view of a domain in a shareable form. One of the most important ontologies in molecular biology is the <database>Gene Ontology</database> (<database>GO</database>) [2,6]. <database>GO</database> is beginning to produce a structured, precisely defined, common, controlled vocabulary for describing the roles of genes and gene products in different species. It comprises three major categories that describe the attributes of biological process, molecular function and cellular component for a gene product. As of August 2003, <database>GO</database> contains about 14000 phrases, representing categories of concepts held within a Directed Acyclic Graph (DAG). Categories can have multiple parents and multiple children along a branch. As they form a standard vocabulary across many biological resources, this shared understanding provides a valuable, computationally accessible form of the community's knowledge about these attributes. Several programs have been developed for profiling gene expression based on <database>GO</database>, and demonstrated to be very useful in translating sets of differentially regulated genes into functional profiles [7-12]. <software>GoMiner</software>[10], <software>MAPPFinder</software>[11] and <software>GoSurfer</software>[12] are standalone software packages while <software>FatiGO</software>[7] and <software>Onto-Express</software>[8,9] are web-based software. Web-based service provides experimental biologists easy access to tools by avoiding problems in installing software locally. However, the two web-based software packages did not visualize the data with the <database>GO</database> hierarchical structure ? the fundamental defining feature of <database>GO</database>. The current implementation (as of August, 2003) of <software>FatiGO</software> is restrictive in that the user must specify ahead of time one particular level of the <database>GO</database> hierarchy that is to be used for analysis of the data. Although <software>Onto-Express</software> allows multilevel analysis, it visualizes the classification in flat view tables and the significantly enriched <database>GO</database> categories are presented as bar charts [8,9].<br>
As the <database>GO</database> categories are held within a DAG and have a natural hierarchical structure, we believe that the tree structure is more intuitive and representative. To create a web-based and tree-based data mining environment for gene sets, we have developed <software>GOTree Machine</software> (<software>GOTM</software>).<br>
<br>
Implementation<br>
Schematic overview of <software>GOTM</software><br>
<software>GOTM</software> is implemented in PHP. It is accessible through IE5.0 or higher and Netscape 7 or higher from multiple platforms. <software>GOTM</software> can be accessed from the website . Figure 1 shows the schematic overview of <software>GOTM</software>. After reading the input parameters and data files from the user, <software>GOTM</software> interacts with the local database <database>GeneKeyDB</database> (S.K. et al., manuscript in preparation) to convert gene symbols, Affymetrix probe set IDs, <database>Unigene</database> IDs, <database>Swiss-Prot</database> IDs or <database>Ensembl</database> IDs to <database>LocusIDs</database>. The hierarchical GOTree structure is then generated using the PHP Layers Menu System [13] and sent back to the user. It is based on the <database>GO</database> annotation for <database>LocusIDs</database> as recorded in GeneKeyDB. The user can browse or query the <software>GOTree</software> for desired <database>GO</database> categories. The GOTree can be exported and stored locally in html format. Bar charts for <database>GO</database> categories at different annotation levels can be generated for publication. The bar chart is created using <software>ChartDirector</software> [14]. Statistical analysis compares the interesting gene set and the reference gene set and provides the user with <database>GO</database> categories with enriched gene numbers. The enriched <database>GO</database> categories are presented in flat view format, sub-tree view format and DAG view format. The DAG is created using <software>Graphviz</software> [15]. Subsets of genes in each <database>GO</database> category can be displayed and additional information for each gene can be further retrieved from <database>GeneKeyDB</database>.<br>
<br>
Database: <database>GeneKeyDB</database><br>
The <software>ORACLE</software> relational database <database>GeneKeyDB</database> was initially built from the NCBI <database>LocusLink</database> database [16]. It has adopted a strong gene-centric viewpoint rather than a sequence entry-centric view. Gene information was further taken from <database>Ensembl</database>, <database>Swiss-Prot</database>, <database>HomoloGene</database>, <database>Unigene</database>, Gene Ontology Consortium and Affymetrix etc. and was integrated into <database>GeneKeyDB</database>. The <database>GO</database> annotation for genes is based on the <database>LocusLink</database> data. However, the <database>GO</database> annotation for genes in the <database>LocusLink</database> data only provides the most detailed information available. Genes are annotated to the most granular <database>GO</database> category(s) possible. For example, the <database>GO</database> biological process annotation for the mouse Birc4 (LocusID 11798) gene is "apoptosis", so Birc4 is directly related to "apoptosis". However, because of the hierarchical relationship between the parent and the child, "apoptosis" is a "programmed cell death"; "program cell death" is, in turn, a "cell death", and so on. This continues until we reach the most general annotation "biological process". Thus, Birc4 is also indirectly related to "programmed cell death", "cell death" etc. If we are interested in all genes involved in "programmed cell death", by using only the annotation provided by the <database>LocusLink</database> data, we will miss the Birc4 gene. Moreover, if we want to find <database>GO</database> categories with enriched gene numbers, failing to implement the parent-child relationship will miss known information. In order to map the granular annotations such as "apoptosis" to general categories like "cell death", <database>GO</database> files for the 3 main categories were downloaded from the current ontologies section from the <database>Gene Ontology</database> consortium website [17] as flat text files and parsed by a Perl script. The relationships between genes and all their directly or indirectly related <database>GO</database> categories are created and stored in tables in <database>GeneKeyDB</database>.<br>
<database>GeneKeyDB</database> is updated periodically. It comprises several independent sub-modules, such as <database>LocusLink</database> and <software>GOTree Machine</software>. Each of the modules is updated independently during the updating. The process is automated by pre-prepared scripts. More detailed information on <database>GeneKeyDB</database> will be presented in a separate paper (S.K. et al., manuscript in preparation).<br>
<br>
Statistical analysis<br>
Identifying <database>GO</database> categories with significantly enriched gene numbers in the interesting gene set compared to a reference gene set will allow the user to focus on biological areas that are most important for the interesting gene set. In order to identify <database>GO</database> categories with significantly enriched gene numbers, we need to compare the distribution of genes in the interesting gene set in each <database>GO</database> category to those in the reference gene set. A reference gene set could be all genes in a genome or another appropriate reference gene set (e.g. the list of genes on the array). We need to mention that an inappropriate reference gene set will lead to possibly false positives and negatives. Unless the user can find the right reference gene set from our stored data, uploading an appropriate reference gene set for the analysis is always suggested. Suppose n genes were identified as interesting genes based on a microarray experiment (such as responsive, up-regulated or down-regulated genes) using an array with N genes. For a given <database>GO</database> category X, a gene is either in the category or not in the category. Suppose further that K out of the N reference genes and k out of the n interesting genes are in category X. If the n interesting genes were effectively a random sample uniformly selected from the reference gene set, the expected value of k would be ke = (n/N)K. If, on the other hand, k exceeds the above expected value, category X is said to be enriched, with a ratio of enrichment (R) given by R = k/ke. Statistical tests that have been used for the assessment of enrichment by related published software include Fisher's exact test, ?2 test, T test and binomial test [8-12]. As genes can be selected only once, this is sampling without replacement and can be appropriately modelled by the hypergeometric distribution [8]. <software>GOTM</software> reports only those enrichments that are statistically significant as determined by the hypergeometric test. The significance of enrichment (P) for a given category is determined by . <database>GO</database> is organized on the basis of the three relatively independent categories: biological process, molecular function and cellular component. The Ns used for each category: biological process, molecular function and cellular component, represent the number of genes having <database>GO</database> annotation in that category.<br>
<br>
<br>
Results and Discussion<br>
Input<br>
Figure 2 shows the input user interface of <software>GOTM</software>. The input identifiers for <software>GOTM</software> can be <database>LocusIDs</database>, Gene Symbols, Affymetrix probe set IDs, <database>Unigene</database> IDs, <database>Swiss-Prot</database> IDs or <database>Ensembl</database> IDs. <software>GOTM</software> currently supports Gene Symbols from human, mouse, rat and fly, and Affymetrix probe set IDs from 8 human arrays and 6 mouse arrays. The user can choose either single gene set analysis or interesting gene set vs. reference gene set analysis. For single gene set analysis, only the file of the interesting gene set is needed, and the result will be a GOTree for the gene set. For interesting gene set vs. reference gene set analysis, the user needs to upload the file of the interesting gene set, and choose an existing reference gene set from our pre-stored gene sets, including all genes in the mouse genome, all genes in the human genome and gene sets from 14 Affymetrix arrays, or upload the file of the reference gene set. The result will be a GOTree for the interesting gene set, and identified <database>GO</database> categories with relatively enriched gene numbers in the interesting gene set compared to the reference gene set. The user can browse his local machine for the input files. The input file should be a plain text file, including the appropriate ID (required) and corresponding microarray ratio (optional), separated by tabs in the format of one ID per row. A unique analysis name is assigned and can be used to retrieve the results for a subsequent user session. Stored results can be accessed through the RETRIEVE TREE button and deleted through the DELETE TREE button at the top of input user interface. The results will be stored until the next periodical upgrading of GeneKeyDB. An email notice will be sent to the users after the updating.<br>
<br>
Output<br>
Figure 3 shows the output user interface of <software>GOTM</software>. The output view is divided into 3 windows. The upper-left window is the GOTree window, the upper-right window is the gene/category list window and the bottom window is the gene information window. The expandable GOTree will be shown in the GOTree window. The user can browse the tree by clicking the "+" symbol. For single gene set analysis, the number of genes in each <database>GO</database> category will be given. If the interesting gene set vs. reference gene set analysis is selected, three parameters will be given for each <database>GO</database> category: O (observed gene number in the category), E (expected gene number in the category), R (ratio of enrichment for the category). For those <database>GO</database> categories with R &gt; 1, the fourth parameter P indicating significance of enrichment will be given. <database>GO</database> categories with significantly enriched gene numbers (P &lt; 0.01) will be colored red. By clicking on individual <database>GO</database> categories, the genes in the category will be shown in the gene/category list window. It might be sometimes difficult for a user to browse and find the <database>GO</database> category in which the user is interested. In this case, the user can do an exact search for a <database>GO</database> category using "GO Term Search" or a fuzzy key word search using "Keyword Search" at the top of the GOTree window. The returned <database>GO</database> categories and genes inside each category will be shown in the gene/category list window. The number of <database>GO</database> categories with enriched gene numbers will also be shown in the GOTree window. By clicking on the number, the names of enriched <database>GO</database> categories will be shown in the gene/category list window. GOTree provides comprehensive classification of the genes in a hierarchical structure, however, due to the complex structure, it's not easily publishable. After browsing the GOTree, the user may pick appropriate annotation levels and get corresponding bar charts for publication using the Bar Chart button (for an example, see ). GOTree can also be exported and locally stored in html format using the Export GOTree button. Enriched <database>GO</database> categories are colored red, and genes in each category are also included in the exported GOTree (for an example, see ).<br>
The gene/category list window shows the genes in a selected <database>GO</database> category, and enriched <database>GO</database> categories in the three main <database>GO</database> categories, biological process, molecular function and cellular component respectively. Each gene is represented by a LocusID, followed by the input ID. In addition, the ratio in the microarray experiment is shown if that information was included in the input file. Up-regulated genes are colored red while down-regulated genes are colored green. A flat view of enriched <database>GO</database> categories doesn't reveal the relationship among the <database>GO</database> categories. When tens or hundreds of <database>GO</database> categories are identified as significantly enriched, it becomes difficult for users to interpret the results. In this case, the user can press the TREE VIEW button to get a sub-tree (Figure 4) or press the DAG VIEW button to get a DAG (Figure 5) for the enriched <database>GO</database> categories in a new window. The <database>GO</database> categories in red in the sub-tree or the DAG are the enriched <database>GO</database> categories while the black ones are their non-enriched parents. The sub-tree and the DAG assemble related enriched <database>GO</database> categories together indicating important biological areas that are worth further study. By clicking on individual <database>LocusIDs</database> in the gene/category list window, related information for the genes will be queried from <database>GeneKeyDB</database> and shown in the gene information window.<br>
The gene information window shows the gene information record for the selected gene, which includes LocusLink ID, organism, gene symbol, gene name, map location, homolog, <database>GO</database> terms, function summary, GRIF (Gene Reference Into Function), phenotype, <database>PubMed</database> record, <database>OMIM</database> (<database>Online Mendelian Inheritance in Man</database>) record, <database>KEGG</database> (<database>Kyoto Encyclopaedia of Genes and Genomes</database>) Map etc. A link is given to external databases such as <database>PubMed</database>, <database>OMIM</database>, <database>KEGG</database> etc when available.<br>
<br>
Application<br>
High-throughput gene expression profiling has become an important tool for investigating transcriptional activity in a variety of biological samples. Data from the published, large-scale expression analysis of Su et al is used here to illustrate the use of this tool [18]. They profiled gene expression from 91 human and mouse samples across a diverse array of tissues, organs, and cell lines and showed a preliminary description of the normal mammalian transcriptome. 311 human and 155 mouse tissue-restricted genes with known function were identified by examining gene expression across a panel of tissues. These genes were hypothesized to perform specific cellular and physiological functions in each tissue. Among the 85 human genes restricted to the testis, the authors only mentioned three genes which were known to be involved in testis function (SOX5, TEKT2 and ZPBP). It would be interesting to show the functional profiles and identify the important functional categories from the tissue restricted gene sets. To do this analysis, 85 human genes restricted to the testis and the 58 human genes restricted to the liver were downloaded from the supporting information on the PNAS web site for the paper. As the HG_U95A array was used for the experiment, all the genes on the HG_U95A array were used as the reference gene set. <software>GOTM</software> was used to identify <database>GO</database> categories with significantly enriched gene numbers (P &lt; 0.01) in the testis gene set and the liver gene set. This analysis was carried out in August 2003 based on GeneKeyDB version GKDB200307.1. <database>LocusLink</database> data used for this version was downloaded on July 18, 2003 from NCBI. The versions for the <database>GO</database> files were 2.378, 2.747 and 2.857 for biological process, molecular function and cellular component respectively. GOTrees were generated for the two gene sets. <database>GO</database> annotation was found for 58, 53 and 50 genes respectively in the biological process, molecular function and cellular component categories for the testis gene set (see text output at ), while 47, 48 and 39 respectively for the liver gene set . 59 and 79 enriched <database>GO</database> categories were identified in the testis and the liver gene set respectively. Examples can be seen from . These examples include bar charts under biological process (at the 4th level from the root), sub-trees, and DAGs of the enriched <database>GO</database> categories.<br>
For the testis gene set, the statistics suggested 36 enriched <database>GO</database> categories in the biological process part of <database>GO</database>. As shown in the DAG  and the sub-tree , these <database>GO</database> categories comprise mainly four groups. The largest group of enriched <database>GO</database> categories includes those related to cell proliferation, cell cycle, mitosis and meiosis. The gametogenic function of the testis is to produce the male gametes or spermatozoa. Formation of the male gamete occurs in sequential mitotic, meiotic and postmeiotic phases. As reviewed by Eddy et al, many germ cell-specific transcripts are produced during this process [19]. The second group contains <database>GO</database> categories that are related to testis specific development, such as sex differentiation and reproduction. The third group of <database>GO</database> categories are those related to protein phosphorylation. Spermatozoa undergo a series of changes before and during egg binding to acquire the ability to fuse with the oocyte. These priming events are regulated by the activation of compartmentalized intracellular signalling pathways, which control the phosphorylation status of sperm proteins. Increased protein tyrosine phosphorylation is associated with capacitation, hyperactivated motility, zona pellucida binding, acrosome reaction and sperm-oocyte binding and fusion [20]. The fourth group consists of <database>GO</database> categories related to glycerolipid metabolism. Some glycerolipids were reported to be responsible for the unique fusogenic potential of sperm plasma membrane domains [21,22].<br>
For the liver gene set, there are 35 enriched <database>GO</database> categories in the biological process part of <database>GO</database>. As shown in the DAG  and the sub-tree , there are mainly two groups of enriched <database>GO</database> categories. One group includes those related to different kinds of metabolism, which is consistent with the key role of liver in the metabolism. The other group includes those related to response to external stimuli and stress, which may be consistent with the roles that liver cells play in response to a variety of physiological states (e.g. production of acute phase proteins [23]). The <database>GO</database> categories, homeostasis and blood coagulation, are also enriched, which may be consistent with the ability of liver to synthesize various protein molecules that are responsible for clotting of blood.<br>
These two examples demonstrate that besides organizing interesting gene sets using <database>GO</database> hierarchies, based on statistical analysis, <software>GOTM</software> can help transfer these expression profiles into functional profiles. This transformation may be useful in helping biologists interpret high-throughput data. <software>GOTM</software> was applied to interpret tissue restricted gene sets identified by microarray experiment in this paper. In fact, it can be applied to any other interesting gene sets.<br>
<br>
Related software comparison<br>
Several <database>GO</database> based functional profiling software packages have been published recently. A complete list of <database>GO</database> Tools can be found at . Zeeberg et al did an extensive comparison of some of these software packages [10]. Table 1 compares <software>GOTM</software> to related software for the main features. It is based on the information available from individual websites as of August, 2003 when the comparison was done. <software>GoMiner</software> and <software>GoSurfer</software> are standalone software packages while <software>FatiGO</software>, <software>Onto-Express</software> and <software>GOTM</software> are web-based software. Different kinds of IDs have been used as input identifiers. The <database>LocusLink Database</database> is one of the most comprehensive resources for gene related information. Using LocusID as the primary identifier enables <software>GOTM</software> to access the abundant gene information resources in <database>LocusLink</database> database. <software>GOSurfer</software> is the only one among the others that includes LocusID as an input identifier. Gene symbol, <database>Unigene</database> ID, <database>Swiss-Prot</database> ID, <database>Ensembl</database> ID, and Affymetrix probe set IDs can also be used in <software>GOTM</software> owing to their broad adoption by end-users. <software>FatiGO</software> is also very flexible in the input identifiers. <software>FatiGo</software>, however, requires the user to specify ahead of time one particular level of the <database>GO</database> hierarchy that is to be used for analysis of the data. Although <software>Onto-Express</software> allows multilevel analysis, the classification information is presented in bar charts and flat view tables. Both of these web-based software packages do not, in our opinion, visualize well the fundamental hierarchical nature of <database>GO</database>. <database>GO</database> was originally organized in DAG, thus <software>GoMiner</software>'s use of a DAG as the visual output format seems appropriate; however, visualization becomes difficult when the gene set is significantly large. The same visualization problem exists for the fixed tree as used in <software>GOSurfer</software>. The expandable tree in <software>GOTM</software> and <software>GOMiner</software> is very similar to the widely used <database>GO</database> browser, <software>AmiGO</software> [24], and is suitable for the visualization of the GOTree structure. All of the software packages provide statistical analysis for identifying important <database>GO</database> categories. <software>GOTM</software> uses the hypergeometric test for assessing significance of enrichment. Since repeated tests are conducted to determine the significantly enriched <database>GO</database> categories, a correction for multiple tests is necessary. <software>FatiGO</software> and the commercial version of <software>Onto-Express</software> have implemented the correction. However, as stated on the webpage of <software>FatiGO</software>, the cost for the correction is the slow speed. This slowness is not desirable for a web based service. Correction for multiple tests is not implemented in <software>GOTM</software>. As a result, the P values can be considered as a relative measure for indicating possible statistical significance. It is not very difficult for an experienced biologist to identify truly interesting areas from the enriched <database>GO</database> categories given by <software>GOTM</software>. Moreover, in <software>GOTM</software>, the unique visualization of the enriched <database>GO</database> categories as sub-trees or DAGs (Figure 4, 5) brings functionally related GO categories together, which can guide users to find interesting biological areas. Although there are usually tens of enriched <database>GO</database> categories, the sub-tree or DAG of enriched <database>GO</database> categories actually focuses on several biological areas. In contrast, tables and bar charts of enriched <database>GO</database> categories in <software>FatiGO</software> and <software>Onto-Express</software> can't reveal such information. <software>GOSurfer</software> and <software>GoMiner</software> highlight the enriched <database>GO</database> categories in the whole GOTree or DAG. Owing to the complex structure of the <database>GO</database> hierarchy, they may not be as intuitive as the visualization of sub-tree or DAG of enriched <database>GO</database> categories in <software>GOTM</software>.<br>
<br>
<br>
Conclusions<br>
As a web-based platform for interpreting sets of interesting genes using <database>GO</database> hierarchies, <software>GOTM</software> provides user friendly data visualization and statistical analysis for comparing gene sets. <software>GOTM</software> complements and extends the functionality of similar data mining tools. Statistical analysis helps users to identify the most important <database>GO</database> categories for the gene sets of interest and suggests biological areas that warrant further study. <software>GOTM</software> should have a broad application in functional genomic, proteomic and large scale genetic studies from which high-throughput data are continuously generated. The application of <software>GOTM</software> is limited by the number of genes that have <database>GO</database> annotation. However, with the bioinformatics effort in automatic prediction of protein functions based on literature, gene expression data and protein sequence information [25-29], rapid growth in <database>GO</database> is expected, and <software>GOTM</software> will become more useful with the improvement of <database>GO</database>.<br>
<br>
Availability and requirements<br>
Project Name: <software>GOTM</software> (<software>GOTree Machine</software>)<br>
Project Homepage: <br>
Operating System: Platform independent<br>
Programming Language: PHP<br>
Other Requirements: IE5.0 or higher, or Netscape 7 or higher<br>
License: GNU GPL<br>
Any Restrictions to use by non-academics: License needed<br>
<br>
List of abbreviations<br>
<database>GO</database>, <database>Gene Ontology</database>; <software>GOTM</software>, <software>GOTree Machine</software>; DAG, Directed Acyclic Graph; GRIF, Gene Reference Into Function; <database>OMIM</database>, <database>Online Mendelian Inheritance in Man</database>; <database>KEGG</database>, <database>Kyoto Encyclopaedia of Genes and Genomes</database><br>
<br>
Authors' contributions<br>
BZ devised the algorithm, wrote program code, formed the website and drafted the manuscript. DS, SK and BZ developed the <database>GeneKeyDB</database> database. JS guided and coordinated execution of the project. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1188055</b><br>
A computational approach for identifying pathogenicity islands in prokaryotic genomes<br>
<br>
<br>
Background<br>
PAIs are distinct genetic elements of pathogens encoding various virulence factors such as protein secretion systems, host invasion factors, iron uptake systems, and toxins [1,2]. PAIs are a subset of genomic islands which have been transferred by horizontal gene transfer (HGT) event and confer virulence upon the recipient. PAIs can be identified by features such as the presence of virulence genes, biased G+C content and codon usage, carriage of mobile sequence elements, and/or association with tRNA genes or repeated sequences at their boundaries [3].<br>
Identification of PAIs is essential in understanding the development of disease and the evolution of bacterial pathogenesis [2]. As complete genome sequences rapidly accumulate, various in silico methods have been developed to detect HGT [4-7]. Most of the methods were based on the detection of genomic regions having atypical G+C content, patterns of codon usage bias, or dinucleotide anomaly. However, compositional approaches may generate many false positives due to other factors such as selection and mutation bias [8,9], and a lot of false negatives owing to adjustment of the transferred sequence in its composition by amelioration [10]. In fact, these methods detect different sets of ORFs as foreign origin when applied to the genome of Escherichia coli K-12 [11]. Thus, combining multiple lines of evidence can be beneficial to determine whether a gene or a group of genes has been acquired by HGT.<br>
While studies on detecting horizontally transferred genes or GIs in genome sequences have been intensively carried out, little has been reported for PAIs. Considering that a PAI is a GI encoding virulence factors, compositional criteria such as G+C content and codon usage is not sufficient for identifying PAIs because genomic approaches can only lead to the identification of GIs [2]. In this work, we designed a computational method for identifying PAIs in sequenced genomes by combining a homology-based method and detection of abnormalities in genomic composition. To do this, we collected published PAI data and checked virulence genes on the PAI loci. We applied this approach to 148 prokaryotic genomes and identified 77 candidate PAIs. Detected regions contain virulence genes and relics of the HGT event.<br>
<br>
Results<br>
Genomic islands in bacterial genomes<br>
As for the 157 chromosomes examined (Table 1S [see Additional file 1]), the length proportion of GIs to the chromosome averaged 10.1%. Nanoarchaeum equitans, the smallest genome of any sequenced microbes, contained the smallest proportion of GIs, which is only 2.9%. Leptospira intrerrogans, which is responsible for worldwide water-borne zoonosis leptospirosis, contained the largest, 34.7% for chromosome I and 32.2% for chromosome II. The genome of L. interrogans was reported to have the biggest number of proteins with structural similarity to eukaryal and archaeal proteins as compared to other bacteria [12]. In general, larger proportions of GIs in pathogens than those in related nonpathogenic species were observed, e.g., 15.7% for Corynebacterium diphtheriae versus 7.6% for C. glutamicum, 12.3% for E. coli CFT073 versus 8.9% for E. coli K-12.<br>
<br>
PAI-like regions<br>
When every ORF contained in 207 PAI loci (see Table 1 and supplementary Table 2S for the complete information [see Additional file 2]) were similarity-searched against the ORFs present in the 148 prokaryotic genomes, 1,490 genomic strips of PAI-associated genes were defined based on the proximity of the homologs of genes from the same PAI accession. Overlapping strips were then merged into 525 genomic regions in 83 chromosomes (Figure 1). Among these regions, 241 contained at least one gene homologous to the virulence genes on the PAI loci, which will be referred to as PAI-like regions in this study. 77 PAI-like regions (total 1,652,758 bp) partly or entirely overlapped GIs, while the remaining 164 regions (total 1,553,923 bp) did not contain any part of GIs. In this report, we call the former candidate PAIs (cPAIs). Figure 2 shows the projection of PAI-like regions in their G+C contents and length-proportion of horizontally transferred genes. 52% of all the PAI-like regions show lower G+C content compared to those of their genomes (average of -0.6%, standard deviation of 3.8), however, 75% of the cPAIs have lower G+C contents (-2.7%, 4.7, respectively). The plot indicates that clusters of PAI-homologs are often located in the backbone sequence while the detected GIs tend to be biased to have lower G+C content.<br>
<br>
Candidate PAIs<br>
cPAIs, PAI-like anomalous regions, were present in 29 bacteria including 6 non-pathogens, and their sizes ranged from 3.7 kb to 137.5 kb with the average length of 21.5 kb (Table 2, supplementary Table 3S [see Additional file 3]). Most of these regions contained transposase, integrase genes or insertion sequence elements, and were associated with tRNA genes at their boundaries, which is indicative of genomic islands. In some instances, our method allowed the detection of the entire PAIs for those only partial sequences have been reported in the original papers (Figure 3). This is due to the fact that PAIs often share conserved regions, and homologous regions of other PAIs can be located in the same PAI locus. Interestingly, cPAIs were detected in six strains which are known to be non-pathogens. Genes contained code for an ABC transporter (Bacillus halodurans), flagellar proteins (Bacillus subtilis), iron transport and fimbrial proteins (E. coli K-12), transmembrane sensors and outer membrane efflux proteins (Nitrosomonas europaea), or nodulation proteins (Bradyrhizobium japonicum). Genes detected in Mesorhizobium loti, a bacterium that forms globular nodules and perform nitrogen-fixing symbiosis with leguminous plants, are involved in the nodulation process and a type III secretion system (TTSS) [13]. However, the unexpected locations of cPAIs in non-pathogens should be interpreted as some clusters of potentially horizontally transferred genes that have homology to virulence genes.<br>
Among the 77 cPAIs, 34 matched to PAIs which have been described in genome sequencing papers (Table 2, Figure 2). 27 cPAIs entirely matched to known PAIs ? a PAI (in Enterococcus faecalis), PAI I, IICFT073 (E. coli CFT073), LEE (E. coli O157 EDL933 and Sakai), cag PAI (Helicobacter pylori 26695 and J99), the TTSS and tc loci (Photorhabdus luminescens), SPI-2,4,5 (Salmonella enterica serovar Typhi Ty2 and CT18, and serovar Typhimurium LT2), SPI-3 (S. typhimurium LT2), SHI-1, 2 (Shigella flexneri 2a 2457T and 301), VPI (Vibrio cholerae), Hrp PAI (Xanthomonas campestris), and HPI (Yersinia pestis CO92 and KIM). One end of PAIs ? SPI-1 (in three S. enterica strains), SaPIm3 (S. aureus Mu50), and SaPIn3 (S. aureus N315) ? were found in 5 cPAIs, and the other end of the PAIs were found in seemingly backbone sequences. ?Sa? in S. aureus MW2 and CTX locus in V. cholerae N16961 were partly matched. Nine cPAIs span the TTSS loci which were not annotated as PAIs in the genome sequencing data.<br>
Regions homologous to a certain PAI were frequently found in genomes of various taxa. Especially, parts of PAIs originally identified from enteropathogenic bacteria were detected not only in enterobacteria but also in phyla other than the Gammaproteobacteria in our study (Figure 4). The number of genomes containing PAI-like regions was drastically reduced when we considered genomic regions that overlap GIs. Elements of PAI I~ III536 in the uropathogenic E. coli strain 536 showed high similarities to other members of the Enterobacteriaceae. This is consistent with the previous report that PAI-specific sequences of E. coli strain 536 were frequently found in pathogenic and commensal E. coli isolates by using "E. coli pathoarray" [14]. Parts of the LEE PAI in enterohemorrhagic E. coli O157:H7, enteropathogenic E. coli E2348/69, rabbit-specific enteropathogenic E. coli 83/89, and rabbit diarrheagenic E. coli RDEC-1 similarly matched to genomic regions of different taxa.<br>
In most cases, distribution of the regions homologous to the PAIs from other enterobacteria such as VPI of Vibrio cholerae, cag PAI of Helicobacter pylori, SaPI1 of Staphylococcus aureus strains were restricted to their host strains. However, widespread distribution in different species was evident for PAGI-1 of Pseudomonas aeruginosa and the Hrp PAI of P. syringae, Xanthomonas spp., Burkholderia pseudomallei, and Ralstonia solanacearum. Variations of cPAIs were observed for EDL933 and Sakai, which belong to the same E. coli O157 group (Table 2). This discrepancy results from the different distribution of prophages in the two genomes. Also, different ORF prediction by different research groups affected the determination of GIs.<br>
<br>
PAI-like regions that did not meet the criteria<br>
164 PAI-like regions in 57 prokaryotes including 16 non-pathogenic bacteria and one archaeon did not overlap GIs (supplementary Table 4S) [see Additional file 4]. Their sizes ranged from 1.9 to 50.6 kb and were averaged 9.5 kb. Most of them encoded flagellar/fimbrial biosynthesis or iron uptake systems. Among these regions, 14 were PAIs published in the genome sequencing papers. Six PAIs ? Hrp PAI (in Pseudomonas syringae pv. tomato DC3000), SPI-3 (S. enterica serovar Typhi strains Ty2 and CT18), SaPIm1 (in S. aureus Mu50), SaPIn1 (S. aureus N315) and ?Sa3 (S. aureus MW2) ? entirely matched, and 5 counterparts of the PAIs that partly match to the cPAIs that overlap GIs were found in these regions. Parts of LIPI-1 in Listeria innocua and two regions of internalins in L. monocytogenes EGD were found. In fact, the Hrp PAI and LIPI-1 have DNA compositions similar to the core genomes, and are suggested to have been acquired a long time ago [15,16].<br>
<br>
<br>
Discussion<br>
By analyzing structures of many microbial genomes, it became obvious that HGT is an important mechanism for bacterial evolution, let alone genome complexity and plasticity [1]. GIs, which are large genomic segments and most likely transferred by HGT, contribute to the survival of the hosting bacterial strain in a particular environment and sometimes to virulence. These two kinds of GIs, of which the former can be referred as 'fitness islands', are often hardly distinguishable from each other because the role of a GI may vary in different ecological niches and the physiology of the bacterium. Up to now, attempts to identify PAIs [5,6,17] have been made by detecting genomic regions which only differ from the rest of the genome in their base composition and codon usage. In this study, we identified "candidate PAIs (cPAIs)" that reflect potential PAIs with anomalous composition, probably due to their recent acquisition. Among the 148 sequenced strains searched in this study, 17 were the strains closely related to the hosts carrying queried PAI loci. From the reports of their genome sequencing projects, 27 PAIs have been described. Among them, 23 PAIs were found in the list of cPAIs and the accuracy of our method can be considered as 85% (Table 2, supplementary Table 4S [see Additional file 4]).<br>
The presence of virulence factors could be a useful criterion for discerning PAIs from other genomic islands. Clusters consisting of only hypothetical genes and/or elements involved in the transfer mechanism (e.g. IS elements, tRNA genes, integrase, and prophage) were filtered out, leaving only 46% of the genomic regions containing virulence factors. Widespread distribution of conserved elements of many PAIs in different species and in even non-pathogens is due to their complex mosaic structures consisting of elements of different origins. PAI I~ III536 in E. coli 536 have mosaic-like structures consisting of many DNA fragments that show high similarities to the chromosomal regions of other pathogenic E. coli strains and Shigella flexneri[18]. SPI-2 is a fusion of at least two genetic elements ? a 25-kb region encoding the TTSS with a low G+C content and a 15-kb region encoding metabolic functions with a G+C content similar to the rest of the genome [19], and the Hrp PAI of Pseudomonas syringae has a tripartite structure [15].<br>
Some virulence factors in PAIs are homologous to seemingly backbone genes. As shown in Figure 4, PAIs having extensive mosaic structures showed highly frequent occurrence in various species, and clusters of seemingly backbone genes could be removed from the list of the cPAIs by checking the presence of a GI in a PAI-like region. Many Gram-negative bacterial pathogens cause diseases by secreting and injecting virulence proteins (effectors) into the host cell via a specialized protein secretion mechanism (TTSS) [20]. They are evolutionarily related to flagellar systems and often hard to distinguish when based only on homology searches [21]. However, TTSSs are frequently transferred laterally between Gram-negative bacteria while flagellar systems are mainly inherited by vertical descent. This fact explains why many regions encoding flagellar biosynthesis genes have hits to PAI-like regions not showing anomalies in DNA composition (supplementary Table 4S) [see Additional file 4], while PAI-like regions overlapping GIs contain lots of TTSSs (Table 2). Iron uptake systems are important for bacterial survival as well as virulence [2]. Many PAIs such as HPI of Yersinia species, SHI-2 of S. flexneri, and SRL of S. flexneri 2a YSH6000 carry genes encoding various siderophore systems that produce and secrete low-molecular-weight siderophores with extremely high affinities for ferric iron. Clusters of homologs of ferric dicitrate transport system (fecABCDEIR, Fec) of SRL [22] were widely distributed in the backbone genomic regions of various species, which implies that Fec might be the most ancient siderophore system (Figure 4, Table 2, supplementary Table 4S [see Additional file 4]). Interestingly, a 7.1-kb fecCDE-homologous region can be found even in Halobacterium sp. NRC-1, the only archaeon possessing the PAI-like region in this study. This region is inserted by a 6-phosphogluconate dehydrogenase gene, 3 hypothetical proteins and tRNA-Arg gene.<br>
One of the difficulties when dubbing potential PAIs in the sequenced genomes is to determine the boundaries. A PAI may have a number of genes which have undergone many evolutionary stages and thus compositionally indistinguishable from the rest of the genome [2,23]. This might be due to some parts highly adjusted to the base composition of the recipient's genome or to the backbone genomic segments added later in evolution [10]. We found that the length proportion of transferred regions contained in the known chromosomal PAIs ? 28.7 kb of LEE in E. coli O157 Sakai, 36.2 kb of Cag PAI in H. pylori 26695, 61.2 kb of VPI-2 in V. cholerae, and 137.5 kb of PAI in Enterococcus faecalis ? vary from 0.19 to 0.65. Thus, compositional approaches cannot predict the boundaries of the detected PAI because they only detect atypical genomic region. To solve this problem, we detected genomic segments homologous to each known PAI, which were then clumped into a large genomic region. This procedure is somewhat like the process of fragment assembly in which a contiguous region (contig) is made from overlapping fragments in shotgun sequencing [24]. Like the conserved sequences of TTSS structural genes [20], PAIs often share conserved regions. In addition, PAIs frequently carry relics of HGT event such as mobile sequence elements and association with tRNA genes at their boundaries [3]. <database>Islander</database> [25], a database of potential integrative islands in prokaryotic genomes, detects GIs by identifying tRNAs or tmRNA genes, and candidate integrase genes. Although many GIs reported from the database were in accordance with our results, large portion was not annotated as cPAIs mainly due to the absence of homologs of virulence genes in known PAIs and PAIs that are not located at the tRNA loci. As illustrated in Figure 3, frequent distribution of conserved regions between PAIs allows our method to find the entire region of a PAI in a sequenced genome even though its similar sequence is partially known.<br>
A typical genome sequencing team uses genes in the gene cluster or the genome sequence of interest as a query to search for any similar genes in the databases. Then, homologs of pathogenicity/virulence genes are inferred by checking whether descriptions of the retrieved genes have any indications that suggest virulence/pathogenicity or they are from pathogens. Because this approach depends on the examiner's knowledge on known PAIs or pathogenicity/virulence genes and entry descriptions of the retrieved genes often are not informative to infer the function, it is never sure whether the searches thoroughly picked up all the genes associated with PAIs or pathogenicity/virulence. To avoid this uncertainty on the robustness of the open-ended search, we first collected all the reported PAI loci and used them as a query to search for homologs in the complete prokaryotic genomes. Our method guarantees that all the potential PAIs related to the known PAIs were searched without the intervention of human interpretation.<br>
In completely sequenced genomes, we detected cPAIs that are homologous to the published PAIs and show anomaly in DNA composition. The methodology we developed in this study has a limitation in that the detected cPAIs are limited by the query data set of the known PAIs. This caveat, however, can be advantageous when the researchers only concern a specific set of PAIs. Furthermore, this approach can be easily extended to identify various genomic islands (e.g. fitness, metabolism, and resistance islands). Among the cPAIs detected in this study, omission of several well-known PAIs such as Hrp PAI of P. syringae and LIPI-1 of L. innocua is due to their DNA compositions similar to the core genomes which may caused by horizontal transfer from closely related strains or very ancient HGT event. Thus, patterns of best matches of each gene to different species, lineage-specific genes or transferred genes from phylogenetically distant species would be helpful in improving the possibility of finding GIs and PAIs. Also, accumulation of PAI sequence data in bacterial families other than the Enterobacteriaceae will lead to detection of more putative PAIs across various taxa. Finally, it should be noted that the identity of cPAIs as bona fide PAIs need to be confirmed by further experimental verification. We are currently improving the detection scheme and are developing a database for cPAIs in sequenced genomes.<br>
<br>
Conclusion<br>
We present the first computational framework combining feature-based analyses and similarity-based analyses. As shown in Figure 3, the similarity-based analysis that is reminiscent of the sequence-assembly procedure was proven to be an efficient method for demarcating the potential PAIs in our study. Also, the function(s) and origin(s) of a cPAI can be inferred by investigating the PAI queries comprising it. With the availability of rapidly increasing complete genome sequences [26] as well as PAI data, the proposed method will be useful in identifying potential PAIs in microbial genomes.<br>
<br>
Methods<br>
Collection of complete genomes and PAI Data<br>
The sequence files of 148 prokaryotic complete genomes consisting of 157 chromosomes, including 17 archaeal ones as of January 2004 were downloaded from the NCBI FTP server (, supplementary Table 1S) [see Additional file 1]. We searched the <database>GenBank</database> database and literature [3,23] for any descriptions of the "pathogenicity island". Forty five kinds of PAIs and 207 <database>GenBank</database> accessions containing either part or all of the reported PAI loci in 120 pathogenic bacteria, are summarized in Table 1. (see supplementary Table 2S for the complete information) [see Additional file 2]. The definition of virulence genes is difficult as their function may depend on growth conditions and host niches. Thus, we attributed this to the biologists who identified PAI loci, and virulence genes of PAI loci were identified by literature survey. Many PAIs, 29 out of 45 kinds of PAIs, came from Enterobacteriaceae. Thirty four PAI loci are completely sequenced ones ranging from 6.8 kb to 153.6 kb (average: 41.3 kb), and the remains are part of PAI. It should be noted that the collected sets do not contain PAIs which were reported from genome sequencing papers.<br>
<br>
Detection of GIs in genome sequences<br>
To detect GIs in a chromosome, we first identified horizontally transferred genes (H) based on the algorithm developed by Garcia-Vallve et al. [4]. To alleviate false positives caused by applying single criterion for identifying HGT regions, we considered a gene as H only if both G+C content and codon usage are aberrant. For each genome, we have computed total G+C content ([G+C]T) and G+C contents at the first and third codon positions ([G+C]1 and [G+C]3) of every ORF. The compositional bias at the first and third positions were reported to be positively correlated to expressivity and genomic G+C content, respectively [10,27]. Extraneous origin of the gene in terms of G+C content was considered if its [G+C]T deviates over 1.5 ? or if deviations of [G+C]1 and [G+C]3 are of the same sign and at least one of them is over 1.5 ?. Mahalanobis distance (dM) was used to evaluate deviation of the codon usage of a gene and mean of the genome [4]. dM is a statistic in unit of standard deviation from the mean of 61 codon frequencies and can be calculated as follows:<br>
dM(X, Xmean) = (X - Xmean)T S-1(X - Xmean)<br>
Where X and Xmean correspond to vectors having relative frequencies of the 61 codons for a gene and the mean values for a genome, respectively. S-1 is the inverse of variance-covariance matrix (S) of all the 61 codon frequencies. The higher this value is the more deviation in codon usage [4]. If Xs are normally distributed, dMs can be converted to p-values using the ?2 distribution function. We considered a gene as extraneous in codon usage if its p-value was less than 0.05. It should be noted that genes longer than 300 bp were used for calculating the mean and standard deviation (?) of G+C contents and dMs. This is from the observation that genes having shorter than 300 bp have much higher chance of anomalies in G+C content and codon usage.<br>
We ran a genome scan of a 10-gene window and identified regions containing four or more H. This threshold frequency of 0.4 was inferred from the observation that the frequencies of H in known PAIs such as LEE of E. coli O157 Sakai, cag PAI of Helicobacter pylori 26695, VPI-2 of Vibrio cholerae, and a PAI of Enterococcus faecalis, were averaged 0.35. Neighbouring regions were merged into larger regions which were referred to as GIs in this study. Some genomic regions had highly biased G+C content compared to the whole G+C content of the chromosome, while their codon usage were not biased. For example, 46.4 kb genomic region ranging from 2,647,129 bp in Yersinia pestis KIM, which contains yersiniabactin genomic island [28] has considerably higher G+C content (55.7% versus 47.6% average for the whole genome), but showed a similar codon usage for the genes contained in this region. Thus, among genomic regions made from genes anomalous in G+C content, the region was added to GIs if its G+C(T) deviates more than 1.5 ?.<br>
<br>
Identification of candidate PAIs<br>
The detection scheme for the regions of cPAIs is outlined in Figure 1. Each ORF from PAI locus was used as the query in <software>BLASTP</software> searches [29] against the set of ORFs from each of the 148 completely sequenced genomes using PAM250 as scoring matrix for retrieving homologous genes in evolutionary distant strains. Likewise, homologs of ORFs, RNA genes and repeat regions of PAI locus on the nucleotide level were searched using <software>BLAT</software>, a modified <software>BLAST</software> alignment program which can stitch matched regions into a larger one [30]. If the identity of the resulting hit is over 80% for DNA sequence or 25% for protein sequence and the aligned region is both over 70% of lengths of query and the hit, the pair of sequences was considered as a homolog. Genomic strips corresponding to each PAI locus were then obtained by identifying the regions containing four or more homologs of the genes from the same PAI accession and by merging the neighboring regions. Overlapping or adjacent genomic strips corresponding to the same or different kind of PAI loci were fused into a large region. Among these regions, PAI-like regions were identified by checking the presence of at least one gene homologous to a virulence gene on the PAI loci. We considered a candidate PAI (cPAI) only if the PAI-like region partly or entirely spans the GI.<br>
<br>
<br>
Authors' contributions<br>
SHY designed the study, developed the software for implementing the devised algorithm, and wrote the manuscript. CH and HK contributed to the writing the software, and YHK collected and reviewed the data, and TKO assessed the biological significance of the results. JFK supervised the project and contributed to the development of methodology and writing the manuscript. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1435940</b><br>
<software>AltTrans</software>: Transcript pattern variants annotated for both alternative splicing and alternative polyadenylation<br>
<br>
<br>
Background<br>
The three major regulatory mechanisms that bring about formation of alternative transcript patterns from an expressed gene act at the choice of alternative sites for transcription start (TS), splicing, and polyadenylation [1-4]. Use of alternative TS site and/or alternative polyA site often accompanies alternative splicing [1,4,5]. Currently there are efforts that collect data &amp; annotation either for TS variants [1,6-9], or for splice variants [10-24], or for polyA variants [3,25-28]. These data sets provide a wealth of value-added annotation (such as tissue specificity, evolutionary conservation, and regulatory motifs). Given that there is a coupling between the machineries responsible for transcription initiation, splicing, and polyadenylation [29-33], it is important to take a coherent &amp; integrated view of these individual variants and to derive a data set of alternate transcript patterns along with consolidated annotation. Previous attempts to integrate variants of splicing and polyadenylation (as well as of transcript start sites) introduced new methods for transcript assembly ? notable examples being that of Kim et al [17], Sharov et al [34], and that of Zavolan et al [4]. General conclusions from these studies were that (i) a majority of transcription units showing multiple splice forms contain transcripts in which the apparent use of an alternative transcription start (or stop) is accompanied by alternative splicing of the initial (or terminal) exon; and that (ii) alternative splicing is a major contributor to transcriptome diversity.<br>
We have been generating data sets on individual variant types; such data sets include <software>AltSplice</software> [11] and <software>AltPAS</software> [25]. Both the <software>AltSplice</software> and <software>AltPAS</software> pipelines generate genome-wide data based on <database>Ensembl</database> [35] gene annotation. The <software>AltSplice</software> pipeline examines gene-transcript alignments and delineates alternate splice events and alternate splice patterns; the pipeline characterises the generated data for various biological features. In the current work, we extend the <software>AltSplice</software> pipeline as <software>AltTrans</software> to annotate the observed splice patterns for terminating polyA site; the information on the polyA site for a splice pattern is derived by examining the transcript sequences that confirm the splice pattern; a splice pattern that could be annotated for terminating polyA site is termed as transcript pattern. The <software>AltPAS</software> pipeline examines gene-transcript alignments and identifies potential polyA sites independently of the underlying splicing patterns; as a result for a given set of genes, <software>AltPAS</software> derives a larger data set of polyA sites as compared to <software>AltTrans</software>.<br>
The transcript patterns as derived by the <software>AltTrans</software> pipeline and the combined list of polyA sites as generated by <software>AltTrans</software> and <software>AltPAS</software> pipelines form the core of the data presented in this work; these basal data are annotated for various biological features. The resulting data for human and mouse is presented to the community in two forms: (i) through an FTP server as flat file distributions; and (ii) through user-friendly web query interfaces. Included in the database are those genes for which at least one transcript pattern (annotated both for splicing and terminating polyA site) was determined.<br>
<br>
Construction and content<br>
The different pipelines discussed below are (i) <software>AltSplice</software> (that delineates splice related data namely, splice sites, splice patterns and splice events), (ii) <software>AltTrans</software> (that delineates transcript patterns along with annotation for terminating polyA site from <software>AltSplice</software> splice patterns, and (iii) AltPAS (that delineates all potential polyA sites on the gene independently of the underlying splicing patterns). Also discussed are the approaches to integrate the resulting data.<br>
Generating splice patterns: the <software>AltSplice</software> pipeline<br>
The methodologies behind the <software>AltSplice</software> pipeline have been previously reported [11-13] and hence are briefly mentioned here. The considered gene set is that of the <database>Ensembl</database> genome annotation project [35]. For each of the considered genes, the nucleotide sequence of a region that extends <database>Ensembl</database>-defined gene boundaries by 3000 bases on either flanking side is extracted as <software>AltSplice</software> gene. Transcript (EST and mRNA) sequences as extracted from <database>EMBL</database> database [36] are used to generate a high quality data set of gene-transcript alignments showing more than one high scoring match between gene and transcript sequences. Any transcript sequence that aligns exclusively to the flanking regions of a gene is discarded. Further, any transcript sequence that aligns with more than one gene or ambiguously with more than one region on a single gene is discarded. Alignment gaps on gene sequences are considered as potential introns and their validation as transcript-confirmed introns is a crucial step in <software>AltSplice</software> pipeline (see [11-13] for validation procedures). Alignment matches on the gene sequence are accepted as a confirmed exon if flanked on either side by a confirmed intron. Thus each transcript sequences that maps to a gene is described by its exon-intron structure. Transcript sequences that map to a gene are then grouped into classes in a manner that the member transcripts from a class show same exon-intron structure. Longest representative from each such class denotes a unique splice pattern for the gene. Overlapping exons and introns from the isoform splice patterns are then examined to delineate alternative splice events.<br>
<br>
Redefinition of <software>AltSplice</software> gene region<br>
An AltSplice gene represents a genomic region containing (5' flanking 3000 bases + <database>Ensembl</database> gene region + 3' flanking 3000 bases). Once the splice patterns are identified, we redefine the <software>AltSplice</software> gene region as below:<br>
(i) If no splice pattern is observed extending into flanking regions, then <software>AltSplice</software> gene is trimmed down to that as annotated in <database>Ensembl</database>.<br>
(ii) If a splice pattern is seen as extending into the 5' flanking region, the genome start location of the splice pattern forms the 5' bound for <software>AltSplice</software> gene.<br>
(iii) If a splice pattern is seen as extending into the 3' flanking region, the genome end location of the splice pattern forms the 3' bound of <software>AltSplice</software> gene.<br>
<br>
Generating transcript patterns and 'terminating' polyA sites from <software>AltSplice</software> splice patterns: the <software>AltTrans</software> pipeline<br>
<software>AltSplice</software> splice patterns and the gene-transcript alignments form the basis of <software>AltTrans</software> pipeline that delineates alternate transcript patterns (see Fig. 1). Each of the gene-transcript alignments confirming a splice pattern from <software>AltSplice</software> is examined for the presence of a polyA site that terminates the transcript sequence. Transcripts displaying a terminating polyA site are grouped in a manner that each class of transcript shares the same exon-intron structure and the same terminating polyA site; representative from each such class is termed a transcript pattern. These different steps are as detailed below.<br>
Detecting polyA sites from transcript sequences that confirm <software>AltSplice</software> patterns. This procedure is of the following three steps:<br>
(i) Detecting polyA tail and polyA cleavage (PAC) site<br>
Each of the gene-transcript alignments is examined for the presence of a 3' dangling end on the transcript sequence. Only those alignments that show 3' dangling ends of length at least 8 bases are considered further. The transcript region -5 to +5 from the end of alignment is examined for the start of a polyA tail. A polyA tail is defined as a string of 8 or more adenosines. It is observed that a higher proportion of the dangling ends are short (5 to 50 bases) and often involve runs of adenosines. While it is possible that mRNAs can possess long polyA tails, we are worried about longer dangling ends since their extra lengths can be results of artefacts in EST sequences or of genomic 'contaminations' at the 3' ends of the ESTs. We do want to include transcript patterns involving those gene-EST alignments with long dangling ends; however, we want to be sure that such dangling ends contain genuine polyA tails and hence we tightened the requirements for polyA tail on long dangling ends as below: (length of dangling end : minimum length of polyA tail) as (= 50 : 8); (&gt;50 &amp; = 100 : 10); (&gt;100 &amp; = 150 : 15); and (&gt;150 : 20). Since it is often the case that a run of adenosines is interrupted by non-adenosine bases, we allow mismatches at up to a maximum of 10% of the positions in the identified string provided the string still contains the required number of adenosines (as per the specification mentioned above); for this purpose, we increase the search window sequence in advance by 10% to take any mismatch into account. If more than one polyA tail is identified starting in the -5 to +5 region, the one with the highest composition of adenosines is chosen as the authentic polyA tail. The gene position corresponding to the start of polyA tail is considered as the cleavage site. As many as 75% of instances of dangling ends showing a putative polyA tail are of shorter lengths (&lt;= 50 bases), 8% are of length 50?100 bases; 4% are of length 100?150 bases, and 13% are of length &gt; 200 bases.<br>
<br>
(ii) Detecting polyA signal (PAS)<br>
A region on the gene sequence that aligns to the 40 nt transcript region 5' to the identified cleavage site is scanned for the presence of one of the 13 variant signals (namely, AAUAAA, AUUAAA, UAUAAA, AGUAAA, AAGAAA, AAUAUA, AAUACA, CAUAAA, GAUAAA, AAUGAA, UUUAAA, ACUAAA and AAUAGA) reported in the literature [3] with the criteria that no mismatch is allowed. For every gene-transcript alignment, all such motifs are identified. Of multiple matches, a representative motif is chosen as per the following criteria: (i) one that occurs within the region of -25 to -15 to the cleavage site is chosen; if multiple such motifs are seen within this region, the one of highest ranking (as ordered in [3] is chosen; of the highest ranking ones, the one that is close to the position of -20 is chosen; (ii) if no signal is seen in the -25 to -15 region, then those identified outside this region are examined; if multiple signals occur outside the region, the one of highest ranking located close to the position -20 is chosen. A higher proportion of gene-transcript alignments with longer dangling ends (that passed the test for presence of polyA tail) gets removed at this step, when compared to those with shorter dangling ends ? we observe the following relationship between length of dangling end and proportion of transcripts failing the signal motif test: &lt; 50 bases : 9%; &gt;50 bases &amp; = 100 bases :14%; &gt;100 bases &amp; = 200 bases :25%; and &gt;200 bases : 33%.<br>
<br>
(iii) Grouping nearby cleavage sites and choosing a representative cleavage site as polyA site)<br>
At this stage, gene-transcript alignments that do not show both a cleavage site and a polyadenylation signal are not considered further. Steps discussed so far identify for every gene a set of cleavage sites along with polyA signals. It is often the case that some of the identified cleavage sites are close to one another. Given that a polyA site can harbour multiple cleavage sites [3,25,37] and also that errors in sequences can lead to small differences in the locations of identified cleavage sites, it is possible that the adjacent sites are not distinct polyA sites. Thus it is essential to have a method in place to identify which of the close by sites can be chosen as an authentic polyA site. The identified cleavage sites are classified onto groups such that a member of a group differs from its immediate 5' neighbour by a maximum of 5 bases; the 5' most site from each such group is then chosen as the representative polyA site for that group of transcripts. Each member gene-transcript alignment of the group is annotated by such a representative polyA site and its associated signal motif.<br>
<br>
Forming the transcript pattern classes<br>
At this stage, for every gene a set of gene-transcript sequences with known exon-intron structure and terminating polyA site is available. The transcript sequences are then grouped into classes in such a manner that members of a class show same exon-intron structure and same terminal polyA site. The longest representative member of each such class is termed as a "Transcript Pattern".<br>
<br>
<br>
Identifying all potential polyA sites (independent of underlying splicing patterns): the <software>AltPAS</software> pipeline<br>
The list of polyA sites as identified by AltTrans pipeline is by no means comprehensive for the reason that <software>AltTrans</software> examines only those gene-transcript alignments confirming <software>AltSplice</software> splice patterns. <software>AltPAS</software> is a generic pipeline that identifies all potential polyA sites irrespective of whether the examined gene-transcript alignments reveal any underlying splicing pattern or not. The methodology used is described in [25] and is briefly discussed below.<br>
EST sequences and full-length cDNA sequences are obtained from transcript resources such as <database>dbEST</database> [38], <database>H-Inv</database> [39], and <database>FANTOM</database> [40]. Of the EST sequences, only those that are annotated as from the 3' end of gene are retained. Trailing polyA or polyT sequences of 5 nt or more are removed from the EST sequences. Both the 3' EST and cDNA sequences (termed transcript sequences) are aligned to the repeat-masked genome using the <software>MegaBlast</software> program [41]. High scoring matches are retained for further analysis. The matches are then clustered in a manner that transcript members from a cluster have their end positions located within a range of 10 nucleotides from each other. Each cluster is then analyzed using a sliding 10-nt window to locate the most likely cleavage site, defined as the position where the window contains the ends of most transcripts. Alignment hits with more than 5 unmatched positions at cleavage site are discarded. Cleavage sites that are flanked by A-rich region (at least 9 out of 10 nt positions are adenosines) in the 50 nt downstream genomic sequence, and those that do not contain one of the known polyA signals in the 30 nt upstream region are discarded. Of the remaining cleavage sites, only those that are supported by at least two transcript sequences are retained as potential polyA sites. The polyA sites, thus identified, are denoted using genome coordinates. Assignment of the detected polyA sites to <database>Ensembl</database> genes is carried out as below: A polyA site is assigned to the Ensembl gene to which the site's genome location can be mapped; if the genome location of a polyA site does not map to any annotated gene, it is assigned to the nearest 5' gene, provided that the distance to gene is less than 3000 bases.<br>
<br>
Integrating polyA sites from the <software>AltPAS</software> and <software>AltTrans</software> pipelines<br>
Of the polyA sites identified by the <software>AltPAS</software> pipeline, considered further are only those that can be mapped within the bounds of <software>AltTrans</software> genes. PolyA sites identified by both pipelines are merged. Adjacent polyA sites are then grouped and a representative polyA site is chosen from each group. The procedure adopted for the grouping process is same as that used for grouping <software>AltTrans</software> polyA sites (discussed in earlier sections) with the following variation. If a group contains sites from both <software>AltTrans</software> and <software>AltPAS</software>, the 5' most <software>AltTrans</software> site is chosen as the representative. Such a set of representative polyA sites is subsequently used to annotate a gene with all potential polyA sites, and to annotate a transcript pattern for potentially "skipped" polyA sites. It is to be noted that a transcript pattern in the data set always ends with an <software>AltTrans</software> polyA site as the terminating polyA site.<br>
<br>
<br>
Discussion<br>
We have been providing to the community an <database>alternative splicing database</database> (<database>ASD</database>) that integrates data from a computational pipeline (<software>AltSplice</software>) and from a manual curation effort (<database>AEdb</database>); such a database specializes on splicing events and their characteristics (see [11,12]). <software>AltTrans</software>, described in this manuscript, is an extension of <software>AltSplice</software>. It considers the splicing patterns as detected by <software>AltSplice</software> and examines the transcript sequences (that confirm each of the splicing patterns) for polyA sites. The transcript sequences thus annotated for both splicing pattern and polyA site are regrouped to form distinct transcript patterns. PolyA sites detected by <software>AltPAS</software>, an independent computational procedure, are also mapped to the <software>AltTrans</software> genes set. Transcripts produced by the <software>AltTrans</software> pipeline are presented as part of the <database>ATD</database> (<database>Alternate Transcript Diversity</database>) database. Figure 2 illustrates the relationship between <software>AltSplice</software>, <software>AltTrans</software>, and <software>AltPAS</software> pipelines/data. <software>AltTrans</software> is an important resource that elucidates the transcript complexity owing to alternative splicing and alternative polyadenylation. <software>AltTrans</software> will be further extended in future to provide information on transcript start as well. The possible applications include derivation of <software>SAGE</software> tags, derivation of exon junction probes for splice arrays, and primers for transcript-specific RT-PCR experiments.<br>
Data sets of transcript patterns were derived for both human and mouse. Statistics on the generated data is presented in Table 1 and is discussed in the following sections.<br>
Human data set<br>
The data set of human transcript patterns contains 7669 gene entries for each of which is derived at least one pattern that is fully annotated for both splicing and terminating polyA site. The total number of transcript patterns is 12559 (at an average of 1.6 per gene) encoded by 10221 terminating polyA sites. In 3179 of the 7669 <software>AltTrans</software> genes, two or more alternate transcript patterns could be observed. Inclusion of <software>AltPAS</software> polyA sites annotated an additional 6883 polyA sites raising the number of polyA sites mapped to 17104.<br>
<br>
Mouse data set<br>
The data set of mouse transcript patterns contains 5862 gene entries. The total number of transcript patterns is 7755 (at an average of 1.3 per gene) encoded by 6976 terminating polyA sites. In 1548 of the 5862 <software>AltTrans</software> genes, two or more alternate transcript patterns could be observed. Inclusion of <software>AltPAS</software> polyA sites annotated an additional 2475 polyA sites raising the number of polyA sites mapped to 9451.<br>
<br>
Extent of alternative splicing versus alternative polyadenylation<br>
Examination of data presented in Table 1 indicates that the proportion of human and mouse genes undergoing alternative splicing (at 74% and 65%, respectively) is higher than the proportion of genes undergoing alternative polyadenylation (at 60% and 42%, respectively). The above estimates are in agreement with those reported in literature ? see [42] for estimate on alternative splicing and [3,25] on alternative polyadenylation. It is also seen that one in two human genes (close to one in three mouse genes) may undergo both alternative splicing and alternative polyadenylation. Considering only the polyA sites detected by the <software>AltTrans</software> pipeline (which requires that transcripts that confirm a polyA site also confirm the splicing of the transcript pattern) reveals a conservative estimate for extent of alternative polyadenylation at 27% for human and 18% for mouse.<br>
<br>
Limitations with regard to low number of reported transcript patterns<br>
There is a large discrepancy in the numbers for observed splice patterns and observed transcript patterns. While the average number of observed splice patterns per human gene is 5.4, the average number of observed transcript patterns is a mere 1.6 (the corresponding numbers in the case of mouse data are 4.6 and 1.3). This discrepancy is due to the fact that for an EST/mRNA sequence to confirm a transcript pattern, it is required that the sequence confirms both the splicing and terminating polyA site. EST sequences do not often cover simultaneously both the internal and 3' regions of the gene ? this is reflected in the observed numbers (see Table 1) for the EST/mRNA sequences that confirm splice patterns and transcript patterns (e.g. of the 837828 EST/mRNA sequences that confirm the human splice patterns, a mere 38731 contain enough information to confirm transcript pattern). As a result, the number of identified polyA sites by the AltTrans pipeline is expected to be reduced <software>AltTrans</software> detected 10221 polyA sites in 7669 human genes. <software>AltPAS</software>, that detects polyA sites independently of the underlying splicing process, mapped a further 6883 polyA sites to the same set of 7669 human genes. It is possible to increase the number of transcript patterns by using the <software>AltPAS</software> polyA sites as well to annotate the gene-transcript alignment for 'terminating' polyA site; however, we have restrained from doing this for the reason that it is our intention to provide a high quality set of transcript patterns, individual structural elements of which are confirmed by same set of EST/mRNA sequences.<br>
<br>
Heterogeneity of cleavage sites<br>
It is known that a polyA site can harbour multiple cleavage sites and that polyadenylation can be an imperfect process [3,25,37,43]. However, it is possible that the small differences in the locations of multiple cleavage sites can be due to artefacts in EST sequences. The method that we adopted to select a polyA site from multiple cleavage sites involve grouping the sites in a manner that each member of the group differ from its 5' neighbour by a maximum of 5 bases. Upon examination of the distance between the locations of the 5'-most and 3'-most member sites in every group, it is seen that such an 'inner group distance' is non-zero only in 25% instances of the observed groups. An inner group distance of = 5 bases is seen only in 8% of instances. It may be possible that the grouping process can be refined further. However, it is often seen that the member cleavage sites of a group have the signal motifs identified at same gene position; and that the distribution of distance between the representative cleavage site and polyA signal (Fig. 3) show the expected normal distribution (while the distribution for the raw members show a bi-modal distribution). It is possible that the 3' ends of mRNAs are marked, in addition to polyA signal and polyA tail, by regions with distinct nucleotide compositional biases [43]. It is expected that incorporation of such signatures and statistical approaches (such as the one implemented in [43]) will lead to improvements in our above-discussed methods.<br>
<br>
Core data and derived annotations<br>
The core of the generated data comprises the following components: (i) Genes and transcript data; (ii) introns/exons, polyA sites; (iii) isoform splice patterns, isoform transcript patterns, isoform peptide sequences; and (iii) alternative splice events, alternative polyadenylation events. Various value-added annotations are also generated, some of which are as described below.<br>
Preservation of splice events across species<br>
An important part of our pipeline is to generate evolutionary profiles of gene expression patterns. Methods based on the identification of conserved introns/exons and of conserved splice events [44] have been standardized to delineate pairs of human and mouse genes that are orthologous to each other. This data enables studies on evolutionary profiles of expression patterns.<br>
<br>
Association with data on genetic variation (SNP, single nucleotide polymorphism)<br>
We have developed methods (as documented in the <database>ATD</database> web pages) to delineate the allele specificity of observed alternative splice patterns.<br>
<br>
Derivation of peptide sequences coded by isoform splices patterns<br>
We have developed methods to delineate the amino acid sequence of the protein sequences encoded by the isoform splice patterns.<br>
<br>
<br>
Data access and query interfaces<br>
The data was generated as part of the European Project on <database>Alternate Transcript Diversity</database> (<database>ATD</database>). The data can be downloaded as flat files or queried through web interfaces. The web interface provides single-box query (where a user can search the database against a keyword or gene symbol or database cross-references) or a detailed query page that searches simultaneously both the human and mouse data or a query page that provides advanced searches to either human or mouse data.<br>
Genes can be queried by chromosomal location, gene names and synonyms, protein keywords, and database cross-references [such as <database>EMBL</database> and <database>UniProt</database> accession numbers [36,45], <database>HUGO</database> gene symbols [46], <database>Gene Ontology</database> identifiers [47] and protein identifiers], types of splice events, types of polyA signal, number of observed polyA sites, and types of variations among isoform transcript patterns (a pair of isoform transcript patterns may differ only in splicing or only in polyadenylation or in both). Queries can be selectively restricted to specific sets of gene entries, such as set of human-mouse orthologous gene pairs or set of gene entries for which data on isoform peptide sequences is available.<br>
<br>
Data presentation (textual &amp; graphical displays) and integration<br>
An output page resulting from a query to the database lists for every gene entry all the available database cross-references; an important aspect being hyperlinks to orthologous genes from other organisms (currently implemented for human and mouse).<br>
Observed PolyA sites and transcript patterns are presented in tabular forms (Fig. 4). Typical information on a polyA site includes gene locations of the cleavage site and of the polyA signal, the signal sequence, and hyperlink to a page that lists the EST/mRNA sequences that confirm the polyA site. Typical information on a transcript pattern includes exon-intron structure of the pattern, locations of the 'terminating' polyA site (along with that of the polyA signal), polyA sites that are skipped in the formation of the pattern, hyperlinks to pages that list EST/mRNA sequences that confirm the pattern.<br>
Observed introns &amp; exons are listed, and are hyperlinked to a page presenting data on EST/mRNA sequences that confirm these features. Observed splice patterns and events are listed (Fig. 5) and are hyperlinked to pages that list information on confirming EST/mRNA sequences. Typical information on a splice pattern includes hyperlinks to pages listing the coding information &amp; sequences of the isoform peptides, detailed exon-intron structures &amp; sequences of the isoform splice sequences, or listing the observed SNP positions and allele specificity. Typical information on a splice event includes information on the type of event, exon/intron feature that undergoes alternative splicing, hyperlink to a page giving details on the exon/intron features involved in the alternative splicing, or hyperlinks to the event in an orthologous gene from another species.<br>
Pattern viewers that give visual presentation of the observed isoform splice pattern structures and of the observed transcript pattern structures are provided. An example of transcript pattern view is presented in Fig. 6A. Each element of the pattern such as exon/intron/polyA site and the pattern as such is hyperlinked to pages giving detailed information (including nucleotide sequence, and detected signals).<br>
The <software>AltTrans</software> data has been integrated with the Ensembl genome annotation project and is visible as DAS (Distributed Annotation System) tracks from the gene view and contigview pages in <software>Ensembl genome browser</software> (Fig. 6B).<br>
<br>
<br>
Conclusion<br>
We present here an integrated data set of transcript-confirmed introns/exons, polyA sites, isoform splice patterns, isoform transcript sequences, isoform peptide sequences, alternative splice events, and alternative polyadenylation events. The data is presented for both mouse and human. Future work will aim to annotate the alternate transcripts for transcription start sites and their variants. In its future extension, this work should ultimately present high quality data on full-length transcript patterns annotated for transcription start site, splice sites, and polyadenylation sites; with each of these individual signals annotated for variations and for biological characteristics such as regulatory motifs and evolutionary profile.<br>
<br>
Availability and requirements<br>
Release 1 of the integrated <software>AltTrans</software> data, presented in this manuscript, is available from . Enquiries on accessing the data can be mailed to asd-ebi@ebi.ac.uk.<br>
<br>
Abbreviations<br>
TS: transcription start; FTP: file transfer protocol; EST: expressed sequence tag; mRNA: messenger RNA; cDNA ? copy DNA; polyA: polyadenylation; PAC: polyadenylation cleavage; PAS: polyadenylation signal; <database>dbEST</database>: <database>database of Expressed Sequence Tags</database>; <database>H-Inv</database>: <database>Human-Invitational Database</database>; <database>FANTOM</database>: <database>Functional Annotation of the mouse</database>; <software>BLAST</software>: <software>Basic Local Alignment Search Tool</software>; <database>ASD</database>: <database>Alternative Splicing Database</database>; <database>AEdb</database> <database>Alternative Exon Database</database>; <database>ATD</database>: <database>Alternate Transcript Diversity Database</database>; <software>SAGE</software>: <software>Serial Analysis of Gene Expression</software>; RT-PCR: reverse transcription-polymerase chain reaction; <database>UniProt</database>: <database>Universal Protein Resource</database>.<br>
<br>
Authors' contributions<br>
DG is responsible for formulating the <software>AltPAS</software> pipeline. TAT is responsible for formulating and developing the methods for the   <software>AltTrans</software>, <software>AltSplice</software>, the data integration pipelines, the annotation   modules, and the database &amp; query interfaces. TAT has written the manuscript and DG has contributed to the drafting process. TAT headed the team at EBI. VLT has developed the software code for the database &amp; interfaces and   for the annotation module of SNP-mediated splicing. JR has developed the software code for the <software>AltTrans</software>, <software>AltSplice</software> and the data integration pipelines. VK has developed the software code for the module of human-mouse conservation. CG has developed the software code for the module of deriving data on protein isoforms. FL has developed the software code for the <software>AltPAS</software> pipeline.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1838436</b><br>
Predicting mostly disordered proteins by using structure-unknown protein data<br>
<br>
<br>
Background<br>
Various kingdoms of life appear to have proteins or protein segments that lack a folded structure [1-3]. These proteins and segments are thought to be intrinsically disordered structures providing essential biological functions [4-8], so predicting such disorder should help us understand protein functions. Disorder have been found in proteins involved in regulatory and signaling events [4,9-11] and may provide conformational flexibility that allows proteins to interact with several structurally different targets [5,12,13].<br>
Many studies have shown that the primary structure of disordered regions is distinct from that of structured regions [14], and this has encouraged the development of many prediction methods based on the amino acid sequence. <software>PONDR</software> [15], <software>GlobPlot</software> [16], <software>DISOPRED</software> [17,18], <software>VL3</software> [19], <software>DISEMBL</software> [20], <software>IUPred</software> [21], and <software>RONN</software> [22] predict the probability of any given residue being in a disordered region by using information about the amino acid sequences near that residue. A different approach predicts disorder by binary classification of amino acid sequences into mostly disordered sequences and mostly ordered sequences [23-25]. The former approach is based on the view that features of the local sequence are a more important than features of the whole structure.<br>
Two methods have been used for binary classification. Uversky et al. suggested that a mostly disordered protein sequence could be discriminated from an ordered one by plotting the average hydrophobicity of the residues in the sequence against the net charge of the sequence [23,24], and that method has been implemented as the web-based FoldIndex application [26]. Garbuzynskiy et al., on the other hand, classified proteins as ordered or disordered by estimating the number of contacts of the whole protein [25]. Both methods classify a target protein by using a linear discriminant function.<br>
Linear discriminant analysis, like other classification methods, infers a discriminant function that minimizes the misclassification of training data. The parameter optimization of the linear equation is therefore strongly influenced by the distribution of training data. In the prediction of protein disorder, the analysis depends on the protein sequences that are already known to be folded or unfolded. The training will be successful if the amount of training data is large enough to approximate the distribution of all protein sequences. If the quantity of training data is too small, however, the classification boundary overfits to a local cluster of protein structures.<br>
It is hard to find disordered proteins not only because protein structures are often determined by X-ray diffraction analysis and information about proteins that could not be crystallized for X-ray analysis is seldom reported but also because not every failure to crystallize is due to disorder. Previous studies have estimated the proportion of disordered proteins in various genomes. <software>PONDR</software> estimated that 60% of eukaryotic proteins and 28% of bacterial proteins include disordered regions more than 40 residues long [1,27]. <software>DISOPRED2</software>, on the other hand, estimated that 33.0% of eukaryotic proteins and 4.2% of bacterial proteins include disordered regions longer than 30 residues and estimated that no more than 0.5% of the sequences in the <database>Protein Data Bank</database> (<database>PDB</database>) include disorder regions longer than 30 residues [2]. Despite the appreciable frequency of disordered proteins, the sequences of few mostly disordered proteins are publicly available. Although the current version (release 3.3) of <database>DisProt</database> [28], which is a public database providing information about disordered proteins, provides the sequences of 458 proteins, only 82 of those proteins are more than 70% disordered.<br>
We therefore think that protein databases might be biased against disordered proteins. If there are a lot of unknown disordered proteins, the structural distribution of proteins in nature will differ from that of proteins whose structure has been determined experimentally. Since training classifiers on data biased in this respect neglects of the actual distribution of natural proteins, the discriminative boundary should be adjusted to compensate the sparseness of the training data. Semi-supervised learning has been gaining increasing attention for dealing with problems due to data sparseness. Conventional supervised-learning methods, including support vector machines (SVMs) and neural networks, use only labeled data when optimizing the parameters of the discriminant function. In binary classification, the labeled data is a set of samples each of which is known to be positive or negative. When semi-supervised learning builds a model to improve predictions, it takes into account not only the labeled data but also the unlabeled data by adapting to the distribution of unlabeled data.<br>
A huge amount of known-sequence data is available. <database>UniProt</database> (UniProt50 release 48.9), for example, which is a widely used database of protein sequences, contains 974,638 nonredundant proteins [29], many of which can be expected to include a lot of disorder. We therefore, think it efficient to utilize the information of structure-unknown proteins by using semi-supervised learning to avoid training data sparseness. And prediction that considers a robust model will provide a new indicator for protein disorder.<br>
In this study we developed a novel method for predicting disordered proteins by using Joachims' spectral graph transducer (SGT) [30], which is a binary classification algorithm based on semi-supervised learning. It constructs a k-nearest neighbor (kNN) graph with both labeled and unlabeled examples as vertices, and the edge weight between two vertices represents their similarity. If the graph is separated into two subgraphs, both labeled and unlabeled vertices are classified into two categories. The SGT takes into account both the prediction accuracy of labeled training data and the distribution of unlabeled data, because it cuts the kNN graph so as to minimize both the misclassification of labeled vertices and the sum of edges weights across the cut. We apply the SGT to the disorder prediction problem with structure-known sequences as labeled data and structure-unknown sequences, including query sequences, as unlabeled data. The proposed method can therefore be used for training both structure-known sequences and a huge amount of structure-unknown sequences, and it creates a model that incorporates a larger protein structural space. We examined how data with no structural information improves the prediction of disordered proteins and we compared the accuracy of the proposed SGT-based method with the accuracy of an SVMs-based method and the accuracies of two other previous methods. We compared this SGT-based binary-classification method with per-residue methods by comparing their predictions for both mostly disordered proteins and mostly ordered proteins. We also estimated the false positive rate when the proposed method was used for partially disordered proteins.<br>
<br>
Results and Discussion<br>
Effect of structure-unknown proteins on disorder prediction<br>
Since the SGT constructs a model on both labeled data and unlabeled data, the accuracy of its predictions is influenced structure-unknown sequences as well as structure-known sequences. Here we examine how structure-unknown sequences affect prediction accuracy.<br>
Does structure-unknown data increase prediction accuracy?<br>
We tested different quantities of unlabeled samples in order to find out whether structure-unknown sequences have a positive or negative effect.<br>
The SGT classifies unlabeled data as either a disordered protein or an ordered protein, so query sequences are also treated as unlabeled data. We tried to increase prediction accuracy by using as unlabeled data not only query sequences but also large numbers of structure-unknown protein sequences.<br>
To investigate the effect of structure-unknown sequences, we prepared different quantities of unlabeled samples that were added to query sequences. Each set of unlabeled samples (structure-unknown sequences) was chosen randomly from the <database>Swiss-Prot</database> database. We prepared 10 different datasets for each experiment in order to avoid sampling bias, and the results we obtained are shown in Figure 1. Note that the x-axis in Figure 1 does not include the number of query sequences, and the total number of unlabeled samples in these experiments was the sum of the number of query sequences and the number of proteins selected from the <database>Swiss-Prot</database> database.<br>
As shown in Figure 1, the maximum, minimum and average Matthews correlation coefficient (MCC) for the 10 datasets were highest in the experiment with 30,000 structure-unknown samples selected from the <database>Swiss-Prot</database> database. The computational cost of building and decomposing a kNN graph increases with the number of examples, so smaller numbers of examples are more practical with respect to computation time. Since the average MCC is almost the same for 30,000, 50,000 and 70,000 examples, 30,000 is the most practical number of examples as well as the one yielding the highest average MCC.<br>
The recent research reported that regions of predicted disorder were found to be conserved within a large number of protein families and domains [31]. The proposed method considers information about conserved regions through similarities among sequences. The results shown in Figure 1 indicate that the proposed method effetely utilized information about conservation of protein disorder.<br>
<br>
Curated data or uncurated data?<br>
To investigate whether classification accuracy is affected by the quality of unlabeled samples, we used unlabeled samples from three different databases: <database>Swiss-Prot</database>, <database>UniProt50</database> and <database>TrEMBL</database>. Each set of 30,000 or 70,000 unlabeled samples was chosen randomly, and we used 10 datasets from each database in order to avoid sampling bias. The results are listed in Table 1. <database>Swiss-Prot</database> outperformed <database>UniProt</database> and <database>TrEMBL</database>, which indicates the quality of unlabeled data is an important factor for prediction accuracy.<br>
<database>Swiss-Prot</database> is a reliable database, which is carefully organized by human curators. <database>TrEMBL</database> is a computer-annotated supplement to <database>Swiss-Prot</database>, which contains amino acid translations of all the EMBL nucleotide sequence entries, including sequences automatically predicted by gene-finding programs. <database>UniProt</database> consists of Swiss-Prot and TrEMBL. This means that <database>TrEMBL</database> and <database>UniProt</database> might include a lot of artificial translation of pseudogenes, which are not translated into proteins in vivo. Such databases containing noise sequences are inferred to have a background distribution distinct from that of native protein sequences, and this distinction would have a negative effect on prediction.<br>
<br>
Which similarity measurement is best?<br>
SGT divides a kNN graph into two subgraphs for binary classification. Since the edge weight of the graph represents the similarity of two vertices, a similarity measurement for two protein sequences has to be defined. We based predictions on three measurements (amino acid composition, composition of physicochemical properties and <software>BLAST</software> score) and examined which is best. The results obtained using 30,000 structure-unknown sequences are listed in Table 2. The amino acid composition yielded the best results (most discriminative predictions), and the <software>BLAST</software> score yielded the worst results (least discriminative predictions). Compositionally based similarity measurements were thus better for predicting dissimilar proteins than was motif or sequence similarity measurement.<br>
<br>
<br>
Comparison with previous methods<br>
We compared the proposed method with two previous methods: <software>FoldIndex</software> [23] and plotting hydrophobicity against the number of contacts (hydrophobicity-contactnumber plot) [25]. <software>FoldIndex</software> output was obtained from the web server provided by the Israel Structural Proteomics Center, and we implemented a method for hydrophobicity-contactnumber plot because no web server or tool was available. We optimized its parameters on the same data we used for training our SGT-based method. For the proposed method, amino acid composition similarity measurement and 30,000 structure-unknown samples were used. The results are listed in Table 3. The proposed method yielded a MCC 0.202 points greater than that obtained using <software>FoldIndex</software> and yielded a MCC 0.221 points greater than that obtained using the method calculating the number of hydrophobic residues in contact. We also show Receiver Operating Characteristic (ROC) curves in Figure 2.<br>
The proposed method has two advantages over previous methods. The first advantage is that it can construct a nonlinear classification boundary taking account of the background distribution of a large amount of protein sequences, which enables the classifier to avoid training data sparseness. The second advantage is that it uses more information than the previous methods do. <software>FoldIndex</software> and hydrophobicity-contactnumber plot postulate that a few physicochemical properties of target proteins are the main factor of disorder. Although this strategy provides a simple and clear indicator, it overlooks many disordered proteins because other complex factors are involved in protein disorder. Classification over a larger feature space should facilitate more accurate prediction. Simple indicators cannot express some features of disorder. Plotting average hydrophobicity against net charge or the number of residues in contact, for example, does not always reflect the sequence complexity, which is an important factor in the discrimination of disordered proteins [32]. If an amino acid that is used repeatedly has average hydrophobicity, net charge and contact number values, such information will remain hidden. Although the composition of the entire sequence cannot be used to distinguish the local sequence complexity, it reflects long-range complexities. We conjecture that the hydropobicity-vs-(net charge) and hydropobicity-vs-(contact number) feature spaces should be considered subsets of the amino-acid-composition feature space. The two advantages of the proposed method enable it to identify more disordered proteins. For the same false positive rate (5%), SGT found 23 disordered proteins that <software>FoldIndex</software> or plotting hydrophobicity against contact number did not find and found nine proteins that neither previous method found. And neither FoldIndex nor plotting hydrophobicity against contact number found four disordered proteins that SGT did not find (Figure 3).<br>
<br>
Comparison with support vector machines<br>
Many forms of biological data are classified using support vector machines, neural networks, or other types of traditional machine learning. These algorithms are supervised learning procedures where the classifier is trained on labeled data. Spectral Graph Transducer, which is associated with semi-supervised learning, differs from other forms of supervised learning in that it uses unlabeled data. We also tested SVMs with the same features we use in SGT in order to investigate whether semi-supervised learning with unlabeled sequences is effective for predicting disorder. We compared SGT with SVMs, which is known to be a powerful classifier and has been widely applied to biological data analyses, using amino acid compositions as the feature vector. The SVMs package <software>libSVM</software> was used; Performance of major three kernels (linear, polynomial, RBF) was compared, and RBF kernel, which gave the best result, was used. All parameters were tuned by grid-search. For SGT, 30,000 structure-unknown samples were used. These results are also shown in Table 3. SGT gave a MCC 0.07 points better than the SVMs did.<br>
Supervised learning methods, including SVMs, are especially sensitive to the training data distribution when the given data set is a small one. Therefore, if biased data are provided, the predictive tendency will differ even if predictions are made using the same data. We compared the predictive tendencies with different training data as follows: (1) evaluation data were divided into three groups (Data-A, Data-B and Data-C); (2) each datum was predicted using a classifier trained on different data; and (3) the correlation coefficient between the two results was calculated. (e.g., a classifier trained on Data-A classifies all sequences from Data-C, another classifier trained on Data-B also classifies all sequences of Data-C. and then the coefficient of the correlation between two results is calculated). The average correlation coefficient for SGT was 0.14 higher than that for the SVMs (Table 4). An SGT-based method, which uses a huge number of unlabeled samples, makes prediction robust with regard to training data sparseness. This result indicates that SGT prediction is less affected by training data bias and provides accurate predictions even with a poor data set. Experimentally determined protein structures can potentially bias the data set. Previous research has shown that discriminating disorder from order is similar to finding the classification boundary between crystal structures and solution structures [14]. This is an unavoidable problem as long as a limited dataset is used, but distribution of structure-unknown data modified the training data bias.<br>
<br>
Comparison with per-residue predictors<br>
There are many studies in which the probability of any given residue being in a disordered region was predicted. Although the methods used in those studies are not directly comparable to our method, comparing the proposed method to per-residue predictors gives helpful information about the accuracy of the proposed method.<br>
We select four successful per-residue predictors for comparison: <software>VL3</software> [19], <software>GlobPlot</software> [16], <software>DISOPRED2</software> [18] and <software>IUPred</software> (long) [21]. The <software>VL3</software>, <software>GlobPlot</software> and <software>IUPred</software> (long) results were obtained from web servers, and the <software>DISOPRED2</software> results were obtained from a stand-alone program [33]. Detailed results are shown in Figure 4, which shows the results of two types of evaluation. In the graphs on the left side, showing results for mostly disordered proteins (at least 70% of their residues are disordered), the sensitivities of each per-residue predictor are plotted against the SGT scores. In the graphs on the right side, showing results for mostly ordered proteins (at least 95% of their residues are ordered), the false positive rates of each per-residue predictor are plotted against the SGT scores. The SGT gives each protein a score that shows how likely the protein is to be disordered. It assigns positive score when it predicts a query protein to be disordered. For example, if a point is plotted in the lower right portion of one of the graphs on the left, the proposed method can correctly classify the target sequence while the corresponding per-residue predictor cannot find a lot of disordered residues.<br>
Table 5 also compares per-residue sensitivity on mostly disordered proteins, sensitivity and specificity on mostly ordered proteins of the proposed method to those of per-residue predictors. When we evaluated SGT prediction, we regarded all residues of the target protein to be predicted to be disordered if the SGT assigned positive score to the protein. And we also regarded all residues of the target protein to be predicted to be ordered if the SGT assigned a negative score to the protein. (I.e., the sensitivity of the proposed method becomes 0.7 if the SGT score is positive and the query sequence includes 70 disordered residues and 30 ordered residues).<br>
<software>DISOPRED2</software> and <software>IUPred</software> have low false positive rates on mostly ordered proteins. <software>DISOPRED2</software> successfully predicts short disordered regions in mostly ordered proteins (proteins with an average disorder length of 2.47 residues per sequence), but does not detect 35.5% of the disordered regions in mostly disordered proteins (proteins with an average disorder length of 228.54 residues per sequence). <software>VL3</software>, on the other hand, successfully finds 78.2% of the disordered regions in mostly disordered proteins but produces a lot of false positives on mostly ordered proteins.<br>
These per-residue predictors try to find exact position of disorder by classifying a fixed window length to be disordered or ordered. Because their prediction thus concentrates on the local trend of disorder, they miss the global trend of disorder. And because a shorter window size includes less information, the local trend of disorder is more difficult to discriminate than the global trend of disorder. When a classification scheme such as neural networks or SVMs is used to determine a classification boundary between similar examples, there is inevitably a trade-off between getting a large number of false positives and getting a large number of true positives. This trade-off is strongly influenced by ratio of positive/negative training examples.<br>
Predicting disorder by classifying proteins as mostly disordered or mostly ordered is a rough approximation but has the advantage of detecting long disordered regions with a low false positive rate by neglecting short disordered regions. The proposed method predicts 83.4% of the disordered regions in mostly disordered proteins, and its false positive rate on mostly ordered proteins is only 0.9%. We therefore think insight is obtained by predict both disordered regions and disordered proteins, since a region-based-prediction provides information local trends of disorder while a protein-based-prediction gives information about large-area trends of disorder.<br>
<br>
Evaluation on partially disordered proteins<br>
Here we describe the results of prediction on partially disordered proteins. Not all proteins are mostly disordered or mostly ordered and many are partially disordered. Evaluating our method on partially disordered proteins gave us practical information we could use for estimating the false positive rates that would occur when it is used for large-scale genome analysis. As shown in Table 6, the proposed method is insensitive for the partially (5?20%) disordered proteins, although the method can predict that 16.67% of the moderately (20?40%) disordered proteins are disordered.<br>
<br>
Prediction of disordered proteins in large databases<br>
To provide illustrative examples of novel predictions made by our SGT-based method, we made predictions on several databases.<br>
Ward et al., using <software>DISOPRED2</software>, estimated 18.9% of eukaryotic genomes and 5.7% of bacterial genomes to be disordered and found long (&gt; 30 residues) disordered segments in 2.0% of archaean proteins, 4.2% of bacterial proteins and 33.0% of eukaryotic proteins [2]. Bogatyreva et al., evaluating the expected number of contacts, estimated that 12%, 3% and 2% of the proteins in eukaryotic, bacterial and archaean proteomes are totally disordered and that long (&gt; 41 residues) disordered segments occur in 16% of archaean proteins, 20% of bacterial proteins and 43% of eukaryotic proteins [34]. The proposed method predicts that an average of 4.14% of archaean proteins, 7.0% of bacterial proteins and 28.5% of eukaryotic proteins are mostly disordered. The frequencies estimated for 5 archaean, 14 bacterial and 5 eukaryotic genomes, in addition to the overall totals for each domain, are listed in Table 7. In line with the results of previous genome-wide analysis, eukaryotic genomes are predicted to code for much more disorder than prokaryotic genomes do. This is consistent with much experimental evidence that has shown that dynamic flexibility of the protein structure is more often related to eukaryotic protein function than to bacterial and archaean protein function [12].<br>
The proposed method also predicts that 15.46% of all sequences in the <database>Swiss-Prot</database> database are disordered. To investigate functional annotations of those sequences that were predicted to be disordered, we calculated the normalized ratio of annotated <database>GO</database> molecular function terms: R(T) = Rd(T)/Rs(T), where Rd(T) is the ratio of the proteins annotated by <database>GO</database> term T to all the proteins predicted to be disordered and Rs(T) is the ratio of the proteins annotated by <database>GO</database> term T to all the sequences in the <database>Swiss-Prot</database> database. The top 10 of the <database>GO</database> molecular function terms that describe more than 50 protein annotations, the 10 with the highest normalized ratios Rd(T) are listed in Figure 5. The proposed method was biased to find transcriptional-factor-related, RNA-binding-related and DNA-binding-related proteins to be disordered. Binding to nucleic acids requires interaction between the nucleic acid phosphate backbone and charged amino acids, which have a propensity for disorder (disorder propensities of amino acids and physiochemical properties are shown in Figures 6 and 7). Therefore it is not necessarily appropriate to suggest that all RNA-binding and DNA-binding proteins need dynamic flexibility, though previous papers have discussed the relation between disorder and proteins binding RNA and DNA [2,10,31,35]. The global analysis over large databases by the proposed method is an on-going study, and in the future we will use the proposed method to find new disordered proteins and will promote its use in further functional analysis of disordered proteins in collaboration with experimental laboratories.<br>
<br>
<br>
Conclusion<br>
In this study we proposed a semi-supervised learning approach for predicting disordered proteins. Disordered proteins are getting more and more attention because many of them are found to be functionally important. Few proteins however, are known to be disordered because information about a protein that could not be crystallized for X-ray analysis is seldom reported, even if the protein might be disordered. We therefore expect the distribution of disorder among proteins whose structure has been determined experimentally to differ from that of disorder among all natural proteins. Since the predictions made by previous methods are based on structure-known data, they are strongly affected by the bias for information about readily crystallized proteins. To avoid training data sparseness and to structure the hypothesis space based on the entire protein distribution, we have proposed a prediction method that uses Joachims' spectral graph transducer and is trained on both structure-known sequences and structure-unknown sequences.<br>
This method yielded MCCs 0.202 points higher than the MCC yielded by the method plotting hydrophobicity vs. net charge (FoldIndex) and 0.221 points higher than the MCC yielded by the method plotting hydrophobicity against the number of contacts. When the false positive rate was 5%, we found 23 disordered proteins that were not found using those previous methods.<br>
The proposed method predicts disorder by classifying proteins as either mostly disordered or mostly ordered. While such binary classification cannot detect partially disordered regions that per-residue predictors can find, it has the advantage of detecting long disordered regions by neglecting short disordered regions. When the proposed SGT-based method was compared with four per-residue predictors-<software>VL3</software>, <software>GlobPlot</software>, <software>DISOPRED2</software>, <software>IUPred</software> (long)-its sensitivity for disordered proteins was 0.834, which is 0.052?0.523 higher than that of the per-residue predictors and its specificity for ordered proteins was 0.991, which is 0.036?0.153 higher than that of the per-residue predictors. <br>
The main contribution of this paper is that it provides a method in which structure-unknown protein sequences are used to increase the accuracy with which disordered proteins can be predicted. We compared the results obtained using the proposed method with the results obtained using a SVMs-based method that used the same features that the proposed method used (the composition of 20 amino acids). The proposed method resulted in a MCC 0.07 points higher than the MCC obtained using the SVMs-based method. When it and the method using SVMs were trained on two different datasets and both methods were tested on a third dataset, it provided an average correlation coefficient that was 0.14 higher than that provided by the method using SVMs. The SGT-based prediction was less affected by training data sparseness and provided more accurate predictions when the data set was a poor one. These results provide convincing evidence for a positive effect of structure-unknown protein sequences, and our SGT-based method is therefore able to serve as a new indicator of disordered protein that considers the overall protein distribution in nature.<br>
<br>
Methods<br>
Materials<br>
Disordered proteins<br>
We downloaded the current version of <database>DisProt</database> (version 3.3) and extracted proteins having more than 70% disorder. Then we clustered those sequences by sequence similarity of 30% using <software>BASTclust</software>, and selected representative sequences. We thereby obtained 82 sequences.<br>
<br>
Ordered proteins<br>
The data was prepared according to the following protocol. Complex proteins were excluded because their folded regions are possible to be unfolded on a single state. Because X-ray crystallographic analysis induces artifactually missing residues, high-quality data and well-refined data were selected in steps (2) and (3). <software>BLASTclust</software> was used for task (6).<br>
1. Extract single-chain proteins from <database>Protein Data Bank</database>.<br>
2. Extract data that has a resolution better than 2 ? and an observed R-factor less than 0.2.<br>
3. Extract data determined by a newer version than Refmac5, SHELXL97 or CNS.<br>
4. Extract proteins that are more than 95% ordered.<br>
5. Exclude proteins that show disorder in the central area (between the 10th residue from the N-terminal end and the 10th residue from the C-terminal end).<br>
6. Choose a representative sequence with 30% similarity to avoid redundancy.<br>
We thereby obtained 526 sequences.<br>
<br>
Unlabeled proteins<br>
We used <database>Uniprot50</database> (downloaded on 12 Jan 2006: 974,638 sequences), <database>Swiss-Prot</database> (release 48.9: 206,586 sequences), and <database>TrEMBL</database> (downloaded on 2 Feb 2006: 2,586,884 sequences). Short sequences tend to have a biased amino acid composition, which adversely affects prediction. We therefore excluded sequences shorter than 30 residues when it is used for semi-supervised training of SGT.<br>
<br>
Partially disordered proteins<br>
the data were prepared according to the following protocol. Complex proteins were excluded because their folded regions are possible to be unfolded on a single state. Because X-ray crystallographic analysis induces artifactually missing residues, high-quality data and well-refined data were selected in steps (2) and (3).<br>
1. Extract single-chain proteins from <database>Protein Data Bank</database>.<br>
2. Extract data that has a resolution better than 2 ? and an observed R-factor less than 0.2.<br>
3. Extract data determined by a newer version than Refmac5, SHELXL97 or CNS.<br>
4. Extract proteins that include more than 5% disorder.<br>
We thereby obtained 417 sequences.<br>
<br>
Protein sequences of 24 genomes<br>
We used proteins sequences for 5 archaean, 14 bacterial and 5 eukaryotic genomes that were downloaded on 18 August 2006 from the NCBI ftp server.<br>
<br>
<br>
Spectral Graph Transducer<br>
The spectral graph transducer (SGT) is a powerful binary classification algorithm that was developed by Joachims [30]. It is based on semi-supervised learning, which for training makes use of not only labeled data (for which the answer is known) but also unlabeled data (for which the answer is unknown). This type of learning method often improves the prediction accuracy obtained when only a small amount of labeled data is available.<br>
A goal of the classifier is to assign a label (either +1 or -1) to unlabeled examples. The SGT takes into account the information of unlabeled data by using a graph composed of both labeled data and unlabeled data. Given a set of labeled examples L = l0,...,lm and unlabeled examples U = u0,...,un, the SGT constructs a k-nearest-neighbor graph G with X = {U, L} as vertices. The graph G has n + m vertices, and edge weights between the vertices represent the similarity of the neighboring examples. The SGT assigns a label (either +1 or -1) to U by dividing G into two subgraphs G+ and G- (?ui ? G+ are assigned +1, ?ui ? G- are assigned -1) That is, G+ and G - define a cut in the graph. The SGT chooses the cut so that it provides a small training error (i.e., an li that is labeled +1 should belong to G+), has a low cut cost (i.e., it minimizes the sum of the edge weights across the cut) and makes the ratio of positive examples to negative examples in U the same as it is in L. This strategy is implemented by minimizing:<br>
miny?y?T(B?A)y?+c(y????)TC(y????)<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaafaqabeqacaaabaacbiGae8xBa0Mae8xAaKMae8NBa42aaSbaaSqaaiqbdMha5zaalaaabeaaaOqaaiqbdMha5zaalaWaaWbaaSqabeaacqWGubavaaGccqGGOaakcqWGcbGqcqGHsislcqWGbbqqcqGGPaqkcuWG5bqEgaWcaiabgUcaRiabdogaJjabcIcaOiqbdMha5zaalaGaeyOeI0ccciGaf43SdCMbaSaacqGGPaqkdaahaaWcbeqaaiabdsfaubaakiabdoeadjabcIcaOiqbdMha5zaalaGaeyOeI0Iaf43SdCMbaSaacqGGPaqkaaaaaa@4C49@<br>
s.t.y?T1=0andy?Ty?=n+m,<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaafaqabeqaeaaaaeaacqqGZbWCcqGGUaGlcqqG0baDcqGGUaGlaeaacuWG5bqEgaWcamaaCaaaleqabaGaemivaqfaaOGaeGymaeJaeyypa0JaeGimaadabaGaeeyyaeMaeeOBa4MaeeizaqgabaGafmyEaKNbaSaadaahaaWcbeqaaiabdsfaubaakiqbdMha5zaalaGaeyypa0JaemOBa4Maey4kaSIaemyBa0MaeiilaWcaaaaa@4547@<br>
where<br>
Aij=wij?k?kNN(xi)wik,Bii=?jAijBij=0(i?j)<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaafaqabeqadaaabaGaemyqae0aaSbaaSqaaiabdMgaPjabdQgaQbqabaGccqGH9aqpdaWcaaqaaiabdEha3naaBaaaleaacqWGPbqAcqWGQbGAaeqaaaGcbaWaaabeaeaacqWG3bWDdaWgaaWcbaGaemyAaKMaem4AaSgabeaaaeaacqWGRbWAcqGHiiIZcqqGRbWAcqqGobGtcqqGobGtcqGGOaakcqWG4baEdaWgaaadbaGaemyAaKgabeaaliabcMcaPaqab0GaeyyeIuoaaaGccqGGSaalaeaacqWGcbGqdaWgaaWcbaGaemyAaKMaemyAaKgabeaakiabg2da9maaqafabaGaemyqae0aaSbaaSqaaiabdMgaPjabdQgaQbqabaaabaGaemOAaOgabeqdcqGHris5aaGcbaGaemOqai0aaSbaaSqaaiabdMgaPjabdQgaQbqabaGccqGH9aqpcqaIWaamcqGGOaakcqWGPbqAcqGHGjsUcqWGQbGAcqGGPaqkaaaaaa@611F@<br>
and yi is the prediction score of xi. If yi &gt; 0, +1 is assigned to xi. The term ?i is the penalty if xi ? L is misclassified. Therefore ?i is positive for xi ? L+, negative for xi ? L- and 0 for xi ? U. The c is a parameter that trades off training error against cut cost, and C is a diagonal cost matrix that allows different misclassification costs for each example.<br>
The spectral graph transducer outperforms other semi-supervised learning methods in many benchmark datasets [30]. We used the SGT package <software>SGTlight</software> in our experiments. Since classification accuracy is little affected by changing the two parameters c (trade-off of wrongly classifying training data) and d (number of eigenvectors) [30], we used c = 10,000 and d = 100. For the number of nearest neighbors, we tried k = 80, 90,...,120 and selected the one giving the best results (k = 100).<br>
<br>
Similarity of two sequences<br>
Because SGT is a kNN-based algorithm, the similarity of two sequences must be defined. We used three types of similarity measurement:<br>
1. Amino acid composition<br>
The amino acid composition is a basic property of proteins. Figure 6 shows the disorder propensities of 20 amino acids. A vector with 20 elements for the amino acid composition was used to calculate cosign.<br>
<br>
2. Composition of physicochemical properties<br>
Previous work has shown that composition alone is sufficient to recognize disorder accurately. Even a reduced alphabet of amino acids is useful for accurate prediction [36]. Figure 7 shows propensity for disorder of 10 physicochemical properties. We used a vector having 10 elements for physicochemical properties to calculate cosign. The binary definition of the physicochemical features is according to Zvelebil et al. [37].<br>
<br>
3. Sequence similarity<br>
Both of the two similarity measurements described above are based on compositional biases of amino acids. We also proposed a measurement based on sequence similarity or local motif. Top k raw score of <software>BLAST</software> search are used as similarity score between query sequence and database sequences for constructing kNN graph. The database of the <software>BLAST</software> search consists of training sequences.<br>
<br>
<br>
Evaluation<br>
We used five-fold cross validation for our experimental evaluation as follows.<br>
1. We separate evaluation data (608 sequences) into five data sets and selected one (e.g., 121 or 122 sequences) for test data. The rest of the data (486 or 487 sequences) was used as training data.<br>
2. Labels of test data are hidden.<br>
3. Construct k-NN graph using training data, test data and proteins which are selected from <database>Swiss-Prot</database>.<br>
4. Separate the k-NN graph into two (disordered or ordered) for prediction. Each sequence of unlabeld data (test data and proteins which are selected from <database>Swiss-Prot</database>) was classified as disordered or ordered.<br>
5. We evaluate the precision of test data.<br>
Steps 1?5 were repeated five times with different training data and test data.<br>
Sensitivity (tp/(tp + fn)), specificity (tn/(tn + fp)), two-state accuracy ((tp + tn)/(tp + tn + fp + fn)), false positive rate (fp/(tn + fp)) and the Matthews correlation coefficient (MCC) were used for the evaluation. Because sensitivity and specificity are trade-off criteria, we needed a balancing criterion for the MCC. This criterion was calculated as<br>
(tn?tp)?(fn?fp)(tp+fp)?(tn+fn)?(tp+fn)?(tn+fp),<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaadaWcaaqaaiabcIcaOiabdsha0jabd6gaUjabgEHiQiabdsha0jabdchaWjabcMcaPiabgkHiTiabcIcaOiabdAgaMjabd6gaUjabgEHiQiabdAgaMjabdchaWjabcMcaPaqaamaakaaabaGaeiikaGIaemiDaqNaemiCaaNaey4kaSIaemOzayMaemiCaaNaeiykaKIaey4fIOIaeiikaGIaemiDaqNaemOBa4Maey4kaSIaemOzayMaemOBa4MaeiykaKIaey4fIOIaeiikaGIaemiDaqNaemiCaaNaey4kaSIaemOzayMaemOBa4MaeiykaKIaey4fIOIaeiikaGIaemiDaqNaemOBa4Maey4kaSIaemOzayMaemiCaaNaeiykaKcaleqaaaaakiabcYcaSaaa@6285@<br>
where tp is the number of true positives, tn the number of true negatives, fp the number of false positives and fn the number of false negatives.<br>
<br>
<br>
Availability<br>
Project Name: <software>POODLE-W</software><br>
Project Home Page: <br>
Operating Systems: <software>POODLE-W</software> is a web application that can be accessed from any OS.<br>
Programming languages: C++, Perl(for CGI programming).<br>
Restrictions to use by non-academics: none.<br>
<br>
Authors' contributions<br>
KS designed the methodology, developed the programs, implemented the experiments and did most of the writing under the guide of YM. SH and KT provided helpful insight in experiment and discussion. TN initiated the project. All authors contributed to the final version of the manuscript and approved it.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1978211</b><br>
Combination of scoring schemes for protein docking<br>
<br>
<br>
Background<br>
Protein-protein interactions and complex formation play a central role in a broad range of biological processes, including hormone-receptor binding, protease inhibition, antibody-antigen interaction and signal transduction [1]. As structural genomics projects proceed, we are confronted with an increasing number of proteins with a characterised 3D structure but without a known function. To identify how two proteins are interacting will be particularly important for elucidating functions and designing inhibitors [2]. Although predicting around 50 percent false positive interactions [3], high throughput interaction discovery methods, such as the yeast two hybrid system, suggest thousands of protein-protein interactions and therefore also imply that a large fraction of all proteins interact with other proteins [4].<br>
Since many biological interactions occur in transient complexes whose structures often cannot be determined experimentally, it is important to develop computational docking methods which can predict the structure of complexes with a proper accuracy [5].<br>
Docking algorithms are developed to predict in which orientation two proteins are likely to bind under natural conditions. They can be split in a sampling step followed by a scoring step. A collection of putative structural complexes is generated by scanning the full conformational space in the first step, taking only geometric complementarity in consideration. Afterwards the putative complexes are ranked according to scoring functions based on chemical and additional aspects of geometrical complementarity.<br>
For the ranking and scoring of potential complex structures we previously published a method based on optimised amio acid specific weighting factors [6]. With the optimised weighting factors a weighted geometric correlation is calculated using a grid representation for the proteins. More than 90% of all near-native structures for the enzyme-inhibitor complexes are found within the top 10% of the ranked output after rescoring with the optimised grid values. For all three complex-classes (antibody-antigen, enzyme-inhibitor and 'others') the number of near-native complex structures (RMSiC? &lt; 5 ?) within the top 100 ranks increased by a factor of 3?5.<br>
The optimised parameters comply partially with known properties of amino acids in protein complex interfaces. Amino acids for which the optimisation produced very low weighting factors are likely to cause clashes in unbound docking especially for the near-native structures. They would be misleading for the docking of unbound proteins. The lowest values (~0) were assigned to the flexible polar amino acids such as ARG, ASP, GLN, GLU or LYS, which also have a very low interface propensity [7]. For the the aromatic residues high values were obtained which comply with the ability of their ring-systems to form ?-stacks and with their high propensity [7] to be in interface regions, together with the rigidity of the aromatic ring system.<br>
Neuvirth et al. [8] reported different interface propensities for different atom-types, and there are specific atoms which play a crucial role in interactions, e.g. by their ability to participate in H-bonds. To take these phenomena into account, we also optimised atom specific weighting factors for the scoring of complex structures and evaluated their ability to identifiy near-native structures. Furthermore the combination of the amino acid specfic scoring with the atom specific scoring is evaluated.<br>
There are other scoring functions available, which consider atoms for the scoring of predicted protein-protein complex structures, especially such that are based on knowledge-based atom-atom potentials (e.g. [9-11]). There are two main differences between this approach and atom-atom contact potentials. The parameters optimised here are derived from complex structures docked with unbound proteins whilst most atom-atom potentials were derived from native structures. Deriving the parameters from unbound structures enables us to consider atom-specific clash probabilities. This became already obvious from the optimised amino acid specific parameters where the flexibility of the amino acids influenced the weighting factors [6]. The other difference is that there is only one weighting factor for each atom type, being independent from the atom type it is in contact with. The advantage here is that wrong conformations of side chains as they appear often in rigid body unbound docking do not necessary result in loosing the contribution of these atoms towards the score.<br>
As already described for the optimisation of the amino acid specific factors, we optimised the atom specific weighting factors for antibody-antigen, enzyme-inhibitor and 'other' complexes following the classification of the docking-benchmark2.0 [12]. Since the optimisation of a factor for each atom-type being present in proteins would have exceeded our computational resources we used the well established atom classification system by Melo et al. [13] consisting of 40 distinct atom types. The optimisation was accomplished using the nonlinear minimisation method (nlm) from the R-package for statistical computing [14].<br>
In parallel to the development of the optimised weighting factors a very successfull comprehensive SVM-based scoring function was developed in our group and is described elsewhere (Martin O. and Schomburg D.; Efficient Comprehensive Scoring of Docked Protein Complexes using Probabilistic Support Vector Machines; submitted 2007) [15]. For this scoring function a support vector machine was trained to combine several scoring functions which were described to be able to identify near-native complexes (e.g. specialised energy functions, evolutionary relationship, class specific residue interface propensities, gap volume, buried surface area, empiric pair potentials on residue and atom level as well as measures for the tightness of fit). The application of the SVM-based scoring function leads to a remarkable improvement of the prediction quality as shown in table 2.<br>
However, there is no factor included in this scoring function which directly describes the geometric fit of the two binding proteins. Thus we also show the results of the combination of the SVM-based scoring function with the scoring based on the weighted geometric correlation.<br>
<br>
Results<br>
Atom specific weighting factors<br>
The weighting factor for each atom class and the estimated value for all cells of the interior of the receptor obtained by the optimisation are shown for the three different complex classes in the supplementary material (additional file 1: supp_table_atm_factors.pdf). For visualisation purposes the weighting factors were divided in 4 classes (very low: 0?1, low: 1?5, high: 5?10 and very high: &gt;10) and mapped in different colours on the corresponding 2-D amino acid structures (figure 1 for enzyme-inhibitor and antibody-antigen complexes and a figure for the 'other' complexes as additional file 2: supp_atm_factors_oth.pdf).<br>
The atoms of the backbone were assigned very low (~0) values for all three complex classes except for the oxygens in antibody-antigen complexes (1.28). Atoms that are part of aromatic ring-systems were allocated high to very high values in all three classes.<br>
For most of the other atom types the obtained parameters differ between the three complex classes. For antibody-antigen and for the 'other' complexes especially atoms that can be part of a hydrogen bond got higher values, while atoms that mainly contribute to the shape of the interface, like the methylene groups of the longer side-chains got low and very low values.<br>
Especially for those atoms which are only present in the short side chains of ILE, LEU and VAL the optimisation yielded higher values for enzyme-inhibitor complexes than for the antibody-antigen complexes.<br>
The values optimised for the interior of the receptor (I1) slightly differ for the three classes. For enzyme-inhibitor complexes the value is 0.67, for antibody-antigen complexes -0.87 and for the 'other' complexes 0.70.<br>
<br>
Improvement of prediction quality<br>
The results of the application of the atom specific weighting factors is shown in table 1 and in figure 2. Figure 2 illustrates the strong enrichment of near native structures within the top 10% of the sorted prediction and in table 1 the number of complexes is shown for which a near native structure is found on the first rank and within the top10, top50, and top100 ranks. Furthermore the average rank for the first near-native structure (RMSD of the interface C? atoms below 5 ?) for each complex is given. Table 1 and figure 2 both show the improvement of prediction quality, compared to the results obtained from a purely geometric ranking, for the reranking based on the previously published amino acid specific weighting factors, for the atom specific weighting factors, and for a ranking based on the arithmetic mean of amino acid and atom specific scores.<br>
The results achieved by the application of the atom specific weighting factors are comparable to those results which can be achieved with the amino acid specific scoring. Depending on the way of quality measurement the atom specific reranking is slightly better than the amino acid specific weighting. Especially the number of complexes for which a near native structure can be found on the top ranks (table 1) is higher with the atom specific factors and the average rank of the first near native structure is considerably lower.<br>
For the four antibody-antigen and for the four 'other' complexes in the validation set the performance of the weighted scoring is worse than the purely geometric scoring with respect to the number of complexes with a near native prediction on the top ranks. However for both of them the average rank of the first near native structure is also considerably lower with the atom specific scoring than with the amino acid specific scoring.<br>
The results obtained by a scoring with the mean of amino acid and atom specific scores are nearly identical to those which can be obtained by the atom specific scoring. Since the enrichment of near native structures on the lower ranks (figure 1) is slightly higher with the combination of both scoring schemes this combination is used for further scorings.<br>
Figure 3 shows the enrichment of near-native structures due to a ranking with the combined atom and amino acid specific weighting factors in comparison to 5 other scoring functions used to rank potential protein complex structures.<br>
Table 2 shows the dramatic improvement of prediction quality due to the combination of the scoring based on the weighted geometric score with the SVM based scoring by Martin [15]. The combination of both scorings further improves the results for all three complex classes. After the application of all scoring functions for 19 of the 22 enzyme-inhibitor complexes, for 12 of the 19 antibody-antigen complexes and for 12 of the 29 'other' complexes a near native structure can be found within the top 100 ranks. Altogether the percentage of complexes for which a near native structure can be predicted within the top 100 ranks increased from 14% with the geometric scoring to 54% with the combination of all scoring functions.!<br>
<br>
<br>
Discussion<br>
The combination of both scoring schemes ? the weighted geometric correlation and the comprehensive scoring function from Martin [15] ? yields a considerable improvement of prediction quality. Especially for the enzyme-inhibitor complexes the results of the ranking are excellent. For half of these complexes a near-native structure can be predicted within the first 10 proposed structures and for more than 86% of all enzyme-inhibitior complexes within the first 50 predicted structures. On average for more than 50% of all complexes a near-native structure is predicted within the top100 ranks.<br>
However, even though most previously described scoring schemes influence the final combined ranking it is not possible to predict all complex structures reliably. Especially for the 'other' complexes the results are not satisfying. For less than 50% of the 'other' complexes a near native structure can be found within the top 100 solutions after the reranking. The reason lies most likely within the inherent heterogeneity of the 'other' complexes (different function, evolutionary restraints, chemical environment, etc.). Furthermore there is no explicit treatment of flexibility included in the docking procedure yet. Even though the optimised weighting factors improve the results when the side chain flexibility is not taken into account, they are not capable to deal with any major movements of the backbone.<br>
The necessary different ways of combining the weighted geometric fit with the SVM scoring is based on the differing success of the SVM-based scoring for the three complex classes. While the SVM-based scoring is highly specific and sensitive for enzyme-inhibitior complexes and antibody-antigen complexes, the sensitivity and performance for the 'other' complexes is worse. Therefore the best combination of weighted geometric score and SVM-based score for enzyme-inhibitor and antibody-antigen complexes is to sort all structures with the same SVM-score again by the weighted geometrical score, while the best results for the 'other' complexes were obtained by multiplying both scores.<br>
The results obtained by the atom specific weighting factors are slightly better than the results obtained by the amino acid specific scoring, which is probably due to the more detailed resolution of an atom specific scoring.<br>
The obtained optimised weighting factors partially mirror the known role of different atom types in protein complex interfaces. Most remarkably are the very low values which were obtained for the backbone atoms. On the one hand these values comply with the low interface propensity for backbone atoms described by Neuvirth et al. [8] and on the other hand it is quite likely that true and false interface regions do not differ significantly with respect to the number of backbone contacts.<br>
For all three complex classes the values assigned to the atoms of the aromatic ring systems are rather high. As already discussed for the amino acid specific weighting factors [6], this can be explained with the high interface propensity of the aromatic residues [7,16], with their ability to form intermolecular ?-stacks and with their rigidity.<br>
The other values differ considerably for the different complex classes. Whilst for the enzyme-inhibitor complexes especially such atoms got higher values which are responsible for the overall shape of the interface and which are quite rigid, for the antibody-antigen and to a certain extend for the 'other' complexes those atoms which show some functionality, e.g. participating in H-bonding, were assigned higher values reflecting the higher importance of hydrogen bonds in the binding region of the antibody.<br>
The higher values for the shape-giving atoms, like the carbon atoms of the shorter side chains of ILE, LEU or VAL and the methylene groups of the longer side-chains, comply with the higher packing density of the enzyme-inhibitor complexes [17]. Whilst the higher values for the nitrogen and oxygen atoms of antibody-antigen complexes (e.g. in ASN, GLN, ASP, GLU, ARG, PRO and O within the backbone) can be explained by the importance of H-bonds and electrostatics for the binding of antigens and antibodies [18].<br>
For some of the atoms, especially those that appear in multiple residues (e.g. backbone atoms or side chain carbon atoms) a more detailed atom classification might yield better results. A finer classification might consider the amino acids or the distance to the backbone. Unfortunately our computational resources would not have been sufficient for the optimisation of a higher number of parameters.<br>
The values optimised for the interior cells of the larger protein are comparable to those optimised for the amino acid specific weighting factors [6]. The values for the enzyme-inhibitor and for the 'other' complexes are positive, while the value for the antibody antigen complexes is negative. The positive value for the enzyme-inhibitor complexes is explainable by the tight fit of the enzymes and their inhibitors. Any clashes which are present in the near-native structures are most likely caused by the flexibility of the proteins. All structures, which are part of the proposed structures for the reranking even though they have some clashes, must have an excellent overall geometrical correlation, which in turn is a property of the near-native structures.<br>
The validation of the obtained weighting factors shows that the optimised enzyme-inhibitor factors also work as expected for complexes which were not present during the optimisation process. Even though the number of antibody-antigen complexes having a near-native structure among the top 100 is lower after the reranking the weighting factors still work for these complexes, which becomes obvious from the considerably lower average rank of the first near-native structure.<br>
The results produced for the validation of the parameter for the 'other' complexes are not convincing, but the number of these complexes was too low for a reliable validation with respect to the heterogeneity of the 'other' complexes. Since the optimised parameters work reliably for the benchmark2.0 set docked with a 12? increment, where especially the false structures are different from the data used for the optimisation, we are still optimistic that these parameters will work for new 'other' complexes.<br>
In comparison with some of the other known scoring functions for protein complexes the weighted geometric correlation is able to enrich more near-native structures within the top 10% of the ranked prediction as shown in figure 3. The combination of SVM-based scoring and weighted geometric correlation more than doubles the percentage of near native structures which can be found in the 1st % of the ranking as compared to the other scoring functions.<br>
<br>
Conclusion<br>
We were able to develop a combination of different scoring schemes yielding a remarkable improvement of prediction quality. In the SVMs most previously described scoring criteria are included. The unsatisfactory results for some complexes, even though most ranking criteria are used, demonstrate again the importance of explicit treatment of flexibility for docking. However, the ranking schemes here should work with any grid based docking scheme.<br>
The docking procedure described in this and our previous papers will be available on the internet very soon, once we are finished with the optimisation of computer time.<br>
<br>
Methods<br>
The docking tool<br>
In a first step we used our docking tool <software>ckordo</software>[19,20] to generate potential complex structures for 83 unbound protein-protein complexes from the Benchmark 2.0 set [12]. <software>Ckordo</software> is a FFT based docking program including further docking arguments such as hydrophobicity and electrostatics. For this work we used only the geometric correlation calculated in Fourier space. For the calculation of the geometric correlation the unbound protein structures are mapped on a 3D grid. Numerical values are assigend to the grid cells depending on their location. Cells representing the surface of the receptor and all cells of the ligand get a value of one. The interior cells of the receptor get -6 (for a more detailed description see [6]). The geometric correlation was calculated with a rotation increment of 12? and a maximum cell size of 1.5 ?. For each rotation the five structures with the highest geometrical correlation were considered. This lead to 43,080 potential structures for each complex showing a reasonable geometric fit.<br>
Since the optimisation procedure (see below) is rather expensive with respect to memory and cpu-power, it was impossible to use 43,080 structures for each complex for the optimisation. An additional ckordo run was done using only 15? rotational increment leading to 22,000 structures per complex, which were used for the optimisation.<br>
Furthermore for each proposed structure the root mean square deviation of the C? atoms in the interface (RMSiC?) was calculated, in comparison to the unbound proteins fitted on the complex. For the RMSD calculation the C?-atoms were defined to be part of the interface if at least one atom of the other protein was within a distance of 10 ?. The fitting of the unbound proteins on the complex was done with CE [21].<br>
<br>
Optimisation<br>
In an extension of the described FFT-based docking procedure we replaced the standard numverical values by optimised atom-specific parameters.<br>
Since the total number of near-native structures produced by a ckordo run is very low compared to the number of incorrect ones, additional solutions with low RMS-values were produced by running ckordo for 1000 randomly chosen angles in the range from -10? to +10? around the correct rotation. From this run all proposed complex structures with RMSiC? &lt; 5 ? were selected. This resulted in up to 4,700 additional near-native solutions for each complex, which were added to the 22,000 structures calculated with rotational steps of 15?. If the ratio between the number of near-native solutions towards wrong ones is too low, the optimisation procedure would not be able to find the optimal parameters.<br>
For the optimisation process ckordo was modified so that for each proposed structure the number of contacts of each atom type with respect to being surface or interior was calculated. This results in one 40 ? 40 matrix for each structure for each possible contact type (surface_protein1 ? protein2, interior_protein1 ? protein2).<br>
For the optimisation we used the same procedure as for optimising the amino acid specific weighting factors [6]. The contact-matrices and the RMS-values for the complexes from the Benchmark 2.0 [12] were used for the optimisation procedure. For each complex class the optimisation was performed independently. The optimisation was done using the nonlinear minimisation method (nlm) from the R-package for statistical computing [14].<br>
The optimisation itself is a minimisation of the quadratic error between an objective function and the scores obtained. For the objective function all near-native structures (RMSiC?: 0?5 ?) were assigned a 100 times higher numerical value (10,000) as those showing a RMS value higher than 10 ?. For those structures between near-native and 'wrong' structures (RMSiC?: 5?10 ?) the target values were calculated by a linear function.<br>
The task for the optimisation was to find suitable weighting factors for each atom type, such that the calculated score for each potential complex structure gets as close as possible to the desired values (i.e. high for near-native structures and low for false structures).<br>
Since the inclusion of the value for the interior of the larger protein in the optimisation yielded better results for the amino acid specific weighting factors we also included this value here.<br>
The nonlinear minimisation function of the R-package[14] uses a Newton-type algorithm [22,23]. This method allows finding a minimum of a function by numerical computation of the derivatives. As a convergence criterion for the optimisation the default parameters were used.<br>
<br>
Validation<br>
Due to hardware limits it was impossible to use all available structures for the optimisation, so that subsets had to be chosen. To prove that several different subsets lead to similar results a 5-fold cross validation procedure was performed. Therefore the different complexes from each class were grouped randomly in 5 groups. The optimisation was run 5 times each time leaving out one of the groups and optimising with the remaining four. The final results were calculated using the average value of the five optimisations.<br>
Furthermore the effect of the obtained parameters was evaluated on 21 enzyme-inhibitor, 4 antibody-antigen and 4 'other' complexes from literature [24], which were not part of the training. These test-cases are identical to those used for validation in Heuser &amp; Schomburg 2006[6]. The docking procedure for these test cases was run with a rotation increment of 12? leading to 43080 potential structures for each complex. The evaluation was done with respect to the number of complexes which do have a near-native solution within the top ranks and to the number of near-native structures on the first ranks.<br>
In addition the ranking schemes were tested on the same complexes which were used for the optimisation, but docked with 12? increment. This leads to a different set of proposed structures. Especially the false structures are different to the false structures used for the training.<br>
The ability of the combined scoring of atom and amino acid specific weighting factors to enrich near native structures within the top 10% of the ranking is compared to 5 other scoring methods. The other scoring methods used are atomic contact energies (ACE) [25], a residue-residue potential [11], an atom-atom potential[9], the scoring function based on complex-class specific residue interface-propensity by Huang et al. [26] and the calculation of packing density. [26,27]<br>
<br>
Reranking<br>
For the reranking the common grid representation as used in ckordo is extended by the weighting factor. Each cell representing the protein gets a value assigned, which is the product of the value for being surface or interior and the optimised weighting factor. To obtain the weighted geometric correlation the values of the overlapping cells are multiplied in the same way as it is done for the calculation of the geometric correlation.<br>
For the combination of atom and amino acid specific scores a ranking based on the geometric and arithmetic mean of both scores was evaluated where the latter yielded better results.<br>
The combination of SVM-based scoring[15] and weighted geometric correlation was done in three different ways. On the one hand the ranking ability of the geometric and arithmetic mean of both scores was evaluated. On the other hand, since the SVM-based scoring classifies many of the potential structures for enzyme-inhibitor and antibody-antigen complexes as being near-native, it was tried to use the SVM-based ranking and rerank all those structures which obtained the same score from the SVM ranking once again with the weighted geometric correlation. The latter turned out to yield best results for enzyme-inhibitor and antibody-antigen complexes, while the arithmetic mean of atom specific, amino acid specific, and SVM based scores produced the best results for the 'other' complexes.<br>
<br>
<br>
Authors' contributions<br>
The described results are obtained by a combined effort of the two authors where PH did the actual computer work including programming and integration of the method whereas the method was developed and results were discussed in frequent discussions between the two authors. The original idea was from DS.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2268656</b><br>
High-throughput bioinformatics with the <software>Cyrille2</software> pipeline system<br>
<br>
<br>
Background<br>
Large-scale computational analysis of biomolecular data often involves the execution of multiple, interdependent operations on an input dataset. The software tools, models and databases that are used in this process need to be arranged in precise computational chains, where output of one analysis serves as the input of a subsequent analysis. Such chains are often referred to as pipelines or workflows. In formal terms, a pipeline can be defined as a graph that describes the order of, and mutual relationships between, the analyses to be performed on an input dataset. In a pipeline representation, an operation performed by a computational tool on input data is represented by a node. The connection between two nodes is represented by an edge and defines a stream of data in-between two analyses. An example of a simple computational pipeline representing part of a genome annotation process is depicted in Figure 1.<br>
Even for a small bioinformatics project with a few interdependent analyses, it is cumbersome to perform all operations manually. For larger projects, e.g. the annotation of a complete eukaryotic genome, which may require the use of dozens of interdependent tools, including gene prediction tools, homology searches against different databases, protein domain analyses and repeat discovery, this quickly becomes excessively complex. The annotation of a genome may require the use of dozens of interdependent tools, including gene prediction tools, homology searches against different databases, protein domain analyses and repeat discovery.<br>
Some of these tools may need to be executed up to tens of thousands of times. The scale and complexity of such computational analyses call for the use of dedicated pipeline software that enables the programming, execution and data-flow management of all analyses required.<br>
With primarily the development of a system for large-scale genome annotation in mind, we have defined the following operational requirements for the design of pipeline management software:<br>
High-throughput<br>
The system should be capable of handling large datasets, complex data analysis workflows and large numbers of jobs requiring long periods of processing time. To this end, the system must be able to employ a compute cluster.<br>
<br>
Ease-of-use<br>
In a high-throughput data production environment, it is important to have a pipeline system in place that is easy to use by non-expert end-users. This can be achieved by a well-designed graphical user interface (GUI) that allows easy and intuitive creation, adaptation, monitoring and administration of a pipeline.<br>
<br>
Flexible<br>
New or upgraded bioinformatics tools, models and databases appear frequently. To remain up-to-date, it is essential that employment of new or upgraded resources within the pipeline is straightforward. The system should therefore be modular and flexible, and able to accommodate complex data relationships required by some tools. Use of an open communication standard can help to achieve this and ensures the system is compatible with remote resources through the use of web services [1].<br>
<br>
Updates<br>
In ongoing projects it is often undesirable to postpone analysis until all data has been generated. Initial analysis must therefore be repeated on a regular basis, for example, when genome assemblies are updated or new reference data (i.e. <software>BLAST</software> databases) become available. Again, adequate data storage and tracking is important, allowing the pipeline operator to identify the affected parts of the pipeline to reschedule and re-execute only the affected parts of a pipeline with minimal redundancy.<br>
There are a number of pipeline systems publicly available, including <database>Ensembl</database> [2], <software>Pegasys</software> [3], <software>GPIPE</software> [4], <software>Taverna</software> [5], <software>Wildfire</software> [6], <software>MOWserv</software> [7], <software>Triana</software> [8] and <software>Kepler</software> [9]. We will not consider systems that are not publicly available (e.g. the NCBI pipeline). This article describes the development of a new pipeline system, <software>Cyrille2</software>. An obvious question is why we would want to develop yet another system? The answer is, in short, that the available systems do not sufficiently comply with the requirements outlined above. We have built the <software>Cyrille2</software> system to provide this distinct set of features. In the discussion we will extend the comparison with other systems in more detail.<br>
<br>
<br>
Implementation<br>
For a detailed description of the structural design and operation of the <software>Cyrille2</software> system several key terms must be defined. Table 1 provides the definitions of the most important terms used. Hereafter, we will start with a general overview of the <software>Cyrille2</software> design, followed by an explanation of how the data-flow is organized within the system. With this knowledge we will continue to describe the core software parts (user interface, Scheduler and Executor) and end with a description of pipeline operation.<br>
System overview<br>
The <software>Cyrille2</software> system architecture is composed of four distinct layers (Figure 2). Layer 1 comprises the main functional and core software components. These core components make extensive use of a modular application programming interface (API) (layer 2). The API allows unified access to three system databases (layer 3). The biological database and the end-user interface that connect to it are third-party systems that can be integrated with the <software>Cyrille2</software> system (layer 4). To allow tracking and debugging of a pipeline in operation, a centralized status and logging system is implemented. This provides a pipeline operator access to detailed information on the status of a pipeline run and errors that might have occurred.<br>
A pipeline system needs to manage and store large amounts of diverse information. To keep different types of data separated, the system employs four databases (Figure 2, layers 3 and 4): (1) the pipeline database which stores pipeline definitions, node settings and associated parameters; (2) the status database which stores the execution state of a pipeline at any given time, tracking all jobs and their respective in- and output.; (3) the biological database which stores and provides access to the results of all analyses and; (4) a failover database which employs a generic method to store all data that does not need to be stored in the biological database.<br>
Consider, as an illustration, the gene-prediction node (node 3) from the example pipeline given in Figure 1. The pipeline database identifies this node as a gene prediction node and stores all instructions (i.e. tool name and parameters settings) on how the gene prediction tool is to be executed on each input DNA sequence from the preceding Load sequence node (node 1). The status database stores information on which of the input sequences the gene prediction has been performed, which genes have been predicted and tracks all objects associated to this analysis in the biological database using unique object identifiers.<br>
Similar to the functional division of the databases, the core software is divided into three distinct functional parts: the Graphical User Interface (GUI), the Scheduler and the Executor (Figure 2, layer 1). The GUI allows a pipeline operator to create, adapt, start and stop pipeline runs and fine-tune pipeline and tool settings. A screenshot of a genome annotation pipeline created with the GUI is given in Figure 3. Additional series of screenshots showing the operation of a small pipeline using the GUI are given in Additional file 1.<br>
The Scheduler is the core of the <software>Cyrille2</software> system. It retrieves pipeline definitions from the pipeline database and schedules all jobs for execution, accounting for dependencies between nodes. A scheduled job is stored in the status database. Further details on Scheduler operation are given below. The Executor loops through all scheduled jobs and executes each of these. The results of each job are stored in the biological database and are tracked with unique object identifiers in the status database. If the number of jobs to be executed is large, a compute cluster is required to keep the total execution time within bounds. To this end the Executor acts as a broker between the <software>Cyrille2</software> system and third-party compute cluster software such as Sun Grid Engine (SGE). It is possible to employ multiple and different types of clusters by running multiple instances of the Executor.<br>
Whereas the <software>Cyrille2</software> pipeline software functionally consists of three separate parts, from a software implementation point of view, the essential application logic of the system is implemented using object oriented programming. The system implements different scheduling strategies in different node types (Figure 4). A new node type (i.e. a node class) can be created from scratch or it can inherit from an existing node class making the implementation of new functionality as easy as possible.<br>
<br>
Data flow and storage<br>
A major challenge for any pipeline system is to devise a fast and robust way to conduct data through a pipeline. This is not trivial, given that even a relatively simple pipeline (such as the one given in Figure 1) may imply that many thousands of separate jobs need to be scheduled and executed, which in turn may result in millions of objects.<br>
Automated execution of a pipeline implies that each node needs to hold information on the nature and format of the objects that enter and leave it and that it has to process such streams in a manner unique for each type of data. For example, a stream of DNA sequences is different from a stream containing <software>BLAST</software> reports. The issue is best approached using a uniform syntax and identification of the data transported in a pipeline. This does not only allow for a standardized implementation of the scheduling strategies of different node types, but also for a generic node interface description and for data tracking.<br>
Several data exchange formats, with varying scope, have been devised and proposed for the handling and communication of biomolecular data, including XML-based formats such as GAME (used by the <software>Apollo genome annotation curation tool</software> [10]) and <database>BioMOBY</database> [11], and flat-file formats such as GFF. An appropriate data exchange format identifies and communicates data in a uniform and unambiguous manner. Such a format must permit unique identification and classification, and it must be extensible to accommodate future incorporation of novel data types. With this in mind, we have chosen to implement <database>BioMOBY</database> [11] as data exchange format for the <software>Cyrille2</software> system. <database>BioMOBY</database> is emerging as an important data standard in bioinformatics and is already used by <software>MOWserv</software> [7] and <software>Taverna</software> [5] when this system is dealing with <database>BioMOBY</database> operations.<br>
The <database>BioMOBY</database> standard contains a specification on how to describe data types, formats, and analysis types. It is a meta-data format, meaning that it does not describe data but defines how to describe data. <database>BioMOBY</database> employs a system of object identification and classification, in which each <database>BioMOBY</database> object is identified with (1) an identification string (id), (2) an object type (articlename) and (3) a namespace. <database>BioMOBY</database> encompasses the description of web services and facilitates interoperability with third-party servers. In the current era of distributed computing, this ability to communicate with systems worldwide is becoming ever more important.<br>
Standardized object identification is also applicable in standardized data storage. With a <database>BioMOBY</database> object stored with a unique id (as provided by the <software>Cyrille2</software> system), articlename and namespace, these three values are sufficient to uniquely retrieve the object from a database. The <software>Cyrille2</software> system is designed to allow the use of different databases schemas and/or engines to ensure flexibility. A database wrapper functions as an intermediate between the object identity on the one hand and database-specific storage and retrieval of these objects on the other hand (Figure 5). The database wrapper contains specific instructions to store, retrieve and delete each different object type in the biological database. This solution combines the unique identification of any object with the freedom to use any database. The database wrapper currently implemented in our system is written for the <database>Generic Genome Browser</database> database schema using MySQL. Two other database schemes have been implemented for use in projects on miRNA discovery and comparative genomics.<br>
Storage of all intermediate data generated during pipeline operation is guaranteed by the failover database that automatically stores any object not stored in the biological database. This may happen for two reasons: firstly, an intermediate object might be of no importance to the end-user querying the biological database, or, secondly, the database wrapper fails to store the object to the biological database for an unexpected reason; in either case, no data is lost.<br>
Apart from transporting data in between separate nodes of the system, a pipeline system needs methods to upload new data into the system and retrieve the results afterwards. To upload data into the <software>Cyrille2</software> system, specific start nodes are provided allowing the upload of data through the user interface or automatically harvesting data from a file system. The resulting data of the pipeline is stored in a domain specific database, for example the <database>Generic Genome Browser</database> database [12], which is commonly used in genome annotation. The web interface is, in this case, a bonus that helps end-users to access the data.<br>
<br>
Scheduler<br>
The Scheduler is the core of the <software>Cyrille2</software> system. Based on a pipeline definition (from the pipeline database) it schedules all jobs for execution, taking mutual dependencies between nodes into account. Various tools used in an analysis pipeline require different arrangements of incoming data. For example, node 2 in Figure 1 uses all DNA sequences from node 1 to create a <software>BLAST</software> database. The Scheduler thus arranges all sequences to be processed by a single job. In contrast, node 4 processes each sequence separately in a <software>BLAST</software> analysis. In this case the Scheduler creates as many jobs as there are input sequences. This is illustrated in Figure 4, which shows the same pipeline as given in Figure 1, but now expanded with detailed information on the objects that are created and the jobs that are scheduled.<br>
Scheduler functionality is embedded in the node classes. This modular, object-oriented implementation of a node allows for complex scheduling strategies. A more complex node implemented in the <software>Cyrille2</software> system schedules groups of objects which share a common grandparent, for example, all repeats that are predicted by several different repeat detection tools, grouped per BAC sequence (the grandparent).<br>
<br>
Pipeline execution<br>
Execution of a pipeline can be considered at two levels: execution of a separate node, and execution of an entire pipeline. A single node in the <software>Cyrille2</software> system executes a variable number of distinct jobs. For example, a <software>BLAST</software> analysis of 10 input sequences requires a <software>BLAST</software> node to execute 10 <software>BLAST</software> jobs. A single <software>BLAST</software> job consists of several processing steps: retrieve the input sequence (in <database>BioMoby</database> format) from the biological database; export the sequence as a <fileFormat>FASTA</fileFormat> file; execute <software>BLAST</software> with the correct parameters; read and parse the resulting <software>BLAST</software> report to a <database>BioMOBY</database> representation; write the <database>BioMOBY</database> formatted <software>BLAST</software> report to the biological database, and; register the results in the status database.<br>
Node operation in <software>Cyrille2</software> is performed by executing three different scripts (Figure 6): (1) data is retrieved from the database; (2) the tool is executed, and; (3) the results are stored back in the database. Communication with the database is handled by two database connection scripts (steps 1 and 3, see also Figure 6: database-get and database-store), which is equivalent to the Ensembl <database>RunnableDB</database> [13]. These two scripts access the database wrapper (Figure 5) and provide generic communication with any database of choice.<br>
A tool wrapper is responsible for the execution of the tool and provides generic interaction with the <software>Cyrille2</software> system. Tool wrappers are implemented in such a way that they can run standalone, be part of a <database>BioMOBY</database> web service, or function as a component of the <software>Cyrille2</software> system. A tool wrapper is equivalent to a Runnable in the <software>Ensembl</software> system [13]. During execution, the tool wrapper is responsible for steps D, E and F from Figure 6.<br>
A further task of the tool wrapper is to register itself in the <software>Cyrille2</software> system. Registration implies that the tool becomes available through the GUI, allowing a pipeline operator to integrate it into a pipeline and allowing the Scheduler to correctly schedule jobs for that tool. The process communicates what type of objects are required as input (e.g. protein sequences for <software>BLASTP</software>), what parameters are accepted (e.g. specification of a protein database) and with what node type it must be associated. This is implemented in a generic registration method where the wrapper registers all required information into the pipeline database.<br>
In a rapidly evolving field like bioinformatics, it is of great importance that new tools can be implemented quickly. In the <software>Cyrille2</software> system this requirement is implemented through modular, object oriented, design of the tool wrapper code. In brief, implementation of a novel tool in the <software>Cyrille2</software> system involves the following procedure: (1) installation and configuration of the new tool on the execution server or cluster; (2) writing of the <database>BioMOBY</database>-compatible tool wrapper; (3) definition of new <database>BioMOBY</database> objects (if required); (4) confirmation of compatibility between object types and the biological database in use, and; (5) registration of the tool in the pipeline database.<br>
A complete pipeline operates by iteratively running the Scheduler and Executor. Results produced by a tool under control of the Executor can result in more jobs to be scheduled by the next Scheduler run. If there are no more jobs to be executed for a node and all its parents, it is flagged as finished in the status database. A complete pipeline is finished if all nodes are in the finished state. Pipeline iteration can be resumed after new data is uploaded into the pipeline, when a database has been updated (e.g. <software>BLAST</software> databases) or when the pipeline definition has changed. Resumption is accomplished by unflagging the finished state of one or more nodes in a pipeline. This is either done manually (through the GUI) or automatically, for example after a <software>BLAST</software> database update.<br>
<br>
Results<br>
Our local implementation of the <software>Cyrille2</software> system runs on a dedicated server (dual AMD Opteron 850, 4 Gb memory, 300 Gb disk) and has a 50 CPU, SGE based Linux compute cluster at its disposal. A list of third party tools currently wrapped in the <software>Cyrille2</software> system is provided [see Additional file 2].<br>
In a test run, the <software>Cyrille2</software> system analyzed 50 Arabidopsis BAC sequences (4.8 Mb) randomly downloaded from NCBI using the pipeline shown in Figure 3. The analyses resulted in over 735.000 objects created in over 10.000 different analyses executed. The results are summarized Additional file 3. Measurement of the pipeline execution time is not relevant as the bulk of the execution time results from executing the actual tools. As an illustration, however, the analysis of a single BAC with the pipeline from Figure 3 takes approximately an hour to complete on a Linux compute cluster with 50 CPUs (hardware specifications as above).<br>
The <software>Cyrille2</software> system is now used routinely for BAC annotation in two solanaceous genome sequencing projects in which our group is involved [14,15]. In addition, we run the system in a comparative genomics project of fungal genomes and a second project on the large-scale prediction of microRNAs in plant and animal genomes. These last two projects require very high data throughput and employ databases different from the <database>Generic Genome Browser</database> database and thus demonstrate the flexibility of the <software>Cyrille2</software> system and its ability to execute complex and computationally demanding pipelines.<br>
<br>
<br>
Discussion<br>
The <software>Cyrille2</software> system was developed with the aim of providing an automated, high-throughput, easy-to-use, flexible and extensible bioinformatics workflow management system. Among its most notable features are the implementation of a powerful job scheduler module, storage of intermediate data, compatibility with different database types for storage of biological data, a generic tool wrapper module, and uniform data transport and data tracking.<br>
Ease-of-use is achieved through implementation of an intuitive user interface with several layers of complexity. A pipeline operator can select from a predefined set of pipelines and nodes to perform complex data analysis tasks while an administrator is able to construct novel, and fine-tune existing, pipelines.<br>
High-throughput operation<br>
The major part of the development effort has been directed towards achieving flexibility and extensibility in architecture and high-throughput operation. In a high-throughput data analysis environment, parallel execution of jobs is important to optimally use the available computational facilities and hence, make pipeline calculation time as short as possible. This requires specific scheduling logic for different node types. The <software>Cyrille2</software> Scheduler prepares jobs for parallel execution as soon as results from preceding analyses become available. The single node type schedules a job immediately after an input object becomes available. This means that subsequent analyses can already start before the parent node is finished. Most pipeline systems implement a scheduling engine able to schedule jobs in parallel [3-9,12]. An important feature of <software>Cyrille2</software> is the modular implementation of the node class allowing a greater variety in scheduling strategies.<br>
Parallel scheduling requires parallel execution, which is controlled by the Executor. There are many solutions available for the distribution of jobs over a compute cluster, including Sun Grid Engine (SGE), Condor, OpenPBS and LSF. For the <software>Cyrille2</software> Executor, we have chosen to employ SGE, which is both stable and able to handle high loads. A port for Grid technologies such as <software>Condor</software> is under development and will allow the <software>Cyrille2</software> system to employ idle Windows desktops.<br>
Another important aspect in high-throughput pipeline analysis is the storage of intermediate results. If this is implemented, the pipeline system will be able to resume calculations close to the point where it may have stopped after a system failure. This feature becomes important when a pipeline requires a long execution time and hence, the chance of a failure, somewhere in the system, increases. If storage of intermediate data is undesirable, for example because of disproportional usage of storage capacity, it is straightforward to either develop a node type which embeds two or more other nodes and directly transfers the data between the nodes in a single Executor run, or to develop a single tool wrapper which executes both steps and behaves as a single tool in the system. In both cases, intermediate data storage is by-passed.<br>
A further advantage of intermediate data storage is that each part of a pipeline can be re-executed when necessary. This is essential when only part of a pipeline needs to be repeated with either different parameter settings, after a database update, or upon the addition of extra nodes to the pipeline. In the current implementation of <software>Cyrille2</software>, the system will remove, prior to a rerun, all data that is affected by the update from both the status and biological databases and rerun the necessary analyses. For example, consider the genome annotation pipeline of Figure 3. Prior to uploading a new version of a sequence, the pipeline operator will flag this sequence, instructing the system to delete all gene predictions and <software>BLAST</software> hits associated with that sequence. The system will subsequently perform only those analyses on which the changed sequence had an impact.<br>
We have encountered two severe problems during <software>Cyrille2</software> development, both related to system overhead: 1) <database>BioMOBY</database> XML parsing is very time-consuming for large data-sets, and; 2) SGE overhead becomes very large for nodes which execute a large amount of small jobs. The import and export of large XML files in general is notoriously slow. In <software>Cyrille2</software>, we have solved this problem by circumventing raw XML transport as much as possible. For example, conversion of the data between steps 1, 2 and 3 in the node depicted in Figure 6, from raw XML to an internal python object representation of the <database>BioMOBY</database> XML boosted the performance of this node significantly. Using serialized objects might, as a drawback, have an impact on backwards compatibility, specifically if objects are stored in the Failover database.<br>
The second major obstacle concerned the overhead involved in executing large numbers of jobs with very short computation time. If each of these jobs is scheduled separately on an SGE cluster, the overhead used by SGE considerably exceeds the time required for the execution itself. This overhead was significantly reduced by implementing a generic batch mechanism which is able to execute an arbitrary amount of pipeline jobs as a single SGE job.<br>
<br>
Flexibility<br>
In the rapidly evolving field of genome annotation, it is critical that a pipeline management system is flexible and easily extensible. The <software>Cyrille2</software> system was designed to allow future incorporation of novel tools, data types and databases in a generic fashion. For example, for a present-day genome annotation project, it is generally sufficient to store all relevant data in a biological database such as the <database>Generic Genome Browser</database> database [12]. However, if one would require the inclusion of data such as multiple alignments or 3D protein structures, a different database is required.<br>
The <software>Cyrille2</software> system is designed to make the addition of a novel object type or the complete change of the biological database as easy as possible. This is achieved by implementation of the database wrapper as a separate module. Addition of a novel data type can be done by adding a 'get', 'store' and 'delete' function for this type of data type to the database wrapper. To create a wrapper for another database schema, a new module must be written with a storage, retrieval and delete function for each object type. This mode of integration of a third-party database with a pipeline system is unique for the <software>Cyrille2</software> system. Many alternative systems do not use a database for storage of intermediate results, (<software>Taverna</software> [5]; <software>GPIPE</software> [4]; <software>Wildfire</software> [6]). Instead, these systems transport the output of one program directly to the next program and/or store intermediate results as flat files. Such an approach is unsuitable for large-scale data analysis. A database system is better suited to keep track of many, possibly millions, of intermediate objects and their mutual relationships. Moreover, a database is better adapted to distribute data in a heterogeneous environment. Other systems do employ a database for data storage (<software>Ensembl</software> [13]; <software>Pegasys</software> [3]; <software>MOWserv</software> [7]) but each of these is strongly linked to a specific database schema, thus limiting their flexibility.<br>
In the current implementation of <software>Cyrille2</software>, the <database>Generic Genome Browser</database> database [12] is used for storage of the genome annotation data with, as an obvious extra, the Generic Genome Browser allowing an end-user easy access to the annotations. Implementing <software>Cyrille2</software> in different projects, such as comparative genomics and miRNA prediction has proven the capability of <software>Cyrille2</software> to operate using different databases schemes.<br>
The use of <database>BioMOBY</database> as a communication standard combined with the storage of standardized object identifiers by the <software>Cyrille2</software> system ensures that any object can be handled and tracked by the system, including binary objects such as images [11]. Other advantages of using <database>BioMOBY</database> are that it ensures easy integration with the growing body of remote <database>BioMOBY</database> web services and optimal interconnectivity between nodes. At the same time, it is possible for external users to access the <software>Cyrille2</software> tools using any <software>BioMOBY</software> client if the tools and toolwrappers are placed, with some minor modifications, on a web server.<br>
Several systems employ specific embedded scripts to translate the output of one tool to the input of the next (<software>Wildfire</software>, [6]; <software>FIGENIX</software> [16]; <software>GPIPE</software> [4]) as opposed to using a standardized data format. Most analysis tools have a unique in- and output format and thus the number of unique translation steps grows quickly with the number of tools wrapped. This can be mitigated by using uniform data transport such as <database>BioMoby</database>, which is used in <software>Cyrille2</software>, <software>MOWserv</software> [7] and <software>Taverna</software> [5] when this system is dealing with BioMOBY operations. The <software>Ensembl</software> system employs a uniform Perl data structure to the same end [17].<br>
On a higher level, import and export of the description of a complete pipeline would improve flexibility by allowing the exchange of workflows with other systems. For example, a pipeline developed in Taverna could then be executed in <software>Cyrille2</software>, or vice versa. There are several candidate languages available such as Scufl, used by <software>Taverna</software>, or the more widely accepted BPEL [18]. Research is necessary to see if the adaptation of such a standard would be worthwhile for the <software>Cyrille2</software> system, both to see if any candidate provides sufficient flexibility as to assess the amount of work necessary to implement such a standard.<br>
<br>
Conclusion<br>
The <software>Cyrille2</software> system has been developed to operate in the environment of a high-throughput sequencing facility with a need for robust, automated and high-throughput genome analysis, and easy creation, adaptation and running of pipelines by non-expert users.<br>
Most of the pipeline systems that have recently been released were developed as a workbench for bioinformaticians. Some systems excel in the way they allow for complex pipelines to be built through a visually appealing but sometimes complex GUI (<software>Taverna</software>, <software>Kepler</software>, <software>Triana</software>). Most systems are not suited for automated, high-throughput operation with as obvious exception <software>Ensembl</software> [13]. <software>Ensembl</software> was, however, not designed to be deployed at other sites or execute ad hoc pipelines.<br>
In view of the distinctive functionality and combination of features implemented in the <software>Cyrille2</software> system we believe that it is a valuable addition to the array of pipeline systems available and particularly useful in environments that require high-throughput data analysis. In the near future we are planning to expand the <software>Cyrille2</software> system in computational workflows for metabolomics data analysis.<br>
<br>
<br>
Availability and requirements<br>
<software>Cyrille2</software> is written in Python on a Linux platform and requires a <software>MySQL</software> database server and an <software>Apache</software>/mod_python web server. The system expects a Rocks Linux cluster for execution of analysis tools, although other SGE based solutions should work. The source code of the <software>Cyrille2</software> system is published under the terms and conditions of the GNU Public License and is freely available from .<br>
<br>
Authors' contributions<br>
MF is the lead developer of the <software>Cyrille2</software> system, has implemented most of the core of the system and has drafted this publication. AvdB and ED have both contributed to the development of tool wrappers, the database layer, limited work on the core and the manuscript. JdG has aided in implementing the GUI. RvH has been the project manager and contributed in discussions, planning and writing of this manuscript. All authors have read and approved the manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2335114</b><br>
Fast NJ-like algorithms to deal with incomplete distance matrices<br>
<br>
<br>
Background<br>
Phylogeny inference methods can be classified into two main categories: character-based (e.g. maximum-parsimony or maximum-likelihood) and distance-based approaches. The latter have low running times which are quite useful (mandatory in some cases) to perform large-scale studies and bootstrap analyses. A number of computer simulations [9-17] have shown that distance methods are fairly accurate, though not as accurate as likelihood-based methods that are much more time consuming. Using any distance-based method first requires to estimate the pairwise evolutionary distances between every taxon pair. These distances are usually estimated from DNA, RNA or protein sequences, but can also be obtained from DNA-DNA hybridization experiments or, e.g., computed from morphological data (see [18] for a review on distance estimation from various data types).<br>
In the last few years, phylogenomic studies (i.e. phylogeny reconstruction from large gene collections [7]) have instigated to the development of fast tree-building techniques being able to infer trees from datasets comprising hundreds of genes and taxa. The low-level gene combination involves concatenating the different genes into a unique supermatrix of characters, and then analyzing this matrix with a standard tree building method. This approach was shown to perform poorly when combined with distance methods, due to inaccurate distance estimations from such large heterogeneous character matrix [6]. Better distance-based trees are obtained by extracting the phylogenetic information from each gene separately, and then combining resulting information sources into a unique distance supermatrix. The Average Consensus Supertree (ACS [19]) and Super Distance Matrix (SDM [6]) techniques input a collection of distance matrices being estimated from each gene separately (the so-called medium-level combination), or being equivalent to the gene trees (the high-level combination). These distance matrices are deformed, without modifying their topological message, and then averaged to obtain the distance supermatrix, which is finally analyzed using a distance-based tree building algorithm.<br>
Estimating the distance supermatrix is fast. However, missing entries may occur in distance supermatrices depending on the extent of taxon overlap within the source matrices. For example, with the two large data sets of Driskell et al. [20], which were collected from <database>Swiss-Prot</database> and <database>Gen-Bank</database> thanks to a computer program, the ratio of missing distances is ~19% and ~1.2%, respectively. These distances are missing because only a few genes are sequenced within each species, meaning that a number of species pairs do not share any sequenced gene in common and cannot be compared using available data. However, Driskell et al. showed that, despite the sparseness of data and the fact that only a small subset of these data is potentially phylogenetically informative, a topological signal still emerges, which provides useful insights into the tree of life (see [20] and below for details). Analogous findings were reported by a number of authors in various contexts [21-23], and tree building from sparse data has become topical, as can be seen from the flourishing literature on supertrees.<br>
However, tree building from incomplete distance matrices is NP-hard [24], and thus practical algorithms are heuristics. The indirect approach involves first estimating missing distances by applying an ultrametric [25], additive [26], decomposition-based [27], or quartet-based [28] completion algorithm. The <package>TREX</package> package [29] provides several implementations of such algorithms to be used before tree building using any standard method with the completed matrix. The direct approach involves using a weighted least-squares (WLS) algorithm and associating missing distances with null weight (i.e. infinite variance), which means that missing distances are simply discarded from WLS computations ([18], pp. 449). The FITCH algorithm [30] from the <software>PHYLIP</software> package [31] and the MWMODIF algorithm [32] from <software>TREX</software> implement this technique. A combination of both direct and indirect methods is provided by MW* [33] (also available in <software>TREX</software>); this algorithm first applies an ultrametric or additive completion algorithm (depending on the density of missing distances) and then infers a tree using MWMODIF, where weights are set to 1.0 for known distances, 0.5 for estimated distances, and 0.0 for missing distances (if any remain). All these (direct or indirect) algorithms have O(n4) time complexity or more, where n is the number of taxa. This limits their application to medium-sized datasets (say 200 taxa without bootstrapping, see below).<br>
Agglomerative algorithms are much faster and allow dealing with thousands of taxa, as soon as the distance matrix is complete. The most popular of them is the Neighbor-Joining (NJ) algorithm [1,2]. Starting from a star tree, agglomerative algorithms iteratively perform the three following steps, until the tree is completely resolved:<br>
(a) select a taxon pair xy that is agglomerated into a new node u;<br>
(b) estimate the length of the two so-created external branches ux and uy;<br>
(c) replace x and y by u in the distance matrix, and estimate the new distances between u and the not-yet-agglomerated taxa.<br>
Step (a) is more time consuming than the two other steps, because of testing all the O(n2) taxon pairs to select the optimal one. To this purpose, NJ optimizes a numerical criterion that is denoted as Qxy. This criterion admits several interpretations related to the Minimum Evolution principle [1,34], but also to the acentrality of the considered pair [35,36]. In this last interpretation (used here), Qxy measures how much the path joining x to y is far from the other taxa i ? x, y. The xy pair maximizing Qxy corresponds to the two taxa which are most distant from the other ones and is the best candidate for agglomeration. Another criterion, denoted as Nxy, is used by <software>ADDTREE</software> [5]; this second criterion is based on the four point condition [37,38] and counts the number of taxon quartets xyij where x and y are neighbors. When the distance matrix exactly corresponds to a tree (it is then said to be additive), Nxy indicates all pairs of sibling taxa in the tree, whereas Qxy indicates just one such taxon pair. We shall see that this property of Nxy is a great advantage when dealing with incomplete distance matrices. Indeed, Qxy is sometimes unusable whereas Nxy is still informative.<br>
Steps (b) and (c) essentially correspond to distance averaging, which requires O(n) run time. These three steps being repeated n - 2 times, agglomerative algorithms require O(n3) time when using the Qxy pair selection criterion, and O(n4) with Nxy [39].<br>
Several refinements of the NJ algorithm have been proposed. BIONJ [3] minimizes the variances associated to the new distances being estimated during each reduction step (c). This way, BIONJ makes use at each iteration of reliable distance estimates to select the new taxon pairs to be agglomerated. To this aim, BIONJ uses a simple Poisson model of the variances and covariances of the distances being contained in the initial distance matrix. BIONJ was generalized into the Minimum Variance Reduction algorithm (MVR [4]), a WLS variant of which can deal with any distance variance model, but which does not account for the distance covariances. It has been shown using computer simulations that this variant (named WLS-MVR in [4] but referred here as MVR for simplicity) has similar accuracy as NJ when applied to distance matrices estimated from one-gene alignments [4]. WEIGHBOR [40] further refines BIONJ approach and uses an agglomeration criterion which accounts for the variances of evolutionary distances. All these algorithms require O(n3) time. Faster, sophisticated distance-based algorithms have been proposed in the last few years [41-46], some of them being clearly more accurate than NJ and BIONJ (e.g. FASTME [42] and STC [44], in O(n2 log(n)) and O(n2), respectively).<br>
In this paper, we propose an adaptation of the agglomerative scheme to quickly infer phylogenetic trees from incomplete distance matrices. We show that the Qxy criterion may be rewritten to express the mean acentrality of the xy taxon pair. In the same way, the Nxy criterion may be rewritten to express the mean number of taxon quartets where x and y are neighbors. By estimating these two means using all available (non-missing) distances, we define the two criteria Qxy? and Nxy? which allow for the selection of taxon pairs in step (a), even when the distance matrix is incomplete. Using these two new criteria in the agglomerative scheme requires O(n3) and O(n4) run time, respectively. A limitation of Qxy? and Nxy? is that they cannot be computed when the distance corresponding to the xy pair is missing (see Methods for more). However, this difficulty is inherent to the problem of building trees from incomplete distance matrices and is encountered (in various forms) by all methods to deal with this problem. Moreover, Nxy? partly circumvents this difficulty thanks to its ability to indicate several relevant pairs, rather than a single one with Qxy? (see Methods for more). As running Nxy? requires O(n4) time, we use a filtering technique: at each step (a) we use Qxy? to select the s most promising pairs for agglomeration, and then use Nxy? to select the best of these s pairs. This computational trick (and other refinements, see Methods) greatly improves the accuracy compared to using Qxy? only, while requiring O(sn3) time, where s is a small constant (s = 15 in our experiments). Finally, the original NJ, BIONJ and MVR formulae corresponding to steps (b) and (c) essentially are distance averaging and are easily adapted to incomplete matrices. The three new algorithms are named NJ*, BIONJ* and MVR*, respectively.<br>
<br>
Results and Discussion<br>
Several computer simulations are presented in this section to assess the performance of NJ*, BIONJ* and MVR*. We first compare the agglomeration criteria Qxy?, Nxy? and their combination with distance matrices that are additive, but contain missing entries. Then, using more realistic datasets, we compare NJ*, BIONJ*, MVR* to FITCH [30] and MW* [33], in terms of both topological accuracy and run times.<br>
Comparison of agglomeration criteria<br>
Our approach is similar to Makarenkov and Lapointe's [33]. We analyze with various algorithms and criteria a distance matrix with randomly deleted entries. The distance matrix we use is additive, i.e. is obtained from a tree by computing the path length distance between every taxon pair. Let T denote this tree and (Tij) be the corresponding distance matrix, where Tij is the path-length (or patristic) distance between taxa i and j in T. When no entry is missing, such an additive matrix uniquely defines T, which is recovered by any consistent algorithms (as are all algorithms being tested here). When entries are missing in (Tij), recovering T becomes a difficult task (see above), and we measure how well the algorithms perform when given an increasing number of missing distances. Such data thus are not realistic from a biological stand point, as evolutionary distances estimated from sequences are not additive, but this is a simple and standard approach to compare algorithms and agglomeration criteria.<br>
We use for the correct tree T the phylogeny of 75 placental mammals from [6]. The percentage of missing entries is Pmiss = 1%, 5%, 10%, 20%, 30%. For each Pmiss value, 500 replicates are randomly generated. From each of these 5 ? 500 incomplete additive distance matrices, a tree T^ is inferred by FITCH, MW* and BIONJ*. Various values of the s parameter are tested for BIONJ*, in order to compare the topological accuracy of Qxy?, Nxy?, and of the combination of these two agglomeration criteria. With s = 1, BIONJ* uses Qxy? only. With, s &gt; 1, the taxon pairs corresponding to the s highest values of Qxy? are reanalyzed with Nxy? (and with other criteria when ties occur; see Methods). When s becomes large (which is denoted as s = max) BIONJ* uses Nxy? only, as all taxon pairs are retained in the first selection step.<br>
Each inferred tree T^ is compared to the correct tree T by using the quartet distance dq [47]. This topological distance measures the number of resolved 4-taxon subtrees which are induced by one tree but not the other, and thus is more precise than the widely used bipartition distance [48] which counts the number of internal branches present in one tree but not in the other. Moreover, the quartet distance is less affected than the bipartition distance by small topological errors, e.g. wrong position of a single taxon [49]. This distance is normalized: dq = 0 indicates that T and T^ are identical, whereas dq = 1 means that both trees do not share any resolved 4-taxon subtrees. Averages of the 500 dq measures for each Pmiss value are displayed in Figure 1, for FITCH, MW*, and BIONJ* with various s values.<br>
All curves in Figure 1 are decreasing; as expected, the correct tree T is better recovered (i.e. the mean dq value between T^ and T decreases) as the proportion of missing distance Pmiss becomes closer to 0. Using Nxy? in BIONJ* greatly improves the agglomeration step; e.g. with Pmiss = 10%, mean dq values of BIONJ* are ~0.0015 and ~0.0008, with s = 1 and s = 15, respectively. However, there is no significant difference between s = 15 and s = max (as assessed by a sign-test [50] based on the 500 replicates, all p-values are much larger than 0.05), meaning that a small value of s (e.g. s = 15) seems to be enough to focus on the most relevant pairs, while avoiding the computational burden of using Nxy? only. Further experiments (see below) confirm this finding. FITCH and BIONJ* (with s = 15 and s = max) have similar accuracy, while MW* tends to perform better than the other algorithms with these data. However, we shall see that algorithm ordering is different with more realistic simulations. These experiments thus confirm the advantage of combining Qxy? and Nxy? within BIONJ*, and similar results (not shown) are obtained with NJ* and MVR*.<br>
<br>
Comparison of reconstruction algorithms with distance supermatrices<br>
We re-use a simulation protocol that we have used previously to compare a number of tree-reconstruction methods in a phylogenomic context [6]. This protocol involves generating sequences and evolving them along trees, and is more realistic than the comparison described above. We first summarize this protocol, and then report the results that are obtained with the simulated datasets by FITCH, MW*, NJ*, BIONJ* and MVR*. To estimate the distance supermatrix that is the input of these algorithms, we use the SDM method ([6], see also Methods) which computes a supermatrix that summarizes the topological signal being contained in a collection {(?ij1),(?ij2),...,(?ijk)} of k distance matrices. Simulations [6] have shown the high-quality of this distance supermatrix in both medium- and high-level gene combinations.<br>
Simulations are as follows (see [6] for more details). Starting from a randomly generated tree T with n = 48 taxa, evolution of k genes is simulated, with k = 2, 4, ..., 20. For each of the k genes, some taxa are randomly deleted. Two deletion probabilities are used: 25% to preserve high overlap between the different taxon sets, and 75% to induce low overlap. From these k partially deleted gene alignments, k distance matrices are estimated to compose the collection C? of source matrices. The SDM method is then run with C? to obtain a distance supermatrix corresponding to a medium-level combination of the k partially deleted genes. To study the high-level combination, a phylogenetic tree is inferred by <software>PhyML</software> [17] from each of the k partially deleted genes; then, the path length distance between each taxon pair for each of the k phylogenies is computed, to form the collection CT of k additive distance matrices that are equivalent to the k PhyML trees. Finally, SDM is applied to CT to obtain a distance supermatrix corresponding to a high-level gene combination.<br>
This simulation protocol is repeated 500 times for each value of k and each deletion proportion. We obtain this way (10 gene collection sizes ? 500 collections ? 2 overlap conditions ? 2 gene combination levels) = 20,000 distance supermatrices, which are denoted as (?ijSDM) and are frequently incomplete. Indeed, if taxon i is missing for gene p, then ?ijp is missing ? which is denoted as ?ijp = ??, and if ?ijp = ? for all p = 1,2, ..., k, then ?ijSDM = ?. With 25% deletion rate, almost all distance supermatrices are complete when k ? 14. With 75% deletion rate, all distance supermatrices are incomplete, but the number of missing distances decreases as k increases (missing distance proportions range from 42% to 11%).<br>
FITCH and MW* are run with default options. In accordance with Figure 1, s is set to 15 for NJ*, BIONJ* and MVR*. With BIONJ*, Vij variances (associated with ?ijSDM distance estimates) are naturally defined by Vij ? ?ijSDM if ?ijSDM ? ?, else Vij = ?. Variances used by MVR* comply with the same rule, but account for other parameters such as the length and the number of sequences being used to estimate each ?ijSDM distance (see Methods). Accuracy of the five algorithms is measured with the topological distance dq, as above, and averaged for the 500 replicates corresponding to each of the conditions. Results are reported in Table 1 for the medium-level gene combination, and in Table 2 for the high-level gene combination. For each value of k, the first- and second-best mean dq values are indicated in bold&amp;underlined and bold, respectively, and a sign-test [50] based on the 500 replicates is used to assess the significance of the difference between these two best values.<br>
In the medium-level gene combination, NJ* and MW* are outperformed by other algorithms. With a 25% deletion rate, BIONJ* has best topological accuracy, followed by FITCH. However, the sign-test indicates that the difference between these two algorithms is moderately significant as the p-value is lower than 0.05 for only five k values (= 6, 8, 12, 16, and 18). With a 75% deletion rate, FITCH is best, but again the sign-test shows that FITCH, BIONJ* and MVR* are broadly equivalent.<br>
With high-level combination distance supermatrices, NJ* and MW* still tend to be outperformed by other algorithms. BIONJ* is in between, and the best mean dq values are observed with MVR* which is followed by FITCH. The sign-test broadly confirms the significance of this observation, though the accuracy difference between MVR* and FITCH is relatively low.<br>
Altogether, these experiments show that MVR* is at least as accurate as FITCH, that BIONJ* has similar performance, while NJ* and MW* are behind these three algorithms. Comparing these findings with the results from (see Figure 2 in [6]), we see that (in the high-level framework, Table 2) MVR* is more accurate than the standard Matrix Representation with Parsimony method (MRP, [51,52]), in most cases; e.g. with k = 10, MVR* has mean dq values of 0.0171 and 0.0663, for 25% and 75% deletion rate, respectively, while mean dq values of MRP equal 0.0175 and 0.1152. MVR* (combined with SDM) outperforms MRP with sparse information (75% deletion rate and/or low number of genes), while both approaches are nearly equivalent when the information is abundant (25% deletion rate). An explanation [53] of this finding could be that the distance approach not only uses the topology of the source trees (as MRP) but also their branch lengths. Distance-based supertrees thus contain more information than MRP supertrees, which makes a noticeable difference when the information is sparse, but does not impact much the results with abundant information (see also following simulation results).<br>
<br>
Results with simulations based on Driskell et al. [20] dataset<br>
This section aims to measure the accuracy of the different tree building algorithms when applied to simulated datasets being more realistic than those commonly used in a phylogenomic perspective. Most notably, uniformly random gene deletion (used in previous section, following [54]) is not fully realistic because some genes (e.g. cytochrome b) are sequenced for most species, while some other genes are rarely sequenced (or rare among living species). It follows that the gene presence/absence pattern is different with real datasets to this being induced by uniformly random gene deletion (see [20,55-57] for illustrative examples). To this purpose, we use the character supermatrix from Driskell et al. [20], which comprises 69 green plant species and 254 genes, and was built via an automated exploration process of <database>GenBank</database>. This matrix contains a total number of 2777 sequences and has 87% missing characters, which are unequally distributed among taxa. Only 3 taxa have more than 50% genes, whereas 42 have 10% genes or less. In the same way, a few genes are present in most taxa (e.g., the 2 most sequenced genes belong to 59 taxa), whereas other genes are rare (e.g. 121 genes are present in at most 5 taxa). However, these k = 254 genes are complementary and the SDM distance supermatrix only contains ~1.2% missing entries. This low proportion of missing entries is favorable to tree reconstruction, but still requires an algorithm able to deal with incomplete matrices.<br>
We use a simulation protocol analogous to that described above [6]. The only difference is the deletion procedure, with random deletion replaced by the gene presence/absence pattern of (see Figure 2B in [20]). We generate 100 datasets this way with n = 69 taxa and k = 254 genes. From these 100 datasets, we infer 100 distance matrix collections C? and 100 tree collections CT. Each of these 2 ? 100 collections is dealt with by SDM, to obtain a distance supermatrix (?ijSDM) that contains the same missing entries as those induced by the original dataset [20]. We use these matrices to compare FITCH, MW*, NJ*, BIONJ* and MVR*, based on dq quartet distance between the correct and inferred trees (see above). Our three algorithms are run with both s = 15 and s = max. Results of MRP are also computed, using <software>TNT</software> [58] to infer the most parsimonious trees. <software>TNT</software> is run with 25 random addition sequences, TBR branch swapping and ratchet. The MRP supertree is defined in the standard way [59] as the strict consensus of the most parsimonious trees. Results are displayed in Table 3, which is similar to Tables 1 and 2; the first- and second-best mean dq values are indicated in bold&amp;underlined and bold, respectively, and sign-tests are used to assess the significance of the differences between MVR* (our best algorithm), FITCH and MRP.<br>
NJ*, BIONJ* and MVR* do not show any significant difference when used with s = 15 and s = max (as assessed by the sign-test, all p-values are much larger than 0.05, results not shown). This confirms the results of the previous experiments to compare our various agglomeration criteria. NJ* has the worst accuracy, especially in the high-level combination framework. MW*, FITCH and BIONJ* show similar performance, while MVR* is best among distance approaches in the two gene combination levels. Moreover, the difference between MVR* and FITCH is highly significant (sign-test p-value ? 0.0). In the high-level framework, MVR* tends to be better than MRP, although the information is quite abundant (254 genes, ~1.2% of missing distances); however, the difference is not significant with 100 replicates (sign-test p-value ? 0.2). The results among distance methods are explained by the fact that MVR* uses fairly accurate estimates (VijSDM) of the variances of the distances in (?ijSDM). Indeed, dataset [20] induces a highly heterogeneous distribution of missing sequences, meaning that some distances are well estimated thanks to a large number of sequences, while some others are poorly estimated via a few sequences. This is accounted for by MVR* in (VijSDM) calculations (see Methods), while MW*, FITCH and BIONJ* lack this information and use inaccurate estimations of (VijSDM). The difference between these two approaches (i.e. MVR* on the one hand, and MW*, FITCH and BIONJ* on the other hand) is somewhat hidden when using uniformly random sequence deletion, because with the latter all distances are broadly estimated with the same number of genes. With biologically realistic pattern of gene presence/absence, the difference becomes important, especially for the high-level combination. Thus, this last set of simulations confirms the findings of the previous ones and supports the capacity of MVR* for dealing with phylogenomic data.<br>
<br>
Run time comparison<br>
Run times with various dataset sizes have been measured on a PC Pentium IV 1.8 GHz (1 Gb RAM) and are displayed in Table 4. We do not report the run times of NJ* and BIONJ*, as they are nearly the same as those of MVR*. In fact, NJ* and BIONJ* are ~2% faster than MVR*, because they are simpler, but these simplifications does not concern the heavy O(n3) parts of the algorithms (see Methods). We also report the run times of SDM [6], which are in the same range as the fastest tree building algorithms, except with Driskell et al. [20]-like datasets, where SDM has to summarize a large number (254) of source matrices, but where the number of taxa (69) is relatively low. In this case, the run time of SDM is analogous to that of FITCH and MW* and remains quite handy (~5 minutes per dataset).<br>
As expected from their mere principle, the run times of the various tree building algorithms are not much affected by the proportion of missing distances, which is induced by the taxon deletion rate (25% or 75%) and the number of source matrices (k). The only apparent exceptions correspond to k = 2 and 75% deletion rate, where all algorithms seem to be quite fast; but in this case the distance supermatrices are of low size (~20, ~42 and ~85 for n equal to 48, 96 and 192, respectively), which explains this finding. Indeed, in this case it occurs frequently that some taxa have no gene (among 2) in common with any of the other taxa, and such taxa cannot be analyzed as all their distances to the other taxa are missing.<br>
With 25% taxon deletion proportion, n = 48 and k = 10, run times of ~3 hours and ~5 hours are required by FITCH and MW*, respectively, to build the 500 trees corresponding to all gene collections in any given gene combination level. The same task, which induces calculations similar to bootstrapping, is achieved in ~30 seconds by any of our agglomerative algorithms. The difference between the agglomerative algorithms and the others increases when the number of taxa increases, as expected given that their time complexity are O(sn3) (i.e. O(n3) as s is kept constant) and O(n4) or more, respectively. With 192 taxa, FITCH and MW* require more than 3 hours to build a single tree, while the agglomerative algorithms require less than 1 minute; this run time makes easy to perform a bootstrap study with our algorithms, but pretty much impossible with FITCH or MW*. With even larger datasets (say, above 500 taxa) neither FITCH nor MW* can be used to build a single tree, while our algorithms still run in a few minutes.<br>
<br>
<br>
Conclusion<br>
Thanks to the ever increasing flow of sequence data, phylogenomic analyses and supertree buildings are more and more frequently used to draw the evolutionary tree of living species. Larger and larger datasets are processed, requiring sophisticated approaches and algorithms. In this context, distance-based methods are quite useful, as they are both very fast and fairly accurate. New techniques, such as SDM [6], allow quickly estimating distance supermatrices that summarize the topological signal being contained in a collection of source distance matrices or gene trees. However, these supermatrices may be incomplete due to low taxon coverage in the selected genes. In this (common) case, fast distance-based tree building algorithms such as NJ, BIONJ, FASTME or STC are no longer applicable.<br>
This paper presents an adaptation to incomplete distance matrices of several agglomerative algorithms, namely NJ, BIONJ and MVR. We show that the formulae forming the basis of these algorithms can be rewritten to account for missing distances. Moreover, the same holds for the quartet-based pair selection criterion of ADDTREE. Combining both NJ and ADDTREE generalized pair selection criteria, we obtain fast and accurate algorithms that require O(n3) run times, where n is the number of taxa, i.e. run times that are similar to NJ's. These three novel algorithms, named NJ*, BIONJ* and MVR*, show (in our simulations) topological accuracy similar or higher to that of FITCH and MW*, which are much more time consuming. MVR* appears to be best, followed by BIONJ*. In a phylogenomic context, MVR* accounts for (and benefits from, regarding other algorithms) the fact that gene distribution among species is very heterogeneous, which implies that some distances are accurately estimated (using numerous genes) while some others are poorly estimated (with few genes). Combined with the SDM method [6] to estimate distance supermatrices, MVR* and BIONJ* are relevant alternatives to standard supertree techniques [7], as MRP [51,52]. JAVA implementations of these algorithms are available in <software>PhyD*</software> software and downloadable from [8]. All our datasets are also available from this URL.<br>
Several research directions would deserve to be explored. The variances and covariances of the distance estimates in the distance supermatrix could be accounted for in a more complete and accurate way, e.g. in the line of WEIGHBOR [40] for the pair selection criterion, or using the generalized least-squares version of MVR [4]. There is a clear need for a pair selection criterion being able to point out xy taxon pairs, even when the corresponding ?xy distance is missing. Theoretical results highlighting the cases where our algorithms will succeed (or fail) in recovering the correct tree, would likely help to improve these algorithms or design new ones. Adapting to missing distances very fast algorithms [41-46] could be promising. Finally, dealing with missing distances is likely required in other (non phylogenomic) applications of phylogenetic trees, and in related problems, as phylogenetic network inference [60].<br>
<br>
Methods<br>
Existing agglomerative algorithms are defined by criteria and formulae which all can be rewritten as distance averages. These algorithms (e.g. NJ [1,2], BIONJ [3] and MVR [4]) are generalized to incomplete distance matrices by estimating these averages using available distances, when some of those are missing. In the following, we first define notation and present a generic agglomerative scheme that covers all the algorithms being discussed here. Then, we describe for each of the three agglomeration steps (pair selection, branch length estimation, and matrix reduction), how NJ is generalized into NJ* to deal with missing distances. NJ* is further refined by BIONJ* that incorporates a first simple estimation of the variance associated to each evolutionary distance. Finally, a second, more accurate estimation of this variance is used by MVR* that generalizes the weighted least-squares (WLS) version of the MVR [4] approach.<br>
Notation<br>
Let Ln = {1,2, ..., n} be the set of all taxa numbered from 1 to n, and (?ij) a distance matrix, where ?ij corresponds to the evolutionary distance between taxa i, j ? Ln, and ?ii = 0, ?i ? Ln. Distance-based algorithms build a tree T (also denoted as T^, depending on the context) from (?ij), and estimate all branch lengths Tuv, where uv is any pair of sibling nodes in T. At each agglomeration stage, a taxon pair xy is selected, connected to a new internal node u, and replaced by u in (?ij). Thus, at each stage, the set Lr = {1,2, ..., r} of non-agglomerated taxa drops in cardinality by 1, and r is changed into r - 1. Tree reconstruction stops when r = 2.<br>
<br>
Agglomerative algorithms with complete distance matrices<br>
A number of existing agglomerative algorithms to deal with complete matrices can be summarized using the following scheme [4]:<br>
? Input Ln = {1,2, ..., n} and (?ij);<br>
? r = n;<br>
? While r &gt; 2, do:<br>
(a) Select the xy pair to be merged into u by optimizing an agglomeration criterion;<br>
(b) Estimate the branch lengths Txu and Tyu:<br>
(1)Txu=?xy?Tyu=12?xy+?i?Lr?{x,y}wi(?xi??yi)<br>
					 with <br>
						?i?Lr?{x,y}wi=12<br>
					;<br>
(c) Reduce the distance matrix (?ij) for all i ? x, y:<br>
(2)?ui = ?i (?xi - Txu) + (1 - ?i)(?yi - Tyu) with ?i ? [0,1]<br>
(d) r = r - 1;<br>
? Output T.<br>
Step (a) in this generic scheme searches for the taxon pair xy to be merged by optimizing an agglomeration criterion. NJ, BIONJ and MVR select the pair which maximizes [1,2]:<br>
(3)Qxy=Rx+Ry?(r?2)?xy,whereRz=?i?Lr?zi.<br>
Let (?ij) be additive [61], i.e. be defined as the path-length distance between taxa in a phylogenetic tree T with positive branch lengths; then, maximizing Qxy over all taxon pairs selects a cherry of T, i.e. a pair of taxa being separated by a unique internal node in T. In other words, Qxy is consistent [36]. However, it is easily shown (using counter-examples) that the second best taxon pair (based on Qxy values) is not necessarily a cherry of T.<br>
Conversely, the ADDTREE [5] pair selection criterion implies that all cherries of T have highest criterion value. The ADDTREE criterion counts the number of times where the xy pair is a cherry in all taxon quartets xyij:<br>
(4)Nxy=?i&lt;j?Lr?{x,y}H(?xi+?yj??xy??ij)H(?xj+?yi??xy??ij)<br>
where H(t) = 1 if t ? 0, and H(t) = 0 if t &lt; 0. This criterion has integer values ranging from 0 to (n - 2)(n - 3)/2, and this maximum value is reached for all cherries (but for the cherries only) with additive distance matrices. Careful implementation [39] of ADDTREE allows for O(n4) run time. NJ, BIONJ and MVR are much faster. They first compute all Rz sums in Equation (3), and then compute in O(1) the Qxy value of each xy pair. Each agglomeration stage thus requires O(r2) time (branch-length estimation and matrix reduction are achieved in O(r)), and the whole algorithm is in O(n3). Moreover, Qxy can be seen as a continuous version of Nxy [62].<br>
After xy pair selection, x and y are connected to the new node u, and the lengths of xu and yu branches are estimated using Equation (1). Assuming that (?ij) is additive and corresponds to tree T, we have Txu = (?xy + ?xi - ?yi)/2, ?i ? x, y. Equation (1) averages these elementary estimators using various (wi) weightings. With NJ, the average is equally-weighted and we have wi = w = 1/(2(r - 2)). We shall see that MVR uses different wi weights.<br>
Finally (step (c)), (?ij) is reduced by replacing x and y with the new node u, and by computing all ?ui distances, ?i ? x, y. When (?ij) is additive and corresponds to tree T, we have ?ui = ?xi - Txu = ?yi - Tyu. Equation (2) averages these two elementary estimators. NJ uses equal weights (?i = 1 - ?i = 1/2) while BIONJ and MVR adjust ?i in order to minimize the variance of ?ui and to have reliable distance estimates during all agglomeration stages. For this purpose, BIONJ and MVR use (approximate) models for the variances and covariances of the distance estimates in (?ij).<br>
<br>
NJ*: generalizing NJ to incomplete distance matrices<br>
When (?ij) is incomplete (missing entries are denoted as ?), the criteria and equations above do not apply. We shall see in this section how they are generalized to define the NJ* algorithm, which keeps NJ's O(n3) time complexity and is nearly equivalent to NJ with complete matrices.<br>
(a) Agglomeration criterion<br>
Let Qxy = Qxy/(r - 2). Maximizing Qxy is the same as maximizing Qxy (Equation (3)), and we have:<br>
Q?xy=Rxyr?2??xy, where Rxy=?i?Lr(?xi+?yi),<br>
which can be rewritten as:<br>
(5)Q?xy=2r?2?xy+1r?2?i?Lr?{x,y}(?xi+?yi??xy).<br>
The sum in Equation (5) relates to terms representing how distant is the path joining x to y from other taxa i ? x, y (?xi + ?yi - ?xy equals twice the distance between u and i), whereas the first term expresses the additional distance induced by ?xy. It has been shown [63,64] that the relative weight of these two factors is unique, due to consistency requirement, and Q?xy can be interpreted as the mean acentrality of the xy pair [35,36]. To extend this criterion to incomplete distance matrices, we estimate it using the set of taxa with non-missing distances: Sxy? = {i ? Lr : ?xi, ?yi ? ?}. Moreover, we assume ?xy ? ?, and thus x, y ? Sxy?. The normalization factor is then equal to |Sxy?| - 2 (instead of r - 2) and we obtain the following generalization of Equation (5):<br>
Qxy?=2|Sxy?|?2?xy+1|Sxy?|?2?i?Sxy??{x,y}(?xi+?yi??xy),<br>
which applies to incomplete distance matrices, and is identical to Qxy with complete ones. This equation further simplifies into:<br>
(6)Qxy?=Rxy?|Sxy?|?2??xy,whereRxy?=?i?Sxy?(?xi+?yi).<br>
Other solutions are possible to extend Equation (5), e.g. preserving ?xy/(r - 2) term rather than transforming it into ?xy/(Sxy? - 2). Simulation results (not shown) indicate that criterion (6) has better topological accuracy than these alternatives. Theoretical results would be desirable to explain these observations and establish the properties of criterion (6), but a first simple explanation is that Equation (6) precisely corresponds to the Qxy value being computed on Sxy? taxon subset. To be consistent on the whole set of taxa (Lr), it is mandatory that the criterion is consistent on taxon subsets (Sxy?, here), and Equation (6) satisfies this requirement.<br>
Maximizing Qxy? seems to require O(r3) time for each iteration, and thus a total time complexity of O(n4). However, efficient implementation allows for O(n3) total run time. At the first stage (r = n), Rxy? and |Sxy?| values are computed and stored for all x, y ? Ln, which requires O(n3) time. In the subsequent agglomeration stages, these values are updated as follows:<br>
? After step (a), for all i, j ? Lr - {x, y} we remove from Rij? and |Sij?| : ?xi and ?xj (if ?xi ? ? and ?xj ? ?), and ?yi and ?yj (if ?ui ? ? and ?uj ? ?).<br>
? After step (c), we compute Rui? and |Sui?| for all i ? Lr - {u}, and<br>
? for all i, j ? Lr - {u}, we add ?ui and ?uj to Rij? and |Sij?| (if ?ui ? ? and ?uj ? ?).<br>
Each of these three updating routines requires O(r2) time, just as pair selection using criterion (6), meaning that using Qxy? instead of Qxy does not change the total O(n3) time complexity of the original NJ algorithm.<br>
However, as discussed earlier, a limitation of criterion Qxy? is that: (1) it cannot be computed when ?xy = ?, and (2) only the best pair is guaranteed (with additive distance) to be a cherry in the correct tree. When xy is the best pair in the complete additive distance matrix, but ?xy is missing in the available distance matrix, then using Qxy? does not provide any guaranty of correctness. This difficulty is partly alleviated when using a generalization of Nxy, as this criterion selects all cherries in the correct tree with complete additive distances. When some of the cherries correspond to missing distances, we are still able to select the others that correspond to non-missing entries. Our generalization of Nxy (Equation (4)) to incomplete distances is defined as follows. Let:<br>
(7)N?xy?=?i,j?Cxy?H(?xi+?yj??xy??ij),<br>
(8)where?Cxy?={(i,j)?(x,y),(y,x):i?j,?xi,?yj,?ij??}.<br>
N?xy? differs from Nxy in that we sum both H terms, instead of multiplying them. This way we exploit all available information. Indeed, when ?xj = ? and/or ?yi = ? but the other entries are available, we still use H(?xi + ?yi - ?xy - ?ij) in N?xy? while a multiplicative solution in the line of Nxy would discard this term. Moreover, it is easily seen that N?xy? = 2Nxy with complete additive distances. To select among taxon pairs, we use the averaged form of N?xy?, that is:<br>
(9)Nxy?=N?xy?|Cxy?|,<br>
which expresses the mean number of quartets where the xy pair corresponds to a cherry.<br>
However, selecting pairs using Nxy? sometimes produces ties. In this case, we select the pair with higher |Cxy?| value, that is the pair which is supported by the larger number of quartets. But ties may still occur, in which case we use:<br>
(10)Mxy?=|Miss(x)?Miss(y)|+|Miss(y)?Mass(x)|,<br>
where Miss(z) = {i ? Lr, ? z : ?iz = ? } corresponds to missing entries for taxon z. Mxy? counts the number of missing entries in the current matrix that will be removed in the next step (see reduction procedure (13)). Maximizing Mxy? tends to quickly fill missing entries in the running distance matrix, which both frees from ?xy ? ? limitation and allows using Qxy pair selection criterion only. Finally, in some (very rare) cases, we still have ties and then maximize the continuous version [62] of N?xy?:<br>
(11)N?xy?=?(i,j)?Cxy?(?xi+?yj??xy??ij).<br>
Pair selection criteria Nxy? (9), |Cxy?| (8), Mxy? (10) and N?xy? (11) are used in a lexicographic way: taxon pairs are ranked based on the first criterion (Nxy?), the second one (|Cxy?|) is used in case of ties, etc. However, using these four criteria only would result in O(n4) time complexity. In order to preserve O(n3) run times, we first select the s top pairs based on Qxy? criterion (6), and then use the other criteria in lexicographic order to select the pair to be agglomerated among these s pairs. As computing Equations (7) to (11) requires O(r2) or less per taxon pair, the total time complexity of pair selection is O(n3) (first selection using (6)) plus O(s?r2) (final selection using (8) to (11)), i.e. O(n3). As explained above, Qxy? does not provide any guaranty of correctness with missing distances, while Nxy? and N?xy? partly circumvent the difficulty. However, Qxy? enables to extract the most promising pairs for agglomeration and we have seen (Figure 1) that using for s a small constant (typically 15) is sufficient to obtain high accuracy, meaning that, in practice, run times are in O(n3).<br>
<br>
(b) Branch length estimation<br>
Equation (1) is easily rewritten using non-missing entries only:<br>
(12)Txu=?xy?Tyu=12?xy+?i?Sxy??{x,y}wi(?xi??yi),where?i?Sxy??{x,y}wi=1/2.<br>
NJ uses the same weight wi for every taxon i. The same holds for NJ*, that is, wi = w = 1/(2(|Sxy?| - 2)). Note that for the selected pair we have ?xy, Sxy? ? ?, meaning that Equation (12) is always applicable. Just as with NJ, branch length estimation (12) requires O(r) time at each agglomeration stage.<br>
<br>
(c) Matrix reduction<br>
Equation (2) averages two elementary estimators, and with NJ this average is equally weighted. With missing distances it may occur that one of these two estimators is not applicable (e.g. when ?xi ? ?), that both are applicable, or that none is applicable. Thus, in NJ* Equation (2) becomes:<br>
(13)?ui={?i(?xi?Txu)+(1??i)(?yi?Tyu)when?xi???and??yi??,?xi?Txuwhen?xi???and??yi=?,?yi?Tyuwhen?xi=??and??yi??,?when?xi=?yi=?,<br>
where ?i = ? = 1/2. In the second and third cases, entries missing in the previous matrix are now present in the new, reduced matrix. We have seen that criterion (10) tends to maximize the number of such entries, in order to fill as fast as possible the missing distances in the running matrix. Just as branch length estimation (12), matrix reduction (13) requires O(r) time at each stage and does not impact total time complexity. Thus, NJ* requires O(n3) run times, when s is kept constant.<br>
<br>
<br>
BIONJ*: improving the reduction step, a first simple solution<br>
BIONJ* uses the same pair selection criteria as NJ*, and adapts to missing distances BIONJ reduction procedure. BIONJ uses the degree of freedom corresponding to the ?i parameter in Equation (2), in order to minimize the variance of the new ?ui estimates in step (c). For this purpose, BIONJ assumes a simple Poisson model of the variances in the original (?ij) matrix, stating that the variance Vij of ?ij is proportional to ?ij. BIONJ also accounts for the covariances in (?ij) (see [3] for more details). It uses a single ? parameter for every xy pair, which does not depend on i and is given by<br>
(14)?i=?=12+12(r?2)Vxy?j?Lr?{x,y}(Vyj?Vxj).<br>
Again, this equation may be seen as an average and can be rewritten using available entries only as:<br>
(15)?i?=??=12+12(|Sxy?|?2)Vxy?j?Sxy??{x,y}(Vyj?Vxj).<br>
The reduction step (c) is achieved by BIONJ* as defined by Equation (13), but using so-defined ?* (instead of 1/2) when ?xi ? ? and ?yi ? ?.<br>
Moreover, BIONJ starts with variance matrix (Vij) = (?ij) and reduces this matrix at each stage using ? value from Equation (14) and equation:<br>
Vui = ?Vxi + (1-?)Vyi - ?(1 - ?)Vxy.<br>
BIONJ* combines this formula with Equation (13) and (15) to reduce the variance matrix, that is:<br>
(16)Vui={??Vxi+(1???)Vyi???(1???)Vxywhen?xi???and??yi??,Vxiwhen?xi???and??yi=?,Vyiwhen?xi=??and??yi??,?when?xi=?yi=?,<br>
Computing ?* using Equation (15) and achieving matrix reductions (13) and (16) requires O(r) run times. Thus, BIONJ* has O(n3) time complexity (when s is kept constant, else O(sn3)).<br>
<br>
MVR*: improving BIONJ* using variances dedicated to distance supermatrices<br>
The BIONJ variance model is well suited for one-gene studies where distance estimations all use the same number of sites (at least when gaps are removed). With phylogenomic studies, some distances are computed using a large number of genes, and thus are reliable, while other distances are based on a few genes and are poorly estimated. Moreover, some distances may be missing due to the absence of common genes between the two species being compared. Altogether, this implies that the BIONJ and BIONJ* variance model can be improved to better fit phylogenomic requirements. This section describes the MVR* algorithm that is intended to this purpose.<br>
Steps (b) and (c) in the generic scheme are based on wi and ?i parameters, respectively. The MVR algorithm [4] generalizes the BIONJ approach and uses these degrees of freedom in order to minimize the variance of the new estimates Tux, Tuy and ?ui. The main difference from BIONJ is that MVR is able to deal with any variance-covariance model of the ?ij distance estimates, while BIONJ is restricted to the Poisson model. The MVR variant that we use here only considers the variances and neglects the covariances, thus assuming a weighted least-squares model (it was called MVR-WLS in [4], but is named MVR here for simplicity). Thus, MVR inputs a distance matrix (?ij) and the corresponding (Vij) variance matrix. We shall see in the next section how (Vij) is calculated to deal with phylogenomic data, and describe now the way MVR and MVR* use and update these matrices all along the agglomeration procedure.<br>
MVR uses Qxy pair selection criterion (3), just as NJ and BIONJ, while MVR* uses the same criteria and selection procedure as NJ* and BIONJ*.<br>
In MVR step (b), i.e. branch length estimation, wi weights in Equation (1) depend on i and are given by:<br>
(17)wi=?Vxi+Vyi,with?normalization?term??=12(?i?Lr?{x,y}1Vxi+Vyi)?1.<br>
MVR* uses Equation (12) (instead of Equation (1)) to deal with missing entries, and adapts above Equation (17) by replacing Lr by Sxy?.<br>
In MVR step (c), i.e. matrix reduction, a different ?i parameter is associated in Equation (2) to each taxon i ? x, y using:<br>
(18)?i=VyiVxi+Vyi.<br>
This value puts more weight and confidence on (?xi - Txu) when the associated variance Vxi is low, compared to Vyi. Equation (18) is also used by MVR*, but combined with Equation (13) to deal with missing distances.<br>
Finally, MVR (just like BIONJ) reduces the variance matrix at each agglomeration stage. To this purpose, MVR uses the following equation:<br>
Vui=VxiVyiVxi+Vyi.<br>
This equation is also used by MVR* in combination with Equation (16).<br>
All the computations described above (except pair selection) require O(r) run times at each agglomeration stage, and thus MVR* has O(n3) time complexity, just as do NJ* and BIONJ*.<br>
<br>
Estimating the variances associated to distance supermatrices<br>
Distance supermatrices are computed [6,19] from source matrices which are first rescaled, and then averaged. SDM [6] inputs a collection C = {(?ij1),(?ij2),...,(?ijk)} of k distance matrices ? each defined on taxon set Lp and estimated from sequences with size sp?, and deforms them, without changing their topological signal, so as to bring them as close as possible to each other before averaging. The first deformation is scaling, which multiplies each (?ijp) distance matrix by a factor ?p. The second (optional in SDM) deformation adds a constant aip to every non-diagonal ?ijp entries. Then, SDM averages the resulting modified matrices to obtain the (?ijSDM) super-matrix that is defined by:<br>
(19)?ijSDM=1Wij?1?p?k,Lp?{i,j}sp(?p?ijp+aip+ajp),whereWij=?1?p?k,Lp?{i,j}sp.<br>
Neglecting the variance of the deformation factors, we obtain a simple expression of the variance of ?ijSDM:<br>
(20)VijSDM=1Wij2?1?p?k,Lp?{i,j}sp2?p2Vijp,<br>
where Vijp is the variance of ?ijp. Note that no covariance terms between any ?ijp and ?ijq estimates appear in Equation (20), as these source distances are estimated from different genes and are independent. Moreover, the covariances between the entries in the SDM supermatrix are neglected, as is the case in a number of (WLS) approaches [30,32,40].<br>
Several studies have shown that the variance Vij associated with the evolutionary distance ?ij (estimated from a single gene) is approximately equal to ?ij?/s with ? ? 2 [11,65]. Based on various experiments (not shown), we have chosen the usual formula Vij=?ij2/s, which corresponds to default option in <software>FITCH</software> program. Equation (20) then becomes:<br>
VijSDM=1Wij2?1?p?k,Lp?{i,j}sp(?p?ijp)2.<br>
<br>
<br>
Authors' contributions<br>
AC designed and implemented the algorithms and experiments, performed the computations that are shown here, and wrote the manuscript. OG supervised these works, participated in the design of algorithms and experiments, and wrote the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2375134</b><br>
<software>Snagger</software>: A user-friendly program for incorporating additional information for tagSNP selection<br>
<br>
<br>
Background<br>
There has been extensive effort to develop and implement strategies for efficient selection of single nucleotide polymorphisms (SNPs) in candidate-gene association studies of complex disease. Due to the prohibitively high cost associated with genotyping every SNP within a given set of genes, methods have been developed to find a subset of these SNPs that capture the same genetic diversity. One of these methods includes a preliminary stage of genotyping in which linkage disequilibrium (LD) or haplotype block structure is estimated by genotyping a set of evenly distributed SNPs across one or more genes for a sample set representative of a given population. Two freely available software applications of note exist to facilitate this preliminary stage [1,2]. <software>SNPHunter</software> automates the filtering and selection of SNPs for genotyping, allowing the user to incorporate desired characteristics, such as chromosomal position and functionality [1]. Once genotyped, <software>htSNPer1.0</software> can be used to define haplotype boundaries and select haplotype tagging SNPs (htSNPs) to capture underlying LD [2]. Using the resulting LD or haplotype information obtained from a first-stage sample, a second stage of genotyping for a smaller set of non-redundant SNPs is typically performed in a larger sample. For example, Haiman et al. demonstrated the use of a small multiethnic first-stage sample with dense genotyping in order to capture the genetic diversity within CYP19. Subsequent haplotype tagging SNPs were then genotyped in a larger case-control second-stage sample examining the association with breast cancer [3].<br>
Recently, it has become common to use the publicly available <database>HapMap</database> database in place of the first stage of genotyping. <database>HapMap</database>, containing genotypes of 270 individuals in four geographically diverse populations for over three million SNPs, has become a reliable source for describing genetic diversity and inferring LD patterns in a target sample population [4,5]. Population genetic studies of underlying LD patterns have demonstrated that data from the HapMap project is sufficient in describing the underlying LD structure across multiple populations [6].<br>
Once genotypes for a set of SNPs is obtained for a representative sample (either from a primary stage of genotyping or the publicly available <database>HapMap</database> database), two approaches can be used to select a minimal set of SNPs to be genotyped in a larger sample: "block-based" and "block-free" [7]. Block-based approaches use haplotype block structure and haplotype frequencies in order to select an informative, non-redundant, minimal set of SNPs that captures the underlying haplotype diversity [8,9]. Block-free approaches do not require this underlying block structure, and instead use pairwise LD between SNPs in order to select a minimal set of tagSNPs that capture all other SNPs at a defined threshold [10]. Block-based approaches have an advantage in that the possible interaction of a group of SNPs that are genetically linked can be measured as a haplotype. In a block-free approach, there is no guarantee that the selected tagSNPs will allow differentiation of haplotypes. However, a drawback of block-based approaches is that they only sample a fraction of the genetic diversity in regions with poor block structure. While both approaches offer advantages, we have focused on developing methods and tools for block-free approaches and we limit our comparison to Tagger [11], which implements a block-free algorithm.<br>
There are several algorithms using block-free approaches to select tagSNPs [7,10,12-15]. Some of these algorithms are based on D' as a measure of LD [9,12], but the majority use r2 [7,10,13-15], as it is a direct measure of association between SNPs [14] and inversely related to statistical power [10,14,16].<br>
The current accessible algorithms, including the commonly used program <software>Tagger</software> [11], have some notable limitations. Some programs enable the user to forcibly include SNPs having a priori importance, such as known functionality [1,7,10,11,14,15], yet they lack the ability to prioritize additional tagSNP picking based on SNP features such as coding status or genomic location. <software>Tagger</software> [11] can consider design scores on a high-throughput genotyping platform when prioritizing tagSNPs, but does not take a SNP's probability of typing failure into account when tagging, nor does it allow SNPs outside of a targeted genomic region to be picked. Recently a few programs have been developed to allow for optimal selection of tagSNPs across multiple populations [17-19], yet they fail to incorporate one or more of the aforementioned features.<br>
Typically, the set of possible tagSNPs in candidate gene studies using a block-free approach is limited to those SNPs which are located within the targeted genomic regions. However, patterns of LD can extend beyond the boundaries of these regions and are often non-contiguous when observing pairwise r2 values between SNPs. This means that a SNP located outside of a targeted region may have a significantly high r2 value with one or more SNPs located within the region, even if SNPs located between them are not in LD. Expanding the set of potential tagSNPs to include SNPs from outside a targeted region allows SNPs with higher probabilities of genotyping success to picked and increases the chance that SNPs unable to be genotyped will be captured.<br>
In this paper we present a user-friendly and efficient block-free tagSNP selection program, <software>Snagger</software>, which improves upon current available SNP tagging algorithms and is available as an extension to <software>Haploview</software>. Our program allows the user to: (1) prioritize tagSNPs based on certain characteristics, including platform-specific design scores, functionality (i.e., coding status), and chromosomal position, (2) select tagSNPs across multiple populations, (3) select tagSNPs outside defined genomic regions to improve coverage and genotyping success, and (4) pick surrogate tagSNPs that serve as backups for tagSNPs whose failure would result in a significant loss of data. While many SNP selection programs and algorithms are designed to pick the minimal set of tagSNPs that will capture the underlying genetic structure, <software>Snagger</software> is designed to pick a set of tagSNPs that will capture the structure while also fulfilling user-defined characteristics and ensuring the best chance for genotyping success.<br>
<br>
Implementation<br>
<software>Snagger</software> was implemented in Java as an extension to the existing open-source software, <software>Haploview</software> (version 3.3). It builds upon <software>Haploview</software>'s user interface and uses its ability to import and filter genotype data in <database>HapMap</database> format and calculate pairwise LD metrics (D' and r2) between SNPs. In addition, it imports a score file containing design scores (e.g., from Illumina) and other relevant annotations for the SNPs in a defined genomic region.<br>
Data selection and filtering<br>
The user can specify a genomic region and ethnic group of interest (Figure 1) for tagSNP selection. Those SNPs passing user-defined filters, such as a minimum minor allele frequency (MAF), make up the set of SNPs, S = {s1, s2,..., sm}. Once data is imported, Haploview generates a table of all possible pairwise r2 values between si and sj (where i,j ? {1,..., m} and i ? j). Parameters and specifications for tagSNP selection can be specified by the user (Figure 2), including a minimum r2 threshold, r2min, when determining the desired LD threshold between SNPs. A set of force-included, I = {i1, i2,..., in}, or force-excluded, E = {e1, e2,..., eo}, SNPs within S can be inputted manually or imported as a separate file.<br>
An option is provided for the user to enforce a minimum design score for tagSNPs as well as a minimum physical distance (in base pairs) between any two tagSNPs.<br>
<br>
Tagging algorithm<br>
<software>Snagger</software> allocates SNPs in set S into three primary sets for use in selecting tagSNPs (see Appendix and Figure 3 for a summary of the algorithm):<br>
- C = {c1, c2,..., cp}, the set of all SNPs to be captured (i.e., "tagged")<br>
- P = {p1, p2,..., pq}, the set of potential tagSNPs<br>
- T = {t1, t2,..., tr}, the set of tagSNPs<br>
Initially, all SNPs in set S are added to set P, and all SNPs within the region of interest and in set S are added to set C. If a set of force-included and/or force-excluded SNPs are specified, all force-included SNPs, set I, are added to set T, and SNPs in LD with set I are removed from set C. <software>Snagger</software> then generates the set of potential tagSNPs, set P, by adding all SNPs in set S except those SNPs in either set I or set E.<br>
A SNP Score is assigned to each potential tagSNP in set P. It is a function of the SNP's probability of genotyping success (Pr [GSm]), MAF (MAFm), functionality (Typem, e.g., synonymous, nonsynonymous), and chromosomal position (Locm, e.g., exon, intron).<br>
SS = {WGS * Pr [GSm]} + {WMAF * MAFm} + {WT * Typem} + {WL * Locm}<br>
The probability of genotyping success, Pr [GSm], is calculated as a function of a SNP's design score.<br>
Pr?[GSm]=exp?(?0.27+3.78?DesignScorem)1+exp?(?0.27+3.78?DesignScorem)<br>
The default parameters for this function were estimated from modeling of failure rates as a function of Illumina design scores using data on 5,848 SNPs genotyped by the University of Southern California Genomics Core Facility. However, analogous scores from other platforms can be used to calculate this probability. The parameters can be changed using the software's interface, provided the user has the estimated parameters for the desired platform.<br>
MAFm is a function of a SNP's MAF across the populations for which the user wishes to select tagSNPs and a user-defined idealMAF. Users can select from any of the four HapMap populations, import custom population data, or combine multiple populations into one. The default value for idealMAF is 0.5.<br>
MAFm=?h=1HmMAFmhHmMAFmh=1?abs(idealMAF?observedMAFmh)0.5<br>
For each SNP m having an observed MAF in a given population h, a MAF Score for that population, MAFmh, is calculated. The population-specific MAF Scores are then averaged across Hm, the number of populations with an observed MAF for SNP m. SNPs having MAFs nearer to the ideal MAF will have MAF Scores closer to 1.<br>
For SNP functionality and chromosomal position, the user can define values between 0 and 1 for specific characteristics (e.g., a SNP located in an exon leading to a nonsynonymous mutation, or a SNP located in an intron).<br>
Weights (WGS, WMAF, WT, WL) are applied to each parameter, and can be modified by the user. If desired, the user can preferentially weight parameters so that tagSNPs having specific characteristics are more likely to be selected. The default value for each weight is zero except for the probability of genotyping success weight, where the default value is one. Thus, without user-specified weightings across parameters, SNP Scores only rely on genotyping success.<br>
For each SNP in set P, a BinA is created containing those SNPs in C for which it can serve as a proxy. The LD threshold, r2min, is used as the entry criteria into respective BinAs.<br>
A secondary bin, BinB, is created for each SNP pi in set P, containing only those SNPs that meet the r2min requirement with the SNP of interest and every other SNP in its BinB. The BinB is formed by first sorting the SNPs in pi's BinA by the size of their BinA bins (highest to lowest). Initially, pi is added to its own BinB. Then, each SNP in pi's BinA is sequentially added to its BinB if it is contained in the BinA of every SNP currently in pi's BinB.<br>
TagSNP picking starts by selecting the SNP from each BinB with the highest SNP Score and adding it to a temporary set, Q. From this narrowed set of potential tagSNPs, the SNP with the most SNPs in its respective BinA, t, is picked as a tagSNP and added to the set T. This tagSNP is removed from every BinB as well as the set of potential tagSNPs, set P. All SNPs in LD with t are removed from every BinA and the remaining set of SNPs to be captured, set C. This algorithm is repeated until either set C or set P become empty. If set P is empty, but set C is not, any SNPs remaining in set C are marked as "untaggable". Singletons are handled in the same way in the selection process, but become untaggable (i.e., uncaptured) if they violate a minimum tagSNP distance or design score requirement.<br>
<br>
Multiple populations<br>
<software>Snagger</software> has the ability to select tagSNPs across multiple ethnic groups. Using a user-defined order, it sequentially picks tagSNPs from the first population, using r2, design score, and surrogate picking parameter specifications (Figure 4), and forces them into the next population (Figure 5). This task is repeated for each population until the last population has been tagged. Since not all groups will share the same set of SNPs to capture (either due to filtering criteria or unavailability of genotypes), the final list of tagSNPs is the union of all tagSNPs in each group.<br>
<br>
Surrogate tagSNP picking<br>
An inherent problem of any tagSNP approach is the possible loss of significant data if a tagSNP that predicts many SNPs fails genotyping. Although similar issues have been addressed in robust tagging software for block-based methods [20], block-free methods require a different solution. Two methods for adding surrogate tagSNPs are available in the software, including one based on probability of genotyping success and one based on the number of SNPs tagged.<br>
The first method uses the probability of genotyping success, Pr [GSm] (Figure 2). As tagSNPs are chosen, every SNP receives a calculated probability of success (CPS) that is derived from the Pr [GS] of all chosen tagSNPs that can act as a proxy for that SNP. Given a SNP m with n tagSNPs:<br>
CPS(m) = Pr [GS1] ? Pr [GS2] ?...? Pr [GSn]<br>
The user can enforce a minimum CPS on all SNPs whose corresponding tagSNPs capture a minimum number of SNPs. With this method, a SNP is not considered captured until its CPS meets or exceeds the user-defined threshold, its tagSNP's "captured" number is less than the cut-off, or there are no surrogates available. The software provides default values of 0.999 and 8 for the minimum CPS and minimum "captured" number of SNPs, respectively. The possible values for Pr [GSm] when using default parameters and Illumina design scores range from 0.43 to 0.98. It follows that any tagSNP predicting at least eight SNPs will at minimum require one surrogate tagSNP in order to achieve a CPS of 0.999 for the predicted SNPs. If either the tagSNP or surrogate has a sufficiently low design score that the threshold is not met, more surrogates will be selected as long as they are available. The default values are chosen so as any SNP in a large bin will have only one chance in a thousand that all the tagSNPs predicting it will fail.<br>
The second method relies on a function that gives the required number of surrogates based on the how many SNPs a tagSNP is tagging (Figure 2):<br>
T = log(?1 ? M^?2) - 1,<br>
where T is the number of surrogates needed and M is the number of SNPs tagged by a tagSNP, with the ?1 and ?2 values specified by the user. Every time a tagSNP is chosen, the above function is evaluated to check if and how many surrogates should be added. The surrogates are chosen from the tagSNP's BinB and added to the list of tagSNPs.<br>
<br>
<br>
Results<br>
SNP Score impact<br>
<software>Snagger</software>'s preferential selection of tagSNPs was evaluated using <database>HapMap</database> Public Release 21a genotype data [21] for 60 CEPH (Utah residents with ancestry from northern and western Europe) founder samples in the following 10 <database>ENCODE</database> regions: ENm010, ENm013, ENm014, ENr112, ENr113, ENr123, ENr131, ENr213, ENr232, ENr321.<br>
For simulation purposes, we randomly marked one-eighth of all potential tagSNPs (936 of 7,479) as located in a coding region of the chromosome, which in practice would include both synonymous and nonsynonymous SNPs. The remaining 6,543 SNPs were marked as non-coding. In calculating the SNP Score, SNPs in the coding region received a weight of 1 and all other parameters had a weight of 0. <software>Snagger</software> selected 1,323 tagSNPs, of which 457 (34.5%) were "coding" SNPs. Favorable weighting for coding SNPs increased the proportion of tagSNPs located in a coding region nearly three times (from 12.5% of all potential tagSNPs to 34.5% of selected tagSNPs), and of all potential coding region tagSNPs, nearly half were selected (457 of 936).<br>
We also compared tagSNPs selected for the <database>HapMap</database> CEPH population across 10 <database>ENCODE</database> regions, using a SNP Score with preferential weighting of MAF in the <database>HapMap</database> Yoruba (in Ibadan, Nigeria) population to no weighting at all. This was done in order to demonstrate the ability of <software>Snagger</software> to preferentially pick tagSNPs in one population (e.g., CEPH) which are common in another (e.g., Yoruba), such that resulting genotypes could potentially be compared in the future to the ungenotyped population. An MAF weight of 1 (for both CEPH and Yoruba populations) on the SNP Score and weights of 0 on all other parameters resulted in 21 percent of picked tagSNPs having an MAF between 0.4 and 0.5 in the Yoruba population. When not weighing on Yoruba MAF, only 12 percent of picked tagSNPs had an MAF between 0.4 and 0.5 in the Yoruba population. Also of note is the reduction in monomorphic tagSNPs in the Yoruba population from 19% with no weighting to 16% with weighting. The distribution of Yoruba MAFs for the chosen tagSNPs is shown in Figure 6.<br>
<br>
Comparison to <software>Tagger</software><br>
The efficiency and coverage of tagSNPs selected by <software>Snagger</software> and the web server <software>Tagger</software> were compared using the same <database>HapMap</database> SNP data and <database>ENCODE</database> regions (Table 1). Design scores were obtained for all SNPs, and used to compare the genotyping reliability of tagSNPs chosen by <software>Snagger</software> and the web server <software>Tagger</software>. Identical parameters were used in identifying potential tagSNPs to ensure comparability between the two software programs.<br>
In terms of the number of tagSNPs selected, <software>Snagger</software> was more efficient than the web server <software>Tagger</software>. Across every <database>ENCODE</database> region, ten to thirty fewer tagSNPs were selected by <software>Snagger</software> than <software>Tagger</software> (Table 1a.). TagSNPs chosen by <software>Snagger</software> had comparable, if not higher design scores than those selected by <software>Tagger</software> (Table 1b.). Also, tagSNPs selected by <software>Snagger</software> provided comparable coverage of all SNPs of interest to those selected by <software>Tagger</software> (Table 1c.).<br>
<br>
Selection of tagSNPs outside a targeted region<br>
To evaluate <software>Snagger</software>'s ability to select tagSNPs outside of a targeted region, we looked across 76 gene regions. There were a total of 6282 common (MAF = 0.05) SNPs targeted for capture using CEPH and Han Chinese genotypes from <database>HapMap</database> Public Release 21a [21] and the Affymetrix GeneChip Human Mapping 500 K Array set [22].<br>
Choosing tagSNPs only from within each region yielded 1702 tagSNPs with an average Illumina design score of 0.902 (possible design scores were 0 through 1, or 1.1; a score of 1.1 is indicative of a successfully designed SNP assay). In either CEPH or Han Chinese populations, 75 SNPs were untaggable because they were within 60 base pairs of another tagSNP or had a design score equal to zero and could not be captured by any other tagSNP. In CEPH and Han Chinese populations there were 56 and 36 untaggable SNPs, respectively.<br>
When we allowed tagSNPs to be picked from outside the region there were 1731 tagSNPs selected with an average design score of 0.917. In either CEPH or Han Chinese populations, there were 61 untaggable SNPs, and in CEPH and Han Chinese populations there were 45 and 28 untaggable SNPs, respectively.<br>
<br>
<br>
Discussion<br>
The development and implementation of tagging SNP selection methodologies have received significant attention in recent years. Our program, <software>Snagger</software>, improves upon other tagSNP picking software by combining preferential tagSNP picking and the ability to select tagSNPs across multiple ethnic populations into one software package. With features not available in other software, including surrogate tagSNP picking to offset the risk of failed assays and the ability to pick better tagSNPs from outside a targeted region, <software>Snagger</software> improves coverage of genomic variation. Like <software>Tagger</software>, <software>Snagger</software> adds flexibility by allowing the user to force-include or force-exclude user-defined SNPs. The software is built on the basis of <software>Haploview</software>'s framework, making it both familiar and graphically appealing to the user, and includes <software>Haploview</software>'s LD plot and haplotype display, which allows the user to visually investigate patterns of variation.<br>
Our program, <software>Snagger</software>, has some similarities to the algorithm used by <software>Tagger</software>. Both programs create a bin (BinA in <software>Snagger</software>) for each SNP that contains the set of SNPs in high LD (e.g., r2 ? 0.8). <software>Snagger</software> distinguishes itself from <software>Tagger</software> by including a second step. To preferentially select SNPs with certain characteristics, <software>Snagger</software> creates a set of bins (BinB) with SNPs that are in LD with every other SNP in the bin. Calculated from a number of user-specified parameters, a SNP Score is assigned to each SNP. The SNP in each BinB with the highest SNP Score becomes the potential set of tags to pick from. Thus, a SNP Score that can be flexibly weighted allows the user to influence the characteristics of chosen tagSNPs. The second set of bins maximizes coverage while minimizing the number of tagSNPs selected. From our evaluation, we show that <software>Snagger</software> on average selects fewer tagSNPs than the web server <software>Tagger</software> when preferentially selecting tagSNPs on design score.<br>
<software>Snagger</software> offers the user the ability to preferentially pick SNPs that are located within a coding region or other genomic location. SNPs that either change an amino acid residue (known as non-synonymous SNPs) or are located in a 5' or 3' untranslated region are suspected to have a greater likelihood of having a biological effect [23,24]. The ability to prioritize these SNP offers added flexibility and many candidate-gene association studies of complex disease have included all known functional SNPs into their selection strategies. To our knowledge, no other available software package includes this feature.<br>
Another key feature of <software>Snagger</software> is the ability to weigh by probability of successful genotyping on specific high-throughput platforms. Genotyping failures can reduce effective genomic coverage, especially when tagSNPs acting as a proxy for many SNPs fail. <software>Snagger</software> addresses this by preferentially choosing tagSNPs with high probabilities of success, while maintaining efficiency in the number of tagSNPs selected. Though it may be necessary for the software to pick tagSNPs with lower probabilities of success in order to capture every SNP, the user can enforce a minimum design score for all tagSNPs. For reference, 18% of <database>HapMap</database> SNPs in the ten <database>HapMap</database> <database>ENCODE</database> regions have a probability of success below 0.776, which corresponds to an Illumina design score of 0.4, the default minimum. In addition, <software>Snagger</software> can select surrogate tagSNPs that will backup low-scoring tagSNPs that act as a proxy for several SNPs. Furthermore, since some genotyping platforms (e.g., Illumina) require that all tagSNPs being genotyped have a minimum base pair distance, <software>Snagger</software> can enforce a minimum distance between tags, which further reduces the chance of genotyping failure. We compared our program to the web server <software>Tagger</software> and show that the tagSNPs chosen by <software>Snagger</software> had comparable, if not higher design scores than those selected by <software>Tagger</software>.<br>
<software>Snagger</software>'s ability to select tagSNPs across multiple populations in a user-friendly manner is advantageous for studies involving multi-ethnic cohorts and admixed populations. Other software programs have focused on the most efficient way to select tags including <software>TAGster</software> [19], but do not include other features available in <software>Snagger</software>. Furthermore, we are currently extending the selection algorithm to incorporate haplotype information in addition to pairwise LD.<br>
<br>
Conclusion<br>
We developed a software application, <software>Snagger</software>, to select an efficient set of tagSNPs that captures the most genetic information and can reliably be genotyped. It is freely available and we include the executable (see Additional File 1), source code (see Additional File 2), user guide (see Additional File 3), sample SNP information (see Additional File 4), and sample <database>HapMap</database> data (see Additional File 5). It performs better than the web server <software>Tagger</software> by choosing fewer tagSNPs when weighting on design score, and performs equally as well in selecting tagSNPs that provide comparable coverage of genomic regions that can be genotyped successfully. In addition, our software program allows the user to conveniently select tagSNPs across multiple populations as well as from outside gene regions of interest, and to include surrogate tagSNPs as another way to offset the risk of failed assays. Moreover, <software>Snagger</software> allows the user to incorporate the probability of genotyping success in the SNP selection process and to give greater priority to, and subsequently choose, particular types of SNPs by functionality, location and MAF. These capabilities significantly improve upon current available tagSNP software packages.<br>
<br>
Availability and requirements<br>
Project Name: <software>Snagger</software><br>
Project home page and availability: <br>
Operating system(s): Platform independent<br>
Programming language: Java<br>
Other requirements: Java Runtime Environment 1.4.2_12 or higher<br>
License: MIT License<br>
Any restrictions to use by non-academics: None<br>
<br>
Authors' contributions<br>
CKE carried out the software engineering effort and performance analysis and participated in the drafting of the manuscript. WHL participated in the development of an alpha version of the program and the drafting of the manuscript. DL participated in the development of an alpha version of the program. DJV and DVC supervised the development of the program and the drafting of the manuscript. All authors read and approved the final manuscript.<br>
<br>
Appendix<br>
Tagging algorithm summary (see Figure 3 for overview)<br>
Input:<br>
? A set of SNPs S = {s1, s2,..., sm} within a contiguous genomic region.<br>
? A table containing r2 values for each pair of SNPs in S having a physical distance less than a user-specified threshold, such that the pairwise r2 value of two SNPs si and sj is defined as: r2(si, sj).<br>
? A set of SNPs I = {i1, i2,..., in} to force-include as chosen tagSNPs, where I ? S.<br>
? A set of SNPs E = {e1, e2,..., eo} to force-exclude from being chosen as tags, where E ? S.<br>
? A user-specified r2 minimum threshold defined as: r2min. All tagSNP-SNP pairs must have a pairwise r2 value that meets or exceeds this threshold.<br>
? A SNP Score function SS based on SNP design scores, other annotations, and user-defined weights.<br>
Output:<br>
? A set of tagSNPs T = {t1, t2,..., tr} such that T ? S and each t ? T tags a subset of SNPs in S.<br>
? A set of "untaggable" SNPs U = {u1, u2,..., us} such that U ? S and no tag SNP in T tags any SNP in U.<br>
Algorithm:<br>
1) Let C = {c1, c2,..., cp} be the remaining set of SNPs to capture, such that C initially contains the SNPs in S that are located within the region of interest.<br>
C ? S.<br>
2) Add all force-included SNPs to the final list of tagSNPs.<br>
For each ii ? I, add ii to T.<br>
3) Remove all SNPs from the set of SNPs that still need to be captured those SNPs that are tagged by the set of force-included tagSNPs.<br>
For all possible pairs of ti ? I and cj ? C, if r2(ti, cj) ? r2min, remove cj from C.<br>
4) Determine the remaining set of SNPs that can possibly be tagSNPs.<br>
Let P = {p1, p2,...,pq} be the set of potential tagSNPs such that P = S - E - I.<br>
5) Determine the set of SNPs for which each potential tagSNP can act as a proxy based on their pairwise r2 values.<br>
For each pi ? P, create a BinA such that p.BinA ? S. For each cj ? C where r2(pi, cj) = r2min, add cj to p.BinA.<br>
6) For each potential tagSNP, find a set of potential tagSNPs that can act as proxies for it and every other potential tag in the set.<br>
For each pi ? P, create a BinB such that p. BinB ? P and pi ? p. BinB, where all possible pairs of bj ? p. BinB and ck ? p. BinB, r2(bj, ck) ? r2min.<br>
7) For each potential tag, determine its best proxy according to the user-defined scoring function SS (e.g., highest probability of genotyping success) and add it to a temporary set.<br>
Let Q = {}. For each pi ? P, let pi-best be the SNP with the highest SNP Score, SS, in pi.BinB. Add pi-best to Q.<br>
8) From the temporary set of best proxies, choose the SNP that tags the most number of SNPs and add it to the final set of tagSNPs.<br>
Let t be the SNP in Q with the largest BinA. Add t to the set of tagSNPs T.<br>
9) Create a set of SNPs that will be removed from the list of potential tags.<br>
Let R = {}. Add t to R.<br>
10) Remove all the newly tagged SNPs from every potential tagSNP's BinA. If a potential tagSNP's BinA becomes empty, it can no longer be a tag and should be removed.<br>
For each pi ? P, p. BinA = p. BinA - t. BinA. If pi.BinA = {}, add pi to R.<br>
11) Remove all newly tagged SNPs from the set of SNPs that are left to be captured.<br>
Let C = C - t. BinA.<br>
12) Remove all the potential tags that were marked for removal from every potential tagSNP's BinB.<br>
For each pi ? P, let pi.BinB = pi.BinB - R.<br>
13) Remove all the potential tags that were marked for removal from the set of potential tagSNPs.<br>
Let P = P - R.<br>
14) If there are no more SNPs to capture or there are no more potential tagSNPs to choose from (i.e., if C = {} or P = {}), the tagSNP picking is done. Otherwise, choose the next tagSNP by repeating from Step 7.<br>
15) Mark any SNPs that still need to be captured as untaggable.<br>
Let U = C.<br>
<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2615452</b><br>
<software>EMAAS</software>: An extensible grid-based Rich Internet Application for microarray data analysis and management<br>
<br>
<br>
Background<br>
The post-genomic technologies, such as transcriptomics and proteomics, are continuing to produce new challenges for the biological community. All new technologies require new skills for interpretation, but in addition the 'omic technologies require expertise in data-management and exploration of multivariate data. All too often researchers are unable to extract the full benefit from their investment in research due to difficulties in applying best bioinformatics practice to their experiments. This problem is set to increase as more researchers adopt these high throughput methodologies, particularly with the move towards integrative and systems biology, where datasets are becoming larger, requiring extremely careful quality control at all steps, before cross-experiment data mining is meaningful or indeed possible.<br>
Additional issues hampering the bench scientist are those of software usability and implementation in a fast-developing research area. Array technologies are under continual development e.g. exon arrays, ChIP-on-chip, SNP chip as well as data now coming online from the non-array high through-put technologies such as ChIP-Seq and more recently high throughput transcriptomic sequencing (RNA-Seq).<br>
This is besides the wealth of statistical methods development leading to a multiplicity of new algorithms and tools, often fast-tracked to the public domain with rudimentary user interfaces or available only as scripts for command-line statistical packages such as <software>R</software> [1]. This leads to a time-lag in adoption for many scientists who may not have the necessary expertise or time to learn how to install or maintain multiple programs, often requiring multiple additional libraries, or to run a number of individual tools from the command-line or via custom scripts. The problem becomes more acute when we consider that the compute requirements for some analyses are becoming too large for comfortable analysis on individual desktop machines, due to a combination of dataset size and algorithm choice. For instance, the requirement to support normalisation across a hundred Affymetrix exon arrays, each with up to 5 million data points using for example the RMA pre-processing algorithm, is by no means unusual in a microarray experiment, but this could be too demanding for the hardware resources available on a desktop computer. This situation can be addressed by harnessing distributed computing resources, but again the start-up time investment is beyond the scope of most individual end-users.<br>
To facilitate microarray data analysis and management we have developed <software>EMAAS</software> (<software>Extensible MicroArray Analysis System</software>), a multi-user Rich Internet Application (RIA), utilising a distributed computing back-end. The <software>EMAAS</software> infrastructure supports:<br>
? Data transfer between specialised sites and data repositories<br>
? Single point access using a bespoke portal to a range of tools and packages for microarray analysis with seamless data flow between the various tools<br>
? Fast-track easy access for biological researchers via the portal to new models and algorithms developed in-house and externally e.g. by statisticians, computer scientists via modular wrapper implementations<br>
? Automated detailed tracking of all the analysis steps performed<br>
? Storage of raw data, analysis steps and analysed data in an underlying relational database<br>
? Ability to access online live expertise in data analysis from local support services for researchers, including remote audio-visual interaction between researchers and staff on different sites (e.g. shared live screen views, audio-video). This can be extended to include access for collaborating experts in other specialities e.g. statisticians.<br>
The system builds upon several open-source technologies already available for microarray data analysis, combining them to form a fully integrated user-friendly system. This allows the user to perform data management and analysis tasks through a single web interface.<br>
Numerous microarray tools are already available for various stages of the microarray data analysis workflow, including several client-server based tools such as the commercial packages <software>GeneSpring GX Workgroup</software>[2](licensed by Agilent) and <software>Resolver</software>[3] (licensed by Rosetta) and freely available packages such as <software>ExpressionProfiler</software>[4], <software>GenePattern</software>[5] and <software>Gecko</software>[6]. Each has its own advantages and disadvantages with respect to parameters such as cost, ability to handle concurrent users in a multi-user environment, scalability, and ease of use of the user interface.<br>
The aim of this project was not to re-write these tools in a static closed system, but to build a modular flexible framework that allows single-point access to existing tools and specialist websites running both on the local server and remotely, and to enable new algorithms, methods and web services to be added as and when they are developed. This enables a user to perform their analysis from start to finish through a single user interface, using the most appropriate data handling and analysis tools, without the requirement to continually update and install multiple programs on their own desktop machine. The system was designed specifically to support microarray analyses and was optimised with this in mind.<br>
<br>
Implementation<br>
Overview<br>
<software>EMAAS</software> is a web based tool developed using one of the latest internet technologies, resulting in a user-friendly and interactive interface accessible from any of the most popular web browsers. All the data analysis takes place on the server side and the user does not have to install any software on their personal computer.<br>
The interface is designed to guide the user through a standard microarray analysis workflow. Each stage of the analysis is tracked, with the results and algorithms, and parameters saved to a database on the server, allowing users to re-visit their data analysis results at a later date and enabling support staff to view the analysis workflow. The interface also includes access to interactive video tutorials and a remote support tool, which allows users and support staff to communicate with each other and to view each others user interface.<br>
On the server side, <software>EMAAS</software> can interact with 3rd party analysis resources located locally, such as <software>R</software> and the <software>Affymetrix Power Tools</software>, or resources hosted further afield at remote facilities such as public data repositories, through the use of distributed computing services. This architecture allows multiple users to access <software>EMAAS</software> resources at the same time as well as enabling users and support staff to share data. It also allows the functionality of the system to be extended with minimal disruption to the users.<br>
The implementation of various technologies integrated into <software>EMAAS</software> will be described in more detail in the following sections. See table 1 for a list of resources and tools integrated into <software>EMAAS</software>.<br>
<br>
<software>EMAAS</software> Architecture<br>
User Interface<br>
The user interface (as shown in figure 1) was developed using the web-based Rich Internet Application (RIA) framework <software>OpenLaszlo</software> [7]. <software>OpenLaszlo</software> is an open source platform for creating zero-install web applications, which results in the end user not having to deal with any complex installation issues.<br>
Laszlo code is compiled into a Flash executable and this gives the interface cross platform compatibility, as such, <software>EMAAS</software> can be used in any browser that has Flash capabilities, which according to a recent survey is over 98% of Internet-enabled computers [8]. This includes access through some of the most commonly used browsers such as <software>Internet Explorer</software>, <software>Firefox</software> and <software>Safari</software>, running on Windows, Mac and Linux operating systems. Although the <software>EMAAS</software> interface is web-based, Laszlo provides many of the capabilities and the look and feel of standalone desktop software, allowing a feature-rich, user friendly interface to be developed, with interactive control and visualisations.<br>
Considerable research into the preferences and work practices of researchers with varied levels of expertise in microarray analysis was used to inform the design of the user interface. We believe that this has resulted in a system that will be equally suited to those just starting to learn microarray analysis, or occasional users, as well as those with more experience, or performing larger, more complex analyses. The interface also enables users to send feedback and comments to <software>EMAAS</software> administrators in real-time, either through emails to a help address, or through a suggestion box integrated into the <software>EMAAS</software> interface.<br>
<br>
Administration Interface<br>
<software>EMAAS</software> is a multi-user application. An administrative interface is available for users who have <software>EMAAS</software> administration rights and is used to set data access privileges for users at the individual or group level. This enables users who are working together on the same project to share their data and the analysis results if required. It also allows 'privileged' support staff to gain access to user's data and analysis pipelines in order to see how the analysis has been performed and to carry out alternative methods of analysis on the same data set where appropriate.<br>
<br>
Server Side<br>
The <software>EMAAS</software> web server is based on <software>Apache-Tomcat</software>. This contains the Laszlo compilation and runtime engine. The EMAAS-Laszlo user interface interacts with the server side components, such as the <database>EMAAS</database> database, grid and distributed resources though Java Server Pages and Java servlets. Data and additional information are fed back to the Laszlo interface as dynamically generated XML.<br>
Several data analysis software packages have been integrated into <software>EMAAS</software>, see Figure 2 for an overview of the system architecture. The main analysis tool integrated to date is <software>R</software> together with <database>Bioconductor</database> [9]. Several technologies for programmatically calling <software>R</software> were assessed including <software>Rserve</software> [10], <software>Taverna</software> [11], Java system calls and the <software>Postgres PLR</software> library [12].<br>
For automated data analysis, the <software>Postgres PLR</software> library has been implemented. <software>PLR</software> allows R functions to be called programmatically within SQL statements. One of the main reasons for using <software>PLR</software> is that it allows data to be passed directly from the database to the R environment and back again in a robust manner. Thus the results from each step of the data analysis, in the form of an R Data Frame for example, can be easily serialised into the <database>EMAAS</database> database, which greatly facilitates data handling, data persistence and analysis tracking. <software>Postgres</software> is used as the underlying <database>EMAAS</database> database technology. To add new analysis R scripts in the current system, the R code is wrapped into a <software>PLR</software> function and the method details are referenced in the 'functions' database table. The Analysis user interface has then to be updated to capture the method parameters required for the new algorithm.<br>
Also integrated in the current version of <software>EMAAS</software> are the <software>Affymetrix Power Tools</software> (<software>APTs</software>) [13], a set of cross-platform command-line programs that implement algorithms for analyzing and working with Affymetrix GeneChip arrays. The <software>APTs</software> were added in particular for processing Affymetrix Exon arrays, but can also be used for processing Affymetrix 3' GeneChips, SNP chips and Gene Expression arrays. <software>APTs</software> have been developed in C++ and have been designed to make efficient use of available hardware resources.<br>
<br>
Distributed Computing<br>
The system has been designed to make use of available distributed computing resources in order to speed up various steps of the data analysis workflow and to allow access to high throughput compute resources to perform analysis that may not be possible on a single desktop machine.<br>
Data workflow tasks such as pre-processing, quality assessment and analysis require a large amount of compute power, particularly in a multi-user environment involving large data sets. Furthermore, some tasks, such as the generation of QA plots, can be done in parallel using distributed computing on grid clusters, in which case the overall duration of the plot generation and data processing can be significantly reduced.<br>
The <software>APTs</software> and <software>R</software>-<database>Bioinconductor</database> have been installed throughout nodes on a compute cluster managed by a Distributed Resource Manager (DRM). In this case <software>GridEngine</software> [14] is used, but other DRMs, such or <software>Condor</software> [15], could also be integrated using the same architecture. The cluster is accessed from the web server using <software>GridSAM</software> [16], an open-source job submission and monitoring web service, designed and implemented in the London e-Science Centre and commissioned by the UK Open Middleware Infrastructure Institute (OMII) [17]. <software>GridSAM</software> endorses the WS-I set of web service standards and the Job Submission Description Language and provides a Web Service for submitting and monitoring jobs managed by a variety of DRMs. The modular design allows third-party to provide submission and file-transfer plug-ins to <software>GridSAM</software>.<br>
The scalability of the system is essential, as the size of data sets increases, more users analyse their data using <software>EMAAS</software> and analysis methods become more sophisticated and resource hungry. This architecture allows any number of additional compute nodes to be added to provide additional compute resources for analysis steps, and together, these DRM tools transparently and seamlessly manage all job scheduling tasks without any user intervention.<br>
<br>
Extensibility to other Microarray Platforms<br>
The statistical analysis component of the pipeline takes place in an R environment and is based around the 'ExpressionSet' R class. The ExpressionSet object holds both the microarray data and the experimental meta-data and was designed to be generic enough to accommodate data from any microarray platform. A bespoke JSP function and <software>PLR</software> wrapper would have to be developed in <software>EMAAS</software> for each microarray platform, such as Illumina or Agilent, in order to import data and to create an ExpressionSet object. Then the analysis can be performed as described in the 'Data Analysis Workflow' section below.<br>
<br>
Distance Support Tools<br>
Several tools have been assessed for integration within <software>EMAAS</software> to provide real time analysis support for users. These tools include <software>EVO</software>[18], <software>NetMeeting</software>[19], <software>VNC</software>[20], <software>VRVS</software>[21], <software>Webex</software>[22] and <software>Copilot</software>[23]. Each has its own advantages (e.g. free, cross-platform compatibility) and disadvantages (cost, single platform, hard for local user to set up, and firewall issues) over the others.<br>
The web conferencing tool <software>EVO</software> (<software>Enabling Virtual Organizations</software>) was selected as the integrated distance support tool of choice. This is a freely available open source tool that allows bioinformatics support staff to set up private meetings with a user or a number of users simultaneously. Tools are available in <software>EVO</software> for instant message, audio and video communication, real-time sharing of screens between user and support staff, whiteboard facility to share on the fly notes and file transfer.<br>
As part of our ongoing commitment to usability and training, a number of short training videos have been developed and are available from the <software>EMAAS</software> web interface to guide users through different aspects of data analysis. Also being currently assessed is the use of several flash-based multi-media tools available in Laszlo for real time user interactivity, such as web camera API's, coupled with the open source streaming media server <software>red5</software>[24] to provide live screen grabs of the support staff and user's interfaces.<br>
<br>
<br>
Data Analysis Workflow in <software>EMAAS</software><br>
Data Import and Management<br>
Data import is supported from a number of different sources. Users can upload their data from their local file system. A file system browser launched from the <software>EMAAS</software> interface allows a user to search and select data files on the local file system. Selected files are transferred to the <database>EMAAS</database> database.<br>
<database>MiMiR</database> is a MIAME-compliant microarray data warehouse supporting over 200 research group at the MRC Clinical Science Centre/Imperial College and 2 international consortia [25]. <database>MiMiR</database> provides a secure environment for collection, capture, comprehensive and consistent experimental annotation and dissemination of data [26]. It is fully integrated with <software>EMAAS</software> through the <database>EMAAS</database>-<software>MiMiR</software> web interface, see Figure 3 for screenshots.<br>
The <software>EMAAS</software> web interface exercises the <database>MiMiR</database> middleware programming interface ? a collection of java classes and methods developed to allow secure access to information in <database>MiMiR</database> from 3rd party applications, such as <software>EMAAS</software>. A user may query and export experimental data and metadata that they have access rights to, including data which has been made publically available from <database>MiMiR</database>. A user may select all, or a subset, of Affymetrix .cel files from a <database>MiMiR</database> experiment. Cel files from different experiments for the same array type can be imported into single <software>EMAAS</software> analysis. Once a selection has been made the .cel files and associated user selected meta-data is then imported via the <database>MiMiR</database> middleware layer into the <software>EMAAS</software> environment for processing and analysis.<br>
Users can also import data from the <database>Gene Expression Omnibus</database> (<database>GEO</database>)[27] without leaving the <software>EMAAS</software> interface. A user selects the Affymetrix platform type from a drop down menu and can enter an optional keyword to limit the search. <software>EMAAS</software> communicates in real time with the <database>GEO</database> server using the <database>GEO</database> web services and a list of experiments are returned and displayed to the user. The experiments are filtered to display only those that have Affymetrix cel files associated with them and contain the user's keyword in the title or experiment description.<br>
Data can also be imported directly from the online public Affymetrix data repository <database>CELSIUS</database>[28]. As well as collating the raw data .cel files and associated meta-data from <database>ArrayExpress</database>[29]and <database>GEO</database>, <database>CELSIUS</database> also regularly reaps data from smaller public data repositories such as UCLA Los Angeles DNA Microarray Core Facility, MIT/Broad Institute and University of Pennsylvania Microarray Core Facility, with very little overlap of datasets between them. <software>EMAAS</software> communicates in real time with the <database>CELSIUS</database> data repository via the <database>CELSIUS</database> web services.<br>
<br>
Experimental Design<br>
An <software>EMAAS</software> Project is a collection of the same array types, allowing arrays to be collated into the same project from several sources. An experimental design describes which arrays belong to which biological factors. One or more experimental designs can be created within the same project. The experimental design view uses the interactive features of OpenLaszlo to drag and drop arrays to set up the biological factors that are to be compared. This dynamically builds up an XML dataset describing the experimental design.<br>
When the experimental design has been set up and saved in the <software>EMAAS</software> interface, the XML schema describing the design is sent to the server and propagated through the data analysis workflow to suggest suitable pre-processing and analysis methods. The experimental design XML is transcribed into an R phenoData file on the server which is then used to read the data into R and to create the expressionSet data object. This expressionSet is serialized to the database for persistence and also saved as a global variable in the R-session for speed of access during the current analysis session.<br>
<br>
Quality Assessment Analysis<br>
A further advantage of using <software>R</software>-<database>Bioconductor</database> for microarray data analysis is that it has a large number of useful quality assessment (QA) plots available such as boxplots, PCA plots, simpleAffy[30] as well as others also found in the <package>RReportGenerator</package>[31] and <package>arrayQualityMetrics</package> packages[32]. However, these plots can take some time to generate ? from a few seconds up to several minutes each ? which can tie-up the analysis resources.<br>
To overcome this problem grid compute clusters are used to generate the plots in parallel using the <software>GridSAM</software> and Grid Engine tools. <software>GridSAM</software> is used to pass data and launch the QA plot jobs on a Grid Engine-managed cluster, with each of the nodes having a locally installed instance of the <software>R</software>-<database>Bioconductor</database> tools. The data, experiment description, <fileFormat>phenoData</fileFormat> text file and a QA R script are passed to a compute node and R is invoked. The resulting QA plot is returned to the <database>EMAAS</database> database and the image displayed in the user interface. The small overhead in time required to copy data is negated by the saving in time it takes to produce all the plots, and the freeing up of resources for other analysis tasks. This system is scalable to allow other computationally intensive analysis algorithms to be integrated in a modular manner.<br>
A further issue with the R plots that are generated is the lack of user interaction, the plots are often static images, and it is sometimes difficult to determine which arrays or data points are the outliers. To overcome this, the data used to generate the charts is passed back to the client interface as xml, which is then used to generate interactive Flash charts.<br>
<br>
Data Pre-processing &amp; Normalisation<br>
Affymetrix 3' Gene Array<br>
There are several pre-processing steps available in <software>R</software>, including <package>MAS5</package>[33] and <package>RMA</package>[34]. After the user has selected a method from the interface, the experimental design expressionSet object is obtained from the <software>PLR</software> session global variable, or unserialized from the database. This object is pre-processed using the selected algorithm and the resulting R dataframe is assigned to a <software>PLR</software> global variable, as well as being serialized to the database.<br>
<br>
Exon Array<br>
The <software>EMAAS</software> GridSam-GridEngine architecture has also been used to facilitate Affymetrix Exon array analysis. <software>Affymetrix Power Tools</software> (<software>APTs</software>) installed on the grid cluster nodes are used to process the raw data files. From the <database>EMAAS</database> web interface, the user selects the pre-processing algorithm of choice, e.g. RMA, PLIER[35], MAS5, and the summarisation level ? either gene or exon, which is used to construct the APT command. The raw data .cel files are automatically transferred to the grid cluster and processed using <software>APTs</software>. The resulting summarised data file is returned to the <software>EMAAS</software> server, imported into R, the expressionSet object is generated and serialized to the database.<br>
<br>
<br>
Differential Expression Analysis<br>
The pre-processed expressionSet object can then be analysed to detect differentially expressed genes or exons using the <database>Bioconductor</database> <package>Limma</package>[36] or <package>Exonmap</package>[37] analysis packages through the <software>EMAAS</software> interface. The infrastructure is in place to allow further analysis algorithms developed within R, or using another technology to be added as required.<br>
Results are passed back to the <software>EMAAS</software> interface as xml and displayed in an interactive table, allowing users to sort data, highlight data and select which columns of information and data are displayed in the table. Double clicking on a gene row generates an interactive expression profile plot and selecting on the plot pulls up the relevant <database>GeneCard</database> [38] for the selected gene. <database>GeneCards</database> is an online accessible database which provides a vast array of information on all known and predicted human genes.<br>
Users can also copy and paste selected rows of interest into 3rd party applications, such as <software>Microsoft Excel</software>, for further analysis.<br>
<br>
Functional Annotation Analysis<br>
Gene annotations are retrieved automatically from the <software>R</software> annotation library packages if available. The <package>R-Biomart</package> package [39] can also be used to retrieve annotation information for genes of interest. Exon annotation is retrieved from an <database>EMAAS</database> database table built using the Affymetrix annotation files [40]. This requires periodic rebuilding as and when Affymetrix update their annotation files.<br>
Gene lists generated during analysis can be passed directly from the <software>EMAAS</software> interface to <software>DAVID</software> (<software>The Database for Annotation, Visualization and Integrated Discovery</software>), a web site for functional enrichment analysis, which enables the discovery of biological groups of potential interest associated with a particular gene list. Submission to <software>DAVID</software> is accomplished through a modified version of the public <software>DAVID</software> programming API. The API allows users to submit a list of genes with all relevant parameters and directly analyze the list in any of <software>DAVID</software>'s analysis modules. This differs from the typical approach of using <software>DAVID</software> by automatically handling the list submission, list property selection, and workflow normally required to begin using a given analysis module. In addition, the modified version of the API does not restrict list size, number of submissions, and duration between submissions as does the public version of the API.<br>
<br>
Download data and Export of Results<br>
The R workspace object for any of the experimental designs can be downloaded from the <software>EMAAS</software> interface. This enables users to continue analysis in their local installation of <software>R</software> if so desired. User-selected data from the results table can also be copied to the clipboard for use in 3rd party applications<br>
<br>
<br>
<br>
Results<br>
See table 1 for a full list of resources integrated into <software>EMAAS</software><br>
The current local version of the <software>EMAAS</software> web server is deployed on a server configured with two dual-core AMD Opteron processors and 8 Gb RAM running Apache Tomcat. The database server has two dual-core AMD Opteron processors and 32 Gb RAM. The database is stored on a local RAID 10 array providing 1.8 Tb of storage. Both the database and web server are running the Red Hat Enterprise 5.2 operating system.<br>
The grid cluster currently used as execution hosts, hosted by the London e-Science Centre, has 248 compute nodes, each with a dual core 2 GHz or 2.8 GHz P4 processor, 2 GB RAM and a Fast Ethernet connection.<br>
<software>EMAAS</software> has been successfully tested with 32 demonstration users concurrently logged into the system, and simultaneously downloading 100 Affymetrix Rat Genome 230 2.0 arrays from the <database>MiMiR</database> data warehouse, setting up experimental designs, RMA pre-processing the data, performing a t-test with multiple testing p-value corrections to determine differentially expressed genes, retrieving gene annotation and passing selected interesting gene lists to <software>DAVID</software>.<br>
The database server has been successfully tested with 64 concurrent virtual users making simultaneous requests to RMA process 132 Mouse 430 2.0 Affymetrix arrays. RMA processing of the data is currently the most computationally intensive step of the data analysis workflow.<br>
<br>
<br>
Discussion<br>
<software>EMAAS</software> has been successfully implemented and can be used by researchers to perform microarray data analysis at the gene level or exon level.<br>
Although we are aware of the large numbers of microarray tools currently available, including the well established and utilised <software>BASE</software>[41], <software>SMD</software>[42] and <software>MeV</software>[43], we believe that the following combination of features of <software>EMAAS</software> gives it a unique place in the microarray data analysis field:<br>
? Web based rich internet application ? no installation requirements on the client side<br>
? User friendly intuitive interface ? drag and drop and interactive visual components facilitate the analysis pipeline<br>
? All integrated resources are accessible from a single user interface, users do not have to learn how to use multiple interfaces<br>
? Seamless integral access to grid compute resources ? enabling improved performance and also facilities the scaling up of the system as more users come online<br>
? User friendly intuitive interface ? all integrated resources are accessible from a single user interface, users do not have to learn how to use multiple interfaces<br>
? Integrated with the <database>MiMiR</database> microarray data warehouse for access to raw data and richly annotated meta-data<br>
? Data and analysis results stored on centrally maintained server ? no risk of loss of data on users local computer<br>
? Integrated access to public data repositories (<database>GEO</database> and <database>CELSIUS</database>) allowing data in the public domain to be searched and imported for analysis with data sets previously imported into <software>EMAAS</software><br>
? Analysis tracking ? each step of the workflow is captured, stored and can be revisited<br>
? Data and analysis results can be shared between users<br>
The <database>EMAAS</database> interface has been created in the RIA technology <software>OpenLaszlo</software>, to create a user friendly, interactive and intuitive web interface.<br>
<software>EMAAS</software> has integrated <software>R</software> into its system. <software>R</software>, in combination with <database>Bioconductor</database>, is considered to be one of the most powerful, flexible and most widely used microarray data analysis tools, yet at the same time can be difficult for the beginner to use. It is driven from the command line and has its own object oriented programming language which is not intuitive, particularly for those who are not familiar with programming concepts or statistical methodologies. <software>EMAAS</software> allows users access to the wealth of microarray functionality available in <software>R</software>.<br>
However, <software>EMAAS</software> analysis functionality is not just limited to <software>R</software>. The <software>EMAAS</software> architecture has been designed to allow other tools to be added where appropriate in a modular fashion, either located locally, such as the <software>APTs</software>, or located at remote facilities such as <software>DAVID</software> and <database>CELSIUS</database>.<br>
Learning the practicalities of undertaking meaningful microarray data analysis is a non-trivial subject area. <software>EMAAS</software> has been designed to facilitate distance support interactions between bioinformatics support services and users in their microarray analysis. Support staff can communicate in real-time with users using the web conferencing tool <software>EVO</software>. The administration interface allows support staff to access the user's data where required, whilst maintaining secure password-protected user and group-access designations.<br>
Future Directions<br>
Although current analysis functionality is somewhat limited to the building blocks of standard Affymetrix data analysis workflows, the framework already in place is flexible and scalable so that new data analysis algorithms and methods can be integrated with relative ease. Additional analysis methods will be added according to user requirements in the beta testing phase. The functionality to analyse imported data from different array platforms such as Agilent, Illumina and custom 2-colour arrays is already available in <software>R</software>, as is the functionality for the analysis of other array methodologies such as SNP chip and ChIP-chip arrays.<br>
<software>EMAAS</software> can currently be extended to use these technologies. In the present version new analysis methods are added via the server side technologies. However development work is currently being carried out to allow users to add new methodologies and R scripts directly via the <software>EMAAS</software> web interface.<br>
Work is already underway to allow users to create and re-use data analysis workflows, to add extra functionality for functional enrichment analysis and to add methodologies for co-expression analysis.<br>
<br>
<br>
Conclusion<br>
<software>EMAAS</software> is a user-friendly, flexible and scalable microarray data analysis platform, which integrates several commonly used data analysis tools including <software>R</software>-<database>Bioconductor</database>, <software>Affymetrix Power Tools</software> and <software>DAVID</software>, making them accessible through a single web interface. <software>EMAAS</software> has been designed with user support in mind to aid users through their data analysis.<br>
<br>
Availability and requirements<br>
? Project name: <software>EMAAS</software><br>
? Project home page: <br>
? Operating system(s): interface: cross-platform; Server side: Linux<br>
? Programming language: Java, <software>OpenLaszlo</software>, R<br>
? Other requirements: <software>R</software>, <software>Postgres</software>, <software>PLR</software>, <software>GridSAM</software>, <software>Apache-Tomcat</software>, <software>OpenLaszlo</software><br>
? License: A tar archive containing binaries for server-side remote installation of the <software>EMAAS</software> system on Linux operating systems is available for download from .<br>
The user interface is publically available at <br>
<database>MiMiR</database> can be accessed via <database>MiMiR Online</database> from the Microarray Centre-MiMiR User Centre web pages .<br>
<br>
Abbreviations<br>
API: Application Programming Interface; <software>APTs</software>: <software>Affymetrix Power Tools</software>; ChIP-chip: Chromatin immunoprecipitation on a microarray chip; <software>DAVID</software>: <software>The Database for Annotation, Visualization and Integrated Discovery</software>; <software>EVO</software>: <software>Enabling Virtual Organizations</software>; MIAME: Minimum Information About a Microarray Experiment; <database>MiMiR</database>: <database>Microarray data Mining Resource</database>; PDF: Portable Document Format; <software>PLR</software>: postgres Procedural Language for R; QA: Quality Assurance; RIA: Rich Internet Application; SNP: Single Nucleotide Polymorphism; XML: eXtensible Markup Language.<br>
<br>
Authors' contributions<br>
GB designed and developed the <software>EMAAS</software> user interface, designed and developed the <software>EMAAS</software> <software>Postgres</software> DB, Wrote <software>PLR</software> functions and integrated <software>APTs</software> and video tutorials. MK wrote the installation scripts, QA pdf report generator and was involved with the integration of the distributed computing tools. NC developed the <software>EMAAS</software> query interface for <database>MiMiR</database>, <database>GEO</database> and <database>CELSIUS</database> and integrated the administrative interface. JMS Developed the parallel computing code, including running QA jobs on the GridEngine-Managed Cluster vie <software>GridSAM</software>, and developed the Administrative interface.<br>
JA set up and maintained the project development and deployment servers and also set up and maintains the project website. AS set up the project infrastructure tools including SVN, ant and the project wiki. BT Initiated the <software>EMAAS</software>-<database>MiMiR</database> interface. YH Initiated the GridSAM Parallel computing code.<br>
SAB first conceived the idea of the system together with MJS, led the project, and together with CT, MS, TA, LG &amp; JD provided insights on software development and testing and critically reviewed the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2689866</b><br>
Automated FingerPrint Background removal: FPB<br>
<br>
<br>
1 Background<br>
The construction of a whole-genome physical map [1-6] has been an essential component of numerous genome projects initiated since the inception of the Human Genome Project [7]. Its usefulness has been proved for whole-genome shotgun projects as a post-assembly validation and recently it has also been used in the assembly step to constrain on BACs positions [8].<br>
High-throughput fingerprinting can produce thousands of BAC clone fingerprints per day. Hence automatic editing of corresponding files can be extremely useful.<br>
From now on the description of the process will be based on fingerprints produced on Applied Biosystems Instruments (ABI) automated DNA sequencers, but the method can be applied to any kind of fingerprint as long as similar input is provided. Moreover, the following terms will be used interchangeably since they correspond to the same entity: fragment, band, and peak.<br>
Fingerprint data is stored in electrochromatograms, i.e. fsa files, output by <software>GeneMapper</software>, ABI sequencer software. Each peak represents a fragment with a certain size and intensity (fig. 1) and it can derive from different sources (see Additional file 1 for details and examples):<br>
? "true peak" derived from a DNA insert digested band;<br>
? low signal peak produced by the machine;<br>
? partial digestion related peak;<br>
? star activity by-product, see [9];<br>
? E. coli genomic DNA band;<br>
? vector band;<br>
? out of size standard range band (with unreliable sizing);<br>
? wide area peak (unreliable, resulting from co-migrating fragments).<br>
Last three kinds of peaks can be removed in a preprocessing step. While, low signal peaks produced by the machine, partial digestion related peaks, star activity by-products, and E. coli genomic DNA bands can be considered as background signal that needs to be subsequently removed to allow a correct assembly by FPC [10,11], nowadays the only physical map assembler. The vast majority of background peaks is usually related to E. coli genomic DNA bands; they tend to exhibit a lower signal than true peaks and therefore they can be removed by computing a threshold below which data needs to be rejected (including low signal peaks produced by the machine). Partial digestion related peaks and star activity by-products tend to produce similar patterns with intermediate signal peaks that, again, can be removed with the use of a threshold on intensity of the signal.<br>
<br>
2 Implementation<br>
2.1 Data<br>
ABI-produced electrochromatograms basically code for "Dye-Sample Peak" (dye and fragment number), "Sample File Name" (clone name), "Size" (fragment length), "Height" (peak intensity), and "Area" (peak's area).<br>
The approach of FPB to remove background includes a preprocessing step where fragments out of size standard range, vector bands, and wide area peaks are removed. Then, to achieve the final goal of a correct map assembly, FPB follows the following principle:<br>
Safety principle<br>
It is better to exclude a few true peaks than to include lots of background.<br>
Such a principle sacrifices sensitivity in favor of specificity for two reasons: first, low sensitivity is counter-balanced by the level of physical coverage needed for a successful physical map, second, high specificity is needed for a correct assembly (remember the example above on vector bands).<br>
The major bottleneck in fingerprinting background removal is represented by BAC-DNA quality: from clone to clone the number of distinguishable true peaks can vary highly. In general, the intensity of true peaks is much higher than that of background peaks, consequently our task may be reduced to the determination of a threshold below which peaks can be rejected. Unfortunately, for a number of different reasons, lots of scenarios can arise:<br>
? clear fingerprints with an evident gap between true and background peaks where to put the threshold, fig. 2(a);<br>
? fingerprints with some low signal true peaks, with no evident gap, fig. 2(b). In this case it is reasonable to remove most, if not all, of the background peaks preserving the most reliable true peaks only, in accordance with the safety principle;<br>
? fingerprints with high signal background peaks, fig. 2(c), very low signal true peaks, fig. 2(d), or without true peaks (empty fingerprints), fig. 2(e). A high threshold should be set or all fragments should be rejected;<br>
? fingerprints with partially digested fragments or by-products of star activity. Such fragments tend to exhibit an intermediate signal intensity, between true and background signal peaks. In accordance with the safety principle they should be rejected.<br>
See table 1 and Additional file 2 for a distribution of possible scenarios occurring in a single project.<br>
At present most of physical mapping projects use fluorescent digestion with SNaPshot method [12] obtaining fragments with four possible dyes and the size standard internal marker. Obviously, the process of background removal has to be iterated for each different dye. FPB has directly been tested on ABI 3730 Genetic Analyzer data exported by Genemapper to tabulated text files from original fsa files. Nevertheless, the method can be applied to different kind of processes, e.g. single dye on agarose gel fingerprints, as long as similar input is provided.<br>
<br>
<br>
2.2 State of the art and novel method<br>
A few solutions to the problem of removing background from fingerprints have already been proposed. In particular, the first tool used for agarose-gel based fingerprints was <software>Image</software> [13], suitable for small projects since high human interaction is required for true bands determination. A more automated alternative to work on agarose-gel based fingerprints has recently been developed [14]. Unfortunately, such fingerprints suffer of a deep problem: the inaccuracy of bands sizing, i.e. a single band can be sized even a few bp apart from its real value. In truth, also capillary electrophoresis based fingerprints slightly suffer of inaccurate band sizing but in a consistent way.<br>
At the moment, the only tools available to remove background from capillary electrophoresis based fingerprints are <software>GenoProfiler</software> [15,16] and its descendant <software>FPMiner</software> [17]. Unfortunately, the algorithm used in the main background removal step requires a fine tuning by the user of empirically determined parameters (see first method below for details); such a task becomes very hard when analyzing thousands of capillary electrophoresis based fingerprints that may be highly variable in signal intensity.<br>
A similar context in which background removal was successfully applied to is "component detection in liquid chromatography/mass spectrometry" [18,19]. In this case, the use of standard background techniques was of limited success [19]. But in [18] the authors propose to use, not only the true signal, but also the background signal to discriminate between the two signals, and they also propose to refine such discrimination in a step-wise manner.<br>
Next, the description of three methods to remove background after the preprocessing step are presented. The first method is the one used in <software>GenoProfiler</software>, while the second and the third approaches exploit similar ideas as in [18] to refine the discrimination of the two signals.<br>
First method<br>
The first method, initially implemented in the lab of DuPont Crop Genetics Research and then coded into <software>GenoProfiler</software> [16] relies on the assumption that the threshold is linearly related to the intensity of few true peaks.<br>
After sorting all peaks of a dye of a clone in descending order by height, the problem of determining the corresponding threshold can be converted to finding the index of the lowest putative true peak or, if an overlap of true and background peaks occur, finding an index k such that the set of the k highest peaks contains as many true peaks as possible and as few background peaks as possible. In practice, the first method considers only a few putative true peaks, let us say from the i-th to the j-th (first i-1 peaks are not taken into account since they can be considered artifacts or not reliable data, e.g. dye blobs), computes the average of their heights to establish, to a certain extent, how high is the signal of true peaks, and multiplies such a value by a constant factor, e.g. 0.35, previously empirically determined library per library, dye per dye (if fluorescence was used); the value obtained is considered the threshold. This is done for each dye of each clone. At this point, apart from the first i-1, only peaks above the threshold are considered true ones. Obviously this approach is quite limited: first of all the determination of constants of multiplication has to be done empirically for each dye and for each library and it relies on the assumption that the signal is highly consistent among different clones in a library. Then, to compute the mean value, it relies only on few peaks without caring of data variability.<br>
<software>GenoProfiler</software> codes for another method to remove background, based on the frequency of peak heights; unfortunately, such a method does not have general applicability: cases as that in fig. 2(c) cannot be solved correctly (we also tried to approximate the distribution of peak heights with curves but with limited success because data variability present in the different scenarios does not respect a fixed distribution; in particular lowest peaks of background are highly abundant and their distribution could be approximated with a gaussian curve while the distribution of higher background peaks could not be distinguished from that of true peaks; an example of unsuccessful application of such a method is directly given by the black line in fig. 1 of the <software>GenoProfiler</software> paper [16]).<br>
Hence the large variation in the quality of fingerprints that is usually present in large fingerprinting projects represents a major difficulty in the correct removal of background peaks that has only been partially addressed by the methods so far adopted that all require a long manual optimization of parameters.<br>
<br>
<br>
Second method<br>
A few definitions are introduced to help the reader in understanding this method.<br>
Definition. high limit (hl), is a value above which peaks are considered to be real one (apart from artifacts).<br>
Definition. high average (ha), is the average value of peak heights greater than hl (apart from artifacts).<br>
Definition. low limit (ll), is a value below which peaks are considered to be background.<br>
Definition. low average (la), is the average value of peak heights lower than ll.<br>
The second method considers more peaks. It still sorts all peaks of a dye of a clone in descending order by height, then it computes ha and la based respectively on given hl and ll. hl usually corresponds to the height of the j-th peak of the first method while ll corresponds to a height below which it can be assured there are no true peaks, e.g. if each dye is supposed to have at most n true peaks then ll can be assumed to be the height of the (n+1)-st peak. Comparing the two values obtained for ha and la and aware of the safety principle, it is possible to forecast a model for true and background peaks: smaller is the ratio of ha and la, closer to hl must be the threshold. A graphical representation of this method is presented in fig. 3. In the case n peaks are not available then ll is set to the height of the 5-th lowest peak, considering the last 5 peaks background. In general, if the ratio between ha and la is low, as in fig. 2(e), then the clone is rejected.<br>
A big problem of this approach stands in the difficulty of finding a good function to convert a ratio to a threshold. Moreover, another problem is the consideration of only few peaks on the true side, i.e. not enough data is considered to determine a reliable high average and consequently the threshold cannot be accurate.<br>
<br>
Third method<br>
Herein the second approach is iterated to consider as many peaks as possible. At each iteration, possibly, more high and low signal peaks are considered and a new ratio is computed. Again, a low number of true peaks is initially considered and at later iterations, if more true peaks are available, it is increased. Before giving the details of the final algorithm, two more definitions are introduced.<br>
Definition. initial gap (ig), is the difference between ha and la as they are computed in the second method. ig = ha1-la1, where indexes represent the current iteration.<br>
Definition. At the i-th iterative step, displacement is a portion of the difference between hli and lli, the difference between hai and lai, and the initial gap. It is used to decrease hli and partially to increase lli at each iteration (background tends to vary less than true peaks) to allow the computation of hli+1 and lli+1, next limits true and background peaks.<br>
In the implementation of FPB the following formulas are used:<br>
<br>
suitable for a fast convergence of the iterative process and reliable enough on splitting correctly true and background data at each iteration (data not shown, based on empirical tests). The iteration stops when a gap is reached (no more peaks are available) or when the two limits meet. In both cases, the threshold is set to the value of hl after the last iteration. It has to be noticed that the 20%ig contribute of the displacement constrains the maximum number of loop iterations to be five.<br>
Implementation<br>
We developed FPB, FingerPrint Background removal, a Perl script with a Perl/Tk GUI, which codes for the third method. The program also converts data to suit FPC input format. The use of GeneMapper exported tables provides accurate peak sizing. The program is freely available at  along with some additional scripts to visualize data on a Linux platform.<br>
<br>
<br>
<br>
3 Results and Discussion<br>
FPB can work with any fragment length and on exported tables from both agarose-gel and capillary electrophoresis projects. Moreover, the program does not need long and manual curations to detect optimized parameters, as stated by the authors of the maize HICF map which relied only on the DuPont method to remove background: "after considerable experimentation with parameters" they were able to automatically define thresholds [20].<br>
It has also been tested on a set of random clones from the grapevine physical map project giving the results reported in table 2. FPB was able to recognize clones from all three expected categories of clones: chloroplast, centromere, and rDNA, while <software>GenoProfiler</software> was partially able to distinguish two of them (chloroplast and centromere) only in the case of manually optimized parameters with the main intent of keeping contigs separated. Resulting contigs were easily veriafiable since BAC-end sequences were available. It is worthwhile noting that all inserts, including chloroplast, centromere, and rDNA, are transformed in E. coli, and contain the main background contaminant, the E. coli bands. If background removal is including background peaks then non overlapping clones may be incorrectly clustered together, as in the Genoprofiler1 assembly of table 2. Vice versa if the choice of the threshold is too conservative then there is no sufficient information to cluster overlapping clones, as in the <software>Genoprofiler2</software> assembly of table 2. Instead, neither of the two cases was produced by using FPB.<br>
Finally, FPB was used to remove background from all fingerprints of three different grapevine physical map projects. The three projects, [21,22] and Scalabrin et al. (submitted to BMC Genomics), consisted of about 45,000, 70,000, and 50,000 fingerprints respectively. In all cases a successful assembly was built (iterative assembly starting with cutoff 1e-50 decreasing stringency up to 1e-20 through DQ and merge steps): only 2982 (6.63%), 1295 (1.85%), and 2,310 (4.62%) Q clones ("Questionables") were produced by the assembly program, meaning that background was properly removed. Moreover, consider that in two of the maps, [21] and Scalabrin et al. (submitted to BMC Genomics) the number of Q clones is highly affected by the heterozygosity of the two selected lines (Pinot Noir and Cabernet Sauvignon): as demonstrated in Scalabrin et al. (submitted to BMC Genomics) FPC assemblies affected by heterozygosity exhibit incorrect positioning of clones in a single contig and therefore lots of "Questionable" clones are produced. In contrast, the assembly of line PN40024, close to full homozygosity, included very few Q clones.<br>
<br>
Conclusion<br>
FPB is freely available at . It requires Perl and Perl/Tk. FPB is effective at automatically removing background in small projects as well as in big projects as demonstrated on the three independent assemblies on different strains of grapevine.<br>
<br>
Authors' contributions<br>
MM proposed the problem and approved the computational solution. SS proposed the main solution, implemented the tool, and verified its efficacy on different maps and datasets. AP improved the solution by proposing the iterative approach and coordinated the computational part of the experiments. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2696450</b><br>
<software>EDGAR</software>: A software framework for the comparative analysis of prokaryotic genomes<br>
<br>
<br>
Background<br>
The mid fifties produced a rather pragmatic definition of the term species, described as a group of cultures or strains which is accepted by bacteriologists as sufficiently closely related [1]. About thirty years later a more fundamental proposition of the term [2] considered measurable quantities including strains' DNA molecules reassociation values and phenotypic traits. However, in recent times, these classical approaches are likely to be outdated by future deductions which may be taken from the increasing collection of genomic information.<br>
Especially methods of pyrosequencing have the undisputed potential to yield huge amounts of genomic sequence information in relatively short time spans. Unsurprisingly, the number of complete genomes being published is rapidly increasing (see ).<br>
As a consequence, one may ask if the genetic variability of a species can be described using only one single strain. A closer look at numerous pieces of circumstantial evidence apparently negates this question, such as the comparison of the well known Escherichia coli strain K12 and its relative O157:H7 revealed 1387 genes to be specific to a certain strain, or the comparison of 17 Streptococcus pneumoniae strains by Hiller et al., where the clustering of similar genes revealed clusters exclusive to one or some strains. Interestingly even isolates taken at nearby locations from patients with similar symptoms showed divergent genotypes [3].<br>
Inspired by the Greek word "pan" for "whole", Tettelin et al. shaped the idea of the pan genome [4]. Using whole-genome shotgun sequencing, they gained genomic information of six strains of Streptococcus agalactiae. In comparison with two additional publicly available genomes of the major pathogenic serotype of Streptococcus agalactiae, called group B Streptococcus (GBS), they found a significant amount of genes not being shared among the compared strains. Their discoveries led to the definition of the pan-genome constituting that "a bacterial species can be described by its pan-genome, which is composed of a 'core genome' [?] and a 'dispensable genome' ".<br>
Furthermore, a differentiation between open and closed pan-genomes has been introduced by Medini et al. [5]. While for example the genomes of Buchnera aphidicola showed almost no gene rearrangements (lateral exchange) and therefore its pan-genome is denominated as closed [6], the compared strains of Streptococcus agalactiae form an open pan-genome, i.e. every newly sequenced strain would contribute new genes into the pool of available genes for that specific species.<br>
Muzzi, Masignani, and Rappuoli pointed out the importance of these concepts, not only to study genetic diversity, but also in terms of medical discoveries and cures [7]. During the design and analysis of potential vaccines, where methods like the reverse vaccinology approach play an important role, the genes of the core genome are most likely the most desirable targets for novel drug candidates.<br>
An automated calculation of the characteristics of a species's pan-genome is highly desirable to identify singleton genes, the dispensable and the core genome. Different tools have been developed to compare the sequences of genomes, comprising for example the <software>VISTA</software> family of tools, <software>xBASE</software> or <software>GeConT</software> [8-11]. However, when these tools were designed, attention was focused on the comparison of the genomes of different species. In the mean time, particularly resulting from the upcoming pyrosequencing technologies, bioinformatics support for the comparison of multiple strains of the same species was needed. Therefore databases like the <database>Comprehensive Microbial Resource</database> (<database>CMR</database>) or the <database>Microbial Genome Database</database> (<database>MBGD</database>) were designed dedicated to the comparison of multiple genomes of related species [12,13].<br>
The <database>CMR</database> provides numerous comparative tools for analysis of 438 genomes stored in its database, including a multi-genome homology comparison tool. This tool allows the user to calculate the number of proteins in a reference genome that have hits to up to 15 selected comparison genomes. The resulting homologous genes of the selected genomes are presented in a circular display of proteins the selected genomes have in common. Special sets of these homologous genes like the core genes or the singletons can be observed and exported in a tabular format. The <database>MBGD</database> provides comparative analysis features for 631 finished bacterial genomes. The genes of selected genomes can be clustered to homologous groups, resulting in a set of ortholog clusters. From this ortholog clusters the core genome, the pan genome and the singletons of a given genome can be calculated. Additional analysis and visualization features are available for the clustered genes like multiple alignments or a comparison of the context of the genes on a genome map. While both databases provide a wide range of highly valuable analysis features, both have limitations in the analysis of groups of related genomes. When using the <database>CMR</database> the user can only view the core genes and singletons for the reference genome. Homologous protein mappings can be analyzed only for the comparison of the reference genome to one other genome, an overall table for all genomes is missing. Additionally, the pan genome can't be displayed using the <database>CMR</database>. The <database>MBGD</database> can calculate the core genome as well as the pan genome or singleton genes, but it is focused on the calculated ortholog clusters. There is a lack of genome wide analysis features like Venn diagrams of the common gene pools of the analyzed genomes or synteny/scatter plots of homologous proteins, and the web interface is not very intuitive. Furthermore, both databases don't feature phylogenetic analyses.<br>
Another crucial aspect when analysing groups of related genomes is the definition of a homology criterion to cluster genes together. Both databases offer a selection of parameters to the user to define a homology cutoff for the genome comparisons. When using the <database>MBGD</database> one can choose amongst 16 parameters in different combinations. The <database>CMR</database> offers three parameters: Minimum percent similarity, minimum percent identity, or maximum p-value. A user can use the default parameters or has to find the parameters best suited for the genomes he wants to compare by trial and error. An automatic estimation of an adequate homology criterion would be a great easement of comparative analyses. Therefore we developed <software>EDGAR</software> as an easy-to-use integrated solution, capable of performing genome comparisons and phylogenetic analyses of multiple strains of a species based on a homology cutoff automatically adjusted to the analyzed genomes.<br>
<br>
Implementation<br>
<software>EDGAR</software> is a bioinformatics approach to provide quick access to orthology information and comparative genomics.<br>
System design<br>
The system design of <software>EDGAR</software> is based on a standard three-tiered architecture. As a data backend (database layer), we use <software>SQLite</software> . <software>SQLite</software> is an easy-to-use file based relational database management system and allows simple transfer of complete data sets from one operating or hardware system to another one. The business logic layer is implemented in Perl  using the DBI package to access the data backend. We created a user interface (presentation layer) based on Perl CGI and some JavaScript. The setup of an <software>EDGAR</software> project requires the selection of related genomes. <fileFormat>FASTA</fileFormat> files for all coding sequences of every genome and their corresponding NCBI protein table files (.ptt), and <software>BLAST</software> databases need to be stored in a project directory. For the precalculated projects data from the NCBI <database>genomes database</database>  was used. Optionally, an <software>EDGAR</software> project can be set up based on an existing project of the automatic annotation plattform <database>GenDB</database> [14], comparing all or a selection of genomes in the project.<br>
We have implemented several maintenance scripts to set up a project and perform all required computations like the creation of phylogenetic trees. All calculations are realized using object oriented Perl. The all-against-all comparisons of the genes of a genus group are distributed over a compute cluster using <software>Sun Grid Engine</software> .<br>
<br>
<software>BLAST</software> score ratio values<br>
As described in the background section the definition of an adequate orthology criterion is a task of vital importance. Following the original definition of orthology by Fitch [15], two genes are orthologs if they diverged through a speciation event. But due to the fact that orthologs are mainly used to propagate functional annotations, the term "ortholog" is often used to describe genes with conserved function. The majority of scientists uses bidirectional best hits (BBHs) of the well known alignment tool <software>BLAST</software> [16] and chooses a certain e-value or an identity threshold over a given alignment length to define orthologous genes. Various more sophisticated orthology identification approaches have been developed in the past, e.g. Clusters of Orthologous Genes (COG), InParanoid, OrthoMCL, Ensembl Compara, Homologene, RoundUp, EggNOG, or OMA [17-24]. Some of these approaches were benchmarked by Hulsen et al. with the result that, while InParanoid performs best, BBHs can give a good orthology estimation for closer related species [25]. Recently, Altenhoff et al. confirmed the good performance of BBHs in a comparison of 11 orthology estimation methods and concluded that BBHs give comparable results to the more sophisticated methods for both the "phylogenetic" orthology definition by Fitch and the widespread "functional" definition [26]. A drawback of BBHs is that only one-to-one orthologous pairs are found, for duplicated genes or paralogs only a single hit will be found, the following ones will be missed. But as the BBH calculation is straightforward and therefore fast enough to handle the huge amounts of sequence information in comparative genomics, this drawback has to be accepted and we use BBHs as orthology criterion in <software>EDGAR</software>. For all calculations protein <software>BLAST</software> (<software>blastp</software>) was used with BLOSUM62 as similarity matrix.<br>
For the completely automated calculation of genome comparisons it is crucial to rely on a generic orthology criterion consistent within the genome group. For this reason orthology thresholds were generated based on so called <software>BLAST</software> Score Ratio Values (SRVs) (as suggested by Lerat et al. [27]) for every compared genus. Instead of using the absolute scores of every <software>BLAST</software> hit, this method employs the <software>BLAST</software> bit scores in relation to the maximum bit score. While Lerat et al. used a fix SRV threshold of 30 for orthology estimation, we use a sliding window approach to estimate the appropriate cutoff for every genus. The maximum score is defined as the score resulting from an alignment of a gene against itself. As a <software>BLAST</software> self hit always has 100% identity over 100% of the query sequence length one gains the maximum bit score possible for the query gene. All hits of a query gene versus genes from other genomes are normalized regarding to this maximum score, so the SRV is defined as the ratio (Observed score/Maximum score), thus giving a value in the range [0, 1]. The SRVs for all <software>BLAST</software> hits of a genome versus another are plotted as a histogram, in most cases forming a bimodal distribution (see Figure 1). The first peak at low similarity values probably represents nonspecific hits, while the second peak represents orthologous genes. To remove all nonspecific hits, a cutoff value has to be identified that discriminates between the two peaks automatically. To find this cutoff, we used a sliding window of a given width over the histogram, thus searching for the lowest scoring window (LSW), which is the window with the lowest count of <software>BLAST</software> hits. Within this LSW, the SRV value with the lowest hit count is estimated, yielding the final cutoff value. A SRV distribution with the search window and the resulting cutoff is shown in Figure 1A. This procedure is repeated for all possible combinations of genomes, resulting in n2 combinations for a number of n genomes. The n2 cutoffs are plotted in a second histogram, in most cases showing a normal distribution (see Figure 1B). To get a comparable general threshold for all subsequent calculations, the peak of this histogram is determined and used as so called master-cutoff.<br>
The orthology cutoff generated by this approach is quite strict, as all low quality <software>BLAST</software> hits are filtered out. For the Xanthomonas project with a calculated master cutoff of 63 this results in only 44% of all <software>BLAST</software> hits passing the filter. The minimum percent identity of BBHs passing the filter is 53.75% and mean <software>BLAST</software> e-value is lower than 1e-10. As a consequence of that strict threshold orthologs found by <software>EDGAR</software>, especially when conserved among numerous genomes like the core genes, could be considered real orthologs, but some potential orthologs might be lost.<br>
Limitations<br>
In some cases the SRV distribution does not show the expected bimodal shape. This is mostly the case if there is a high variation within the genomes of a genus. A good example is the genus Corynebacterium, where the genomes are very diverse, leading to a SRV distribution with only one peak for low similarities and a broad plateau of medium scores with a decay at the highest scores. The cutoff calculation described above leads to cutoffs at very high SRVs, thereby omitting the majority of all <software>BLAST</software> observations (see Figure 2). To overcome this problem the master-cutoff is set to 35% if the majority of SRV histograms do not show a bimodal distribution. This value has shown to be a good cutoff value in the genomes compared with <software>EDGAR</software>.<br>
<br>
<br>
Data model<br>
<software>EDGAR</software> stores the bidirectional best <software>BLAST</software> hit information of the all-against-all comparison of the genomes and all needed sequence information in a <software>SQLite</software> database. A modular data scheme and the project based approach allow to arbitrarily update single projects of the <software>EDGAR</software> database backend whenever a new genome becomes available for the respective genus. A local copy of the NCBI <database>Bacteria database</database> used by <software>EDGAR</software> is updated regularly. Based on this update newly included genomes are added to existing <software>EDGAR</software> projects to keep the database up-to-date.<br>
<br>
Calculation of the core genome and pan genome<br>
The core genome is calculated by iterative pairwise comparison of a set of genomes G. One genome is selected as reference genome, the gene content of this genome is taken as base set for the following calculations. This set A of genes is compared to a set B of genes contained in another genome of the set G. For each gene in set A, a lookup is performed to check if it has a reciprocal best hit in set B. The lookup is performed on the BBHs filtered according to the orthology criterion calculated based on the SRVs. Every gene from set A that has no reciprocal best hit in set B is removed from the set. The resulting set A' is then iteratively compared to the genes of the remaining genomes in G, resulting in a final set of genes that have hits in all genomes of G, thus forming the core genome. The pan genome is also calculated in a similar way. A set B of genes is compared to the base set A of genes. Every gene of B that has no ortholog in A is added to the reference set. This process is repeated iteratively for all genomes in the set G, extending the base set A step by step to the pan genome. The selected reference genome has nearly no impact on the resulting core genome, its main purpose is that the genes of the reference genome appear first in the results. However, there may be some small bias (&lt; 1%) due to paralogous genes appearing in different order during the calculation.<br>
<br>
Phylogenetic trees<br>
To support the comparison of different genomes, phylogenetic trees (see Figure 3) can be generated using a slightly adapted version of the pipeline proposed by Zdobnov et al. [28]. The core genome is calculated as described above. In the next step multiple alignments for all core genes are created using <software>MUSCLE</software> [29]. Non matching parts of the alignments are masked using <software>GBLOCKS</software> [30] and then removed. The matching parts are concatenated to one big multiple alignment of more then 1 Mb length. Finally, a phylogenetic tree is generated from this long alignment using <software>PHYLIP</software> [31].<br>
<br>
User interface<br>
The user interface is based on an <software>Apache Web Server</software> using mod_perl and CGI. The web interface is separated into three parts: The HTML code is organized in static HTML templates. These templates use XHTML 1.0 strict, which is supported by all modern browsers. The graphical layout is implemented using CSS stylesheets. Both the XHTML and the CSS code were validated to be compliant to the standards of the W3C consortium.<br>
<br>
<br>
Results<br>
<software>EDGAR</software> is designed to support the high throughput comparison of related genomes. A comparison of the genomes of all genus groups of the NCBI <database>genomes database</database> with more than three sequenced strains was performed, and the resulting orthology information is made available to the scientific community.<br>
The <software>EDGAR</software> web application<br>
<software>EDGAR</software> provides a precomputed database with orthology information for all genomes of a genus, based on a generic orthology criterion calculated from score ratio values (SRV ? see Methods). The calculated orthologous genes as well as a number of visualization features are accessible via a full featured web interface. Genomes of identical genus are clustered together, where for each compared genus group a separate project database is created to store the <software>BLAST</software> score ratio based orthology information. The SRV histograms and the derived cutoffs can be plotted for every genome combination. The resulting histogram of SRV-cutoffs for all possible genome combinations is also available. Using these plots the user can validate the orthology criterion calculated by <software>EDGAR</software>.<br>
Genes that have no orthologs in any other genome of the genus are called singletons. Using <software>EDGAR</software> the singletons of every genome in comparison to its group can be estimated within seconds. The singletons of a bacterial strain can be exported as fasta file for further analysis.<br>
The core genome can be calculated for a selected reference genome in comparison to every combination of genomes of its genus. The genes of the reference genome are used as the starting set for the iterative core calculation (see Methods). The calculated core genome is presented as a table of orthologous genes of all selected genomes and their functions, starting with the selected reference genome in the first column (see Figure 4).<br>
To observe the differences between the orthologous genes of the core genome, <software>EDGAR</software> features multiple alignments of the core genes, created using <software>MUSCLE</software> [29]. Furthermore multiple alignments are created for the upstream region of the core genes. This allows the researcher to quickly find conserved sequences in the upstream region, e.g. when searching for promotor binding sites or recognition sites of regulatory elements. The pan genome, the set of all unique genes of the compared genomes, can be calculated for user defined sets of genomes. The pan genome is also listed as a table of orthologous genes with their gene functions, beginning with all genes of the reference genome, followed by the genes added to the gene pool of the genus by the other species. The table comprises unique genes only, meaning that a gene that is orthologous to a previously listed gene will not be added to the list. The pan genome and core genome can be exported as fasta files or as TAB separated tables of locus tags.<br>
The genomic context of orthologous genes can be observed via the comparative view. The gene names in the core genome table and the pan genome table are linked to a linear view (see Figure 5) of all orthologous genes in their genomic neighborhood, which allows e.g. for the analysis of operon structures or genomic rearrangement events. The comparative view can also be accessed via the sidebar menu. Another feature of the <software>EDGAR</software> web interface is the creation of synteny plots. Here, stop positions of two orthologous genes of two bacterial strains are used as coordinates and plotted to a diagram with the sequence length of the compared strains serving as x/y-axis. The resulting synteny plot can reveal an insight into genome rearrangements that occurred during the evolution of the strains.<br>
To visualize differential gene content between several genomes <software>EDGAR</software> can create Venn diagrams for up to five genomes. Every area in this Venn diagram represents a subset of the compared genomes and is labeled with the number of genes in this subset. To simplify the assignment of an area to a genome set every genome has a base color. The areas of the Venn diagram are colored in the averaged color of the associated genomes (based on the RGB color model). Venn diagrams for more than five genomes can be created theoretically, but as higher level diagrams get extremly complex they were not implemented in the current version of <software>EDGAR</software>. Projects were created for all genomes of the NCBI <database>Bacteria database</database> where three or more genomes of one genus were available. This resulted in 75 genus groups containing 582 genomes (as at 15.02.2009). All these projects are freely accessible via the <software>EDGAR</software> web interface. In order to analyze unpublished data using <software>EDGAR</software>, private projects with access control by a user management system can be created upon request.<br>
<br>
A use case study: comparative analysis of Xanthomonas genomes<br>
In a use case study, <software>EDGAR</software> has been employed to compare the genomes of Xanthomonas strains. Xanthomonads are plant-associated and usually plant pathogenic bacteria [32]. These Gram-negative bacteria affect a broad set of host plants, among them important agricultural crops like rice and other grains [33-35], soy beans, cotton, citrus plants [36], but also tomato, pepper [37], or Crucifera [36,38] including cabbage, rape, and the model plant mouse-ear cress (Arabidopsis thaliana). Besides their pathogenicity-based agricultural relevance, some Xanthomonas species, especially X. campestris, are also commercially important due to their production of the polysaccharide xanthan, which found many industrial applications, mainly as a viscosifier [39-41].<br>
Understanding the taxonomic relation of Xanthomonas strains has become an awkward endeavor. In the early days of microbiology, each bacterial isolate identified from a host plant for which no member of this bacterial genus had been described previously was classified as a new species [42]. Later many of these species were merged on the basis of in vitro tests, but the original name identifying the main host plant was conserved in the term "pathovar" [43]. Incorporation of information derived from partial knowledge of DNA sequences, such as 16S rDNA sequences or RFLP patterns, led then to a reassessment of the Xanthomonas taxonomy [44], which is still in progress [45,46]. This phylogenetic analysis provides not only the basis for a systematic order of the Xanthomonas bacteria, but also a deeper understanding of the evolution of the Xanthomonas strains. However, all attempts so far to reconstruct the true evolutionary relationships between the Xanthomonads did not lead to a taxonomy that is generally applied within the community. Instead, the differing classifications of the strains resulted in inconsistent naming in the literature. Thus, exploiting the emerging genome data may now open the door to obtain a well-established Xanthomonas taxonomy on a definite basis. We have used <software>EDGAR</software> to assess this approach.<br>
All Xanthomonas genome data currently available from public sequence repositories have been employed for a comparative analysis of these bacteria. The genome data were from the X. campestris pv. campestris (Xcc) strains ATCC 33913 [36], 8004 [38], and B100 [39], the X. campestris pv. amoraciae (Xca) strain 756C, the X. campestris pv. vesicatoria (Xcv) strain 85-10 [37], the X. axonopodis pv. citri (Xac) strain 306 [36], the X. oryzae pv. oryzae (Xoo) strains KACC10331, [33] MAFF311018 [34], and PXO99A [35], and the X. oryzae pv. oryzicola (Xoc) strain BLS256. The main features of these Xanthomonas strains and their genomes are summarized in Table 1.<br>
In a first analysis, the pan genome of the Xanthomonas chromosomes was computed to consist of 12,951 coding sequences (CDS). Among these genes, a core genome of 2,156 CDS was determined. Besides genes encoding basic features like the central metabolism and the cell envelope, the core genome comprised genes important for survival in the bacterial environment. Such genes coded i.e. for the flagella and chemotaxis, for putative glycosidases and sugar uptake systems. Furthermore, pathogenicity factors like the type I-IV secretion systems seemed basically conserved among all so far analyzed Xanthomonas strains, as well as the xanthan production machinery encoded by the gum genes.<br>
To get an overview on the true taxonomy of the sequenced Xanthomonas strains, the determined core genome data was used for a phylogenetic analysis. The genome divergence was quantified with <software>EDGAR</software> using a similar approach to one which recently proved to be of value for eukaryotes [28]. The result can be displayed in a tree with two main branches: the first comprising almost all X. campestris genomes and the second with the genomes of the X. oryzae strains, which includes the genomes of X. axonopodis and of X. campestris pv. vesicatoria 85-10 in a separate branch (see Figure 3). In the X. campestris branch, all Xcc genomes were very close to each other and the genome of Xca 756C was only marginally more distant from these strains. The second branch was more heterogeneous. Beside the branch with Xac 306 and Xcv 85-10 the genome of Xoc BLS256 was more distant from the Xoo strains. This phylogenetic clustering was in good accordance with the phytopathogenicity of the Xanthomonas strains; the X. campestris strains, which were grouped together, are all pathogens of cruciferous plants, while the X. oryzae strains are pathogens of rice (Oryza sativa). Also, Xoc BLS256, which forms a distinct branch within this group, causes bacterial leaf streak disease, while the Xoo strains provoke bacterial blight. Xcv and Xac that diverge from the second branch leading to the rice pathogens, affect pepper and citrus plants, respectively. The phylogenetic tree was derived from the core genome that comprised 2,156 CDS and thus less than half the genome of each Xanthomonas strain. To shed more light on the relation of the Xanthomonads, the synteny of genome pairs was plotted, which gave a rough survey on the conservation of gene order between individual strains (Figure 6). For the analysis Xcc B100 was set as a reference. The comparisons show that the gene order of Xcc B100, which despite some striking inversions was generally well conserved in the Xcc and Xca chromosomes, was increasingly disintegrated along the phylogenetic tree that indicates their divergence (Figure 4). While there was considerable conservation in the Xca/Xcv chromosomes, the number of rearrangements increased dramatically in comparison to Xoc, with the gene order almost comminuted in Xoo. Such chromosomal rearrangements have been linked to IS elements [35,38], mobile genetic elements that were found frequently in Xanthomonas genomes, with numbers ranging from 58 in Xcv 85-10 to 267 in Xoo PXO99A [35,37].<br>
The degree in gene order conservation among the Xanthomonas chromosomes as apparent from the synteny analysis seems well correlated with the phylogenetic order computed for the core genome CDS. Two taxonomic groups became evident, comprising of X. campestris strains pathogenic for crucifers and of X. oryzae strains pathogenic for rice. In between there was a third group consisting of Xca and Xcv 85-10. These three groups have been further characterized by analyzing the distribution of orthologous CDS within the groups (Figure 7). Among the crucifer-pathogenic X. campestris strains (Figure 7A) there were particular overlaps between the genomes of strain Xcc 33913 and Xcc 8004. For the genome of strain Xca 756C, that had been classified to the distinct pathovar "amoraciae", no outstanding role became obvious when compared to the Xcc strains. Among the X. oryzae chromosomes (Figure 7C) the Xoo strains had a large number orthologs in common, thus reflecting the different symptoms provoked by Xoc when affecting rice. The Xac/Xcv chromosomes that branched off the remaining Xanthomonads conjointly between the X. campestris and X. oryzae groups, shared many orthologs with an Xoo representative that was also included in the comparison (Figure 7B).<br>
Altogether these analyses conveniently performed with <software>EDGAR</software> lead to a more comprehensive view on the phylogeny of the Xanthomonads that so far was clouded to some extend by previous contradictory taxonomic classifications. Now <software>EDGAR</software> facilitates a high-resolution analysis for the sequenced strains. The results for the available genomes imply two phylogenetic groups that constitute crucifer-pathogenic and rice-pathogenic strains, respectively. While the genome-based analyses reflect the distinct disease symptoms caused by an infection with X. oryzae pv oryzicola, the classification of Xca 756C in a separate "pathovar" is questioned. Furthermore, the X. axonopodis pv. citri and X. campestris pv. vesicatoria strains are related. This is in accordance with previous phylogenetic analyses that caused X. campestris pv. vesicatoria strains to be reclassified as X. axonopodis [44,46]. The substantial distance between Xcv 85-10 and the group of the remaining X. campestris strains suggest that a renaming for Xcv 85-10 should be considered.<br>
<br>
<br>
Discussion<br>
As we demonstrated by the use case <software>EDGAR</software> provides various useful features for the comparative analysis of closely related genomes. While some of the presented features are available also in the <database>CMR</database> or the <database>MBGD</database>, <software>EDGAR</software> adds some novel aspects like the phylogenetic analysis or the Venn diagrams of common gene pools. The intuitive web interface and the auto-generated SRV based orthology cutoffs allow researchers to analyze genomes of their interest as quick as possible. The SRVs have been shown to be a useful method for a generic orthology threshold estimation. These generic thresholds are crucial for the high throughput comparison of genomes, as it is much too laborious to observe every genome group manually. While working well in the vast majority of cases, in some genus groups (e.g. Corynebacterium) the SRV cutoff calculation fails due to very dissimilar genomes. A proper method to estimate the threshold in these cases has yet to be developed, up until then a fix threshold is used. As most other orthology estimation approaches also use static thresholds, this is no major drawback.<br>
<software>EDGAR</software> will be continuously enhanced, there are several features planned for the future. The identification and visualization of segmental duplications in analyzed genomes will be one of the main topics in the further development of <software>EDGAR</software>. Another planned feature is the integration of additional visualization features to the web interface like e.g. circular plots of orthologous genes.<br>
A script based prototype of <software>EDGAR</software> was successfully used for the comparative analysis of Neisseria meningitidis strains. Schoen et al. [47] compared disease and carriage strains of N. meningitidis to gain insights into virulence evolution. Some techniques used in this work like a curve fitting approach to test for an "open" pan genome are also planned to be integrated into <software>EDGAR</software> in the near future.<br>
Another feature to come is a search mask for boolean queries on sets of genomes. Furthermore, the integration between <software>EDGAR</software> and the automatic annotation framework <database>GenDB</database> [14] will be expanded by integrating direct links from <software>EDGAR</software> to <database>GenDB</database> annotations.<br>
With regard to the large numbers of unfinished genomes that are expected to arise from next generation sequencing technologies, <software>EDGAR</software> is not limited to completely assembled genomes. As the calculation is gene based, <software>EDGAR</software> has the capability to analyze multiple contig draft genomes, provided that a gene finding approach like <software>GLIMMER</software> or <software>CRITICA</software> [48,49] was performed on the contigs. The comparative view of <software>EDGAR</software> could actually support the annotation of unfinished genomes.<br>
Space requirements of an <software>EDGAR</software> project depend on the size and number of the analyzed genomes. Among the precomputed projects the space requirements vary from 9 MB (Buchnera ? four genomes of about 500 genes) to 1.2 GB (Mycobacterium ? 19 genomes of about 4500 genes). The compute time for one project also highly depends on the number of genes to be compared (e.g. via BLAST). Processing all 582 genomes in the precomputed projects took about three days on a compute cluster (127?Sunfire V20z dual Opteron 1.8 Ghz, 27 ? SunFire X2200 dual cpu dual core Opteron 2,4 Ghz, 3 ? SunFire V880 8 ? Ultra Sparc III).<br>
<br>
Conclusion<br>
With the rapidly emerging ultra-fast sequencing technologies the trend moves towards analyzing not just one genome, but groups of related genomes. <software>EDGAR</software> is the ideal tool for analyzing connatural genomes by providing a quick insight into the similarities and differences among the sequenced genomes. <software>EDGAR</software> was used to analyze all suitable sequences of the NCBI <database>genomes database</database>. All genomes were sorted by their genus, and every genus-group with three or more sequenced species was processed with <software>EDGAR</software>. This resulted in 75 genus groups containing a total of 582 genomes.<br>
All these groups are accessible via the <software>EDGAR</software> web interface located at . Since only published genomes are used in the analysis, no access control is needed. However, it is possible to create private <software>EDGAR</software> projects for unpublished data upon request. The <software>EDGAR</software> web frontend provides convenient access to all data stored in the <software>EDGAR</software> databases, allowing for the fast and easy calculation of singletons, core genome, and pan genome of any combination of related genomes available in the NCBI <database>Genomes database</database> so far. The web based access to comparative data via <software>EDGAR</software> stages an ideal platform for cooperative work of researchers all over the world.<br>
Additionally, when comparing newly sequenced genomes to a well annotated one, the orthology information applied by <software>EDGAR</software> can be used to transfer annotation information from the old to the new genomes. Visualization features include synteny plots for pairs of genomes, as well as Venn diagrams of up to five genomes. Phylogentic trees as presented in the use case study make a powerful expert system for evolutionary analyses available to the scientific community. The visualizations stated above as well as the singleton, core genome or pan genome tables can be easily exported for further use in other tools.<br>
Additionally, the overview tables generated by <software>EDGAR</software> are the perfect means to give a review of the analyzed genomes and to identify promising genes for further inspections and specific analyses. All these features make EDGAR a valuable gain for scientists in the field of comparative genomics.<br>
Concerning the Xanthomonas use case, the advancements in ultra-fast sequencing technology imply the arrival of further Xanthomonas genome data in the future. Easy-to-use tools like EDGAR will allow constant and timely enhancements in understanding the phylogeny of these organisms upon arrival of genome data. As increased taxon sampling greatly reduces phylogenetic errors [50], remaining obscurities in Xanthomonas taxonomy may be resolved efficiently by extensively exploiting genome data by means of <software>EDGAR</software>.<br>
<br>
Availability and requirements<br>
? Project name: <software>EDGAR</software><br>
? Project home page: <br>
? Use case study: project "BMC_Xanthomonas" on the <software>EDGAR</software> home page<br>
? Operating system(s): Platform independent<br>
? Programming language: Perl, JavaScript<br>
? Other requirements: JavaScript enabled web browser<br>
? Software license: GNU GPL<br>
? License agreement required for non-academic users<br>
<br>
Authors' contributions<br>
DD has designed and implemented the initial <software>EDGAR</software> framework. JB has supervised the prototype development and implemented the final version. FJV contributed the Xanthomonas use case and the phylogenetic analyses. SPA helped in the creation of the web server and in the drafting of the manuscript. MZ developed the comparative viewer. AP contributed biological background knowledge. AG supervised the development of the initial system design and approved the final manuscript. All authors have read and approved the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2751785</b><br>
<software>Epitopia</software>: a web-server for predicting B-cell epitopes<br>
<br>
<br>
Background<br>
The detection of highly immunogenic regions within a given protein, specifically those that elicit a humoral immune response i.e., B-cell epitopes, is central to many immunodetection and immunotherapeutic applications [1,2]. An unguided experimental search for such regions is clearly laborious and resource-intensive. Thus, computational approaches that are able to perform this task are desired.<br>
Extensive studies regarding the physico-chemical and structural aspects of antibody-antigen molecular recognition have provided several important characteristics of a typical epitope [3-6]. With this rich information at hand and the availability of state-of-the-art pattern recognition and classification algorithms, a computational tool that predicts the most antigenic regions in a protein, which can thus be approximated as immunogenic, is called for. Indeed, several such tools have been developed over the years. Some only rely on properties that can be extracted from the linear sequence of the antigen (<software>ABCpred</software> [7] and <software>COBEpro</software> [8]), while others rely on an available three-dimensional (3D) structure (<software>CEP</software> [9] and <software>DiscoTope</software> [10]). Other structure-based tools can be applied to linear sequences if a structural homolog can be found (<software>ElliPro</software> [11]). Yet, to date, no tool has been reported to perform its predictions either on the structure or directly on the sequence, if a structure is unavailable.<br>
Here we present the <software>Epitopia</software> server, which implements a machine-learning based algorithm to predict immunogenic regions as candidate B-cell epitopes using either the 3D structure or the sequence of a given protein. We compare the performance of <software>Epitopia</software> to several other tools that either predict B-cell epitopes given a protein 3D structure or sequence alone and show that it has greater predictive power.<br>
The Epitopia algorithm infers the immunogenic potential at the single amino-acid site resolution. <software>Epitopia</software> computes an immunogenicity score for each solvent accessible residue if a 3D structure was provided as input or a score for every amino-acid if a sequence input was provided. In addition, <software>Epitopia</software> combines a powerful visualization tool that color-codes the immunogenicity scores on either the protein sequence or the 3D structure to provide the users with a perceptible image of the immunogenic nature of their studied protein.<br>
Herein we provide a short description of the <software>Epitopia</software> methodology. More detailed descriptions are available under the 'OVERVIEW', 'GALLERY', and 'QUICK HELP' web sections. We exemplify the use of <software>Epitopia</software> by predicting immunogenic regions for both a 3D structure and a sequence input. Finally, we report its performance on a benchmark dataset and compare it to other available tools.<br>
<br>
Implementation<br>
The Epitopia algorithm [12] uses a Na?ve Bayes classifier to predict the immunogenic potential of protein regions. The classifier was trained to recognize immunogenic properties using a benchmark dataset of 66 non-redundant validated epitopes derived from antibody-antigen co-crystal structures (an updated dataset compared to [11]), and 194 non-redundant validated epitopes derived from antigen sequences (for further reading about the data and immunogenic properties please refer to [13] and the 'OVERVIEW' web section, respectively).<br>
A given antigen input is divided to overlapping surface patches (or stretches in the case of a linear sequence input), with the size of a typical epitope. <software>Epitopia</software> then computes for each patch (or stretch) the probability that it was drawn from the population of epitopes on which the classifier has been trained, with respect to each one of its physico-chemical and structural-geometrical properties. The immunogenicity score is thus the sum of logs of these probabilities and is assigned to the central residue of the patch (or to the middle residue in the linear stretch) [12].<br>
The immunogenicity score reflects the immunogenic potential of a certain residue relative to all residues in the antigen. In order to have a more intuitive measure of immunogenic potential, we also provide a probabilistic score. To this end, we first divided all site-specific immunogenicity scores in the training data to quantiles (octiles for the structure data and noniles for the sequence data). For each quantile, we computed the fraction of validated epitope residues out of the total number of residues in the quantile. This number approximates the probability that a residue with a given immunogenicity score that falls in this quantile is an epitope residue.<br>
We note that in structure-based predictions our method refers only to solvent exposed residues since, similar to other types of protein-protein interfaces, buried residues are not actively participating in the interaction. In cases where a studied protein may undergo cleavage which results with peptides that may become B-cell epitopes themselves [14], the 3D structure may not be relevant for the prediction and the sequence-based prediction should thus be used.<br>
Epitopia input<br>
For a protein 3D structure input, <software>Epitopia</software> requires a <database>protein data bank</database> (<database>PDB</database> [15]) file (or its identifier), which can either be an X-ray crystal model or a representative NMR model of the protein of interest. In addition, the user should specify the relevant chains to which <software>Epitopia</software> should relate in one of the following options: (1) if all of the chains in the model should be related to, either all chain identifiers or the term "all" should be specified; (2) if only a subset of chains in the model should be related to, the corresponding chain identifiers should be specified. All non-selected chains will thus be removed from the model file in the preprocess stage; (3) the non-selected chains can be kept by marking the relevant checkbox. In this case, the structural-geometrical considerations for computing the immunogenicity scores will be affected by all the chains in the model, but immunogenicity scores will only be computed for the residues of the selected chains.<br>
For a protein sequence input, the amino-acid sequence may either be pasted or a local sequence file can be uploaded. In either case, the sequence should be in <fileFormat>Fasta</fileFormat> format and should contain only standard amino acids.<br>
The input is then preprocessed and several stand-alone executables are used to extract some of the physico-chemical and structural-geometrical properties required for <software>Epitopia</software>. Further details regarding the preprocess stage are available under the 'OVERVIEW' web section.<br>
<br>
<software>Epitopia</software> output<br>
The immunogenicity and corresponding probability scores are computed by <software>Epitopia</software> for each surface residue for a 3D structure input or for every amino-acid for a sequence input. In either case, these scores are given as a text file link. In addition, the immunogenicity scores are color-coded and projected onto the protein. The visualization tool that is used for the 3D structure case is the <software>FirstGlance</software> in <software>Jmol</software> interface [16], which enables a wide range of display options. Along with that, <software>Epitopia</software> also provides a <software>RasMol</software> command script for viewing the results locally with the <software>RasMol</software> program [17].<br>
For the sequence output case, an automatic search procedure for clustering highly immunogenic amino acids on the linear sequence is performed since it is not naturally evident as in the case of 3D structure output. Briefly, the clustering procedure divides the sequence to stretches and assigns each stretch a corresponding p-value, which is defined as the probability of randomly obtaining an equally-sized stretch with such a score or higher. The score of a stretch is the sum of immunogenicity scores of the amino acids comprising it. Practically, the p-value is computed by shuffling all the scores in the sequence and repeating the search procedure a large number of times. Eventually, these clusters, ranked according to their statistical significance (detailed in the 'OVERVIEW' web section) are given as a text file link.<br>
<br>
<br>
Results and discussion<br>
Case studies<br>
To illustrate the performance and functionality of the <software>Epitopia</software> server two examples are given, one for a 3D structure input, and one for a sequence input. The 3D structure model is of the human vascular endothelial growth factor (VEGF), which was co-crystallized with its binding antibody (<database>PDB</database>: 1BJ1[18]). Figure 1 illustrates <software>Epitopia</software>'s prediction, when only the VEGF chain of the complex (chain identifier W) was selected. The immunogenicity and probability scores (partly displayed in Figure 1A) are color-coded and projected onto the structure model using the <software>FirstGlance</software> in <software>Jmol</software> interface (Figure 1B). Figure 1C shows that the region predicted to be the most immunogenic largely overlaps the genuine epitope of the neutralizing antibody, making it a highly successful prediction. The <software>FirstGlance</software> interface further enables a wide range of display options for the graphical output such as increasing the display quality, zoom control, and different chain display modes.<br>
Figure 2 illustrates the prediction of <software>Epitopia</software> given the amino-acid sequence of the Plasmodium falciparum Merozoite surface antigen 2 (MSA-2) [<database>Swiss-Prot</database>: P19599]. Figure 2A presents a sample of the immunogenicity and probability scores computed for this sequence, where Figure 2B displays the graphic visualization of these scores color-coded and projected onto the sequence, along with the predicted surface accessibility status for each amino acid (whether it is buried or exposed). It is evident that the region spanning amino acids 121 to 142 is highly immunogenic. Correspondingly, the most significant immunogenic stretch according to Figure 2C lies between amino acids 122 and 150. According to the <database>Bcipep</database> database [19], a validated epitope for this sequence includes the stretch between amino acids 125 to 131.<br>
<br>
Comparing <software>Epitopia</software> to other B-cell epitope prediction tools<br>
Conventionally, the area under the receiver operating characteristic (ROC) curve (AUC) [20] is used for diagnosing the performance of prediction methods (e.g., Ponomarenko and Bourne [21] used the AUC measure for evaluating several B-cell epitope prediction methods). Yet when it comes to assessing the performance of epitope prediction methods, the AUC is somewhat inadequate. In order to be able to compute the AUC, one has to define which residues are true epitope residues and which are non-epitope residues. It follows that any predictions which are not part of any validated epitope are regarded as false predictions. However, it is quite possible that the tested antigen harbors a far larger number of epitopes than are currently known, and thus the AUC underestimates the actual predictive power of the prediction method (this limitation was also noted by Ponomarenko et al., [11]). We thus consider an additional measure to evaluate the accuracy of prediction. Intuitively, in a successful prediction, genuine epitope residues should be scored higher than the average score of all residues. Hence, we considered a prediction (for a single protein input) to be successful if the average score of genuine epitope residues exceeds the average score of all considered residues. Accordingly, we define the success rate of a method as the number of successful predictions divided by the total number of predictions. Our method's parameters were optimized to achieve such maximal ratio. We also provide the AUC scores, which as noted above, provide a lower bound to the method's performance.<br>
We compared Epitopia's performance to three other structure-based epitope prediction tools, <software>CEP</software> [9], <software>DiscoTope</software> [10], and <software>ElliPro</software> [11], on the same data and using exactly the same assessment measures. <software>Epitopia</software> succeeded in 59 out of the 66 predictions, yielding a success rate of 89.4%. In comparison, <software>DiscoTope</software> and <software>ElliPro</software> succeeded in 54 and 53 predictions, giving success rates of 81.8% and 80.3%, respectively. Since <software>CEP</software> does not individually score amino acids its performance could only be assessed using the AUC (computed as described in [21]). <software>CEP</software> achieved a mean AUC of 0.53 (over 65 cases, since a prediction for one of the datasets, <database>PDB</database> ID: 3FFD could not be obtained), which is substantially lower than that of all other methods (mean AUCs of 0.6, 0.62, and 0.59 for <software>Epitopia</software>, <software>DiscoTope</software>, and <software>ElliPro</software>, respectively).<br>
<software>Epitopia</software> was additionally compared to two sequence-based tools, <software>ABCpred</software> [7] and <software>COBEpro</software> [8], which also implement machine-learning algorithms and were trained on very similar data as Epitopia. <software>Epitopia</software> succeeded in 156 out of 194 predictions (success rate = 80.4%) with a mean AUC of 0.59. <software>ABCpred</software> succeeded in 130 out of 194 predictions (success rate = 67%) with a mean AUC of 0.55. <software>COBEpro</software> succeeded in 119 out of 178 predictions (success rate = 66.9%), (16 antigen sequences were discarded since they exceed <software>COBEpro</software>'s sequence length limit) with a mean AUC of 0.55.<br>
We have selected the leave-one-out cross-validation procedure so that the performance of <software>Epitopia</software> is evaluated on data different from that used to train the classifier (thus avoiding over-fitting). In contrast, the performances of the methods to which <software>Epitopia</software> was compared were not achieved using cross-validation (thus, in most cases the compared classifier was trained and evaluated on the same data). Clearly, training and evaluating a method on the same data can artificially bias (increase) its performance.<br>
<br>
<br>
Conclusion<br>
The Epitopia algorithm treats the problem of epitope prediction as a classical classification problem, applying the most suitable methodology for tackling it. To this end, Epitopia relies on an extensive set of physico-chemical and structural-geometrical features that characterize epitopes [6], which was optimized to yield maximal predictive power [12]. Although the Na?ve Bayes classifier is often claimed to be over-simplified [22], we note that a support vector machine (SVM) classifier was also applied to this problem but did not perform as well as the Na?ve Bayes classifier (data not shown). Thus, as the SVM classifier is claimed to be second-to-best for most of the classification problems, we feel that the Na?ve Bayes classifier is an appropriate choice. Finally, it is worth emphasizing that the performance assessment measure defined here serves as a good alternative to the commonly used AUC measure, so long as the validated data remain scant. Although this new measure reports higher values than the AUC, it does so for all the compared methods without favoring any method in particular.<br>
The <software>Epitopia</software> server provides ease of use, bifunctionality (in handling both 3D structure and sequence inputs), and rich output and visualization options that enable users to delve into the prediction results. These features along with the superiority of the Epitopia algorithm make up the main advantages of the <software>Epitopia</software> server over other related servers.<br>
<br>
Availability and requirements<br>
Project name: <software>Epitopia</software><br>
Project home page: <br>
Operating system(s): Platform independent<br>
Programming languages: C++, Perl<br>
Any restrictions to use by non-academics: for non-commercial research purposes only<br>
<br>
Authors' contributions<br>
NDR, IM, and TP conceived the algorithm. NDR developed the server. EM developed the graphical tool implemented in the server. NDR drafted the manuscript. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2753852</b><br>
Comparison of sequence-dependent tiling array normalization approaches<br>
<br>
<br>
Background<br>
Tiling microarrays are widely used to detect enriched DNA or RNA fragments generated by, for example, chromatin immunoprecipitation or expression profiling. Tiling microarrays allow for detecting simultaneously the enrichment of many fragments on a genome wide scale in an unbiased manner. This is facilitated by a high number of small probes, usually in the range of 25 to 60 nucleotides long. In order to achieve high coverage of the genomic sequence it is not possible to select the probes for their hybridization properties. Thus, the affinity of the probes towards their targets varies in a sequence-dependent manner. Recently, a number of approaches have been developed in order to remove the bias introduced by the sequence-dependent hybridization properties of the probes (e.g. methods dealing with tiling microarrays MAT [1]; MA2C [2]; PMT [3]; [4] and methods dealing with gene expression arrays [5,6]). These approaches have been shown to enhance the ability to detect enriched DNA fragments compared to other methods. However, MAT, MA2C and PMT not only normalize the measured intensities by removing the sequence-dependent bias but also implement a peak calling procedure that is different to other methods. Hence, the improved ability to detect enriched regions may be due to the peak calling procedure.<br>
In order to test this possibility, we employed recently published data obtained by so-called spike-in experiments [7]. We compared the effect of sequence-dependent probe-level normalization on the ability to detect enriched genomic regions, whose genomic locations we know beforehand. Specifically, we tested three normalization methods: MAT [1], MA2C [2] and an updated version of PMT developed by us [3]. As control, we normalized the measured intensities by subtracting the mean and scaling the resulting deviations from the mean such that they have a variance of 1 on a logarithmic scale, referred to as standard normal normalization (SNN). To separate the effects of the normalization procedure from peak calling we used the normalized intensities as input for a single peak-calling algorithm. To our surprise we found that the three sequence-based normalization methods showed an almost identical performance compared to SNN. Thus, the sequence-dependent probe level normalization has only a minor impact on the ability to detect enriched genomic regions. We propose that the intensities measured for the control samples are the best guide to remove sequence-dependent biases.<br>
<br>
Results and Discussion<br>
Comparison of normalization methods<br>
In order to assess the effect of sequence-dependent probe-level normalization on the ability to detect enriched genomic regions, we analyzed the data generated by so-called spike-in experiments [7]. The spike-in samples contained about 100 DNA fragments of approximately 500 base pair length at different concentrations. Otherwise the spike-in samples were exactly the same as the control samples consisting of sonicated whole genome DNA. Spike-in and control samples were either directly labeled and hybridized to tiling microarrays or were diluted such that these samples required amplification prior to hybridization. Moreover, different microarray platforms were used, namely tiling microarrays manufactured by Affymetrix, Agilent and NimbleGen covering the <database>ENCODE</database> regions.<br>
We used three sequence-dependent (MAT [1]; MA2C [2]; PMT [3]; this study) and one sequence-independent normalization procedure (standard normal normalization (SNN); this study). We implemented all four methods in R [8] in order to assure that the same input data is used for normalization (see Methods). We selected for each of the tiling microarray platforms datasets, which were performed on both amplified and unamplified samples, namely the ones provided by the Struhl and Gingeras (Affymetrix), Farnham and Green (NimbleGen) and McCuine (Agilent) labs. We did so because we wanted to compare the effect of the amplification step on the ability to detect enriched genomic regions.<br>
We used the raw data as input for the four normalization strategies. The resulting normalized intensities (on a logarithmic scale) were subsequently used in a single peak calling procedure adapted from Johnson et al. (2006) [1]. Thereafter, the predicted spike-in regions were ordered by their enrichment score and represented by the start and end coordinates of the scored window. We considered predictions, which overlapped the spike-in region as true positive and all others as false positive. In order to assure that each spike-in region is counted only once as true positive prediction, we removed all lower ranking predictions falling within the boundaries of already predicted spike-in regions.<br>
We adopted the same assessment approach as in the original study ([7], see also Methods). Briefly, we determined the cumulative number of true and false positives by traversing the rank ordered list of the predictions. These numbers were divided by the total number of spike-in regions (98 for the unamplified and 100 for the amplified samples). We determined the area under this ROC (receiver operating characteristic)-like curve (AUC). Here, the AUC is bounded by zero (almost random prediction) and one (perfect performance, i.e. 100% sensitivity and 100% specificity). In the original study, the AUC is determined only in the interval between 0 and 10% false positives and than multiplied by 10 (Li personal communication), i.e. there are only about 10 false positive predictions allowed. In order to compare our results to the findings reported by the original study, we adhered to this procedure.<br>
The results of this analysis are summarized in Figure 1. We included as a reference the AUC of the best performing algorithm using the data of each lab. Note that we used the ranked list for each of these algorithms and calculated the AUC values by our method, i.e. we removed multiple hits to the same spike-in region. Irrespective of the platform and the sample type we found that the three sequence-dependent normalization methods perform very similarly to each other. Moreover, we observed that none of them performed better than the sequence-independent SNN approach. The AUC values of each of the four methods compared to the best performing one were in almost all cases only 0.06 smaller. However, our implementation of the MAT method performs worse than the original implementation using both the unamplified (Figure 1a, open red bar) and amplified sample (Figure 1b, open red bar). We recomputed ranked lists using the original implementation and the same input data provided to our implementation. Performance analysis revealed that AUC values obtained by the recomputed ranked lists are very similar to our implementation of MAT (Figure 1 hatched red bar), suggesting that different input data and/or parameters led to the different results rather than errors in our implementation of the MAT approach. The other exception was the AUC value obtained by MAT in the amplified sample hybridized to an Agilent microarray by the McCuine lab (Figure 1b marked by an asterisk). Given that the confidence interval of the AUC value has been determined to be around ? 0.07 [7], we conclude that sequence-dependent normalization does not increase the ability to detect enriched genomic regions.<br>
In contrast to our conclusion, earlier studies reported that sequence-dependent probe-level normalization increases the ability to identify enriched genomic regions [1-3]. We think that the major performance increase is not due to the normalization procedure but rather due to the peak calling approach utilized after pre-processing the raw data. Peaks are predicted by calculating the trimmed mean of signal values in a sliding window, where the 10% highest and lowest values are discarded. The resulting trimmed mean is multiplied by the square root of probes contained in the corresponding window (see Methods and Johnson et al. (2006)[1]). We refer to this approach as trimmed mean, which seems to perform very well compared to other methods [1].<br>
In order to check whether the trimmed mean approach performs exceptionally well, we compared the performance of this approach to algorithms that have been reported to perform best for the respective dataset [7]. In almost all cases the trimmed mean procedure received AUC values within the aforementioned confidence interval of the best performing algorithm. Only for the amplified sample hybridized to the Agilent microarray by the McCuine lab the results for MA2C, PMT and SNN were much better than for the reported ADM-1 algorithm. However, we note that most of the ranked lists reported by the original study contained only a single chromosomal coordinate per predicted spike-in region, such that the AUC results may not be directly comparable. Irrespective of this concern, we conclude that the trimmed mean procedure is not better but also not worse than other existing methods.<br>
Finally, we compared the performance of the four normalization methods in the two types of samples, i.e. amplified or unamplified. We reasoned that we could minimize the effects of different experimental procedures by comparing datasets generated by the same lab. Amplification had a negative impact on the ability to detect enriched regions only in the experiment employing Affymetrix tiling arrays, while it had no effect using Agilent or NimbleGen arrays (compare Figure 1a and 1b). The decrease in specificity was due to a decline of the recall rate across all spike-in concentrations in the experiments involving Affymetrix tilling arrays. Comparison of the concentration-dependent recall rates for the amplified and unamplified samples interrogated by Agilent and NimbleGen tiling arrays showed no such behavior (Figure 2). However, we think that this negative effect of amplification cannot be attributed to the Affymetrix platform, as datasets generated by the Brown lab interrogating an amplified sample by another Affymetrix tiling array yielded comparable results to the ones obtained by Struhl and Gingeras using an unamplified sample (data not shown; [7]). The difference between the two Affymetrix tiling arrays is the number of probes spotted on the array, i.e. the spatial resolution. The array used by the Struhl and Gingeras lab had ~3 times more probes than the one used by the Brown lab. We speculate that PCR amplification results in a preferential enrichment of short DNA fragments, which are more easily mistaken for real enrichment in tiling arrays with higher spatial resolution. Thus, we conclude that amplification per se has no negative impact on the ability to detect enriched genomic regions, but limits the detection of these loci using tiling arrays with high spatial resolution. Furthermore, we speculate that the lower performance of the Affymetrix array interrogating the unamplified sample has a similar explanation. Based on this assumption we propose that tiling arrays with high spatial resolution require a refined peak calling procedure that accounts for the possibility of false positive detection of small DNA fragments.<br>
<br>
Standard normal normalization<br>
Contrary to our expectation that sequence-dependent probe-level normalization increases the ability to detect enriched genomic regions, we found no evidence for such an effect. Our "control" normalization procedure, SNN, performed in most of the cases similar to the three sequence-dependent normalization approaches, suggesting that it effectively removes biases due the sequence composition. The SNN procedure consists of three steps: (1) transformation of the intensities to a logarithmic scale (log intensity); (2) subtraction of the mean log intensity, which is intended to remove systematic shifts in the intensity distribution between arrays (and/or channels for two color arrays); (3) division by the standard deviation, which is intended to remove differences in the intensity scales.<br>
These operations are in general not able to remove the sequence-dependent biases of the intensities measured by each individual array and/or channel. The correlation between the SNN normalized intensities and, for example, the GC content remained unchanged. The picture dramatically changes if we consider the dependency between the ratio of the intensities measured for the spike-in and the control samples on a logarithmic scale (log-ratio) and the GC content. Here, we found that the division of the standard deviation effectively removed most of the sequence inherent biases observed by only subtracting the mean log-intensity (compare Figure 3a and 3b). Thus, the control hybridization is the best guide to remove sequence-dependent biases, but it has to be properly rescaled in order to display its normalizing character.<br>
<br>
<br>
Conclusion<br>
We have demonstrated that sequence-dependent normalization hardly improves the detection of enriched genomic regions. We were able to draw this conclusion by utilizing spike-in experiments, which emphasize the requirement of clear-cut benchmarks to recover the advantages and disadvantages of analysis approaches. We found that the "success" of the sequence-independent normalization method, referred to as SNN, can be attributed to the normalizing character of the control experiment, which depends on a proper scaling of the measured intensities.<br>
<br>
Methods<br>
Raw data for spike-in Experiments<br>
We downloaded the raw data from NCBI <database>GEO</database> [9]. For the amplified samples processed and hybridized by the Struhl and Gingeras lab we downloaded 6 raw <fileFormat>CEL</fileFormat> files corresponding to the identifiers [GEO:GSM249008 to GSM249010] (3 spike-in samples), [GEO:GSM249011 to GSM249013] (3 control samples); by the Farnham and Green lab 6 raw files corresponding to the identifiers [GEO:GSM254805 to GSM254807] (each channel separately); by the McCuine lab 2 raw files corresponding to the identifiers [GEO:GSM248658 and GSM248666] (both channels). For the unamplified samples processed and hybridized by the Struhl and Gingeras lab we downloaded 12 raw <fileFormat>CEL</fileFormat> files corresponding to the identifiers [GEO:GSM248996 to GSM248998] and [GEO:GSM249002 to GSM249004] (6 control samples), [GEO:GSM248999 to GSM249001] and [GEO:GSM249005 to GSM249007] (6 spike-in samples); by the Farnham and Green lab 8 raw files corresponding to the identifiers [GEO:GSM254930, GSM254971 to GSM254973] (each channel separately); by the McCuine lab 2 raw files corresponding to the identifiers [GEO:GSM248654 and GSM252509] (both channels).<br>
The array designs and probe sequences were taken from files corresponding to the identifiers [GEO:GPL6129] (Affymetrix, Struhl and Gingeras lab), [GEO:GPL4559] (NimbleGen, Farnham and Green lab) and [GEO:GPL6189] (Agilent, McCuine lab).<br>
<br>
Sequence-dependent probe-level normalization<br>
MAT was originally implemented by Johnson et al. (2006) [1]. As reference we downloaded the source code of <software>MAT</software> and re-implemented the algorithm in R [8] with only one modification, i.e. we removed the term which models the effect of the number of times a probe occurs in the genome (variable named ci in [1]). We estimated the parameters of the model using a standard linear model fitting procedure. Using this model we computed the expected intensity due to non-specific hybridization and the residuals of the measured intensities. The probes were then divided into 3000 bins based on the expected intensity. The residual intensity of each probe was then divided by the standard deviation of the bin the probe belongs to [1]. We reran the original <software>MAT</software> implementation (version 2.09232006) on the input data using the following parameters: Bandwidth = 200, MaxGap = 200, MinProbe = 10, Var = 0, Pvalue = 1e-5.<br>
MA2C was originally implemented by Song et al. (2007) [2]. As reference we downloaded the source code of <software>MA2C</software> (REF) and re-implemented the algorithm in R [8]. Specifically, we implemented the so-called robust version of their algorithm. Here, the probes are grouped into bins with equal GC content and for each GC content bin the mean intensity and the standard deviation per channel is computed and is used to estimate the enrichment of sequences corresponding to a probe [2]. We modified the algorithm such that it can also be used to normalize data from single color arrays. Furthermore, we compared each spike-in sample to each control sample.<br>
The basic approach of PMT is outlined in Chung et al. (2007) [3]. We did not change the estimation procedure of the free energy change due to unspecific hybridization. However, we substituted the linear model for the estimation of the intensity due to unspecific hybridization with a non-linear model, which is referred to as Hill equation:<br>
<br>
Here, the Ii corresponds to the intensity measured for probe i and ?Gi corresponds to our estimated free energy change due to unspecific hybridization. I0, ? and ? are parameters that have to be estimated. We can however attribute some meaning to two of these parameters, namely I0 corresponds to the average intensity of the sample and ? corresponds to the logarithm of average concentration (or better activity) of free probes.<br>
We would like to note that this model could be approximated by a linear model if exp [-? ?Gi - ?)] is very large compared to 1. This is the case when ? is very small, i.e. if the concentration of free probe is very small. If true this suggests that the higher the concentration of sample gets the higher will be the effect of unspecific hybridization. Thus, we predict that if one uses too much sample one measures mainly the physical properties of the probes, while by using fewer samples one can minimize the effect of unspecific hybridization.<br>
<br>
Standard normal normalization<br>
The standard normal normalization (SNN)is very similar to quantile normalization [10]. However, SNN normalizes each array and/or channel separately and the normalized intensities on a logarithmic scale are taken directly without further processing. It involves three steps: (1) the raw intensities are transformed to a logarithmic scale, the resulting values are referred to as log intensities; (2) the mean log-intensity is subtracted from the log intensities and (3) divided by the standard deviation.<br>
<br>
Peak detection<br>
We adapted the peak calling procedure proposed by Johnson et al. (2006) [1]. Specifically, we computed the mean of the normalized intensities of probes mapping to a sliding window of 500 base pairs. We discarded the 10% lowest and 10% highest intensities, if the number of probes within a window was more than nine and requested that each window contained at least three probes. The window means were either separately calculated for each array and/or channel (for the MAT, PMT and SNN normalized intensities), or for the combined values (for the MA2C normalized intensities). We summed the window means corresponding to the spike-in samples and subtracted the sum of the window means of the control samples. The resulting score was finally multiplied by the square root of the number of probes within the window. In a final step we extracted local score maxima, such that none of the overlapping windows had a score higher than the chosen one.<br>
<br>
Performance assessment<br>
We adapted the assessment scheme of Johnson et al. 2008 [7]. Here, the predicted windows were ranked according to their score and it was checked whether the window overlapped any of the 98 (unamplified) or 100 (amplified) fragments. If it overlapped, we scored it as true positive, if not, as false positive. In order to assure that each fragment is only predicted once, we discarded all further overlapping predictions, i.e. they were neither counted as true nor false positive. We calculated the true positive rate as the number of true positives divided by the number of spike-in fragments and the false positive rate as the number of false positives divided by the number of spike-in fragments. As a performance measure we determined the area under the ROC-like curve (AUC), where we plotted the true positive rate against the false positive rate as defined above. We computed the AUC up to a false positive rate of 10%, i.e. 10 false positives and multiplied it by 10 to arrive at AUC values between 0 (random prediction) and 1 (perfect prediction).<br>
In order to check whether the sensitivities were dependent on the enrichment of the spike-in fragments, we computed the percentage of correctly predicted spike-ins in four different enrichment classes with maximally 10% false positive predictions, namely high(enrichment between 64 and 192 fold), medium (enrichment between 6 and 10 fold), low (enrichment between 3 to 4 fold) and ultra low (enrichment between 1.25 and 2).<br>
<br>
<br>
Authors' contributions<br>
HRC wrote the analysis software, conducted the analysis and drafted the manuscript. MV drafted the manuscript. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2832898</b><br>
An automatic method for identifying surface proteins in bacteria: <software>SLEP</software><br>
<br>
<br>
Background<br>
Bacterial diseases are among the major causes of mortality and morbidity in humans. Antibiotics are the first line of defence against bacteria, however more and more bacteria are antibiotic resistant and the phenomenon is spreading at an alarming rate [1,2]. Many diseases are becoming increasingly difficult to fight. There are several examples of microbial infections that are becoming resistant to all existing therapies and for which a vaccination strategy is deemed to be appropriate, such as gonorrhea, tuberculosis, pneumonia, septicaemia and childhood ear infections [3-7].<br>
Among the proteins encoded by bacteria, secreted and surface proteins are particularly important in bacterial pathogenesis. The former can be involved in host cell toxicity and lead to more or less subtle alterations of the host cell for the benefit of the pathogen. Bacterial surface proteins play a fundamental role in the interaction with the cell environment [8-12]. They can be involved in adhesion and invasion of the host cells as well as in defending against host responses. Because of this, surface proteins are potential drug targets [13]. Moreover, surface proteins are likely to interact with the host immune system and are ideal candidates for vaccine development [14-16].<br>
Surface proteins include integral or transmembrane proteins that span the membrane and have a hydrophilic cytosolic domain, which interacts with internal molecules, a hydrophobic membrane-spanning domain that anchors it within the cell membrane, and a hydrophilic extracellular domain that interacts with external molecules. Lipid anchored proteins are instead covalently-bound to one or more lipid molecules. Other membrane proteins are peripheral, i.e. they are attached to integral membrane proteins, or associated with regions of the lipid bilayer.<br>
Gram-positive bacteria possess a thick cell wall containing many layers of peptidoglycan and teichoic acids. In contrast, Gram-negative bacteria have a relatively thin cell wall consisting of a few layers of peptidoglycan surrounded by a second lipid membrane containing lipopolysaccharides and lipoproteins. This is reflected in their membrane protein composition. Cell wall proteins are found in Gram+ bacteria while ?-barrel membrane proteins are only found in the outer membranes of Gram- organisms, in mitochondria and chloroplast [17].<br>
Despite the biological relevance of bacterial surface proteins, their characterization is still incomplete. There are two main routes to identify surface proteins. In one approach, membrane and cell wall fractions are separated from the cytoplasmic fraction and then proteins are identified by two-dimensional (2D)-electrophoresis or 2D-chromatography coupled to mass spectrometry [see for example [18-23]]. The other possibility is to take advantage of bioinformatics and attempt their prediction on the basis of one of the many specifically developed algorithms.<br>
There is a plethora of available tools for predicting the membrane localization and topology of a protein and the presence of specific localization signals in its sequence, but not every method is equally accurate and, especially, an end user is not always well informed about novel developments in the field. The order in which these tools are used might also make a difference, as we will show here. Furthermore, each of them tends to use different input formats and not always self explanatory output formats.<br>
The aim of the work described here is to bring these tools in a coordinated and easy-to use form to the bench scientists who, on one side, should not need to be familiar with the ins and outs of each and every tool, but, on the other, should be given sufficient information to assess the reliability of the methods.<br>
<br>
Implementation<br>
<software>SLEP</software> and all the related tools have been implemented locally on a linux SLES 10 server.<br>
The programs included in the <software>SLEP</software> automatic procedure are <software>Glimmer</software> [24-26], <software>TMHMM</software> [27,28], <software>prodiv-HMM</software> [29,30], <software>pSORTb</software> [31,32] and <software>LipoP</software> [30] all ran with default parameters.<br>
If the user inputs a genome, putative genes need to be identified and translated into their amino acid sequence. This is achieved using <software>Glimmer</software>, a gene finding program based on Interpolated Markov Models (IMMs) [24-26]. The accuracy of gene identification by <software>Glimmer</software> depends upon the length and the GC-content the genome and is reported at http://www.cbcb.umd.edu/software/glimmer/.<br>
The translated gene products, or the input proteins (if the user selected to start with a known proteome) are analysed for the presence of transmembrane regions using <software>TMHMM</software> [27,28] and <software>prodiv-HMM</software> [29], two independent Hidden Markov Model-based prediction methods. It is a known problem in the field that signal peptides might often be mispredicted as transmembrane helices and vice versa. To alleviate this problem, we only assign the "membrane protein" tag to proteins for which more than three transmembrane helix are predicted by at least one method. As described later, proteins for which no signal peptide is identified are re-submitted to the transmembrane prediction tools.<br>
Proteins not assigned to the "membrane" bin are analysed using <software>LipoP</software> [30], a tool for identifying signal peptides of both type I and II in a protein sequence. Because all clearly detectable membrane proteins have been already filtered out in the previous step, the number of false positives, i.e. the number of times <software>LipoP</software> predicts as a signal peptide what is in reality a transmembrane helix, is reduced. Table 1 shows the comparison between the accuracy obtained using <software>LipoP</software> on the complete dataset and that achieved by running it only on the filtered set of proteins, i.e. on proteins not including predicted transmembrane proteins with three or more helices, according to the <software>SLEP</software> protocol. The decrease in the number of false positives, although rather small, justifies our choice in using the tool only after filtering out the predicted multiple membrane spanning proteins.<br>
The next step consists in running <software>pSORTb</software> [31,32] on the remaining set of proteins. <software>pSORTb</software> is used for recognizing cell wall proteins (in Gram+ bacteria) and outer membrane proteins (in Gram- bacteria) as well as exported proteins. The remaining proteins are reanalysed by <software>TMHMM</software> and <software>prodiv-HMM</software> in order to identify proteins with a single membrane spanning helix. As mentioned before, we remove clearly detectable membrane proteins before attempting the prediction of the presence of signal peptides. Only if no signal peptide has been identified in the sequence, we look for single membrane spanning helix.<br>
The statistical parameters used for evaluating the accuracy of the predictions are:<br>
Where TP, TN, FP and FN are the number of True Positive, True Negative, False Positive and False Negative results, respectively.<br>
<br>
Results and discussion<br>
<software>SLEP</software> is based on an automated optimal combination and succession of usage of some of the most reliable available tools. The user needs to input either the genomic sequence of the bacterial organisms under study or its proteome together with the information of whether the bacterium is Gram+ or Gram-. The main purpose of <software>SLEP</software> is to provide users with an easy-to-use tool for the prediction of protein localization with the highest possible accuracy achievable today. The user interface of the tool is illustrated in Figure 1.<br>
The output of the system is an organized list of proteins (or putative proteins if the input is a genome) classified according to their predicted localization. In particular it will separately list lipoproteins, membrane, exported and secreted proteins, cell wall proteins or outer membrane proteins in Gram+ or Gram- bacteria, respectively (Figure 2).<br>
We tested the accuracy of the procedure using the manually curated database <database>SwissProt</database>. This dataset, named SP, contained 18,510 protein sequences of known localization (as reported in the SUBCELLULAR LOCATION field), roughly equally populated by protein from Gram- and Gram+ bacteria (9,946 and 9,564, respectively). For Gram+ bacteria we used Enterococcus (EN, 228 proteins), Listeria (LI, 749 proteins), Staphylococcus (SP, 3981 proteins), Streptococcus (ST, 179 proteins) and a pool of Bacilli Gram+ organisms (B+, 4427 proteins). For Gram-, the datasets contained data from E. coli (EC, 3891 proteins), Legionella (LE, 421 proteins), Pseudomonas (PS, 3369 proteins), Salmonella (SA, 2019 proteins) and a pool of Bacilli Gram- organisms (B-, 246 proteins).<br>
The overall accuracy of the predictions that can be achieved in a single click using <software>SLEP</software> is illustrated in Table 2.<br>
In Table 3 we compare our results with the use of <software>PSORTb</software> alone. For completeness, we report in the same Table the accuracy of other available methods for the relevant datasets [33-38]. Notice that the tools included in <software>SLEP</software> have been selected for their accuracy, but also for their availability as stand-alone programs since they are all implemented locally to speed up the procedure.<br>
<br>
Conclusions<br>
Bioinformatics tools are extremely useful for the bench scientists and most of them are mature enough to be considered part of a toolbox that should be readily and easily accessible to all.<br>
The appropriate usage of the tools is however essential. This is far from being trivial: one of the most cogent problems in bioinformatics is that way too often obsolete tools remain available and are used by experimentalists who are unaware of more recent developments. Users are confronted with too many available tools, not all properly benchmarked and updated and this can result in a waste of time and effort. The problem is even more relevant when the methods need to be used as start points of a set of experiments where an incorrect selection/usage of the methods can seriously affect the end results.<br>
The initial selection of the set of transcripts/proteins from a pathogen to be used as targets for the development of vaccines and/or inhibitor screening is one such case and yet no comprehensive easy-to-use system was available so far. Perhaps the most complete resource available is <software>Augur</software> [39] which includes a precompiled list of protein localizations and other useful features, but does not allow users to supply their own genome/proteome or set of proteins as is the case in <software>SLEP</software> and is limited to Gram negative bacteria.<br>
We have described here an automatic procedure designed to achieve an accurate prediction of bacterial protein localization via an appropriate sequence of usage of the available methods that is, at the same time, extremely easy to use.<br>
<software>SLEP</software> uses a combination of state of the art methods that have been shown to be the most accurate available [29,30,32]. The specific order of usage of these programs has been designed to reduce the chance of misclassification by each of the tools.<br>
The system relieves the bench scientists from the burden of selecting the most accurate programs for the task at hand. <software>SLEP</software> will be continuously updated to reflect novel developments and plans to be the one-stop shop for the analysis of bacterial protein localization that is perhaps the most important aspect of therapeutic target selection.<br>
<br>
Availability and requirements<br>
? Project name: <software>SLEP</software><br>
? Project home page: http://www.caspur.it/slep<br>
? Operating system(s): Platform independent<br>
? Programming language: Perl and Python<br>
? Any restrictions to use by non-academics: None<br>
<br>
Authors' contributions<br>
EG implemented the system. MO wrote the scripts for running <software>glimmer</software>. DC designed and developed the SLEP website. AT was involved in coordinating the work and in drafting the manuscript. All authors read and approved the final manuscript<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2836305</b><br>
Enrichment of homologs in insignificant <software>BLAST</software> hits by co-complex network alignment<br>
<br>
<br>
Background<br>
Comparative genomics involves large scale investigations to identify which parts of different genomes are of common descent in order to predict function or to study genome evolution. A common first step towards detecting homology between genes or proteins within a genome or between different genomes, is to do a <software>BLAST</software> search with a set of genes or proteins against a database and regard each hit with an E-value below a certain cutoff to be homologous [1]. Additionally, several filters and clustering algorithms can be applied to separate sets of homologs into orthologous groups (e.g[2,3]). Usually, a stringent score cutoff is used to ensure that the hits that are included are indeed homologs. Naturally, homologs whose sequences have diverged strongly, are incorrectly excluded.<br>
On a smaller scale, more sensitive searches based on profiles of groups of related amino acid sequences (such as <software>PSI-BLAST</software> or <software>HMMer</software>) or, if available, protein three dimensional structures are commonly used to avoid False Negatives without losing confidence in the putative homologs returned [4,5]. In these searches, instead of using the same scores or probability for each position, a multiple sequence alignment is used to define position specific substitution scores or transition probabilities.<br>
Besides improving sequence based homology searches, one can also use information on the genomic context of sequences to aid detection of a common descent of sequences. Genome alignments can be very useful when there are difficulties in determining homology between sequences, for example between intergenic regions. Boekhorst and Snel showed that genome alignments can be used to select candidates from a set of insignificant <software>BLAST</software> hits in prokaryotes [6]. In eukaryotes, gene order is less conserved across large phylogenomic distances such as between fungi and animals and therefore less likely to make a valuable contribution to the detection of homology at these large evolutionary distances [7]. As a result, conserved synteny is mainly employed in eukaryotes for the detection of orthologs, between closely related species, e.g. within ascomycete fungi or within vertebrates [8,9].<br>
The availability of protein interaction networks allows for the comparison of genomes and the functional context simultaneously. Information on the functional context of proteins is already used in comparative genomics of eukaryotes to select from a set of inparalogs, the protein that is functionally similar to the query sequence (the 'functional ortholog') [10-12]. In the comparative analysis of protein interaction networks, spurious protein interactions can be separated from biologically relevant interactions if the protein-protein interaction occurs in different species. We here test if the reverse is also in principle applicable: can the network alignment help to separate spurious homology links from real ones? Analogously to genome alignment, we test the expectation that the insignificant <software>blast</software> hit between protein a from species A and protein b from species B is more likely to reflect homology if protein a is functionally closely related to proteins which are readily identifiable as orthologous to proteins in species B that are functionally closely related to b (Figure 1). To answer this question, we select candidate pairs from <software>BLAST</software> hits between human and yeast proteins based on conserved functional context, in this case homologous complexes, and determine whether this selection contains relatively more homologs than a background of hits with similar <software>BLAST</software> scores.<br>
<br>
Results and discussion<br>
Are hits with conserved functional context more likely to be homologous?<br>
We perform an all-against-all <software>BLAST</software> search between the human and yeast proteomes using a substantially more inclusive threshold than normally is applied to allow a comprehensive survey of insignificant <software>BLAST</software> hits. For each query-hit pair <software>BLAST</software> returns an E-value. We bin the E-values into 8 bins ranging from [E &lt; = 10-5] to [10 &lt; E &lt; = 100]. We define co-complex networks for human and yeast based on two curated complex datasets per species, and use Inparanoid clusters between human and yeast to align these networks (Figure 1 and Methods) [2,13-15]. If the query and the hit contain a domain which belongs to the same Pfam clan, we consider them to be True Positives. For each of our 8 bins, we calculate the fraction of query-hit pairs which are True Positives, with and without co-complex network alignment.<br>
We find that the use of co-complex information results in a considerable increase in the fraction of true homologs among the returned hits, compared to <software>BLAST</software> without co-complex information (Figure 2). This difference is most eminent in bins representing E-values normally considered to be insignificant (the 'gray-zone'). At E-values between 1 and 10 almost 90% of the returned hits share a Pfam clan, which means a substantial, 8 fold increase in the percentage of True Positives. This is not due to a bias resulting from being a member of the co-complex network or being in a conserved region of the co-complex network, as only after alignment of the co-complex network we see a big improvement in the fraction of True Positives (Figure 2).<br>
Only a small subset of yeast and human proteins (~12% of human and ~26% of yeast proteins) is part of a co-complex network within each species. Moreover, many of those are not functionally linked to proteins that have readily identifiable orthologs in the other species. As a consequence, this method is applicable to only a small fraction of query-hit pairs (Table 1). If we include high-throughput co-complex datasets for yeast and human, the coverage is increased a little at a cost of a slightly inferior performance (see Additional file 1).<br>
We show that alignment of co-complex networks can facilitate the identification of true homologs among gray zone <software>BLAST</software> hits. In a simple and completely automated procedure, we obtain a subset of hits which, despite very high E-values, is substantially enriched for homologs. This allows us to infer homology for pairs with co-complex network alignment with an E-value ranging between 0.1 and 1 with the similar confidence as for pairs before co-complex network alignment and an E-value of 0.01 (Figure 2). Our framework would likely be improved if we could use statistics on (locally) missing connections in both co-complex networks. To date, protein complex datasets are too fragmentary to make any sensible estimates of the number of missing connections.<br>
<br>
Detection of missing complex subunits<br>
Previous large scale investigations towards presence and absence of protein complex subunits in prokaryotes and eukaryotes reveal that most complexes are only partially present in other species [16-18]. In these studies, an orthology definition based on <software>BLAST</software> is used to determine presence and absence of subunits in different species and part of the subunits regarded absent may be missing due to detection problems. Hence the disrupted co-evolution of protein complexes might partly be an artefact.<br>
The use of co-complex information is potentially useful in the detection of yeast homologs of subunits of human protein complexes. Especially as the most important disadvantage, the lack of coverage of the co-complex networks, is less urgent because the queries are subunits and hence are all part of the human co-complex network. We take the opportunity to test the applicability of our method to a problem in comparative genomics and assess the added value of co-complex network alignment in detecting homologs in yeast for subunits of human complexes. For the complexes in the <database>CORUM</database> dataset, we initially find homologs for complex components by running a <software>BLAST</software> search with all subunits against the yeast proteome, applying a commonly used E-value cutoff of 0.001. Then, for the subunits we did not find homologs for, we use a less stringent E-value cutoff of 1, in combination with co-complex network alignment, to see how many additional subunits we pick up.<br>
Using <software>BLAST</software> only with an E-value cutoff of 0.001, we find yeast homologs for 1199 out of 1901 (63.07%) subunits. We find that 172 out of 710 complexes (24.23%) have a homolog in yeast for all subunits, 427 (60.14%) have homologs for some subunits and 111 (15.63%) complexes are completely absent. Even when only comparing two species, we find that for most human complexes, only part of all subunits have a homolog in yeast. However, as we have argued before, some subunits may be called absent due to detection problems.<br>
For the 702 subunits for which we detected no homolog in yeast, we select, using the co-complex network alignment, candidate homologs in yeast for 52 additional subunits, belonging to 62 complexes (some subunits are part of multiple complexes). Using <software>Pfam</software>, <software>CDD</software> and <software>PSI-BLAST</software>, we confirm that the 49 out of 52 candidates recovered with co-complex network alignment are in fact homologous. With the 49 confirmed homologs we retrieved, an additional 19 complexes are completely present in yeast (see Additional file 2).<br>
One striking observation when comparing individual complexes in human to complexes in yeast, is that there is very little congruence between human and yeast complex definitions (see Additional file 3). Factors such as the incompleteness of data in both species, individual decisions on what does belong to a complex and what does not, and proteins belonging to multiple complexes, obscure a one-to-one relation between yeast and human complexes, assuming such a correspondence exists.<br>
Fortunately, because we align human and yeast complexes on a network level rather than as individual complexes, we are able to retrieve homologs with the co-complex network alignment for complexes which do not exist as such in yeast. A good example is the Multisynthetase complex (Figure 3). This complex is composed of 8 aminoacyl-tRNA synthetases and 3 auxiliary proteins. The individual tRNA synthetases all have a homolog in yeast found with the initial straightforward <software>BLAST</software> search. The yeast homologs of the tRNA sythetases are not known to be organized in a complex, with one important exception: methionyl and glutamyl synthetases MES1 and GUS1 associate into a complex with ARC1, an auxiliary protein (and homolog of the human auxiliary protein p43, SCYE1) which increases catalytic efficiency and ensures correct localization into the cytoplasm. Via this complex, human JTV1, a scaffold required for the assembly and stability of the multi-tRNA synthetase complex, is linked to a short N-terminal stretch of yeast GUS1, whose C-term is unambiguously homologous to the glutaminyl synthetase in human, QARS (3). When we do a <software>PSI-BLAST</software> with human JTV1 as a query protein, we retrieve GUS1, aligned to the GST_C domain in JTV1 (E-value 1e-05) after three iterations.<br>
We recovered a homolog for another subunit of the Multisynthetase complex via an unrelated complex: the Ribosome. Human EEF1E1 has a hit with yeast EFB1 with an E-value of 0.015. EFB1 is located at the ribosome, as is YHR020W, which is the readily identifiable yeast ortholog of the human bifunctional glutamyl-prolyl tRNA synthetase EPRS. Both EFB1 and EEF1E1 are translation elongation factors (EEF1E is a translation elongation factor 1 epsilon and EFB1 a translation elongation factor 1 beta) and both contain a domain which belongs to the GST_C_superfamily. The HSP lies in the regions where the GST_C_superfamily domain lies in both proteins and these regions in the protein sequences are, albeit very distantly, evolutionary related. EFB1 has a much more similar homolog in human (namely EEF1B2, <software>BLAST</software> E-value 1e-33), suggesting that EEF1E1 and EFB are related through a very old duplication event and the translation elongation factor 1 epsilon EEF1E1 ortholog is lost in yeast.<br>
Applying the co-complex network alignment to the set of protein complex subunits in <database>CORUM</database>, we select candidate homologs in yeast for 52 proteins, out of which we could confirm homology with <software>Pfam</software>, <software>CDD</software> or <software>PSI-BLAST</software> for 49 pairs. The observation reported in both large and small scale investigations [16-20], that most complexes are 'incomplete' in many species, remains unchallenged because we can only show for a few complexes that their incompleteness is a result of an undetected homology<br>
<br>
Are the recovered distant homologs orthologs?<br>
Exploiting the co-complex network alignment we find yeast homologs for 52 subunits of human complexes that are not revealed by standard <software>BLAST</software>. This is markedly less than the 405 human queries for which co-complex information is applicable (Table 1). The likely crucial difference between our initial survey of all <software>BLAST</software> hit pairs and the detection of missing complex subunits is the fact that in the latter, we applied the co-complex alignment only to those query proteins for which we could not find a homolog with <software>BLAST</software> alone. Therefore we expect that many query proteins for which we recover a distant homolog with the co-complex network alignment in the initial survey, have an additional, significant hit in yeast and are therefore not used as a query when looking for additional homologs for complex subunits. Indeed, we find that this is the case for no less than 85% (347 out of 405) of the query-hit pairs with an E-value &gt; 0.01.<br>
There are a few possible evolutionary histories that can explain the fact that for a certain query protein in human, we find a close homolog and a distant homolog with conserved functional context in yeast. First of all, distant homologs recovered by co-complex network alignment could be ancient paralogs (outparalogs with respect to branching of fungi and metazoa), in which the high degree of divergence is due to time rather than rapid sequence evolution (Figure 4a). For instance, EEF1E1 and EFB1 in the Multisynthetase example discussed above are ancient paralogs. Another possibility is a more recent duplication in yeast followed by asymmetric divergence in the duplicates, in which case the divergence is caused by accelerated evolution on one branch (Figure 4b)[12,21]. Finally, the two yeast hits may be homologous to different regions of the query protein due to fusion, fission or domain recombination events (Figure 4c), in which one domain/region has a markedly higher rate of sequence evolution than the other.<br>
In the fusion/fission/domain recombination scenario, the two yeast hits of the human query protein in yeast are not homologous. For 29 of our 347 trios the two yeast hits are not a significant hit in <software>BLAST</software>, neither do they share a homologous domain according to <software>Pfam</software>. For 27 of these trios the best scoring <software>BLAST</software> HSP of the two yeast proteins is in a different region in the human protein. In the remaining two pairs, the distant homologs that we retrieve share only a short KOW motif with the query protein, while the best hit shares both the KOW motif (not recognized by <software>Pfam</software>, but part of the HSP) and also the adjacent Ribosomal L27e domain.<br>
If we consider only those 318 trios of proteins in which the two yeast proteins are homologous according to <software>Pfam</software>, we find that in 307 of them, the distant homolog has a significant hit in human, suggesting it is in fact an ancient paralog (Figure 4a). A recent study towards the fate of duplicated protein complex subunits showed that 31% of duplicates resides in different complexes, 31% stayed in the same complex and in 38% of the cases one of the duplicates is not known to be part of any complex [22]. We investigate the fate of the 307 yeast pairs that are outparalogs according to our analysis. The yeast pairs are not a random sample of ancient yeast duplicates, on the contrary. Because one of the yeast paralogs is a close homolog of a human protein which is part of a complex which is homologous to the complex the other yeast paralog is part of, we expect a bias towards duplicates remaining in the same complex.<br>
We find that for 139 pairs (45.3%), both duplicates are in the same complex, for 25 pairs (8.14%) the duplicates are in overlapping complexes (sharing more than half of their subunits), for 108 pairs (35.2%) they are in a different complexes and for 35 pairs one of the duplicates (the one most closely related to the human query protein) is not known to be part of any complex in yeast. We expect that the human homologs of the 108 pairs in which the yeast ancient duplicates belong to distinct complexes, are more often part of multiple complexes, indicating that the yeast duplicates subfunctionalized. We do observe a significant overrepresentation of proteins that are part of multiple complexes in ancient paralogs when compared to all subunits in the <database>CORUM</database> complex dataset (P = 0.007), but not in ancient paralogs which are part of the same complex when compared to those which have ended up in distinct complexes (P = 0.58).<br>
The lion's share of distant homologs we recover using the co-complex network alignment consists of ancient paralogs rather than orthologs. Duplications in general are very important in the evolution of protein complexes [22,23] and many structures are known to consist of subunits resulting from very old duplications (e.g. the proteasome). We find that in most cases both duplicates are part of the same or overlapping complexes. This suggests that the duplicates we detect have sub- or neofunctionalized within one complex, although some might be the result of outparalogs that have been independently recruited to a biological process.<br>
<br>
<br>
Conclusions<br>
We test whether contextual information from the functional network, in this case conserved co-complex relations, can aid homology detection. Functional context information has been used before to help in choosing functional orthologs from a set of inparalogs, but to our best knowledge, this is the first time functional networks are used to aid distant homology detection. Using an aligned co-complex network, we can identify a subset highly enriched for homologs of <software>BLAST</software> hits with an E-value which would normally be regarded as insignificant. This shows that, even though evolution takes place at the sequence level, one can use co-complex networks as circumstantial evidence to improve confidence in the homology of distantly related sequences.<br>
The interspecies co-complex network includes only a small fraction of all proteins, which impedes applicability. As more high-throughput datasets become available in more species, we expect that the proof of principle we established here, can be applied and tested on a larger scale, between more distantly related species and with other types of functional relations. We apply our co-complex network alignment to a dataset of human complexes in order to determine how many homologous subunits we can detect that we missed in an initial <software>BLAST</software> search. We thereby recovered homologs for only a few additional subunits, despite the fact that coverage is less a limiting factor in this context. We find that one reason we retrieve less additional subunits than expected, is that with the co-complex alignment, we mainly detect outparalogs rather than orthologs.<br>
It has been shown that subunits of a protein complex diverge at similar rates, presumably because subunits of a protein complex are functionally strongly interdependent and subject to very similar evolutionary constraints [24]. In contrast, the co-complex network alignment method is based on the fact that some subunits diverged between human and yeast to such an extent that they are not picked up in a regular <software>BLAST</software> search and other subunits are conserved such that the human and yeast orthologs are still detected by Inparanoid. In this light it is not surprising that most homologs we recover with our method are ancient paralogs rather than orthologs: the difference in the extent of divergence is due to difference in time, as opposed to difference in evolutionary rates between subunits of the same protein complex.<br>
Researchers studying the evolution of individual protein complexes have used functional information to find diverged homologs successfully despite absence of proof from a large scale study [25,26]. Our results provide this proof. Numerous predictions made in these small scale studies were subsequently confirmed by profile vs profile alignments or the comparison of protein three dimensional structures upon availability. Interestingly, many of these predictions represent initial <software>BLAST</software> hits with E-values even higher than 100 (the cutoff used in this study). Hence it is possible that in our study still many homologs have gone undetected, and the formal integration of functional context with more sensitive homology detection methods might help in the development of automatic bioinformatic methods to uncover these distant homologs and improve our insights into the ancient evolution of the protein interaction network.<br>
<br>
Methods<br>
Co-complex network<br>
To construct a human co-complex network, we download the set of <database>CORUM</database> Core complexes from http://mips.gsf.de/genre/proj/corum and stored the complexes as sets of co-complex pairs [15]. We added 'direct complex' pairs downloaded from <database>Reactome</database> http://www.reactome.org/download/current/homo_sapiens.interactions.txt.gz[13], which, in combination with pairs from the <database>CORUM</database> dataset, results in a co-complex network containing 32415 unique pairs in total. For the yeast co-complex network, we stored <database>MIPS</database> complexes from ftp://ftpmips.gsf.de/yeast/catalogues/complexcat as binary co-complex relationships [14] and complexes from <database>SGD</database> <database>GO</database> cellular component annotation [27] as in [22]. This resulted in 20075 unique pairs in total.<br>
<br>
<software>BLAST</software> and <software>Pfam</software><br>
We downloaded 46704 human protein sequences from <database>Ensembl</database> [28], (Homo_sapiens.NCBI36.50.pep.all.fa) and yeast protein sequences from the <database>Saccheromyces Genome Database</database> (orf_trans_all.fasta) in July 2008. We run <software>BLAST</software> between human and yeast with the maximum returned E-value set to 100, maximum number of hits and alignments set such there it is no limiting factor [1]. We did not adjust the database size. If two proteins have multiple HSPs (regions aligned by <software>BLAST</software>), we keep only the HSP (High Scoring Sequence Pair) with the lowest E-value. We downloaded <software>Pfam</software> HMMs (version 23, July 2008) and data on homologous <software>Pfam</software> families (Pfam clans) from the <software>Pfam</software> website ftp://ftp.sanger.ac.uk/pub/databases/Pfam, searched for domains in human and yeast proteins using hmmpfam in the <software>HMMer</software> package [29] with default cutoffs.<br>
For each <software>BLAST</software> hit, for both the human query protein and the yeast hit, we determine the overlap between the HSP and each <software>Pfam</software> domain and divide the number of amino acids in the overlap with the length of the shortest region (either <software>Pfam</software> domain or HSP) to get a percentage of overlap. If a query and a hit have a Pfam domain that belongs to the same clan and the overlap of the domain and the HSP is greater than 50%, we call this <software>BLAST</software> hit a True Positive. <software>BLAST</software> hits for which this overlap is less than or equal to 50% in either the human query or the yeast target protein are ignored as the gold standard for homology (<software>Pfam</software> clans) can't be fully applied to these proteins.<br>
<br>
Co-complex network alignment<br>
To align the co-complex networks of yeast and human we look for yeast orthologs for all proteins in the human co-complex network using <software>Inparanoid</software>. We run <software>Inparanoid</software> 3.0 with default parameters, so for each bidirectional best hit which forms a seed pair for an <software>Inparanoid</software> cluster, it is required that the minimum <software>BLAST</software> bitscore is 50 and the overlap of the alignment relative to the shortest of the two proteins is at least 50% [2].<br>
For each <software>BLAST</software> query-hit pair, if the human query protein has at least one direct neighbour in the human co-complex network that is orthologous (in one <software>Inparanoid</software> cluster) to the direct neighbour of the yeast hot protein in the yeast co-complex network (Figure 1), we assign this pair to the 'co-complex network alignment' category (Figure 2). We bin E-values in 8 bins ranging from [E &lt; = 10-5] to [10 &lt; E &lt; = 100] and calculate for each bin the percentage of True Positives (hits that each have a <software>Pfam</software> domain belonging to the same clan), also known as the Positive Predictive Value. Each bin contains at least 60 pairs and at least 50 query proteins (Table 1). Normalizing for family size gives similar results (see Additional file 4).<br>
<br>
Detection of missing complex subunits<br>
To avoid biases due to overlapping complexes as much as possible, we removed 803 complexes which are a subcomplex of another complex from the set of <database>CORUM</database> Core complexes. If we remove all supercomplexes instead of all subcomplexes, we get qualitatively the same results. We first attempt to find a yeast homolog with <software>BLAST</software> and an E-value cutoff of 0.001 for all subunits. Subsequently, on those subunits we did not find a homolog for, we applied the co-complex network alignment with an adjusted E-value of 1 (expected percentage of False Positives &lt; 3% (Figure 2)).<br>
<br>
<br>
Authors' contributions<br>
BS conceived the study and assisted in writing the manuscript. LF, SB and JB performed the analysis and LF wrote the manuscript. All authors have read and approved the manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2876130</b><br>
Fine-tuning structural RNA alignments in the twilight zone<br>
<br>
<br>
Background<br>
Introduction<br>
The biological function of an RNA molecule is often determined by the three dimensional structure of the molecule. The structure is often more conserved than the exact sequence of bases in the course of evolution. Therefore, a strong structural consensus among related, but diverged sequences can be taken as an indicator of a preserved functional role.<br>
Following a classification introduced in [1], the structural consensus for a family of RNA molecules can be computed following three different "plans": We can either (A) align the sequences, then fold them jointly, or (B) simultaneously align and fold, or (C) first fold sequences individually, and then align their structures.<br>
Plan B is theoretically optimal, as it jointly solves the two optimization problems of alignment and folding under arbitrary scoring functions [2]. However, its computational costs of O(n3m) time and O(n2m) space, where n is sequence length and m is the number of sequences, are rather high. Implementations are for example <software>Foldalign</software>, <software>Dynalign</software>, <software>PMComp</software> [3-7], making pragmatic restrictions to improve efficiency.<br>
Plan C applies to sequences that are too diverged to be meaningfully aligned based on sequence conservation. First, sequences are folded individually. This must be done with care. One cannot simply compute MFE structures and then align them with a structure alignment program. In an evaluation of 69 sequences from 10 RNA families, the MFE predictions had the same abstract shape (i.e. arrangement of helices) as the consensus structure only in 32 cases (see Table two in [8]). Such lack of consensus in predicted MFE structures makes their structure alignment meaningless - but it does not rule out the existence of a good consensus structure hidden in the near-optimal folding space. Therefore, a representative subset of near-optimal structures must be obtained for each sequence, for example by abstract shape analysis [9]. Then, those structures which have a consensus abstract shape are aligned, for example via tree comparison (<software>RNAforester</software> [10,11], <software>MARNA</software> [12]). <software>RNAcast</software> [8] was the first Plan C approach; a more recent one is <software>R-Coffee</software> [13].<br>
Plan A is probably the most widely used approach and is the one we are going to strengthen further here. It applies when sequences can be meaningfully aligned using an off-the-shelf multiple sequence alignment tool (e.g. <software>ClustalW</software> [14], <software>T-Coffee</software> [15], <software>MAFFT</software> [16]). Then, the aligned sequences are folded jointly (e.g. <software>PFOLD</software> [17], <software>RNAalifold</software> [18,19], <software>ILM</software> [20], <software>ConStruct</software> [21]).<br>
The above listing of Plan A, B, and C methods is far from complete, as the difficulty of the problem is reflected by a large number of approaches. Numerous heuristics have been suggested to retain the power of Plan B approaches, but reduce its high computational cost and overcome its limitation to pairs of sequences, e.g. <software>MURLET</software> or <software>MXSCARNA</software> [22,23]. The practically minded reader is referred to the <software>WAR</software> web server for aligning structural RNAs [24], where presently 14 such methods are on display. Four of them can be categorized as Plan A approaches, which is our concern here. Our contribution presented here should not be considered as yet another approach, but rather, a fine-tuning step which is worthwhile to combine with Plan A methods.<br>
Plan A loses its raison d'?tre when sequence conservation is above 90%. While the sequence alignment is certainly reliable, it carries little extra information compared to folding the sequences individually. The method works best around 75% of sequence similarity. Below 55%, measurements [25] show a decline of performance. This effect has been confrmed in [26] (Figure four). Here, we show how to alleviate this situation to a certain extent. The resulting method is named planACstar, as it constitutes an iterated combination of steps from Plan A and Plan C.<br>
<br>
Motivation<br>
To evaluate the performance of Plan A in detail, consider Figure 1. The score of Plan A diverges most from the reference alignment in a range of 30-55% sequence identity. In this "twilight zone", the performance of <software>RNAalifold</software> drops, which is expressed in a lower structure conservation index (SCI) [27]. The lower index indicates a quality drop of the produced consensus structure, but does not rule out that a better consensus exists, which has not been detected. In fact, this is often the case.<br>
Let us look at the situation in some detail. It is generally known that with increasing divergence, sequence alignment will fail to align bases representing compensatory mutations and carry the covariance signal to be exploited in the subsequent phase. In fact, in the presence of gaps, sequence alignment will systematically obscure the covariance signal. As this observation has not been reported in the literature (to the best of our knowledge), let us explicate this phenomenon here.<br>
We use the familiar dot-bracket notation, where paired bases are indicated by matching parentheses, and unpaired bases by dots. For structures ..((....)).. and ..(....).., the expected structure alignment<br>
might show good sequence conservation, two gaps and a compensating base pair change. In any sequence alignment of AAGGAAAACCAA and AACAAAAGA, aligning two bases GG with a single base C, the algorithm has to insert a gap either to the left of C, or to the right. As the resulting score is not affected by this choice, the gap position may be chosen arbitrarily. Let us assume that the alignment algorithm first produces the mismatch, and then inserts a gap to the right of the mismatched base. So far, so good. Exactly the same situation arises in the alignment of CC and G. Again, the algorithm inserts the gap at the right side of G, and aligns the sequences this way:<br>
Now, joint folding of the aligned sequences will produce:<br>
which prevents the second base pair in the upper sequence from being formed: Pairing the second G with the second C would create non-nested base pairing, which is not allowed in the folding algorithm.<br>
<br>
Outline of planACstar<br>
In the 30-55% sequence identity range, we conjecture that the resulting consensus structure is often correct in its overall shape, while structural detail is lost due to the obscured covariance signal. Therefore, we disassemble the alignment and refold the sequences separately, with respect to the preliminary consensus. This separate folding will produce additional base pairs compatible with the consensus. We then align the resulting structures with a structure alignment program [10]. This aligns base pairs irrespective of the concrete bases, thus finding compensatory base changes and recovering the covariance signal. Any structure alignment, so obtained, entails a sequence alignment, which we extract. This sequence alignment may now fold into a better consensus than before.<br>
There is a clear limitation in this approach: We may recover base pairs missed in the first alignment folding, but we never undo consensus base pairs which were actually formed in the initial step. These base pairs will always persist in the improved structures, although they may be aligned in a different way. This is why we consider our approach a fine-tuning add-on to Plan A, rather than (yet another) new approach. This is in good analogy to tuning your guitar without reference to a perfect pitch device. You assume that one of the six strings, say A, is on pitch, and tune the other strings to the A string. While your guitar now sounds in harmony, in absolute terms, it may be more out of pitch than before. Our assumption is that, in the twilight zone, the initial alignment will be good enough to point to the string A which is closest to pitch in absolute terms.<br>
This idea has been implemented using the tools <software>ClustalW</software>, <software>RNAalifold</software>, and <software>RNAforester</software>. The quality of the consensus is assessed by <software>RNAalifold</software>'s SCI score, the structure conservation index, which according to Gruber et al. [28] is the best measure for structural conservation. Folding an alignment A with <software>RNAalifold</software> results in the consensus minimum free energy (MFE) EA as output. EA includes pseudo-energy contributions from observed covariation. Folding the sequences B1,.. Bn separately, we can compute their average MFE  from the separate MFEs. The SCI is the quotient of EA/.<br>
Thus, we measure an improvement in terms of <software>RNAalifold</software>'s own quality criterion, the achieved SCI.<br>
<br>
Adequacy of SCI improvement<br>
Note that the SCI is not a performance measure in the sense that an alignment optimizing the SCI score is closest to a "true" alignment. It serves as an indicator of structural conservation in diverged sequences, whether or not the conserved structure is "true". Mechanistically, the SCI accounts for the energy required to refold the sequences from their MFE structures into the predicted consensus X. It does not preclude the existence of yet another, low energy consensus structure X', which may be structurally quite different and may actually be the relevant one for the RNA's function. Refolding all sequences from their MFE structures into X' instead of X might require only marginally more energy than with consensus X, but again, this does NOT imply that X and X' are structurally similar. X' might be the "true" structure, but Plan A would not notice it at all. Admittedly, the larger the number of sequences considered, and the more diverged they are, the more unlikely is this situation to occur. Plan A approaches could, in principle, be augmented to safeguard against this situation, but only at the high cost of performing suboptimal consensus folding.<br>
Our new approach is based on the premise that it is worthwhile to increase SCI scores. However, we have also evaluated the resulting consensus structures against curated structures, and report on this in the Conclusion and in Additional File 1.<br>
<br>
<br>
Results and Discussion<br>
Algorithm<br>
planACstar is an iterated combination of elements of Plan A and Plan C. It uses separate structure predictions, as done in Plan C, but includes information from a multiple sequence alignment and alignment folding as in Plan A. We use <software>ClustalW</software> for the initial alignment step, and <software>RNAalifold</software> for folding the sequences into the consensus structure, because they are most widely used tools for these tasks in practice. Let A be the initial sequence alignment, C is the preliminary consensus and X is its SCI-score. Ci is the projection of the basepairs of the preliminary consensus C to a sequence Bi from the input set of size n, where 1 ? i ? n. Si is the individual folding of a sequence Bi from the input set into the preliminary consensus structure C. Y is the multiple structure alignment of the folded structures. The multiple structure alignment implies a sequence alignment, which is the improved sequence alignment A* with SCI-score X*.<br>
In pseudocode, the procedure is as follows:<br>
Given a set of RNA sequences B1,..., Bn,<br>
1. A ? ClustalW(B1,..., Bn) - initial sequence alignment<br>
2. C ? RNAalifold(A); X ? sci-score(C) - preliminary consensus<br>
3. Ci ? Projection of basepairs in C to Bi - (see below)<br>
4. Si ? RNAfold -C Ci(Bi) for i = 1,.. n - individual folding of each Bi relative to consensus<br>
5. Y ? RNAforester(S1,..., Sn) - multiple structure alignment<br>
6. A* ? sequence alignment implied by Y - extract improved sequence alignment<br>
7. C* ? RNAalifold(A*); X* ? sci-score(C*) - fold improved alignment<br>
8. if X* &gt;X, set A ? A* and iterate from step 3, else exit with result A, C and X<br>
The "projection" (Step 3) takes the base pairs from the consensus, removes the gaps with respect to sequence Bi, and yields an unsaturated structure Ci for Bi. The call to <software>RNAfold</software> with option -C Ci (Step 4) may produce additional base pairs aside from those of Ci. These will, in general, be different for each Bi.<br>
<br>
Test data<br>
In the evaluation, we used the most recent version of <software>RNAalifold</software> [29] and <software>ClustalW</software> 2.0.11. The SCI was computed with the formula given in [28]. In computing average sequence similarity, we did not use the original BRAliBase computation, but used the improved calculation suggested by Torarinsson et al. [30]. planACstar was evaluated on data set 1 of the <database>BRAliBaseII</database> benchmarking database [31]. <database>BRAliBaseII</database> was created in 2005 by Gardner, Wilm and Washietl within a benchmark of multiple sequence alignment programs applied to structural RNAs [25].<br>
Dataset 1 of <database>BRAliBaseII</database> contains various RNA sequences of Group II introns, 5S rRNA, tRNA and U5 spliceosomal RNA. The sequences were obtained from the <database>Rfam</database> database [32]. Each RNA family was chopped into 100 subalignments using a procedure described in [26]. Those subalignments contain 5 sequences each and cover a wide range of sequence identities.<br>
Currently, the full IUPAC code is not supported by <software>RNAforester</software>, therefore our method is restricted to concrete RNA sequences as input sequences. This restriction reduces the size of our test data set to 340 out of 388 subalignments.<br>
For comparison, we include the score of the reference alignment. It must be kept in mind that it is scored from the full family alignment. Therefore, the reference alignment does not necessarily represent the optimal subalignment.<br>
<br>
SCI improvements<br>
In Figure 2, we see the average SCI score at variable sequence similarity. The results of Plan A, planACstar and the reference alignment were scored and a filtering with a Savitzky-Golay filter was applied afterwards. The filter uses local polynomial regression to compute a smoothened value for each point.<br>
The results suggest a boundary at 55% sequence similarity. Above 55%, the SCI scores are in agreement, below 55%, the two approaches are outperformed by the reference alignment. This confirms the observation by Gardner, Wilm and Washietl:<br>
The results suggest that 60% sequence identity is a crude threshold, whereby the structural content of predicted sequence alignments diverges from reference structural alignments[25].<br>
(The 5% shift of the boundary results from the use of the improved formula for calculating similarity.) Our working hypothesis was that the overall shape of the prediction might still be correct in this twilight zone. In fact, planACstar shows an improvement in the zone between 30% and 55% similarity, compared to Plan A. While the underlying RNA sequence is not conserved well, the additional information we extract from its structure improves structure conservation in the overall alignment. Compared to Plan A, planACstar reduces the area in the twilight zone to roughly two thirds of its original size.<br>
Figure 3 connects the individual alignments made by planACstar and Plan A. Highlighted in color is the interesting sequence identity range, the twilight zone. Note that almost all significant improvements are within this region.<br>
Looking at the full data set, 90% of the dots are on or very close to the diagonal. As expected, above the 55% boundary, our fine-tuning is not necessary. In the twilight zone, we notice that almost 50% of the SCI scores improved.<br>
We also tested whether the improvement is related to the gap content in the alignment, but no correlation was observed. This test is documented in Figure 4.<br>
<br>
A detailed observation<br>
Below, a typical path through the pipeline of planACstar is shown. As an example, we picked an alignment located in the sequence identity's twilight zone, alignment 83 of the Group II introns. The alignment has a sequence identity of 31% and planACstar improves its SCI score from 0.8807 to 1.005.<br>
The first step is the initial sequence alignment A, conducted by <software>ClustalW</software>. The result is the following:<br>
Using <software>RNAalifold</software>, its preliminary consensus C is calculated. Figure 5 (left) shows the structure of the consensus, scored by <software>RNAalifold</software> with -27.92, and a SCI score of 0.8807. The string representation of C is the following:<br>
The next step in our pipeline is an individual folding of the single sequences relative to the consensus with <software>RNAfold</software> -C. A direct comparison of this procedure to the optimal MFE folding without any consensus is shown in Figure 6. <software>RNAforester</software> calculates the multiple structure alignment Y, from which an improved sequence alignment A* can be easily extracted. Both are shown below:<br>
Note that there is a new gap at the end of our alignment. The effects on the improved consensus structure C* are shown in Figure 5 (right): the number of basepairs increased. The <software>RNAalifold</software> MFEA improves to -31.88, that is a SCI score of 1.005. A SCI score above 1.0 indicates a very well conserved secondary structure.<br>
<br>
Evaluation summary<br>
Structural conservation in RNA alignments in the twilight zone of 30-55% sequence identity can often be improved. We achieved this by combining already available and widely used RNA tools in the pipeline described above. While the average improvement lies at 10%, a percentage range of improvement cannot be stated, as sometimes, the original SCI value is very close to 0. In the leftmost data point in Figure 3, a structure has two parts, of which the original alignment only allows to identify one. <software>RNAfold</software> finds the second part in each individual sequence, and the structure alignment matches these parts. Hence, the SCI value is raised from (almost) 0 to a significant positive value.<br>
Comparing Figure 1 and 2, we notice that, in the twilight zone, planACstar reduced the discrepancy between predictions and reference to about two thirds of its original area. This goes hand in hand with the direct comparison of the SCI scores of planACstar and Plan A shown in Figure 3. Almost all improvements occur within the twilight zone.<br>
These improvements are costly to compute. With our data set and particular set of tools employed, we found that planACstar is slower than Plan A by a factor of 30 (sequence length 80) to 50 (sequence length 150). To be concrete, the runtime on a single subalignment (n = 5, sequence length 150) was increased from 0.297 seconds to 14.896 seconds on a 2 GHz Intel Core 2 Duo processor. This extra efforts results from the individual calls of <software>RNAfold</software> for n sequences and the multiple alignment of the refolded structures afterwards, where the former effort grows with O(n), and the latter grows with O(n2). Since each of the pipeline's components may be replaced with a functional equivalent, resource requirements in another implementation may be different. In particular, we are working on a faster version of the pure structure alignment algorithm for the special case when the aligned structures are known in beforehand to share a common overall shape.<br>
On the side, our study also confirmed the recent improvements made to the <software>RNAalifold</software> program [29]. Our data were originally computed with the previous version of <software>RNAalifold</software>, yielding an average SCI improvement near 20%. With improved gap handling in <software>RNAalifold</software>, our fine-tuning makes a smaller improvement of the SCI score. But note that the new <software>RNAalifold</software> does not change the alignment, it only scores it more cleverly.<br>
<br>
<br>
Conclusions<br>
Our evaluation shows that, with a fairly simple combination of available tools, structure conservation in RNA alignments in the twilight zone can be improved. The degree of achievable improvement varies significantly. In a context of large screening for conserved secondary structures, such as genome-wide RNA gene prediction with <software>RNAz</software> [27], Plan A is chosen for its high speed, and using planACstar in place of Plan A must be considered too expensive. Moreover, many folded alignments fall outside the twilight zone, and no improvement is to be expected from fine-tuning anyway. However, once certain alignments have been determined as promising and to be are used for model building, we suggest that fine-tuning with planACstar should be applied. Our implementation is available on the Bielefeld Bioinformatics Server at http://bibiserv.techfak.uni-bielefeld.de/planACstar/.<br>
While we have built our evaluation on the three tools <software>ClustalW</software>, <software>RNAalifold</software> and <software>RNAforester</software>, note that each of these constituents may be replaced by a functional equivalent to create another instance of planACstar. Recovering the covariance signal obscured by sequence alignment remains the underlying general idea.<br>
While often interpreted in this way, a high SCI does not necessarily imply that the associated structure is close to the truth. It is not known currently to what an extent this interpretation is valid, in particular within the twilight zone. We have compiled a second test data set from curated structures, taking their consensus as a standard of truth. We created 80 data sets of 5 sequences each, randomly selected from the <database>Szymanski 5S Ribosomal RNA database</database> [33], taking care that the corresponding subaligments fall within the twilight zone. In 55 (of 80) cases, planACstar either confirmed the initial prediction (23 cases), or it improved the SCI and also made the alignment more similar to the reference (22 cases). In 28 cases, the SCI was improved but the alignment became less similar to the reference.. In 26 cases, this was due to moderate local rearrangement of base pairs, while the overall structure was retained. In one case, we found that planACstar was fine-tuning to the wrong string, and in the other remaining case, one (correct) helix was strengthened on account of the other (correct) helix, with an overall negative effect. Details of this study are given in Additional File 1, and the complete data set in Additional File 2. There, we also look at the worst case in detail, and show how the pipeline performs when applied to the "true" alignment. As a final observation, we note that there were 7 cases where the alignment improved whereas the SCI did not (and our pipeline hence reports the original SCI and alignment). This indicates that there are a few extra cases where fine-tuning could improve the predicted consensus structure. However - since in practice we have no reference structure available - we have no criterion to take advantage of this fact.<br>
<br>
Authors' contributions<br>
RG and SS designed the study. AB implemented the planACstar pipeline and performed the first evaluation. SS performed the evaluation of the SCI improvement against reference structures. Both AB and SS contributed equally to this work and should be considered as joint first author. All three cooperated closely on the manuscript. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2920274</b><br>
A comparative study of conservation and variation scores<br>
<br>
<br>
Background<br>
A protein's amino acid sequence is considered to carry information about structure and function of the protein. However, it is still difficult to predict residues important for structure and function from a single sequence. One effective way of extracting such information is the comparison of homologous sequences, since proteins sharing a common ancestry often are similar in structure and function. Therefore, the residues critical for function or structure have been conserved in homologous proteins during the course of molecular evolution. In other words, we can predict the residues or alignment sites under strong constraints by comparing amino acid sequences of homologous proteins and identifying conserved alignment sites. Not only conservation, but also variability sometimes provides important information about proteins. As an example, consider a viral peptide antigen, which is a target for the immune system of the hosts. The amino acid residue at a site recognized by the immune system of hosts would change rapidly to escape an attack by the immune system. Therefore, the antigenicity-determining sites can be predicted by evaluating the variability of alignment sites.<br>
A quantitative measure for conservation or variability of alignment sites is useful for identifying sites under constraints, and various methods to quantitatively evaluate the conservation or variability of alignment sites have been developed. The methods are hereafter referred to as scoring methods or simply as scores. Such scores have been reviewed and classified based on calculation method by Valdar [1], although new scores have been developed since then. There is however yet no report which systematically examines the practical similarities or performances of scoring methods. Such information would be useful when several scoring methods are available to analyze a multiple sequence alignment.<br>
We present here an empirical comparison of scoring methods. We have collected programs for scoring methods--some were implemented by ourselves and others were provided by the developers. We apply the methods to a subset of the <database>Catalytic Site Atlas</database> [2], which is a dataset containing alignments as well as information about catalytic sites. We calculate a distance matrix using the correlation coefficients between scoring methods and perform a cluster analysis on the scoring methods. We also evaluate the scores' performance in predicting catalytic sites, which are sites under strong evolutionary constraints.<br>
<br>
Results<br>
One simple way of evaluating the similarity between a pair of scores is to calculate a correlation coefficient between the two scores over alignment sites in a whole dataset. However, if one score is highly affected by the number of sequences in one alignment, this simple correlation does not reflect actual similarity. We therefore first examine the dependency of each score on alignments size. Strictly speaking, we should consider effective alignment size by taking sequence weights into account. We however take a simpler way of using the number of sequences as measure of alignment size since some methods do not use sequence weights and are therefore considered to depend on alignment size directly.<br>
The correlations between alignment size N and mean score is shown in Table 1, and we can see that there is a variety of correlations, ranging from highly negative to highly positive. The majority of scores show a negative correlation, which may be explained in part by a possible higher sequence diversity for higher N. This can however not explain the positive correlations. The positive correlation of Lockless99 is explained by the fact that it calculates binomial probabilities for the number of occurences of amino acids. The probabilities for nk become very small for increasing values of N, giving a higher conservation score (the denominator in the definition of Lockless99 is not affected as much, since the average occurence n generally does not deviate much from the expected occurence qN).<br>
It is not straightforward to exclude the dependencies found. For example, we divided the real-valued evolutionary trace Mihalek04 by N - 1 (since it is a summation of N - 1 terms) which did not abolish the dependency on alignment size (data not shown). The conclusion will have to be that a comparison of scores in different alignments should not be done, and we therefore adopt the procedure of calculating correlations only within alignments as described in Methods for the cluster analysis.<br>
Clustering results<br>
The result of the cluster analysis using average linking is shown in the dendrogram in Figure 1. Each node in Figure 1 shows where scoring methods under it are joined at an average correlation coefficient (acc). In cases where only two scoring methods are joined, we refer to it as simply the correlation coefficient (cc). Each node is labeled with the result from a bootstrap analysis, where the number is to be interpreted as the percentage of bootstrap samples that have an identical node. Generally, the dendrogram was found to be very stable by the bootstrap procedure, as can be seen by the fact that many nodes have been labeled with 100. We define two major groups (A and B) which are illustrated by squares in the dendrogram. They are clustered with bootstrap probabilities of 100% and 87% respectively.<br>
Cluster A<br>
The top node in cluster A is labeled as A0 in Figure 1 and is found at acc = 0.94 with a bootstrap probability of 100%. The internal topology of cluster A is also very stable, as can be seen on the bootstrap probabilities on the nodes internal to this cluster. This cluster holds all the scores described above as "substitution matrix scores" (see Methods), except for Mihalek07.<br>
The highest correlations in cluster A are found between Karlin96, Pei01sp(w) and Sander91sp, which are all clustered at acc ? 0.99. The scoring methods Karlin96 and Sander91sp are very similar already in their formulations, but we can also conclude that Pei01sp(w) is very similar in practice even though the score formulation is quite different. Hence, at least for this dataset, the weighting of Pei01sp does not make much difference. We may also note that even though Sander91sp uses sequence weights for each pair, this does not make any practical difference compared to Karlin96 and Pei01sp that do not use sequence weights at all. All these scores also use the same matrix MK in their calculations, which may explain their similarities. The other scores in cluster A are Valdar01, Thompson97 and Liu08w. Valdar01 is by its definition quite similar to Karlin96 and Sander91sp, using different sequence weighting and a different scoring matrix. Thompson97 measures the deviation from a hypothetical "consensus residue". It measures this deviation using a scoring matrix just as the other scores in cluster A, which is seen to give Thompson97 a very similar behaviour. Liu08w compares each occuring amino acid to the most common amino acid at the site, also using a similar scoring matrix.<br>
<br>
Cluster B<br>
A variety of scores are classified into cluster B, which is formed at node B0 located at acc = 0.90 in the dendrogram in Figure 1. However, all scores in this cluster share the property that they consider individual amino acids rather than amino acid substitution as the scores in cluster A do. Cluster B is divided at node B0 into phylogeny- and non-phylogeny based scores, labeled by B1 and B2 respectively in Figure 1. Subgroup B1 contains all scores that use a phylogenetic tree for their analyses. There is a very close relation (cc = 0.98) between Mihalek04 and Zhang08, and they do indeed share a summation over the same phylogenetic tree. Zhang08 uses von Neumann entropy, which considers the amino acid similarities that the Shannon entropy used by Mihalek04 does not. The summation over the same phylogenetic tree seems to erase most differences between these entropies, however. We may compare this pair to their phylogeny-free counterparts Shannonw and Caffrey04w (see Subgroup B2), which show a slightly larger difference (cc = 0.96).<br>
Mayrose04 is the most distant relative in cluster B, even though its closest relatives are the other phylogeny-based scores. Mayrose04 is a computationally different method, but its score is also conceptually different. It uses mutation probabilities from the JTT matrix [3], and calculates evolutionary rates given these probabilities. This has the effect of estimating a lower evolutionary rate (hence higher conservation score) for the conservation of an amino acid that typically mutates easily (e.g. Ala) than for an amino acid that typically does not mutate easily (e.g. Trp). This is in stark contrast to the relative entropy scores Wang06w and Capra07w, considering the fact that rare amino acids are often also amino acids that do not mutate easily.<br>
Subgroup B2 contains scores that do not consider phylogeny in their scoring. This subgroup is vaguely divided into scores that do or do not consider background distributions, where the scores that do consider a background are located to the right of node B2. The exception to this rule is Pei01var and Pei01varw. By inspecting the correlation matrix, we could however see that Pei01var had a quite high correlation (cc = 0.97) with Lockless99 while correlating with cc = 0.98 to Shannon. Hence, Pei01var(w) is not as unrelated to the relative scores Lockless99, Wang06w and Capra07w as it may seem by the dendrogram. Wu70 correlates with cc = 0.99 to both Shannon and Shannonw, and we may conclude that the formulation of Wu70 is in practice very similar to an entropy. Caffrey04w is also clustered together with these "non-relative" scores, though at some distance. Wang06w and Capra07w are clustered together at cc = 0.99, which is not surprising since they are very similar by their definitions. Wang06w uses relative entropy while Capra07w uses the Jensen-Shannon divergence that has relative entropy in its definition. As mentioned above, Lockless99 is clustered together with these scores which is expected since Lockless99 is a score that also measures deviation from a background.<br>
<br>
Other scores<br>
Outside the clusters described above we find scores that do not relate closely to any other score, which may be concluded from the dendrogram where all joining nodes outside clusters A and B are found at acc ? 0.86. All scores that are based on a manual grouping of amino acids are found among these scores. Williamson95 measures relative entropy on an alphabet of nine amino acid groups, Mirny99 measures entropy on an alphabet of six amino acid groups, and Taylor86 and Zvelebil87 measure conservation of properties. They all have different approaches on how to group the amino acids, and this gives quite different behaviour for each of these scores.<br>
Mihalek07 measures the relative entropy of residue pairs at an alignment site, where the background is given by the substitution matrix MM. This matrix is normalized so that each row and column sum to approximately one, and the element MM (?, ?) may be interpreted as a probability that the given amino acid ? will mutate to amino acid ? (or be conserved if ? = ?). A feature of this score, which may be what makes it different from most other scores, is that rare mutations show a large relative entropy (hence a large conservation score). Mihalek et al. [4] argue that a rare mutation should imply an important meaning of the site.<br>
<br>
Other clustering methods<br>
Dendrograms created by single linking and complete linking are shown in Additional file 1 and 2 respectively. There are many similarities to the average linking shown in Figure 1, though there are also many differences. Nodes close to the leaves are similar using any method, which may be expected since all methods are identical if measuring distance between single units.<br>
With any clustering method, cluster A is formed with the same topology and high statistical significance, except for Valdar01 in complete linking. This is due to the fact that Valdar01 correlates very strongly (cc ? 0.95) to most scores in cluster A, except for Thompson97 to which it correlates with only cc = 0.91. By complete linking, Valdar01 is then put in a different cluster, where it correlates to all other scores with cc ? 0.92. This illustrates a weakness of complete clustering, where the similarities between Valdar01 and other scores in cluster A are not visible in the dendrogram created by this method.<br>
Neither single linking nor complete linking recreates cluster B. With single linking it is instead spread out in what is often referred to as the "chaining effect", meaning that links may be created without regard to the shape of the emerging cluster. However, the subcluster consisting of Pei01var, Pei01varw, Wu70, Shannon and Shanonw is found with the same topology and statistical significance by any clustering method, suggesting that symbol frequency methods except for Lockless99 are highly similar to symbol entropy methods.<br>
Using complete linking, subcluster B1 of Figure 1 is merged with cluster A, together with Mihalek07, Mirny99 and Caffrey04w. These are all scores which consider the stereochemical similarities of amino acid, and this may be the reason why they are placed close to cluster A in this dendrogram.<br>
Both Taylor86 and Zvelebil87 belong to the stereochemical properties group, and are clustered together by any clustering method. Since this subcluster is distantly located from other scoring methods also by single linking we can see that they are not closely related to any other score.<br>
Thus, the results of these two cluster analyses provide supporting evidence for the classification of a cluster and some subclusters created by the average linking method, despite the difference in clustering pattern.<br>
<br>
<br>
Performance evaluation<br>
We evaluate the performance of scores in predicting catalytic sites, the result of which is shown in Figure 2. The figure shows performance measured by AUC for subsets of the original dataset derived by removing alignments containing more than N sequences. As expected, all methods perform worse for alignments containing fewer sequences. The color coding in Figure 2 shows that performance generally follows the classification given by the clustering discussed above, and that scores in cluster B perform best, followed by scores in cluster A. An exception is however Caffrey04w, which performs worse than most scores in cluster A. This is a quite surprising result, which calls for further study.<br>
The scores outside clusters A and B are the least successful among all scores in predicting catalytic sites. As discussed above, four of these scores (Williamson95, Mirny99, Zvelebil87 and Taylor86) define their own alphabets of amino acid groups. We could conclude above that the manually designed amino acid groups gave scores that did not correlate strongly with other scores, and we can conclude from Figure 2 that this also makes the scores perform worse in predicting catalytic sites. We should however point out that some of these scores were not designed with catalytic sites in mind. For example, Mirny99 was designed to detect sites in the core of protein structures and Williamson95 was designed to detect ligand binding sites within the hydrophobic environment of the cell membrane. Mihalek07 was evaluated on protein-protein interaction sites, but is seen to lack some performance on our dataset. We suggest that the argument posed by Mihalek et al. that rare mutations should imply an important meaning of the site, is actually a drawback of their method. The possibility that rare mutations also implies non-conservation cannot be excluded. Mihalek07 is also sensitive to smaller alignments sizes, and is seen in Figure 2 to lose performance steadily as alignment sizes decrease.<br>
Scores considering deviation from a background distribution generally perform better. One example of this is Williamson95 (relative Shannon entropy) which performs better than Mirny99 (Shannon entropy). The advantage of relative scores can also be seen from the scores in cluster B, which contains five scores measuring deviation from a background (Capra07w, Wang06w, Pei01var(w) and Lockless99). These scores occupy five of the top six rankings shown in the legend of Figure 2. Interestingly, these five scores also show greater tolerance for small alignment sizes. This can be compared to Mayrose04 (Rate4site) which performs among the best scores for large alignment sizes, but loses performance for alignments containing less than about 50 sequences.<br>
With one exception, the scores from cluster A are seen to perform similarly across the entire spectrum of alignment sizes. The exception is Valdar01, which performs better for smaller alignments and also outperforms all "nonrelative" scores from cluster B for small enough alignments. There is indeed one property that separates Valdar01 from the other scores in cluster A--the ranking of completely conserved sites. While all other scoring methods in cluster A give equal scores to complete conservation, Valdar01 gives a higher conservation score to amino acids that have a high "self-similarity" value according to the substitution matrix used. While this is not the same as a relative score discussed above, there are similarities in practice. As an example, we may consider the rarest amino acid Tryptophan which also is the amino acid with the highest self-similarity value according to BLOSUM62. It is reasonable that this difference between Valdar01 and other scores in cluster A is amplified for alignments containing fewer sequences, since there may be more completely conserved sites in such alignments.<br>
<br>
<br>
Discussion and Conclusions<br>
It can be difficult to evaluate similarities among computational methods. A formal classification based on method formulation does not always agree with similarity in performance in the actual application. One way of evaluating similarities among any computational methods is the empirical approach taken in this study. This approach was also recently taken to evaluate similarities of feature selection methods in microarray analysis [5]. In this study, we can also see some differences between practical performance and method formulation. The scores defined as stereochemically sensitive entropy scores were not clustered together, but instead seen to behave quite differently. The von Neumann entropy in Caffrey04w performed similarly to the Shannon entropy, while none of Mirny99 and Williamson95 were clustered close to any other score.<br>
One key factor for this empirical approach is the selection of data to which the methods are applied. We used the <database>Catalytic Site Atlas</database> (<database>CSA</database>) data for this study, but we also tried to evaluate the scoring methods on Balibase [6] and Homstrad [7], which are alignment benchmark datasets. On both these datasets we could regenerate cluster A, cluster B and the other scores (data not shown), just as for the <database>CSA</database> data. The internal topology for these clusters were however slightly different on each dataset. We could for example see that sequence weights made a larger contribution to the internal topologies in clusters A and B on the Balibase dataset, suggesting that this dataset contains more alignments where sequence weighting is important.<br>
To illustrate the difference between clusters A and B, we used as a representative case the site in the <database>CSA</database> dataset showing the largest difference in ranking between scoring methods Karlin96 and Shannon. All scoring methods' ranking of this site are shown in Table 2, where the scores have been normalized so that 1.0 implies that the site is judged to be the most conserved site in the alignment. This site has an amino acid profile containing 40% V and approximately 20% each of L, I and M. These are all hydrophobic amino acids which are deemed similar in BLOSUM62, and we can see that the scores from cluster A consistently give higher ranking than any method from cluster B. We can also see that Mirny99 judges this to be a completely conserved site, which is clear from the definition of Mirny99 where V, L, I and M are explicitly considered identical. For a conservation measure like the Shannon entropy, however, this site is far from conserved, since the most prevalent amino acid occupies only 40% of the amino acid profile.<br>
This obvious difference in behaviour between clusters A and B highlights the question of how much attention should be given to "conservative mutations". The site depicted in Table 2 does indeed show a strong conservation of hydrophobicity. We can see that the scores in cluster B do not perform well in discovering this site, and a score that takes amino acid similarities into consideration would be preferred. From our performance evaluation we may conclude that conservative mutations should however not be considered when detecting catalytic sites. Inspection of the dataset gives that 60% of all catalytic sites are completely conservered, and 79% of all catalytic sites have an amino acid profile consisting of at least 90% of one specific amino acid. The interpretation of this is that conservative mutations are not tolerated in catalytic sites, and those scores that do consider amino acid similarities give false positives such as the one depicted in Table 2.<br>
However, the conservation of features such as hydrophobicity may be a fruitful feature for other uses of conservation scores. For example in structural studies, it may be very useful to be able to discover such sites. Indeed, the Mirny99 score was developed for detecting the core of protein structures. We may finally notice that Caffrey04w gives a ranking of the site in Table 2 that is somewhere between clusters A and B. This might be expected since Caffrey04w is an entropy just like Shannon, but uses a substitution matrix like the scores in cluster A. However, as shown in Figure 2, the performance of Caffrey04w is not between clusters A and B, but instead below most scores from cluster A. We examine this score further and suggest an improvement in a separate study [8].<br>
In summary--the biggest effect on the evaluation of an alignment site is the choice of whether amino acid similarities should be considered or not. We have concluded that, regarding catalytic sites, there is no benefit to be gained from considering amino acid similarities since it introduces false positives. The second biggest effect is the inclusion of background information. We could see that scores comparing the site with an expected background distribution perform better in predicting catalytic sites.<br>
<br>
Methods<br>
Scoring methods<br>
We evaluate 25 scoring methods, all of which are shown in Table 3, with explanation of notations in Table 4. As far as possible, we present the scores following the same categorization as Valdar [1]. As can be seen in Table 3, several scores use a probability distribution pk for the amino acids at alignment site k. This distribution is easiest estimated by the observed frequencies of amino acids, but several methods add sequence weighting to solve taxonomic bias in the estimation of pk. The scores marked with "w" as suffix in the score name in Table 3 use sequence weighting by the Henikoff-Henikoff method [9].<br>
Pei et al. [10] suggested scores both with and without sequence weights and we include both alternatives in our analysis. Some other scores do not estimate pk but nevertheless have sequence weighting inherent in their definitions (Sander91sp, Valdar01).<br>
Programs of 24 scoring methods were implemented by ourselves and out of these, 14 methods are available in the <software>SEALA</software> package, while 10 can be downloaded from the author's website. The Rate4Site method can be downloaded from its developers' website [11]. Table 3 shows the availability of methods.<br>
Symbol frequency scores<br>
Wu and Kabat [12] introduced the first widely accepted variation score to evaluate the variability of antigen recognition sites on antibodies. The score is simply the number of different amino acids at the site divided by the frequency of the site's most common amino acid. Lockless and Ranganathan [13] introduced a score that measures how amino acids found at the site deviate from the average distribution in the alignment. This is done by modelling occurences as binomial probabilities and calculating a root-squared sum of log-odds ratios for amino acids. Pei and Grishin [10] introduced a similar score, that takes a more straightforward approach of comparing distributions directly. Their score is a root-squared sum of distribution differences.<br>
<br>
Stereochemical property scores<br>
Some scores consider only the stereochemical properties at alignment sites, disregarding the exact identity of amino acids. Taylor [14] introduced a score in which 61 sets T1, ?, T61 were created, where each set contains amino acids sharing a stereochemical property. For example, the smallest set is T4 = {D, E} containing the negative amino acids Asp and Glu. The sets are not disjoint, and T4 is a subset of larger sets. The score is defined as the size of the smallest set Tj that contains all amino acids at the alignment site. It is an integer between 2 and 20, and a smaller score indicates a higher conservation.<br>
Zvelebil et al. [15] introduced a score that counts stereochemical dissimilarities occuring at a site. If only one amino acid is present at the site the score is set to 1, otherwise it is calculated according to the formula given in Table 3. This is done by considering ten stereochemical binary properties, and ndis is the number of properties that are found to vary at the site.<br>
<br>
Symbol entropy scores<br>
The symbol entropy scores use either the Shannon entropy S [16] or the relative entropy R (also known as Kullback-Leibler divergence), both of whose definitions are shown in Table 4. Sander and Schneider [17], Shenkin et al.[18], and Gerstein and Altman [19] all used a variant of Shannon entropy in their scores. Relative entropy measures the deviation of a probability distribution from a background distribution and was used by Wang and Samudrala [20], where they used the background distribution from the BLOSUM62 matrix. The rationale behind this score is that rare amino acids should get a higher score if they are conserved. A further development on this theme was done by Capra and Singh [21] who used the Jensen-Shannon divergence. This measure is very similar to relative entropy, with the advantage of being symmetric and limited between 0 and 1.<br>
<br>
Stereochemically sensitive entropy scores<br>
Entropy scores have also been adjusted to account for the stereochemical properties of amino acids. Mirny and Schakhnovich [22] did this by replacing the original alphabet of 20 amino acids by a smaller alphabet consisting of six amino acid groups. Williamson [23] developed a similar score, using a slightly different amino acid alphabet and using relative entropy with the distribution in the whole alignment as background. More recently, the von Neumann entropy (shown as V (pk) in Table 4) was introduced by Caffrey et al. [24]. This entropy was originally developed for quantum mechanics, and has been adapted to bioinformatics to account for amino acid similarities.<br>
<br>
Substitution matrix scores<br>
Substitution matrix scores do not consider residues by themselves, but rather consider the mutations that have occured. Sander and Schneider [17] introduced a sum-of-pairs score that sets a weight for each pair, depending on the sequence similarity. They use a scoring matrix where all values in the diagonal are one, which has the effect of giving the same score for any amino acid that is conserved. Karlin and Brocchieri [25] used an almost identical score, that only differed in that the score Karlin96 did not use sequence weighting. Valdar and Thornton [26] also used a similar score, but instead set individual sequence weights using a method from [27]. They normalize the substitution matrix M so that their matrix MV takes values in the range [0; 1] as shown in Table 4. Pei and Grishin [10] introduced a score that uses the same matrix as Sander91 and Karlin96, but sums over the alphabet of amino acids rather than the sequences in the alignment.<br>
A vectorial view was proposed by Thompson et al. [28] in which one vector per sequence is defined for an alignment site k. The vectors are defined in the space of amino acids using a substitution matrix. A mean vector is then calculated for the site and the final score is the average euclidean distance to the mean vector. Mihalek et al. [4] developed a score that resembles relative entropy, considering amino acid pairs as the unit to measure. It counts the number of unordered pairs of amino acids nk(?, ?) to estimate the probability pk(?, ?). It then assumes a background distribution created by a substitution matrix normalized so that each row and column approximately sum to one (it is not possible to do exactly and keep a symmetric matrix). It is a score that rewards mutations (or lack of mutations) that deviate from this background. Liu and Guo [29] introduced a score that focuses on the most common amino acid (?0) in each site, and sums over comparisons of each amino acid with ?0.<br>
<br>
Phylogeny scores<br>
Mihalek et al. [30] developed the real-valued evolutionary trace method, which is a score to quantitatively measure evolutionary trace [31]. They do this by constructing a phylogenetic tree using the UPGMA method, and summing entropies on the groups created by cutting the tree at different nodes.<br>
Zhang et al. [32] later introduced an extension of Mihalek04 in which the Shannon entropy was replaced by the von Neumann entropy. Mayrose et al. [33] developed Rate4Site, a computationally demanding method that estimates evolutionary rate (thus conservation). It does this by constructing a phylogenetic tree and inferring an evolutionary rate for each site using a Bayesian inference scheme, given a statistical model for evolution. This score is the default method at the <software>ConSurf</software> web server [34], and is the successor of the original ConSurf method [35] which we do not include in this study.<br>
<br>
Scores not included<br>
Several scoring methods are not included in this study. For example, cumulative relative entropy [36] and quantitative evolutionary trace [37] are not included. They are designed to detect alignment sites involved in the functional difference between subsets of the aligned proteins. Both methods require information about the classification of aligned sequences into subsets corresponding to the functional difference, and we therefore do not include the methods in this study.<br>
Integer-valued evolutionary trace and zoom methods are associated with real-valued evolutionary trace [30]. Integer-valued evolutionary trace was used to demonstrate the superiority of real-valued evolutionary trace, whereas zoom focuses on a single sequence. Therefore, the two scores are excluded from our analysis.<br>
We do not include 3 D cluster analysis [38], which calculates the score at an alignment site by considering the site itself along with alignment sites of spatially close residues. We do not consider structure in this study, and do not include this score.<br>
Valdar [1] suggested a score that is a multiplication of three scores--Shannon entropy, a score similar to the score by Thompson et al. [28] and a gap penalty. The score is highly dependent on the weight for each factor in this multiplication, so we do not include it in this study. It also includes a gap penalty factor and we do not evaluate score performances on gaps in this study.<br>
<br>
<br>
Score evaluation<br>
We evaluate scores on a subset of the <database>Catalytic Site Atlas</database> (<database>CSA</database>) [2], compiled in [21]. We analyze all alignment sites without gaps or undecided amino acids (B, X, Z) which leaves us with a dataset consisting of 455 alignments with a total of 89446 sites, of which 1149 are catalytic sites. Hence, the scores' evaluations of gaps is not included in this study. Any score could however be extended with a gap penalty to handle this.<br>
The purpose of a conservation or variation score is generally to compare sites within an alignment, and we analyze specifically this property of the scoring methods. In order to do this, we negate scores that give high scores for high variability, so that all scores can be viewed as conservation scores. Further, when comparing sites within an alignment, the scores Sander91, Shenkin91 and Gerstein95 give equal comparisons since they are all variations of a Shannon entropy. We therefore evaluate them together as a score which we name Shannon (which is identical to the Sander91 score). We also evaluate the score Shannonw as suggested by Pei et al. [10] which uses sequence weights for the estimation of pk.<br>
We also show that a comparison of scoring methods other than within alignments is difficult, by evaluating how the scores depend on alignment size (the number of sequences in the alignment). We do this by, for each scoring method, calculating the Pearson correlation between alignment size and mean score on the alignment.<br>
Cluster analysis<br>
We perform hierarchical cluster analysis on the scoring methods, in order to see how scoring methods relate to each other. For this analysis we calculate a distance between every possible pair of scoring methods. We do this by calculating a correlation matrix C(i) for each alignment i, where the element  is the correlation between methods k and l on sites in alignment i. We subsequently calculate the average correlation matrix  for all alignments (m = 455 which is the number of alignments). Hierarchical clustering is then done on the distance matrix . The symbol 1 denotes a unit matrix, i.e. a matrix consisting only of ones. Since we are interested in how the scores compare sites, i.e. the ranking of sites, we use the Spearman's rank correlation which also solves possible problems with outliers and nonlinear correlations.<br>
At each stage of the hierarchical clustering, the two closest clusters (created at earlier stages) or single units are merged into a new cluster. Above we defined the distance between units (the scoring methods) but we also have to define the distance betweeen clusters, i.e. we have to define a clustering method. We choose to mainly present average linking as clustering method, which means that clusters are joined based on the average distance between all inter-cluster pairs. Other possible methods include single linking and complete linking. In single linking the distance between two clusters is equal to the distance between the closest inter-cluster pair, while in complete linking it is equal to the distance between the most distant inter-cluster pair. Both these methods are vulnerable to outliers, since they merge clusters based only on local measures. Therefore, we mainly discuss the clustering of scoring methods based on the average linking method. For comparison, we however also perform hierarchical clustering using single and complete linking. We also perform a bootstrap procedure to determine the reliability of each node in the resulting dendrogram. One bootstrap sample j is obtained by randomly picking m alignments from the original dataset (where one alignment may be picked more than once) and calculating a distance matrix  in the same way as above. We obtain 1000 bootstrap samples in this way, and each node in the original dendrogram is labeled with the number of times (in percent) that an identical node is found in a bootstrap dendrogram (identical in the meaning of having the same set of leaves under it).<br>
<br>
Performance evaluation<br>
We evaluate the performances of scoring methods in predicting catalytic sites in the alignments. We do this by calculating the receiver operator characteristic (ROC) curve for each score and alignment. We subsequently calculate the area under each ROC curve (AUC), and calculate an average AUC score on all alignments for each score. We also evaluate how performance depends on the size of alignment, by setting upper bounds on alignment size and evaluating AUC scores only for alignments with a size smaller than that.<br>
<br>
<br>
<br>
Authors' contributions<br>
HT conceived the study. FJ implemented methods and performed the analysis. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2949893</b><br>
<software>Sigma-2</software>: Multiple sequence alignment of non-coding DNA via an evolutionary model<br>
<br>
<br>
Background<br>
Evolutionary models seek to describe the process by which DNA changes over time, while sequence alignment is the computational task, given two or more sequences of DNA, of determining which stretches of nucleotides may have arisen from a common ancestor. It seems logical to combine these goals, and we present an attempt to do so here. We specifically address non-coding DNA of unknown function, but it is straightforward to include functional models of DNA (such as selection for protein-binding), and we hope in the future also to extend this approach to protein-coding DNA and amino-acid sequences.<br>
The motivation for using an evolutionary model in multiple sequence alignment is this: rather than simply optimise the "similarity" of two sequences by some "metric", we want to assess which similarities are unlikely to have occurred by chance. In other words, given two sequences, or two sets of sequences, we want to know whether or not to align them, by estimating the likelihood of observing such sequences under two hypotheses: that they are related (under our evolutionary model, with an unknown common ancestor); or that they are not related.<br>
This is less of an issue in aligning protein-coding genes, which tend to be rather well conserved. Non-coding DNA, however, can contain strongly conserved regions (such as regulatory regions) interspersed among weakly-conserved regions. In an extreme case, we recently found [1] that the centromeric regions in two closely-related yeast species have no detectable homology, even though the neighbouring ORFs are well-conserved and syntenous, and most intergenic regions are well-conserved too. It is important that a sequence alignment program, when used on non-coding DNA, be able to distinguish genuine ancestral relatedness from chance similarity.<br>
We start with a quick review of several evolutionary models, but it is important to note the difference in motivation: while most previous researchers have been interested in estimating evolutionary distances and constructing phylogenetic trees based on observed substitution patterns, we are interested in using known or estimated evolutionary history to evaluate differing hypotheses relating to the evolution or function of individual subsequences. Specifically, we have recently, in the motif-finder <software>PhyloGibbs</software>, [2,3] used an evolutionary model, as described in Methods, to evaluate the competing hypotheses that short stretches of sequence may be "regulatory" or "background". Here we use a similar evolutionary model to perform multiple sequence alignment by evaluating the hypotheses that two (sets of) subsequences from two longer (sets of) sequences may, or may not, be homologous. The word "homology" is used, throughout, in the sense of "evolutionary relatedness" [4], and not merely "similarity". A principal goal of the alignment program described here is that alignments reported by it should indicate homology, to a high degree of confidence.<br>
Most evolutionary models trace their lineage to the work of Jukes and Cantor [5]. Their model assumes neutral evolution, independent evolution of nucleotides and a uniform mutation rate from any nucleotide to any other. Improvements to that model have largely consisted of using realistic mutation matrices that take account of differing mutation rates between different nucleotides: in particular, the fact that transitions (purine-purine or pyrimidine-pyrimidine) are much more common than transversions (purine-pyrimidine or vice versa). Kimura [6,7] accounted for differences in transition and two types of transversion rates. Further work along these lines has been done by Tamura [8], Tamura and Nei [9] and others. The most general reversible model was described by Tavar? [10], and the general 12-parameter model was discussed by Rodr?guez et al. [11]. Meanwhile, Felsenstein [12] introduced a model, that we discuss further below, where mutation rates represent equilibrium frequencies for nucleotides. Hasegawa et al. [13] amended this method to take account of differing frequencies of transition and transversion. Heterogeneity of sequence and differing rates of fixation at different loci have been considered by various authors, starting from Uzzell and Corbin [14]. We do not consider this problem in detail here, but our model can be modified to include prior knowledge of sequence function and heterogeneity of sequence composition.<br>
One shortcoming of such models is that they do not explain some significant observed features of DNA, the most basic of which is the fact that nucleotides are correlated, not independent. If one considers abundances of neighbouring nucleotides (dinucleotides), they differ significantly from what would be expected from their individual frequencies: for example, AA is usually over-represented while CG is underrepresented (in vertebrates, CG is severely underrepresented because methylation of the C makes it likely to mutate to a T [15]). Attempts have been made to address this by various authors. Arndt and Hwa [16] use dinucleotide substitution matrices instead of single-nucleotide matrices. While sufficient to account for the most important effects, this assumes that the mutation of certain dinucleotides is preferred. Sometimes this is true (for example, the CG dinucleotide in vertebrates), but in other cases selection forces (some of which are discussed below) could well be operating. Also, such an approach still does not account for longer-ranged correlations in DNA, which exist to significant distances in non-coding DNA, as first noted by Peng et al. [17]. Baele et al. [18] observe complex substitution behaviour, and argue that incorporating context-dependent substitution effects is worthwhile.<br>
We argue that, even in the absence of known function, mutating intergenic sequence can have a cost in fitness, and selection and fixation could be operating on large parts of the genome--perhaps the majority. In a recent study of centromeric DNA in two Candida species [1], we calculated a substitution rate of 27% between those species from synonymous codon substitutions; correcting this with known codon biases gave a substitution rate of 42%, which was our best estimate at a neutral rate. However, the substitution rate in conserved intergenic sequence is much lower than either of these estimates (about 17%). Meanwhile, the centromeres appear to have diverged much faster than our best neutral rate would suggest--implying either that the centromeres evolve neutrally while the rest of the genome is under significant selection pressure, or that centromeres evolve at a "faster than neutral" rate, or both. It is possible that structural and stability requirements, the necessity to bind nucleosomes [19], and other biophysical considerations constrain the evolution of DNA.<br>
We recently used an evolutionary model, in the context of the motif finder <software>PhyloGibbs</software> [2,3], that represents the polar opposite of neutral evolution: it assumes that fixation of nucleotides after mutation is perfect--that is, the distribution of mutated nucleotides matches the distribution found in sites elsewhere of similar function (which may be very different from a genomic average distribution). A similar approach was used in the cis-regulatory module predictor <software>Stubb</software> [20]. This is in fact the model of Felsenstein [12], with a slightly different interpretation and a very different motivation. The model is reviewed in Methods, "Evolutionary Model". However, while it is important to consider fixation (especially in the motif-finding context), the assumption of perfect fixation may be extreme and unrealistic. We address that issue here, thereby connecting with other models from the literature: we have a model that resembles the "general reversible model", with the inclusion of fixation but not "perfect fixation".<br>
We then use this evolutionary model to address the problem of sequence alignment: specifically, we use this model to calculate the log-likelihood ratio of sequences being related, to being unrelated. We modify our previously published multiple sequence alignment program <software>Sigma</software> [21] to use this as a scoring scheme. The key goal of <software>Sigma</software> is to minimise spurious alignments (that is, alignments of sequence that are not likely to be homologous), a significant issue in non-coding DNA, where highly conserved segments can be interspersed with long insertions and deletions. This was achieved by calculating the p-value for the score of each locally aligned region, that is, calculating the probability of observing such a score under the "null hypothesis" that the sequences are not ancestrally related: only matches with sufficiently low p-values are considered for alignment. While one other program that we are aware of, <software>Dialign 2</software> [22], also used a p-value as a criterion, our calculation of the p-value is different in details, as described in Methods. We tested several programs in the earlier paper [21] and showed that they produce spurious alignments even for randomly-generated DNA, and show significant error rates in aligning synthetic sequence; while <software>Sigma</software> (version 1) was much less sensitive (that is, it aligned a smaller fraction of nucleotides compared to other programs), we showed that the motif finder <software>PhyloGibbs</software> [2] exhibited better performance when its input data was aligned with <software>Sigma-1</software>, suggesting that its alignments were biologically more realistic.<br>
<software>Sigma-2</software>, the modification of <software>Sigma-1</software> that features the evolutionary model described here, proves to be substantially more sensitive than <software>Sigma-1</software> on synthetic data (its sensitivity is now comparable to other programs), while maintaining a very low error rate and refusing to align sequence that is not related. We demonstrate this on both synthetic and genomic (yeast) DNA. The results indicate the benefits of including selection and fixation in an evolutionary model, of basing the problem of multiple sequence alignment on such a model, and of comparing results with the "null model" of unrelatedness, and insisting on stringent p-values to report alignments.<br>
Ours is not the first attempt to include evolutionary considerations in sequence alignment, but it differs in details. Thorne et al. [23,24] have previously considered including an evolutionary model in pairwise sequence alignment. Their main focus was the treatment of insertions and deletions. Steel and Hein [25] extended that approach to sequences on a tree. The focus of our work is different: we focus on gapless local alignments, assuming that non-coding DNA will contain large insertions and deletions which will be accounted for by assembling the gapless alignments; and rather than consider the overall "maximum likelihood" alignment, we insist on a stringent p-value for the log-likelihood-ratio that we calculate for each local alignment. Below we benchmark our program against ten other widely-used multiple sequence alignment programs.<br>
<br>
Results and Discussion<br>
We performed three sets of benchmarks, on synthetic and real (yeast) data, comparing <software>Sigma-2</software> with eleven other programs: the previous version of <software>Sigma</software> (version 1.1.3), <software>DiAlign-TX</software> version 1.0.2 [26], <software>T-Coffee</software> version 8.06 [27], <software>ClustalW</software> version 2.0.11 [28,29], <software>KAlign</software> version 2.04 [30], <software>MLagan</software> version 2.0 [31], <software>Muscle</software> version 3.7 [32], <software>PCMA</software> version 2.0 [33], <software>FSA</software> version 1.15.3 [34], <software>Pecan</software> version 0.8 [35], <software>MAVID</software> version 2.0 build 4 [36].<br>
Benchmark on yeast data: discriminativeness<br>
While synthetic benchmarks are better quantifiable, real DNA exhibits complexities difficult to capture in synthetic data. Here we describe the performance of <software>Sigma-2</software> and other programs on yeast data. "Reference alignments" being unavailable, we measure performance indirectly: we compare the alignments produced by various programs for orthologous DNA, with alignments by the same programs for non-orthologous DNA.<br>
We used 947 genes for Saccharomyces cerevisiae for which there existed a kilobase of upstream intergenic (non-coding) sequence, and for which the orthologous genes in four other species (S. paradoxus, S. mikatae, S. bayanus and S. kudriavzveii) also had a kilobase of upstream non-coding sequence. Thus, the benchmark consisted of aligning 947 files, each containing 1000 bp of orthologous non-coding sequence. We also generated 947 "shuffled" files, that contained the same upstream sequences from the same five species in each file, but entirely non-orthologous: that is, each sequence in the original set was present in exactly one shuffled file, but no two sequences in a given shuffled file were orthologous. This was accomplished by ordering the genes and the species, and selecting upstream sequence from the n + 100k(mod2)'th gene for the k'th species (k = 0, 1, 2, 3, 4).<br>
While we cannot quantify the accuracy of alignment on the orthologous sequences, we can say with some confidence that very little sequence from the "shuffled" set is likely to be genuinely homologous; so a program whose alignments indicate homology rather than mere "similarity" should not report significant similarity in the second set of sequences. At a minimum, there should be significant gap in results on the two sets.<br>
Table 1 reports the average number of aligned nucleotides per input nucleotide for each program and each data set. That is, it shows the total number of matches per nucleotide summed over all nucleotides, divided by the total number of nucleotides. Since there are five sequences of equal length in each set, the theoretical maximum for this number is 4. <software>Sigma-2</software> detects a significantly greater degree of similarity in the "orthologous" files, and a lesser degree of similarity in the "shuffled files", than its predecessor, <software>Sigma</software>-1.1.3. Both versions report a little under two matches for each nucleotide in the orthologous files, and very few matches per nucleotide in the shuffled files. Of the remaining programs, only <software>DiAlign-TX</software>, <software>FSA</software> and <software>Pecan</software> report a significant gap in results in the two data sets. Some programs, in fact, produce significantly more alignment in the shuffled set than in the genuinely orthologous set (approaching, in fact, the theoretical maximum of 4): an odd result that throws doubt on the utility of those programs in alignment of non-coding DNA sequence.<br>
All programs were run with their default command lines, except as follows: for <software>Sigma-2</software>, a file providing background dinucleotide frequencies, and another file providing transition rates, both files derived from yeast, were supplied. For <software>Sigma</software>-1.1.3, only the background file was supplied. <software>DiAlign-TX</software> was run with the parameter -12, the most stringent (and least sensitive) mode. <software>FSA</software> was run with the parameter --gapfactor 5, which increases its specificity. <software>Mavid</software> was run using the bundled perl script to automatically generate the phylogenetic tree. <software>Pecan</software> was fed the phylogenetic tree (((S. cer, (S. par, S. mik), S. kud), S. bay)).<br>
Table 1 reports the most stringent options that we used for each program. In Table 2, we compare the effect of parameter changes in <software>Sigma-2</software>, <software>Dialign-TX</software> and <software>FSA</software>. In <software>Sigma-2</software>, we removed one or both of the background model option and the transition matrix option, resulting in assumptions of uniform nucleotide frequencies and/or uniform transition probabilities. It appears that assuming uniform background frequencies increases the number of erroneous alignments (in shuffled sequence) by a factor of more than 4, but slightly increases the number of alignments in orthologous sequence. Assuming uniform transition rates (with a realistic background model) hurts performance in both data sets. Making both the background and the transitions uniform causes a nearly tenfold increase in the alignments for shuffled sequence. If the threshold p-value for local alignments is increased from the default 0.002 to 0.2, and the background model and transition matrix are made uniform, then the alignment rate in orthologous sequence exceeds 2.5, while the alignment rate in non-orthologous sequence is about 0.06, still substantially less than all other programs.<br>
Of other programs, <software>FSA</software> and <software>Dialign-TX</software> still show substantial gaps between orthologous and shuffled sequence sets when run with their default settings; however, at their most stringent settings, both align much more shuffled sequence than <software>Sigma-2</software> does at the least stringent setting tested above. <software>Mavid</software> was run with a tree corresponding to the yeast alignments, but the results did not greatly differ from the automatically-generated tree.<br>
If the most basic task of a sequence alignment program is to distinguish homologous and non-homologous sequence, it seems that all but a few programs fail badly at that task, and <software>Sigma-2</software> is by far the most stringent in rejecting non-homologous sequence.<br>
Finally, one can ask: even in the alignment of orthologous sequence, to what extent do various programs agree with one another? We consider four programs that perform the most discriminative alignments in Table 1, namely <software>Sigma 2</software>, <software>FSA</software>, <software>DiAlign-TX</software> and <software>Pecan</software>. In the orthologous set, 4714791 pairs of nucleotides in total were identified by <software>Sigma-2</software> as orthologous. Of these, 3889882 were identified by <software>FSA</software>, 3995809 by <software>DiAlign-TX</software> and 4022407 by <software>Pecan</software>. In other words, nearly 20% of the nucleotide pairs aligned by <software>Sigma</software> were not aligned by the other programs. We then ask, what about the alignments made by other programs and not by <software>Sigma-2</software>? 2073056 pairs of nucleotides are aligned by <software>FSA</software> and not by <software>Sigma</software>. Of these, 1829902 are also aligned by <software>Pecan</software>, but only 1465189 by <software>DiAlign-TX</software>. Meanwhile, <software>Dialign-TX</software> aligns many nucleotides that are omitted by <software>Sigma-2</software> and <software>FSA</software>, and Pecan aligns many nucleotides that are omitted by all three programs. This level of disagreement, in a task of aligning five closely-related yeast species, indicates the difficulty of underlining non-coding DNA and the desirability of a conservative approach.<br>
<br>
Motif-finding benchmark on yeast data<br>
To test our motif-finder <software>PhyloGibbs</software> [2] and <software>PhyloGibbs-MP</software> [3], we benchmarked its ability to identify transcription factor binding sites in yeast from the <database>SCPD</database> database [37]. Conversely, in the previous paper on <software>Sigma</software> [21], we measured the performance of <software>PhyloGibbs</software> 1.0 in detecting binding sites using sequence alignments generated from various programs. We repeat that benchmark here, using <software>PhyloGibbs-MP</software>. The reason to use <database>SCPD</database> is that it is a large database of experimentally validated binding sites. So measuring the performance of a motif finder in detecting these sites is an objective measure of its performance in the real world. While this benchmark does not directly measure the quality of the alignment, it is hoped that a more "correct" alignment will improve the performance of a motif-finder. We use a recently retrieved version of the <database>SCPD</database> database, after filtering out sites smaller than 3 bp. We were left with 512 sites upstream of 205 genes. Up to 1000 bp (or upto the next coding region, whichever was smaller) was extracted for each gene in S. cerevisiae and its orthologues from S. paradoxus, S. bayanus, S. mikatae and S. kudriavzveii. These were aligned using each of the alignment programs studied here, <software>PhyloGibbs-MP</software> was run on the aligned files individually (with a motif width of 10 bp, a predicted "site density" of 0.01 and "number of motifs" 3 for each file), and its site predictions compared with the annotated sites. Since the <database>SCPD</database> sites vary greatly in length (and, in addition, come from a variety of experimental methods), while our assumed motif width was 10 bp, an overlap of a single basepair was counted as a "hit".<br>
The results are plotted in Figure 1, which shows the "precision" of <software>PhyloGibbs-MP</software>'s predictions (the fraction of predictions that agree with SCPD) as a function of "sensitivity" (the fraction of <database>SCPD</database> sites that were found by <software>PhyloGibbs-MP</software>). The sensitivity is varied by changing the "cutoff" for the significance score reported by <software>PhyloGibbs-MP</software>. While not too many conclusions should be drawn from this limited benchmark. both versions of <software>Sigma</software> perform well over most of the sensitivity range, as does <software>DiAlign-TX</software>. Other good performers are <software>Kalign</software> and <software>T-coffee</software>. With several alignment programs, however, the motif-finding performance of <software>PhyloGibbs-MP</software> is surprisingly poor. Meanwhile, <software>Sigma-1</software> mostly seems to fare better than <software>Sigma-2</software>: our hypothesis is that, though it is less sensitive in alignments than <software>Sigma-2</software>, it performs well in aligning functional binding sites (since these are probably better conserved) and this, in turn, helps bias <software>PhyloGibbs-MP</software> towards those sites (since the scoring in <software>PhyloGibbs-MP</software> rewards conserved sites). Perhaps this argument also helps explain the better performance of <software>Sigma-2</software> compared to most other programs; but it does not explain the poor performance of <software>FSA</software> and <software>Pecan</software>. We cannot directly conclude from this benchmark that <software>Sigma</software>'s alignments are more "correct" than others, but we can view it as supporting the use of <software>Sigma</software> in real-world applications where the correctness of the alignment is important.<br>
<br>
Benchmark on synthetic data<br>
We generated sets of synthetic DNA that conformed to the evolutionary model described above, where each set was evolved from a 500 bp ancestral sequence and contained five descendants, each descendant sequence had a proximity q to the ancestor, and substitutions from the ancestor were made according to equation 9, with dinucleotide frequencies and an inverse substitution matrix estimated from yeast data. Values of q from 0.10 to 0.80, in increments of 0.05, were considered. In addition, insertions and deletions of short stretches of sequence (from 1 to 200 bp) were made with a small probability (0.02): in other words, around 10 insertions or deletions were expected per sequence. Each insertion and deletion applied only to a single descendant sequence (since each sequence was assumed to be independently evolved from the common ancestor). For each value of q, 100 independent sets of 5 sequences each were generated. This method of generating sequences also gave us the theoretical "correct" reference alignment for each set of sequences. Alignments were assessed on sensitivity to the reference alignments (that is, the fraction of aligned nucleotide pairs that were aligned in the program's output), but also on the error rate (the ratio of incorrectly aligned nucleotide pairs to the total number of aligned nucleotide pairs in the reference alignment) and the precision (the fraction of nucleotide pairs reported aligned that are aligned in the reference alignment). That is, if there are Nref aligned nucleotide pairs in the reference alignment, Ncorrect aligned pairs in the reported alignment that are also aligned in the reference alignment, and Nincorrect aligned pairs in the reported alignment that are not aligned in the reference alignment, we define<br>
(1)Sensitivity=NcorrectNref<br>
(2)Error?rate=NincorrectNref<br>
(3)Precision=NcorrectNcorrect+Nincorrect<br>
Figure 2 shows the sensitivity, Figure 3 shows the error rate, and Figure 4 shows the precision. Like its predecessor <software>Sigma</software>-1.1.3, <software>Sigma-2</software> shows a very low error rate, but is much more sensitive, and comparable with the better performers in this aspect. The error rates in Figure 3 show a striking separation of <software>Sigma</software> (both versions), <software>Dialign-TX</software>, <software>FSA</software> and <software>Pecan</software> from the other programs. The precision data in Figure 4 show <software>Sigma-2</software> outperforming all programs by far for weakly-conserved sequence (low q), and FSA somewhat outperforming it for intermediate conservation rates. For highly conserved sequence (q &gt;0.5), <software>Sigma</software> (both versions), <software>FSA</software>, <software>DiAlign-TX</software> and <software>Pecan</software> show precisions close to 1; <software>Muscle</software>, <software>MLagan</software> and <software>Mavid</software> do just a little worse; and there is a substantial gap to the other programs.<br>
<br>
<br>
Conclusions<br>
Benchmarking on synthetic data is of limited benefit in analysing real-world performance, but it is quantifiable. Kim and Sinha [38] recently did an exhaustive benchmark of six programs, and claim that their method generates data that "truly represent the variability observed in genomic data in terms of the difficulty of the alignment task". They observe degradation in performance with insertions, which is probably attributable to our observation that most programs spuriously align non-homologous sequence. They also observe that <software>Pecan</software> is not susceptible to this problem and that its performance was superior to all other programs, in agreement with what we see in yeast data (they did not benchmark <software>FSA</software> or <software>Sigma-1</software>), but in contrast with our observation in these synthetic data benchmarks. This supports their claim that their generated data are biologically realistic.<br>
However, our "homology discrimination" benchmarks on yeast data are, we believe, of greater interest because of their simplicity and the somewhat unexpected results. Arguably the goal of sequence alignment should be to detect homology and not similarity, since the former is a well-defined biological concept meaning "having a common ancestry" [4] and the latter is not always unambiguous or even meaningful. We argue further that a sequence alignment program should err on the side of caution, that is, though it may fail in some cases to detect genuine homology, it should not incorrectly claim homology where none exists. Other than <software>Sigma-2</software>, all programs tested here fail, in differing degrees, on this criterion. The most effective at rejecting spurious alignments is <software>FSA</software> with a stringent gap factor. The only other programs that strongly distinguish the homologous sequences from the shuffled sequences are <software>Pecan</software> and <software>Dialign-TX</software>, but they still spuriously predict a homologous nucleotide for half, or more, of the nucleotides in the shuffled set. This performance, meanwhile, is far superior to all the other programs tested, which predict over two homologues per nucleotide in the shuffled set, and in some cases predict more homology in the shuffled set than in the genuinely homologous set. We feel therefore that these programs should not be used to align non-coding DNA (which was, in any case, not their primary purpose). This is particularly important since it is increasingly important to align, not just non-coding DNA, but whole genomes, and some of the programs described here have been used for that task; and the error rates seen here on the shuffled yeast data are a matter of concern.<br>
With the default settings of <software>Sigma-2</software> (which cause it to predict only 0.003 homologues per nucleotide in shuffled sequence), it predicts just under 2 homologues per nucleotide in orthologous sequence. With the loosest settings that we tested--a p-value of 0.2 for alignment, uniform background model, uniform transition matrix--<software>Sigma-2</software> predicts close to 0.06 homologues per nucleotide in shuffled sequence (an error rate nearly 20 times larger) and over 2.5 homologues per nucleotide in non-shuffled sequence. <software>FSA</software>, run with a gapfactor of 5, performs worse on both counts: it predicts fewer homologues in the orthologous set and more homologues in the shuffled set. Other programs predict more homologues in both sets. Based on the predictions by <software>Sigma-2</software>, <software>FSA</software>, <software>Pecan</software> and <software>Dialign-TX</software>, we estimate that the true conservation rate between these species is probably around 2.5 homologues per nucleotide, and the significantly higher predictions of the other programs are unreliable. This is probably because of the abundance of insertions and deletions in intergenic sequence.<br>
<software>Sigma-1</software> was originally designed to reject such spurious alignments, and benchmarks on synthetic and real data showed that it performed well on this criterion, but was also less sensitive than other programs in detecting genuine homology (at least on synthetic data where this can be quantified). Here we have shown that the incorporation of an evolutionary model into <software>Sigma</software>'s scoring scheme improves its sensitivity to the point where, on synthetic data, <software>Sigma-2</software> is competitive with all other programs; while its precision is much higher, and error rate much lower, than all other programs that we tested.<br>
Meanwhile, the motif-finding benchmark shows <software>Sigma</software> to be one of the best performers in a real-world application.<br>
<br>
Methods<br>
Evolutionary model<br>
<software>Stubb</software> [20] and <software>PhyloGibbs</software> [2,3] use a model of evolution that differs in motivation from the Jukes-Cantor model and its descendants (including Felsenstein's model [12], which it resembles). Where Jukes-Cantor ask, "Given an observed rate of substitutions between two species, what is the evolutionary distance between them?", <software>Stubb</software> and <software>PhyloGibbs</software> ask "Given an evolutionary history that describes two or more organisms, and given a functional model that describes homologous loci in those organisms, what is the likelihood of the sequence observed at those loci?" The goal here was to distinguish between competing functional models (specifically, binding sites for transcription factors, statistically represented by "position weight matrices" [39,40]; and "non-functional", represented by a "background model".)<br>
Calling the functional model M, let the probability of observing a nucleotide ? at a particular locus be M?. Here, the vector of values M? could be a column of a position weight matrix, or the background probabilities of the four nucleotides, or something else. The assumption in the <software>Stubb</software>/<software>PhyloGibbs</software> model is that fixation operates sufficiently strongly that, if a site is mutated, it is also selected for, so that its distribution after mutation is again given by M. Suppose the nucleotide has descended from an ancestral nucleotide ?, and the conservation rate or "proximity" (the probability of the nucleotide not having mutated) is q. The proximity is related to the mutation rate: if there are ? mutations in unit time, and the evolutionary time between the species is t, then q = exp(-?t). In is model, the "transition probability" (the probability of observing ? given an ancestor ?, the proximity q, and the model M is<br>
(4)T(?|?;q,M)=q???+(1?q)M?.<br>
In other words, with probability q the nucleotide is unmutated from the ancestor; and with probability 1 - q it is mutated (possibly multiple times), but also fixated, so that its distribution is given by M. If ? is the mutation rate, and the evolutionary time since the ancestor is t, then q = e-?t. This equation is the same as equation 7 in Felsenstein [12], with M? being his vector of equilibrium probabilities. The chief difference is that where Felsenstein had no functional model and his equilibrium probabilities were site-independent "background" probabilities, <software>PhyloGibbs</software> detects binding sites for transcription factors (TFs) as described by "position weight matrices" (PWMs), so M? is a single column of a PWM Wn?, where n is the position within the putative binding sequence; thus the "equilibrium probabilities" are not site-independent, but--if a site is a TF binding site--are assumed to be precisely equal to the PWM that describes the binding of that TF. <software>PhyloGibbs</software> considered two possible functional models: binding sites for TFs, or background. Here we leave the model unspecified, but retain the assumption that the model describes the equilibrium probabilities.<br>
This transition matrix has some desirable properties. It has reasonable limits as q ? 0 (zero conservation, where it reduces to M) and as q ? 1 (perfect conservation from the ancestor), and the correct composition with intermediate ancestors:<br>
(5)??=A,C,G,TT(?|?;q1,M)T(?|?;q2,M)=T(?|?;q1q2,M).<br>
With <software>PhyloGibbs</software>, the model worked well, in the sense that the motif-finder based on it proved effective at finding regulatory sites in conserved sequence. However, it has some shortcomings that we address here.<br>
First, the assumption of "perfect fixation" seems extreme in general, because restoring the original nucleotide requires at least two mutations at the same site--a doubly-rare event. (Felsenstein [12] is aware of this, but appears to define a "mutation" as a substitution of a nucleotide with any nucleotide, including possibly itself; he calls it a "useful compromise between realism and tractability.") Second, not all mutations are equally likely: transitions are much more common than transversions, and different transitions (and different transversions) occur at different rates, too.<br>
The second shortcoming is easily addressed, and in doing so we move back in the direction of "standard" evolutionary models. Suppose that, if a mutation occurs, the probability of nucleotide ? mutating to nucleotide ? is given by the matrix P??. (The diagonal elements of this matrix are zero, since a nucleotide does not mutate into itself; and its columns sum to 1.) The probability of ? changing to ? after k mutations is given by the k'th power of this matrix. Given a mutation rate ?, the probability of k mutations in time t is given by the Poisson distribution<br>
(6)P(k?mutations)=(?t)ke??tk!<br>
and the transition probability, summed over all possible numbers of mutations, is<br>
(7)T(?|?)=e??t???+?k=1?[(?t)ke??tk!???1,?,?k?2P??1P?1?2?P?k?2?]=(e?t(P?I))??=(qI?P)??<br>
where we have used the earlier definition of the proximity: q = exp(-?t).<br>
This equation is widely used with a different derivation and a slightly different notation: usually ?t(P - I) is defined as a single matrix that appears in a rate equation (eg, equation 2.7 in [10], or equation 3 in [11]).<br>
To this framework we would like to add model-based selection and fixation. We make the assumption that selection operates faster than mutation: that is, while mutations are rare events at any given locus, the spread or disappearance of mutations at individual loci through a population occurs relatively rapidly. In that case, one can effectively replace P with an effective mutation probability matrix P' which includes the effect of fitness selection on mutations. So the probability of observing the nucleotide ? at a particular locus, given that it has recently mutated from an ancestor ?, depends (as above) on ? and the mutation matrix P; but also by its function (described by a functional model M), for which it is being selected. Neighbour-dependence effects can in principle be absorbed into M: that is, the fitness of a nucleotide at a given position may depend on its neighbours. In this way, we incorporate correlated "background models" into our formalism. The neighbour-dependence of the background model is the only form of position specificity that we consider here, but in principle we can consider entirely different locus-specific functional models M, that describe transcription factor binding, nucleosome occupancy, or other features. We hope to extend Sigma in this manner in the future.<br>
Given M, ? and P, what is the probability of observing ?? We use a Bayesian answer:<br>
(8)P(?|?;M)=P(?|?)P(?|M)???P(?|??)P(??|M)<br>
Here P(?|?) is an "inverse mutation matrix", the probability that the ancestor was ? given that the descendant is ?. Defining P???=P(?|?;M), we have for our evolutionary model<br>
(9)T(?|?;q,M)=(qI?P?)??.<br>
The transition matrix allows us to evaluate the likelihood of a set of observed aligned nucleotides, given the phylogenetic tree that connects them and a functional model M. The likelihood is the product of transition probabilities over all branches of the tree, summed over the allowed nucleotides at the ancestor and at all intermediate nodes. For example, for a set of N descendants ?i that all diverged from a single common ancestor ? with proximities qi (i = 1 ...N), the likelihood is<br>
(10)P({?i}|?;qi,M)=??=A,C,G,Tp??i=1NT(?i|?;qi,M)<br>
and for the tree shown in Figure 5, the likelihood is<br>
(11)P(A1T2C3C4A5|T)=?aPaT(A1|a,q1)??bT(b|a,qb)T(T2|b,q2)??cT(c|b,qc)T(A5|c,q5)??dT(d|c,qd)T(C3|d,q3)T(C4|d,q4).<br>
(Superscripts to nucleotides here indicate the leaves where they occur.)<br>
We next describe how to apply these ideas to multiple sequence alignment.<br>
<br>
The sequence alignment algorithm<br>
<software>Sigma</software> builds a global alignment progressively out of gapless local alignments. (<software>Dialign-2</software> [22] previously used a similar strategy, but assembled the global alignment after evaluating all possible local alignments, rather than build it up progressively.) Local alignments are sorted according to their p-value, that is, the probability that an alignment with a similar or better similarity score would be found under the "null hypothesis" that the sequences are unrelated; and are made in increasing order of p-value. Alignments whose p-value is above a certain threshold, chosen by default to prevent alignment of random sequence, are rejected.<br>
The alignment strategy of <software>Sigma</software> is described in detail in the earlier paper [21]. Briefly, it works as follows: the basic data structure is a "sequence fragment". At any point in time, the alignment is given by a collection of "sequence fragments", corresponding to gapless local alignments of the input sequences. The input sequences are numbered, and initially every sequence is in its own fragment; as the multiple alignment progresses, each sequence fragment may contain multiple sequences (each numbered with the input sequence from which it originated), representing local gapless alignments. At each step, local alignments are made with all existing pairs of sequence fragments provided that the pairs are disjoint in the sequence numbers that they contain, and that aligning them would be consistent with previous alignments (synteny is preserved in alignments, and consistency is maintained via a labelling scheme). This is portrayed in Figure 6.<br>
The local alignments are sorted by their p-values, and made in order of increasing p-value. Alignments whose p-value are greater than a given threshold are rejected. Each local alignment consists of "fusing" gapless stretches of two existing fragments into a single fragment containing the union of their nucleotides. The two aligned sequence fragments are replaced by five new fragments: the fused fragments, and two unfused fragments on either side (some or all of which may be of zero length).<br>
So, initially, there are N fragments belonging to N input sequences (for example, 3 sequences in Figure 6). After a single round of local alignments, there is a larger number of fragments (for example, the 9 fragments in part (c) of Figure 6). After this, a new set of local alignments is calculated (with consistency conditions imposed) and performed on these fragments, which could result in a still larger number of fragments. This is repeated until there are no more possible local alignments whose p-value is below the minimal threshold. Finally, the "fragments" are "assembled" into the final alignment.<br>
The only major difference in algorithm with the previous program is in how the p-value is calculated. <software>Sigma-1</software> calculated, in a rather crude way, the probability of seeing m mismatches in a local alignment of length ?, given total fragment lengths L1 and L2. In <software>Sigma-2</software>, as described in the "Evolutionary Model" subsection, the measure of the quality of a local alignment is the log of the ratio of the likelihood of nucleotides in that alignment arising from a single common ancestor, to the likelihood that nucleotides in each column in a single fragment are related (having been previously aligned) but the two fragments are unrelated. This is illustrated in Figure 7. Each possible gapless local alignment has a log-likelihood ratio S as a measure of its quality, which is the sum of such log-likelihood ratios over every "column" of nucleotides in the alignment. From S we calculate the p-value of the alignment using the central limit theorem: the total score S is a product over positions i, within the alignment, of individual scores si. Before performing the alignment, therefore, the mean s? and variance ? of s for 1000 individual pairs of positions selected randomly from the two fragments are calculated. If the fragments are unrelated, the expected mean log-likelihood-ratio of an alignment of length m would be ms? and the expected variance would be m? (from the central limit theorem, for sufficiently large m). The probability that the observed log-likelihood ratio observed in unrelated sequence fragments would be S or lower is<br>
(12)p?=12erfc(S?ms??2m)<br>
where erfc x is the "complementary error function" of x: erfc x=2??x?exp?(?x2)dx. This applies to a particular local alignment of length m. But we want to consider all possible local alignments of length m in the given fragments. Suppose the fragments have lengths L1 and L2: the probability that no pairwise alignment exists, of length m and score S or lower, is<br>
(13)???P(no?alignment)=(1?p?)(L1?m+1)(L2?m+1)<br>
where the exponent is the number of possible ways subsequences of length m can be chosen from the two fragments. So the probability that at least one local alignment of length m and score S or lower exists in the two fragments, that is, our desired p-value, is<br>
(14)P=P(at?least?one?alignment)=1?(1?p?)(L1?m+1)(L2?m+1).<br>
For sufficiently small p', this reduces to p'(L1 - m + 1)(L2 - m + 1). We note that <software>Dialign-2</software> [41] uses a similar formula, p'L1L2, which amounts to assuming that L1 and L2 are large compared to m. However, L1 and L2 for <software>Dialign-2</software> are the lengths of the original input sequences, while for us they are the lengths of the "sequence fragments" currently being considered under our "progressive alignment" scheme. If L1 and L2 are small (comparable in size to m), the significance increases: a small local alignment that would be rejected in the "first pass" may prove to be significant in context of local alignments that have previously been carried out. For example, if the original input sequences were each 1000 bp, a 10 bp local alignment may initially be insignificant; but if two large local alignments are carried out on either side of this 10 bp stretch, reducing the "available" sequence fragments to 50 bp each, the 10 bp local alignment may now become significant.<br>
The best (lowest p) local alignment is found by a dynamic programming algorithm similar to the Smith-Waterman method [42]; but since the alignments are gapless, the algorithm requires only linear space but quadratic time, O(L1L2). Estimating the full running time of the program is less straightforward, since many local alignments are performed.<br>
This algorithm requires a phylogenetic tree. Given input sequences, in a preliminary run <software>Sigma-2</software> runs a multiple alignment with a "star phylogeny" tree where each sequence has a proximity of 0.33 from its ancestor. It uses this interim alignment to calculate all pairwise proximities. It then uses these pairwise values to construct a phylogenetic tree that is then used to perform the final alignment. While in principle this could be iterated to convergence, it seems to be unnecessary to do so.<br>
<br>
<br>
Availability<br>
<software>Sigma-2</software> is available from http://www.imsc.res.in/~rsidd/sigma2/ and is free software, distributable under the GNU General Public Licence.<br>
<br>
Authors' contributions<br>
RS conceived the work. GJ and RS implemented the program and performed the benchmarks. RS primarily wrote the manuscript. Both authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1440920</b><br>
Identification and Classification of Conserved RNA Secondary Structures in the Human Genome<br>
The discoveries of microRNAs and riboswitches, among others, have shown functional RNAs to be biologically more important and genomically more prevalent than previously anticipated. We have developed a general comparative genomics method based on phylogenetic stochastic context-free grammars for identifying functional RNAs encoded in the human genome and used it to survey an eight-way genome-wide alignment of the human, chimpanzee, mouse, rat, dog, chicken, zebra-fish, and puffer-fish genomes for deeply conserved functional RNAs. At a loose threshold for acceptance, this search resulted in a set of 48,479 candidate RNA structures. This screen finds a large number of known functional RNAs, including 195 miRNAs, 62 histone 3?UTR stem loops, and various types of known genetic recoding elements. Among the highest-scoring new predictions are 169 new miRNA candidates, as well as new candidate selenocysteine insertion sites, RNA editing hairpins, RNAs involved in transcript auto regulation, and many folds that form singletons or small functional RNA families of completely unknown function. While the rate of false positives in the overall set is difficult to estimate and is likely to be substantial, the results nevertheless provide evidence for many new human functional RNAs and present specific predictions to facilitate their further characterization.<br>
<br>
Introduction<br>
Many new classes of functional RNA structures (fRNAs), such as snoRNAs, miRNAs, splicing factors, and riboswitches [1?3], have been discovered over the last few years. These structures function both as independent molecules and as part of mRNA transcripts. These recent discoveries verify that fRNAs fulfill many important regulatory, structural, and catalytic roles in the cell, and suggest that perhaps only a small fraction of these fRNAs are currently identified [1,3,4].<br>
The development of computational methods that can efficiently identify fRNAs by comparative genomics has been hampered by the fact that fRNAs often exhibit only weakly conserved primary-sequence signals [5]. Fortunately, the stem-pairing regions of fRNA structures evolve mostly with a characteristic substitution pattern such that only substitutions that maintain the pairing capability between paired bases will be allowed. This leads to compensatory double substitutions (e.g., GC ? AU) and to a few types of compatible single substitutions (e.g., GC ? GU); the latter made possible by RNA's ability to form a non?Watson-Crick pair between G and U. This evolutionary signal can be exploited for comparative identification of fRNAs [6?12].<br>
The many non-human vertebrate genomes now sequenced can be aligned against the human genome, leading to a multiple alignment with considerable information about the evolutionary process at every position [13?15]. Given a diverse enough set of genomes, comparative methods that can make effective use of this evolutionary information should in principle be able to efficiently identify the conserved human fRNAs. We have developed a comparative method called <software>EvoFold</software> for functional RNA-structure identification in multiple sequence alignments. <software>EvoFold</software> makes use of a recently devised model construction, a phylogenetic stochastic context-free grammar (phylo-SCFG) [10,16,17], which is a combined probabilistic model of RNA secondary structure and sequence evolution. Phylo-SCFGs use stochastic context-free grammars (SCFGs) [18,19] to define a prior distribution over possible RNA secondary structures, and a set of phylogenetic models [20?22] to evaluate how well the substitution pattern of each alignment column conforms with its secondary-structure annotation. <software>EvoFold</software> uses a very general model of RNA secondary structures that allows it to model everything from short hairpins to complex multiforking structures, including novel structures not seen in its training set. The substitution process explicitly models co-evolution of paired bases within the structure using the phylogenetic tree and evolutionary branch lengths relating the sequences of the alignment. Stem-pairing regions are detected not only by the presence of compensatory substitutions, but also by the presence of compatible single substitutions and the overall slower rate of evolution. We have built a human-referenced eight-way vertebrate whole-genome alignment and used EvoFold to search for functional RNAs in the human genome. This search resulted in a total of 48,479 candidate RNA structures. Based on estimates of the false-positive rate, which unfortunately are associated with very large uncertainties, we estimate that the candidate set contains approximately 18,500 substructures of approximately 10,000 RNA transcripts. These numbers are derived using an estimated false-positive rate of 62%. Among the highest-scoring candidates, where the estimated false-positive rate is much lower, this screen finds a large number of known functional RNAs, and contains new candidate miRNAs, selenocysteine insertion sites, RNA editing hairpins, RNAs involved in transcript auto regulation, and many folds that form singletons or small functional RNA families of completely unknown function.<br>
<br>
Results<br>
We constructed a whole-genome alignment of the human [23], chimpanzee [24], mouse [25], rat [26], dog, chicken [27], zebra fish, and puffer fish [28] genomes using the <software>MULTIZ</software> program [13,29]. From this alignment we assembled a set of human genome segments where at least four other species are aligned and the pattern of substitution shows evidence of negative selection using the PhastCons method [15]. These segments were further filtered to remove retroposed genes, simple/low-complexity repeats, segments with mitochondrial chromosome homology, and segments that were not clearly in the orthologous locations with respect to neighboring genes in both the human and mouse genomes (?nonsyntenic human-mouse matches?). The resulting set defines 1,181,107 conserved segments spanning 3.7% of the reference human genome. We applied the EvoFold algorithm, illustrated in Figure 1, to each of these conserved segments. This resulted in a total of 48,479 candidate RNA folds with more than five pairing bases that span 0.07 % of the human genome at the base level (see Figure S1 for length distribution). These can be interactively explored or retrieved in bulk from the University of California Santa Cruz (UCSC) <database>Genome Browser</database> (http://genome.ucsc.edu, Protocol S1).<br>
We classified these candidate folds according to three different criteria: their size, their genomic location, and their overall shape. We distinguished two size ranges: short (between five and 15 pairing bases, 39,075 folds) and long (more than 15 pairing bases, 9,404 folds); five types of genomic location: coding (12,736 folds), 3?UTR (3,331 folds), 5?UTR (334 folds), intronic (11,777 folds), and intergenic (20,301 folds); and four shape-types: hairpins (42,964 folds),Y-shaped (3,479 folds), clover-shaped (250 folds), and more complex shapes (1,786 folds). This scheme results in 40 different RNA fold prediction categories. Candidate folds were also clustered by proximity in the genome or overlap with cDNAs into sets of folds that are likely to be part of a single underlying RNA transcript. This grouped the 48,479 candidate RNA folds into 23,287 candidate structure?containing transcripts. Finally, the folds within each category were ranked by a length-normalized likelihood-ratio score that we call the folding potential score (fps), and a shuffling scheme was used to tentatively estimate the rate of false-positive predictions in each category as a function of score (Materials and Methods, Figures S2 and S3, Tables S1 and S2).<br>
We mapped all available human and non-human mRNAs and ESTs to the human genome and determined the enrichment of hits to our set of candidate RNA folds relative to the background hit rate in genomic DNA. These were found to vary from 3.6? (cDNA from humans) to 11.4? (non-human EST). This is significantly higher than the enrichments observed for the full set of conserved elements from which these candidates were chosen (Figure S4).<br>
We also found that predictions at known fRNAs generally score higher on the strand of the fRNA compared to its reverse complement (this is, e.g., the case for 89% of the known miRNAs we predict). The asymmetry is primarily caused by the ability of GU (or UG) to pair, but not its reverse complement AC (CA). Since the most common types of substitutions in RNA stems involve GU (or UG) pairs, this can have a pronounced effect on the <software>EvoFold</software> score, thus allowing the strand association of a fold to be inferred by comparing the score of an alignment with the score of its reverse complement. In cases where the candidate RNA is contained in a known transcript, the <software>EvoFold</software> score for the sense strand (i.e., the strand complementary to the template strand for transcription) is often significantly higher than for the anti-sense strand (Table S3). Because this is similar to the effect observed for known fRNAs, this provides circumstantial evidence that many of these predictions are new fRNAs. However, part of this effect may be due to compositional asymmetries, possibly due to transcription-mediated repair [30], or the influence of other sense-strand associated functional elements (see Protocol S1).<br>
Using a shuffling approach, we estimate that the set of 48,479 candidates contain 18,500 partially correct fRNAs (see Materials and Methods, Validation section). However, this estimate is associated with huge uncertainties inherent to the shuffling approach and should only be viewed as a first approximation based on the available data (see Discussion). Based on the shuffling approach and the genomic distribution of the candidates, we estimate, conditional on the above-mentioned uncertainties, that our predictions comprise about 10,000 human RNA transcripts: 2,200 of which are transcripts of protein-coding genes that harbor functional RNAs in their UTRs or overlapping their coding region, and the remainder being fRNA genes. After correcting for the shuffling-based estimates of false-positive rates, the folds break down into the different sizes, locations, and shapes as shown in Figure 2.<br>
Three quarters of the predicted folds are short. These are likely to represent a mix of small complete folding units and partial predictions of larger folds, where only a small core element had sufficient evolutionary covariation to be detected by our method. Among the long folds, about 82% are intergenic or intronic, 5.5% are in 3?UTRs, 0.5% in 5?UTRs, and a surprising 12% (550 folds) overlap known coding regions. These are discussed further below. As expected, the small folds are predominantly single hairpins; there are usually not enough paired bases in these to support more complex stable structures. The long folds show a more varied shape distribution, but are also dominated by simple hairpins. Again, since these are often partial structural predictions, this breakdown is likely to be somewhat biased toward the simpler fold types.<br>
Because <software>EvoFold</software> is designed to look for RNAs that are conserved in structure and remain in the same genomic context in all vertebrates, there are likely to be additional fRNAs not detected in this survey. There are some classes of known functional RNAs that are too mobile or rapidly evolving for <software>EvoFold</software> to detect, such as tRNAs and snoRNAs. The vertebrate tRNAs spawn many lineage-specific copies that land in different places in the genome, most of which are pseudogenes, so that the remaining functional copies often end up in a different genomic context in different vertebrate lineages [27]. As a result, more than 99% of the functional human tRNAs fail the filter we applied that removes nonsyntenic matches between human and mouse, and hence are absent in our set of predicted folds. In contrast, most snoRNAs are missing from our set of predicted folds either because they have too few base pairs (bp), e.g., 4?5 bp in the CD-box snoRNAs, or have experienced too many structural changes in vertebrate evolution. We observe that 32% of the bp of known deeply conserved snoRNAs could not be formed in fish or chicken, causing a conflict with the overall structural signal EvoFold is designed to detect. The signal recognition particle RNA and the Y RNAs are also missed due to their evolutionary mobility. On the other hand, RNase P RNA and both the U11 and U12 spliceosomal RNAs are well conserved and detected by this screen. Based on our current methods, we cannot predict how many more, as-yet-undiscovered, classes of highly mobile or rapidly evolving RNAs there are in vertebrate genomes.<br>
For other known classes of RNAs, such as miRNAs, <software>EvoFold</software> achieves a high rate of sensitivity, finding nearly all known members. To evaluate <software>EvoFold</software>'s sensitivity, we performed a 5-fold cross-validation test using various curated sets of known RNAs. These tests showed that <software>EvoFold</software> is quite good at detecting some known classes of RNAs, such as miRNAs and Histone 3?UTR stem loops (Table 1). Despite the fact that Histone 3?UTR stem loops have stems containing only 6 bp, they are predicted very accurately: 97% predicted with 100% correct structure.<br>
Since the fps used by <software>EvoFold</software> ranks deeply conserved compact folds highly, we also defined an alternative score directly based on the substitution evidence and used it to define a ranked set of 517 ncRNA candidates (see Protocol S1). This score, for example, top-ranks the U11 and U12 spliceosomal RNAs mentioned above. The second-highest ranked clover-shaped fold from this set is currently being investigated experimentally.<br>
We evaluated the relative benefit of using an eight-way alignment instead of a pair-wise alignment by redoing the sensitivity experiments and part of the shuffling experiments using only the mouse?human subalignment. The sensitivity on the mixed set of <database>Rfam</database> Seed decreased by 59% and the false-positive rate increased slightly (Table S4). Overall, <software>EvoFold</software> made fewer predictions on the pair-wise alignments.<br>
New miRNAs among Long Intergenic and Intronic Hairpins<br>
The higher-ranked candidate RNAs in several of the fold classifications are greatly enriched for certain classes of known RNAs. In particular, we see a strong enrichment for known miRNAs among the higher-ranked candidates in the class of long intronic and intergenic hairpins (Tables 2 and 3): 36 of our top 100?ranked long intergenic hairpins and 33 of our top 100 long intronic hairpins are known miRNAs. At the time we first computed our set of 48,479 candidate fRNAs, 157 of them were known miRNAs. Since then 38 more of them have been confirmed to be miRNAs in three recent papers [31?33], giving a total of 195 known miRNAs in this set. Altogether, these three recent papers found 55 new miRNAs from among the 1,181,107 conserved segments that were input to <software>EvoFold</software>; thus, <software>EvoFold</software>'s sensitivity was 69% (38/55) on these new miRNAs.<br>
The known miRNAs tend to reside in short conserved segments (70% in segments of at most 200 bp), and their stems have relatively few bulges (86% have at most 20% of their bases in bulges). Using these additional criteria we defined a more specific set of 277 miRNA candidates from among the 3,500 predicted long intergenic and intronic hairpins. This set contained 90 known miRNAs and 187 novel candidates, with an estimated false-positive rate of 15% (see Materials and Methods). Xie et al. [31] ended up testing five of our predicted miRNAs and validating four. Bentwich et al. [32] validated 14 of our predicted miRNAs, and Berezikov et al. validated six [33]. Since six candidates were validated multiple times, this gives a total of 18 validated candidates.<br>
While miRNAs probably comprise a significant fraction of the high-scoring intergenic and intronic hairpins, it is quite possible that the majority of the folds in these categories have other functions. In particular, the three highest-scoring long intronic hairpins all are found in introns of ion channel genes, which are frequently targets of RNA editing by A-to-I conversion involving hairpins such as these [34?36]. In A-to-I conversion, the enzyme ADAR (adenosine deaminase acting on RNA), acts on a hairpin RNA structure to change a specific adenosine (A) to inosine (I). One of these genes, GRIA4, is already known to harbor an A-to-I editing hairpin in its coding region [37], which we also detected. Thus, there is a possibility that these three intronic hairpins are involved in similar editing on the pre-mRNA.<br>
<br>
New Coding fRNAs<br>
The candidate RNAs contain a surprising number of long folds that overlap coding regions. Coding folds are fascinating for at least two reasons. First, they often function in genetic recoding, which, as in the RNA editing in GRIA4, causes the protein made by the ribosome to differ from what would be obtained by a direct translation of the genomic sequence using the genetic code [38]. Second, their primary sequence encodes information both on the protein and the fRNA level, and these dual functional constraints lead to a highly constrained evolutionary process [39].<br>
The 15 top-ranking long-coding hairpins contain eight well-studied RNAs, five of which are involved in genetic recoding in the form of RNA editing (R-G site of GRIA2, GRIA3, and GRIA4) [37] and programmed frameshifting (OAZ1 and OAZ2) [38,40] (Table 4). Two of the remaining three play roles in regulating translational efficiency (COL1A1 and COL1A2) [41], and one is a miRNA [42,43] overlapping what appears to be a spuriously annotated open reading frame.<br>
Among the seven novel candidate RNAs in the top 15, we predict at least three to be involved in genetic recoding. Two of them are associated with the known selenoproteins SEPN1 and SELT [44]. Selenoproteins constitute another important example of genetic recoding: they contain in-frame UGA stop codons that are recoded as insertion sites for selenocysteines. The recoding of these stop codons is directed by a hairpin called the selenocysteine insertion sequence (SECIS). In eukaryotes the SECIS has previously only been found in the 3?UTR of selenoprotein transcripts [38,44,45], but in prokaryotes it is found in coding regions downstream of the UGA codon [38,46]. Both of these transcripts have an annotated SECIS in their 3?UTR [44,47], but the hairpin structure given in the <database>Rfam</database> database is only partly conserved. The predicted coding hairpins of both SEPN1 and SELT are located less than ten bases downstream from the selenocysteine insertion site (the UGA codon) (Figure 3). We therefore hypothesize that both of these hairpins are involved in the recoding of the UGA codon, and that they may constitute the first examples of Eukaryotic SECIS hairpins in coding regions. During review, we became aware of recent independent experimental work that shows the SEPN1 hairpin does indeed facilitate UGA readthrough [48].<br>
The third is the highest-ranking long-coding hairpin, found in the UBE1C gene (Figure 4). This shows the characteristics of many other hairpins found at sites of A-to-I RNA editing [34?36] by overlapping the intron?exon boundary, and by having a single 1-bp symmetric bulge with consecutive adenosines flanking it. This provides good evidence that this hairpin may function as an A-to-I editing site that is altered in the primary mRNA transcript. An inspection of the human cDNAs spanning this region also revealed a cDNA with a single genomic discrepancy showing a guanosine (G) instead of an adenosine (A). Since inosine is sequenced as guanosine, this evidence further supports the hypothesis that this hairpin can function as an A-to-I editing substrate for ADAR.<br>
Of the four remaining candidate long-coding hairpins, two are in genes of unknown function (KIAA1190 and KIAA0924), one is in the Wolf-Hirschhorn syndrome candidate-1 gene, WHSC1L1 [49], and perhaps the most interesting is in the DGCR8 (DiGeorge syndrome critical region) gene. The DGCR8 gene is known to harbor two double-stranded RNA binding domains [50]. DGCR8 has recently been shown to be associated with Drosha and to play a crucial role in the processing of primary miRNA transcripts to precursor miRNAs [51,52]. This gene harbors not only a high-scoring hairpin in its first exon but also the longest and second highest?scoring hairpin of the 5?UTR category (Figure 5). The 5?UTR hairpin resembles the folds predicted for known miRNAs, and receives a very significant score by mirScan [53] (see Protocol S1). It is therefore possible that these folds are involved in self-regulation of DGCR8, potentially through the cleavage of the 5?UTR hairpin by the DCGR8/Drosha microprocessor complex described above.<br>
<br>
New Clover-Shaped Folds<br>
In addition to new examples of previously known RNA families, the high-ranking candidate RNAs also include several completely novel families. One of these is represented by the highest and fourth-highest ranking candidates in the category of long clover-shaped folds. These are located less than 3,500 bases apart, and both are overlapped by transcripts of the little-characterized gene ZNF207 [54] (Figure 6A). Both folds contain several supporting substitutions (Figure 6B). The shorter of the folds is located in the 3?UTR of the gene and the longer in an intron of an alternative splice variant. The primary sequence of these two folds (Figure 6C) aligns well: the central stem-pairing regions are almost identical with only a few compensatory and compatible substitutions, while the loops differ both by substitutions and insertions/deletions (Figure 6D). This evolutionary relationship suggests a common functional constraint, which has preserved the central part of both clover-shaped folds. The close proximity, the high scores, and the systematic evolutionary differences within as well as between these folds suggest that they may constitute members of a new family of fRNAs.<br>
<br>
Paralogous Families<br>
In the spirit of the last example above, we grouped the RNA-fold predictions into paralogous families based on their primary-sequence homology. We disregarded sequences that could cause homology to be inferred for trivial reasons, i.e., repeats, pseudogenes, coding regions, etc. (see Materials and Methods). This approach resulted in 299 families with a mean family size of 2.7.<br>
Known families of fRNAs were recovered, such as the histone 3?UTR stem loops (46 known folds, one family), families of known miRNAs (72 known folds, 29 families), and families of RNA editing hairpins in GRIA genes (three known folds, one family). But most of the families were completely new. Some contain long intergenic and intronic hairpins and are likely to be new families of miRNAs (e.g., 17 of our miRNA candidates are found in 11 families). Others contain hairpins in ion-channel genes not previously characterized as undergoing RNA editing (e.g., a cluster of three coding hairpins overlapping sodium channel exons in SCN3A, SCN8A, and SCN2A2. But the majority involves more complex folds, which we currently have no functional hypotheses for. A complete definition of the families is given online (http://www.cbse.ucsc.edu/jsp/EvoFold).<br>
<br>
<br>
Discussion<br>
We have conducted a survey of the human genome to identify functional RNA structures through comparative genomics using an eight-way whole-genome sequence alignment. While this alignment contains considerably more evolutionary information than has been previously available, these currently available genomes are still quite limited in terms of their statistical power to detect negative selection [55], a situation that will change in the coming years as more vertebrate genomes are sequenced. Nevertheless, this study shows that we already have sufficient evolutionary information for efficient discovery of many classes of fRNAs. Further information from additional genomes and additional experiments should be able to weed out many of the false-positive predictions and refine the individual candidate structures.<br>
This initial survey suggests that there are many more functional RNAs in the human genome than are represented in the current RNA sequence databases. We estimate that these databases annotate 1,207 RNA genes in the human genome (see Materials and Methods). Our results suggest that there may be 10-fold more functional RNAs there, and 7-fold more RNA genes. However, these values depend on the ability of the shuffling experiments to correctly estimate the false-positive rate. It is not clear how well shuffling experiments can estimate false-positive rates, and thus our current estimates are associated with very large and difficult to quantify uncertainties. Previous scans for ncRNAs based on pair-wise alignments have found that only a small fraction of the predictions are experimentally verifiable [56,57], thus caution is warranted. Further experimental work will be necessary to reliably characterize the number of human fRNAs. However, combined with the presence of additional evidence (sense-strand bias, transcription evidence, biologically plausible folds, and existence of paralogous families), our results do suggest that there are many additional RNAs to be found. The exploration of RNA genes and RNA structural elements within protein-coding genes represents a huge opportunity, and a huge challenge, as we try to fully explore the key functional elements of the human genome sequence.<br>
The RNA folds we predict with the highest confidence include many known fRNAs, such as miRNAs and genetic recoding signals, as well as thousands of new fRNA candidates, a large fraction of which are supported by the presence of compensatory substitutions. Some of these new fRNAs enlarge existing families while others group into small new families. Detailed analysis of individual candidates has revealed additional supporting evidence and has allowed specific functional hypotheses to be formulated in some cases, including the new SECIS elements, RNA editing hairpins, regulatory hairpins, and miRNA candidates discussed above. We estimate that about 500 coding regions contain overlapping functional RNA structures, and that a non-negligible fraction of these may contain undocumented examples of genetic recoding.<br>
The EvoFold method we have developed was trained to only predict RNA stems that are well-supported by a consistent evolutionary signal in clearly orthologous copies from many species. To guarantee orthology, the alignments used require that aligned sequences from different species appear in the same genomic context, i.e., have orthologous flanking DNA, in each species. This greatly reduces the number of false-positive predictions due to mobile elements such as transposons and retroposed pseudogenes. However, it causes us to miss some highly mobile known fRNAs, such as tRNAs and snoRNAs, even with a relatively liberal threshold that allows an estimated 62% false positives in our overall set of predictions. Identifying mobile fRNAs with a general model of molecular evolution will require logic for lineage-specific duplication and loss of function in addition to the simple evolution of orthologous copies that the <software>EvoFold</software> model embodies.<br>
Alignment errors can also disrupt the evolutionary signal of true fRNAs, and thus improvements to the current sequence-alignment scores might improve the results. Local alignment errors involving only a few bases are unlikely to affect the entire structure and thus should normally allow at least a partial structure with a reduced signal to be identified. However, more extensive errors, where non-orthologous regions are aligned, will most likely cause the fRNA to be missed completely as discussed above.<br>
<software>EvoFold</software>'s rate of false positives is much lower among the highest-scoring predictions, but it never goes completely to zero, even for the largest predicted structures. One problem is that the elements where negative selection is strongest, the ultraconserved regions [58], often have too few substitutions within the available vertebrates for the evolutionary approach to distinguish conservation of RNA secondary structure from other kinds of functional conservation. Until more genomes are available, for these elements we are faced with something like the problem of predicting RNA structure in a single sequence, without benefit of comparative genomics.<br>
Sequence comparisons between novel predicted fRNAs verify that some of these can be grouped into small paralogous families, but most appear as singletons. Since many fRNAs undergo lineage-specific expansions [2,32], we find it likely that a search for paralogs in the human genome will show many of these singletons to be founders of phylogenetically shallow families. However, lineage-specific expansion and rapid diversification may make family members difficult to recognize in searches based on primary-sequence identity.<br>
The <software>EvoFold</software> scoring scheme very highly ranks compact folds with a high ratio of paired to unpaired bases, such as miRNAs and histone 3?UTR stem loops. Indeed, these two families stand out prominently in this survey, and their existence would have been a clear-cut new outcome of this study had it not already been known. One of the reasons they rank so highly is because the fps is a length-normalized likelihood ratio, which tends to emphasize the ratio of paired to unpaired bases rather than the total number of paired bases. Other normalization schemes may emphasize other families of fRNAs as shown by the substitution-ranked ncRNA candidates (see Protocol S1).<br>
This set of fold predictions represents what we believe is the first general survey of evolutionarily conserved human fRNAs. (Another survey, based on our multiple alignments and PhastCons detection of conserved segments as well, has come to our attention during the final stages of preparing this paper [59]. The authors appear to have reached similar conclusions regarding the expected number of human RNA genes.) We have attempted to create a comprehensive set, which still maintains a relatively low false-negative rate, in hopes that it would be a useful resource for further studies of fRNAs. To facilitate these further studies, the complete set of predictions is available through the UCSC <database>Human Genome Browser</database>, including detailed structure-labeled alignments as in Figures 3?6 (http://genome.ucsc.edu). Additionally, ranked lists of folds of each category, the set of miRNA candidates, the set of ncRNA candidates, and the set of paralogous families can be accessed from the <software>EvoFold</software> Web site (http://www.cbse.ucsc.edu/jsp/EvoFold).<br>
<br>
Materials and Methods<br>
EvoFold algorithm.<br>
The <software>EvoFold</software> program takes a multiple alignment and a phylogenetic tree as input, and outputs a specific RNA secondary-structure prediction and an fps (Figure 1). The phylogenetic tree, which includes branch-length estimates, specifies the evolutionary relationship between the sequences of the multiple alignment. <software>EvoFold</software> is based upon two phylo-SCFGs: an fRNA model that describes regions possibly containing fRNAs and a background model that describes regions with no fRNAs. The score is a log-likelihood ratio under these two models. A Linux (i386) executable of the <software>EvoFold</software> program can be downloaded from the <software>EvoFold</software> Web site (http://www.cbse.ucsc.edu/jsp/EvoFold). Source code is available upon request.<br>
<br>
The phylo-SCFGs.<br>
Phylo-SCFGs were developed by Knudsen and Hein in 1999 and can be seen as an extension of phylo-HMMs [60?62]. They combine SCFGs' ability to model RNA secondary structure [18,19,63] with phylogenetic models' [21,22] ability to describe the substitution process along the branches of a tree. One of the strengths of this model construction is that it can handle multiple alignments with any number of sequences and weigh their information content in a way that reflects phylogeny.<br>
Two types of phylogenetic models are used by the phylo-SCFGs: a single-nucleotide model and a di-nucleotide model (Figure 1E). The single-nucleotide model describes the substitution process of the nonpairing regions of the RNA secondary structures (i.e., loops and bulges) as well as the nonstructural regions of the genome. The di-nucleotide model describes the substitution process of the stem-pairing nucleotides. These two models differ in various ways, in particular the single nucleotide model makes many kinds of substitutions relatively likely and the di-nucleotide model strongly favors compensatory substitutions.<br>
The phylo-SCFGs are composed of two components: a structural and a nonstructural one (Figures S5 and S6). The structural component describes structural regions whose first and last bases are paired. Such regions can correspond to a single hairpin or a more complex structure, and will be referred to here as folds (Figure 1D). This component contains both a di-nucleotide and a single-nucleotide phylogenetic model. The nonstructural component describes the regions outside folds and contains only a single-nucleotide phylogenetic model.<br>
The fRNA model contains both the structural and the nonstructural component. In contrast, the background model contains only the nonstructural component. See Protocol S1 for a complete specification of the phylo-SCFG parameterizations.<br>
<br>
Structure and score predictions.<br>
<software>EvoFold</software> uses the fRNA model to assign a specific RNA secondary-structure prediction to an input alignment (Figure 1C). The most probable structure given the information in the multiple alignment will be predicted. A prediction devoid of structure is possible due to the nonstructural component of the fRNA model. All the predicted folds, which pass the fold elimination described below, are included in the candidate set.<br>
The fps measures the overall tendency for the alignment to contain any fRNA. It is calculated as a log-odds score between the likelihood of observing the alignment (x) under the fRNA model (?fRNA) and the background model (?bg): fps = log(P(x|?fRNA)/P(x|?bg). The background model is carefully designed to model alignment sequences using the same nucleotide distribution as the fRNA model, thereby alleviating the problem of overpredicting in, e.g., GC-rich regions. The fps scores are length dependent; length-normalized versions of the fps scores are therefore used in this paper. The scores are used to rank the folds within each subclass.<br>
<br>
Validation.<br>
The false-positive rate of <software>EvoFold</software> was estimated by applying it to a set of alignments that have been randomized to remove the signal of any true fRNAs, but which retain the same base composition, substitution pattern, and conservation pattern as the original alignments. The false-positive rate can be seen to depend on the size of the predicted folds (Figure S2A): ranging from 76% for folds with five or fewer pairing bases to 42% for folds with more than 25 pairing bases. Our set of fold predictions thus contains some false positives, but we decided to retain all but the very short folds to sustain a comprehensive set of folds for downstream analysis. Subsets of folds with a much lower occurrence of false positives can be defined by focusing on only the top-ranked predictions, e.g., there are only an estimated 5% false positives in the top 100 scoring folds with more than 15 bp (Figure S2B and S2C). We also find the false-positive rate to depend on the degree of sequence conservation, the number of bulges found in stems, the genomic location, and to a lesser extent the overall shape of the folds (Figure S3).<br>
<br>
Training data.<br>
The alignments used to train <software>EvoFold</software> were prepared from a conserved subset of the <database>Rfam</database> Full database (version 6.0) [47] as follows: all human entries from Rfam Full were aligned to the human genome using <software>BLAT</software> [64], and only perfect matches were retained. The conserved human?mouse syntenic matching elements (see below) that overlap these human matches were selected and annotated with the secondary structures given in <database>Rfam</database> Full. Annotated stem pairs that could not form in the human sequence were treated as unpaired. Then all tRNA matches were discarded (many were found to be pseudogenes) and alignment sequences with poor secondary-structure conservation were removed. Finally all alignments with fewer than four sequences left were discarded. The resulting set contained 262 annotated alignments. Maximum-likelihood estimates of the phylo-SCFG parameters were found using a combination of the EM algorithm and a quasi-Newton method (see Protocol S1).<br>
<br>
Genomic alignment and conserved elements.<br>
EvoFold was applied to the conserved elements of an eight-way multiz [13] alignment of the following vertebrate species (UCSC assembly designations given in parenthesis): human (hg17), chimpanzee (panTro1), mouse (mm5), rat (rn3), dog (canFam1), chicken (galGal2), fugu (fr1), and zebra fish (danRer1). The <software>PhastCons</software> program [15] was used to identify an initial set of highly conserved elements, which was then processed by joining consecutive elements fewer than 30 bases apart. The joining avoids splitting fRNAs with fast-evolving loop regions across several conserved elements. Since computational constraints limit the size of the elements that can be handled by EvoFold, elements longer than 750 bases were substituted by a tiling of 300 base?long windows each offset by 100 bases. Alignment segments corresponding to both strands of the conserved elements were extracted from the eight-way alignment.<br>
<br>
Phylogenetic tree.<br>
A single phylogenetic tree, including branch lengths, was estimated from the genomic alignment using the <software>PhastCons</software> program [15] and subsequently used with every alignment segment.<br>
<br>
Known fRNA annotations.<br>
The fold predictions were compared against different classes of fRNAs: the 207 human micro RNAs found in the <database>miRNA Registry</database> version 5.1 [43]; the subset of 3?UTR histone stem loops annotated in <database>Rfam</database> Full version 6.0 [47] that overlaps histone-associated transcripts (as defined by the known gene annotation of the UCSC <database>Human Genome Browser</database> [65]); the set of human tRNAs as defined by <software>tRNAscan-SE</software> predictions scoring above 55 bits [66]; the set of snoRNAs defined in <database>snoRNA-LBME-db</database> [67]; and against the more broadly representative set of human fRNAs found in <database>Rfam</database> Seed version 6.0 [47]. When combined, these databases contain a total of 1,207 distinct fRNAs.<br>
<br>
Protein-coding gene annotation.<br>
The known gene annotation from the UCSC <database>Human Genome Browser</database> (May 2004 assembly) [65] was used to annotate the folds with a genomic location. Some folds overlap the boundaries of genomic regions, in these cases a single assignment was chosen according to the following prioritized list: coding &gt; 5?UTR &gt; 3?UTR &gt; intronic &gt; intergenic. The gene names of the known gene track, which are used in Tables 2?4 as well as in the text, are based on <database>RefSeq</database> or <database>HUGO</database> gene symbols.<br>
<br>
Fold elimination.<br>
Folds likely to be nonfunctional based on other annotations, alignments, or genomic location were discarded from the initial set. The filtering comprised certain types of repeats (many trivial folds), regions with synteny breaks (many pseudogenes), and regions homologous to the mitochondrial genome (many pseudogenes). The filters were based on the following UCSC <database>Human Genome Browser</database> data: simple and low-complexity repeats from the <software>RepeatMasker</software> track, synteny information from the mouse net track [68], and homology information from the <software>Blastz</software> self track.<br>
<br>
RNA transcripts.<br>
5?UTR, coding, and 3?UTR folds were considered part of the same transcript if overlapped by a known gene annotation (see above). Intronic and intergenic folds were considered part of the same transcript if separated by fewer than 250 bases. The false-positive rate was estimated from the folds of the relevant genomic types using the randomization procedure described below (see also Validation).<br>
<br>
Randomized alignments.<br>
All input alignments shorter than 450 bases (98% of total) were randomized by first permuting columns with no substitutions and then permuting columns with some substitutions. The resulting alignments thus maintain the conservation pattern, the substitution pattern, and the nucleotide bias of the original alignments, but have lost the signal of any true fRNA stems.<br>
<br>
Paralogous families.<br>
The folds were clustered according to primary-sequence homology, as given by the human <software>Blastz</software> self track of the <database>UCSC browser</database>, thereby defining a set of paralogous families [58]. To avoid inferring homology for trivial reasons, we disregarded sequences annotated as coding, repeats, retro-genes, or pseudo-autosomal regions in the UCSC <database>Human Genome Browser</database> (May 2004 assembly).<br>
<br>
<br>
Supporting Information<br>
Accession Numbers<br>
Accession numbers from <database>Swiss-Prot</database> (http://www.ebi.ac.uk/swissprot) are: COL1A1 (P02452), COL1A2 (P08123), DGCR8 (Q8WYQ5), GRIA2 (P42262), GRIA3 (P42263), GRIA4 (P48058), KIAA1190 (Q6ZSY6), KIAA0924 (Q5H9Q0), OAZ1 (P54368), OAZ2 (O95190), SCN2A2 (Q99250), SCN3A (Q9NY46), SCN8A (Q9UQD0), SEPN1 (Q9NZV5), SELT (P62341), UBE1C (Q8TBC4), WHSC1L1 (Q6ZSA5), and ZNF207 (O43670).<br>
The <database>GenBank</database> (http://www.ncbi.nlm.nih.gov/Genbank) accession number for cDNA of UBE1C gene is BC022853.<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1848004</b><br>
Correction: Specificity and Evolvability in Eukaryotic Protein Interaction Networks<br>
<br>
<br>
In PLoS Computational Biology, volume 3, issue 2: doi:10.1371/journal.pcbi.0030025<br>
The set of four equations in the Materials and Methods section had three extra minus signs, which were incorrect. The following are the correct equations.<br>
				<br>
				<br>
				<br>
				<br>
			<br>
<br>
<br>
<p><hr><p>

<b>PMC1866358</b><br>
HIV-1 Subtype B Protease and Reverse Transcriptase Amino Acid Covariation<br>
Despite the high degree of HIV-1 protease and reverse transcriptase (RT) mutation in the setting of antiretroviral therapy, the spectrum of possible virus variants appears to be limited by patterns of amino acid covariation. We analyzed patterns of amino acid covariation in protease and RT sequences from more than 7,000 persons infected with HIV-1 subtype B viruses obtained from the <database>Stanford HIV Drug Resistance Database</database> (http://hivdb.stanford.edu). In addition, we examined the relationship between conditional probabilities associated with a pair of mutations and the order in which those mutations developed in viruses for which longitudinal sequence data were available. Patterns of RT covariation were dominated by the distinct clustering of Type I and Type II thymidine analog mutations and the Q151M-associated mutations. Patterns of protease covariation were dominated by the clustering of nelfinavir-associated mutations (D30N and N88D), two main groups of protease inhibitor (PI)?resistance mutations associated either with V82A or L90M, and a tight cluster of mutations associated with decreased susceptibility to amprenavir and the most recently approved PI darunavir. Different patterns of covariation were frequently observed for different mutations at the same position including the RT mutations T69D versus T69N, L74V versus L74I, V75I versus V75M, T215F versus T215Y, and K219Q/E versus K219N/R, and the protease mutations M46I versus M46L, I54V versus I54M/L, and N88D versus N88S. Sequence data from persons with correlated mutations in whom earlier sequences were available confirmed that the conditional probabilities associated with correlated mutation pairs could be used to predict the order in which the mutations were likely to have developed. Whereas accessory nucleoside RT inhibitor?resistance mutations nearly always follow primary nucleoside RT inhibitor?resistance mutations, accessory PI-resistance mutations often preceded primary PI-resistance mutations.<br>
<br>
Introduction<br>
HIV-1 is a highly mutable pathogen. In the decades since it entered human populations, it has accumulated extensive sequence variation leading to the development of different subtypes and recombinant forms [1]. Although the enzymatic targets of therapy are among the most conserved parts of the HIV-1 genome, these too can develop marked variation, particularly in the setting of selective antiretroviral drug pressure. Indeed, it is not uncommon for drug therapy to select for protease and reverse transcriptase (RT) variants containing substitutions at more than 10% of their amino acids [2]. However, despite this high degree of mutation, the spectrum of possible virus variants appears to be limited by patterns of amino acid covariation.<br>
In 2003, we published two studies that examined the extent of covariation among RT and protease residues in the presence and absence of antiretroviral therapy [3,4]. Despite the relatively large size of the datasets in these studies?2,244 protease sequences and 1,210 RT sequences?there were insufficient data to examine patterns of covariation of different mutations at the same position. As more sequence data have become available, we are now analyzing covariation among mutations (rather than positions) in protease and RT. This expanded analysis uses a highly specific measure of covariation, the Jaccard similarity coefficient, and a multidimensional scaling based on this coefficient. In addition, we examine the relationship between conditional probabilities associated with a mutation pair and the order in which those mutations develop in viruses for which longitudinal sequence data are available.<br>
<br>
Results<br>
Protease<br>
Protease sequences from 3,982 protease inhibitor (PI)?naive individuals and from 3,475 PI-experienced individuals were available for analysis. The PI-experienced individuals had received a median of 1 PI (interquartile range, 1?3).<br>
Jaccard similarity coefficients and their standardized Z scores were calculated for all pairs of mutations at different positions present three or more times among the sequences from PI-naive and PI-experienced individuals. Among 19,203 pairs of mutations from the PI-experienced individuals, 161 pairs were significantly associated after adjusting for multiple comparisons by controlling the family-wise error rate at &lt;0.01. Of these 161 pairs, 92 (57%) were positively associated (Z &gt; 5.1, unadjusted p &lt; 4.4 ? 10?7) and 69 (43%) were negatively associated (Z &lt; ?5.0, unadjusted p &lt; 4.8 ? 10?7). Table 1 shows the Jaccard similarity coefficients and conditional probabilities of the 40 strongest positively associated protease mutation pairs and the ten strongest negatively associated protease mutation pairs. Table S1 shows the complete list of 161 statistically significant mutation pairs.<br>
For the positively associated mutation pairs, Table 1 also contains two columns with data on the temporal order in which correlated mutations occurred in sequences with both mutations from persons in which an earlier sequence was available that contained only one of the two mutations. For example, the first row shows that among persons with both I54V and V82A in whom an earlier sequence contained only one of these two mutations was available, I54V occurred first in nine (26%) of 34 people, and V82A occurred first in 25 (74%) of 34 people (p &lt; 0.01). In contrast, the fourth row shows that among persons with both A71V and L90M, each of the mutations was as likely to occur first (26 of 51 versus 25 of 51; p = NS). Figure S1 plots the relationship between the log of the ratio of the conditional probability of two mutations versus the log of the ratio in which two mutations develop, indicating that the conditional dependence between mutations is highly correlated with the order in which the mutations develop when they occur together (r2 = 0.56, p &lt; 0.001).<br>
Among the 18 positively associated pairs in Table 1 containing a major and an accessory PI-resistance mutation (as defined in Methods), the accessory mutation appeared first more often in 12 of the 18 pairs. There were several striking patterns of temporal association among these 18 pairs of correlated major and accessory mutations. The major mutation L90M preceded the accessory mutation G73S in 31 of 34 persons for whom temporal data were available. In contrast, the accessory mutation L63P preceded L90M in 160 of 172 persons, and the accessory mutations L10I and A71V preceded the major mutation I84V in 51 of 59 and 35 of 38 persons, respectively.<br>
The Jaccard dissimilarity coefficients associated with 595 pairs of 35 mutations were used for a multidimensional scaling. The mutations included in this analysis were the 22 positively associated mutations in Table 1 and 13 additional clinically relevant PI-resistance mutations (L10F, V32I, L33F, I47V, I50V/L, F53L, I54L/M, Q58E, L76V, V82T, and N88S). Figure 1 plots the mutations along axes representing the first two principal components. The first principal component accounted for 10% of the total inertia and separates the nelfinavir-resistance mutations D30N and N88D from the main group of PI-resistance mutations. The second principal component accounted for 7% of the total inertia and separates V82A-associated mutations (I54V, L24I, and M46L) from L90M-associated mutations (M46I, G73S, and I84V). Finally, the lower-left part of the figure contains a cluster with seven of the 11 mutations recently reported to be associated with phenotypic and clinical resistance to the newest PI, darunavir (V32I, L33F, I47V, I50V, I54L/M, and L76V).<br>
At several positions, there was sufficient data to contrast covariation patterns for different mutations. For example, M46I/L were each significantly associated with L10I, L24I, V32I, L33F, I54V, V82A, and L90M. However, M46I was uniquely associated with F53L, G73S/T, V82F/T, I84V, and N88S. I54V was significantly associated with L10F, L24I, L33F, M46I/L, G48V, F53L, V82A/F/T, I84V, and L90M. In contrast, I54L/M were significantly associated only with L33F, M46I, I47V, I84V, and L90M. N88D was positively associated with D30N and negatively associated with M46I, whereas N88S was negatively associated with D30N and positively associated with M46I. Of note, the divergent associations of different mutations at positions 46 and 88 have previously been reported by Hoffman and coworkers [5].<br>
Among 7,131 pairs of mutations in sequences from PI-naive persons, 65 pairs were significantly associated (family-wise error rate &lt; 0.01; Table S2). All but three of the positive associations among PI-naive persons were weaker (i.e., had a lower Z score) than the positive associations among treated persons in Table 1.<br>
<br>
Reverse Transcriptase<br>
RT sequences from 2,601 RT inhibitor?naive and from 5,188 RT inhibitor?experienced individuals were available for analysis. The RT inhibitor experienced individuals had received a median of three nucleoside RT inhibitors (NRTIs; interquartile range, 2?4) and zero nonnucleoside RT inhibitors (NNRTIs; interquartile range, 0?1).<br>
Jaccard similarity coefficients and their standardized Z scores were calculated for all pairs of RT mutations at different positions present three or more times among the sequences from RT inhibitor?experienced and ?naive persons. Among 65,624 pairs of mutations from the RT inhibitor?experienced persons, 327 pairs were significantly associated after adjusting for multiple comparisons by controlling the family-wise error rate at &lt;0.01. Of these 327 pairs, 213 (65%) were positively associated (Z &gt; 5.2, unadjusted p &lt; 2 ? 10?7) and 114 (35%) were negatively associated (Z &lt; ?5.0, unadjusted p &lt; 5 ? 10?7). Table 2 shows the Jaccard similarity coefficients and conditional probabilities of the 40 strongest positively associated RT mutation pairs and the ten strongest negatively associated RT mutation pairs. Table S3 shows the complete list of 327 statistically significant RT mutation pairs.<br>
Positively associated mutation pairs consisted primarily of Type I or II thymidine analog mutations (TAMs; as defined in Methods); accessory NRTI mutations that occurred in combination with Type I or II TAMs (K43E, E44D, V118I, H208Y, D218E); and Q151M-associated mutations (V75I, F77L, F116Y). Among the top 40 associated mutation pairs, there were only three positive associations between Type I and II TAMs (M41L, L210W, and T215Y with D67N). The strongest significant association between an NRTI and an NNRTI mutation was between L74V and Y181C (J = 0.17, Z = 8.9, unadjusted p &lt; 1 ? 10?11). Of note, the associations between the five accessory mutations listed above and Type I and II TAMs have also previously recently been described by Svicher and coworkers [6] and Cozzi-Lepri and coworkers in independent datasets [7]. The conditional probabilities and the temporal data columns show that each of the accessory NRTI mutations consistently follows the Type I or II TAMs. Among 12 pairs with a TAM and an accessory mutation, the TAM occurred first more often in all 12 pairs and was preceded by the accessory mutation in only 6% of pairs. In addition to the five accessory mutations in Table 2 (K43E, E44D, V118I, H208Y, and D218E), other NRTI mutations that consistently followed TAMs included the known treatment-selected mutations T69D and T69N. Figure S2 plots the relationship between the log of the ratio of the conditional probability of two mutations versus the log of the ratio in which two mutations develop, indicating that the conditional dependence between mutations is highly correlated with the order in which the mutations develop when they occur together (r2 = 0.81, p &lt; 0.001).<br>
The Jaccard dissimilarity coefficients associated with the 561 pairs of 34 mutations were used for a multidimensional scaling. The mutations included in this analysis were the 23 positively associated mutations in Table 2 and 11 additional clinically relevant NRTI-resistance mutations (K65R, A62V, T69ins, L74I/V, V75M, Y115F, M184V, and K219R/E/N). Figure 2 plots the mutations along axes representing the first two principal components. The first principal component accounts for 13% of the total inertia and separates the TAMs from the Q151M-associated mutations, whereas the second principal component accounts for 9% of the total inertia and separates the Type I and Type II TAMs. A62V, K65R, and Y115F are mutations that cluster with Q151M but may also occur with Type II (but not Type I) TAMs. D67N is a Type II TAM that can also occur with Type I TAMs, and it therefore occurs between Type I TAMs and Type II TAMs in terms of the second principal component. The non-TAM mutations, M184V and L74V, demonstrated no clustering with other NRTI-associated mutations.<br>
At several positions, there was sufficient data to contrast covariation patterns for different mutations (Table 2, Figure 2, and Table S3). The Type I TAM, T215Y, clustered with other Type I TAMs, whereas the Type II TAM, T215F, clustered with other Type II TAMs. K219Q/E were Type II TAMs that cluster with other Type II TAMs. In contrast, two less common mutations at this position (K219N/R) were positively associated with Type I TAMs. T69D was associated with both Type I and Type II TAMs, whereas T69N was associated only with Type II TAMs. L74V was associated with the NNRTI-resistance mutations L100I, K103N, and Y181C, whereas L74I was associated with M41L. V75I was associated with Q151M-associated mutations, whereas V75M was positively associated with the Type I TAMs.<br>
Among 19,431 pairs of mutations in sequences from RT inhibitor?naive persons, 41 pairs were significantly associated (family-wise error rate &lt;0.01; Table S4). However, all of the positive associations among RT inhibitor?naive persons were weaker (i.e., had a lower Z score) than the positive associations among treated persons in Table 2.<br>
<br>
<br>
Discussion<br>
In this analysis of amino acid covariation in protease and RT sequences from more than 7,000 persons infected with HIV-1 subtype B viruses, we confirmed several previously reported patterns of amino acid covariation and identified many new patterns of covariation. Multidimensional scaling further organized many of the correlations into clusters of co-occurring mutations. RT covariation was dominated by the distinct clustering of the TAMs and Q151M-associated mutations, and by the separation of the Type I and Type II TAMs. Protease covariation was dominated by the clustering of nelfinavir-associated mutations (D30N and N88D), two main groups of PI-resistance mutations associated either with V82A or L90M, and a newly identified cluster of the mutations V32I, L33F, I47V, I50V, I54L/M, and L76V. This new cluster of mutations is associated with decreased susceptibility to all PIs, including the salvage therapy PIs amprenavir and lopinavir and the recently approved PI darunavir. Although none of the sequences in this study were from patients who received darunavir, this drug is highly similar to amprenavir and is affected by the same PI-resistance mutations.<br>
Previous studies of HIV-1 covariation have used either the Pearson correlation for binomial random variables or mutual information [3?6,8?10]. The correlation coefficient is overly sensitive to rare pairs of mutations because its statistical significance is based on a departure from equality between the diagonal and off-diagonal products of a 2 ? 2 contingency table. In contrast, mutual information is insensitive to rare pairs of mutations, approaching a high level only for commonly occurring pairs of mutations. We therefore used the Jaccard similarity coefficient, which uses only those sequences in which at least one of a pair of mutations is present, and we assessed the significance of this coefficient using a distribution based on the underlying data.<br>
We also used a conservative correction for multiple comparisons (Holm's method) because our analysis was not designed to identify all covarying mutations but only those with the strongest association. Without a correction for multiple comparisons, 753 pairs of protease mutations from PI-experienced persons and 2,061 pairs of RTI mutations from RTI-experienced persons had a significant Jaccard similarity coefficient at a p-value of 0.01 but with the Holm's correction, only 161 pairs of protease mutations and 327 pairs of RTI mutations were significantly associated using a family-wise error rate of 0.01.<br>
Covariation between two mutations may result from the shared inheritance of the mutations from a founder virus, from a shared evolutionary pressure (e.g., an antiretroviral drug) that independently selects for each mutation, or from a functional dependency between the mutations. In our analysis, covariation was unlikely to result from shared inheritance because the most strongly covarying mutations occurred solely among treated HIV-1 isolates, consistent with the repeated selection of the correlated mutations in many different isolates as a result of selective drug pressure rather than the inheritance of the correlated mutations from a small number of ancestral viruses.<br>
However, the possibility that many of the covarying residues resulted from similar selective pressures rather than from functional dependency cannot be excluded. For example, it is possible that some pairs of covarying protease amino acids result from the selective pressure of the same PI or possibly pair of PIs. Shared selective pressure is a possible explanation for why covarying mutations are not necessarily close to one another in tertiary structures (Figure S3) [4]. An analysis of covariation that controls for treatment history would be better able to distinguish functional dependency from shared selective pressure. However, for most PIs and NRTI combinations, insufficient data are available for such an analysis. Identifying similar patterns of covariation in one or more independent lineages (e.g., other non-B subtypes) would also provide additional independent evidence for functional dependency.<br>
Our examination of conditional dependency between mutation pairs, the temporal order in which mutations occur, and the relationship between these two types of data provided new insights into the evolution of protease and RT in persons receiving antiretroviral therapy. A strong positive relationship between the conditional dependency ratio of two mutations and the order in which the mutations occur would represent the most parsimonious mechanism for HIV-1 to develop multiple mutations (i.e., the mutation that occurs more often in a pair of mutations would be on average more likely to occur first). Nonetheless, we found that the positive relationship between conditional dependency and the order of mutation occurrence was stronger for covarying RT (r2 = 0.81) compared with protease (r2 = 0.56) mutation pairs. This suggests that the number of mutational steps required to develop multiple PI-resistance mutations may be greater on average than that required for developing the same number of multiple NRTI-resistance mutations.<br>
We also found that accessory NRTI-resistance mutations nearly always followed primary NRTI-resistance mutations (particularly the TAMs). In contrast, the commonly recognized accessory PI-resistance mutations were as likely to precede as to follow major PI-resistance mutations. This frequent precedence of accessory PI-resistance mutations results in part from the fact that many of the accessory PI-resistance mutations are polymorphic and therefore present prior to the start of therapy. However, this alone does not explain the marked dependency of some major mutations on polymorphic accessory PI-resistance mutations that occur only at low levels in untreated persons.<br>
The strong positive relationship between conditional probabilities and temporal data that we describe support the validity of previous research, which used cross-sectional data to infer mutational pathways [11] and causality [12,13]. Our results also suggest that there is a complex process underlying the order in which major and accessory PI-resistance mutations develop during PI therapy, and that the designation of major PI-resistance mutations as primary and accessory PI-resistance mutations as secondary often refers only to their roles in causing resistance and not to the order in which they develop.<br>
<br>
Materials and Methods<br>
Virus sequence data.<br>
Sequences included HIV-1 subtype B RT and protease sequences from published studies in the <database>Stanford HIV Drug Resistance Database</database> (http://hivdb.stanford.edu) [14]. For patients with more than one sequence, only the latest sequence obtained while receiving treatment was analyzed. For each gene, separate analyses were done for the sequences from treatment-experienced and treatment-naive individuals.<br>
RT positions 1?240 and protease positions 1?99 were analyzed. Mutations were defined as differences from the consensus wild-type subtype B amino acid reference sequence (http://hivdb.stanford.edu/pages/asi/releaseNotes/index.html). For each pair of mutations (X, Y), the numbers of sequences containing both mutations (X and Y), only one mutation (X or Y), or neither mutation (not X, not Y) were counted and used to populate a contingency table. Sequences containing mixtures at either of the two positions were excluded from analysis of that pair of positions.<br>
Antiretroviral treatment?selected mutations were defined based on the results of a previous study, as mutations that were significantly more common in treated than untreated persons after adjusting for multiple comparisons [15]. PI-selected mutations included L10I/V/F/R, V11I, K20R/M/I/T, L23I, L24I, D30N, V32I, L33F/I, E34Q, E35G, M36I/V, K43T, M46I/L/V, G48V/M, I50V/L, F53L, I54V/M/L/T/A/S, K55R, Q58E, L63P, I66F, C67F, A71V/T/I, V72L, G73S/T/C/A, T74A/P/S, L76V, V77I, V82A/T/F/S/L/M, I84V/A/C, I85V, N88D/S/T/G, L89V, L90M, T91S, Q92R/K, I93L, and C95F. Several PI-resistance mutations?particularly those that occur in the substrate cleft or that have a major impact on drug susceptibility?are considered major PI-resistance mutations [2,16]. For the purposes of this study, we defined mutations at positions 24, 30, 32, 46, 47, 48, 50, 53, 54, 76, 82, 84, 88, and 90 as being major PI-resistance mutations. Several PI-resistance mutations?including several that are polymorphic in untreated persons?are commonly considered accessory drug resistance mutations that either compensate for the decreased replication associated with many of the major mutations or that reduce drug susceptibility further when present with a major mutation. Mutations at positions 10, 20, 33, 36, 58, 63, 71, 73, 74, 77, and 93 are usually considered to be accessory mutations. Little attention has been given to the remaining PI-selected mutations, and for the purposes of this paper, we leave them unclassified with respect to the designations major and accessory.<br>
NRTI-selected mutations included T39A, M41L, K43E/Q/N, E44D/A, A62V, K65R, D67N/G/E, T69D/N/S/insertion, K70R, L74V/I, V75I/M/T/A, F77L, V90I, K104N, Y115F, F116Y, V118I, Q151M, M184V/I, E203K, H208Y, L210W, T215Y/F/D/C/E/S/I/V, D218E, K219Q/E/N/R, H221Y, K223Q, and L228H/R. These mutations included the Type I TAMs M41L, L210W, and T215Y, and the Type II TAMs D67N, K70R, T215F, and K219Q/E [7]. Recently described accessory NRTI mutations included T39A, K43E/Q/N, E44D/A, V118I, E203K, H208, D218E, H221Y, K223Q, and L228H/R [3,6,17]. Q151M-associated mutations included A62V, V75I, F77L, F116Y, and Q151M [18,19].<br>
NNRTI-selected mutations included A98G, L100I, K101E/P/N/H, K103N/S, V106A/M, V108I, V179D/E, Y181C/I/V, Y188L/C/H, G190A/S/E/Q, P225H, F227L, M230L, P236L, and K238T.<br>
<br>
Pairwise correlation.<br>
We used the Jaccard similarity coefficient (J) to assess covariation among protease and RT mutations. For a given pair of mutations X and Y, the Jaccard similarity coefficient is calculated as J = NXY /(NXY + NX0 + N0Y) where NXY represents the number of sequences containing X and Y, NX0 represents the number of sequences containing X but not Y, and N0Y represents the number of sequences containing Y but not X. This coefficient represents the probability of both mutations occurring together when either mutation occurs and, therefore, does not inflate the correlation between two mutations that may appear correlated by other measures when both mutations are nearly always absent.<br>
To test whether observed Jaccard similarity coefficients were statistically significant, the expected value of the Jaccard similarity coefficients (JRAND) and its standard error (JSE) assuming two mutations (X and Y) occur independently were calculated for each pair of mutations. JRAND was calculated as the mean Jaccard similarity coefficient after 2,000 random rearrangements of the X or Y vector (containing 0 or 1 for presence or absence of a mutation, respectively). JSE was calculated using a jackknifed procedure, which removed one sequence at a time, repeatedly for each sequence. The standardized score Z, Z = (J ? JRAND) / JSE, indicates a significant positive association (Z &gt; 2.56) or a significant negative association (Z &lt; ?2.56) at an unadjusted p &lt; 0.01.<br>
Holm's method was used to control the family-wise error rate for multiple hypothesis testing [20]. The p-values of observed Jaccard similarity coefficients for all pairs of mutations were ranked in descending order. Starting from the smallest p (rank r = n, where n is the number of pairs), we compared each p of rank r with a significance cutoff of 0.01 / r as long as pr ? 0.01 / r. All p-values from pr?pn were considered to be statistically significant.<br>
To deal with contingency tables containing 0 for NXY (potentially leading to Z scores of ??), we generated a conservative nonzero approximation of JSE using the following procedure. Given a dataset of n sequences, x with mutation X and y with mutation Y, we computed the probability of both mutations (PXY), mutation X but not Y (PX0), mutation Y but not X (P0Y), and neither mutation (P00) under the null hypothesis of independence by PXY = (x / n) ? (y / n), PX0 = (x / n) ? (y / n) / n, P0Y = (n ? x) / n ? (y / n) and P00 = 1 ? PXY ? PX0 ? P0Y. These probabilities were used to create 200 two-by-two contingency tables with cells containing randomly distributed numbers adding up to 20,000 based on the null hypothesis probabilities of independence.<br>
<br>
Multidimensional scaling.<br>
Given the matrix of dissimilarity coefficients (1 ? Jaccard similarity coefficient) for a list of mutations (X1, X2, ..., Xn), multidimensional scaling was used to construct points in 2-D space such that the Euclidean distances between these points approximate the entries in the dissimilarity matrix [21]. For a given k, it computes points X1, X2, ? , Xn in 2-D space such that S =<br>
					 is minimized where dist(Xi, Xj) is the Euclidean distance between Xi and Xj and dij is the dissimilarity between Xi and Xj in the matrix D. This was performed using the R function cmdscale (classical multidimensional scaling).<br>
				<br>
Multidimensional scaling captures the inertia in a dataset in terms of a set of variables (or principal components) that define a projection that encapsulates the maximum amount of inertia in a dataset and is orthogonal (and therefore uncorrelated) to the previous principal component. Using the first and second principal components, we summarized the relationship among mutations in a graphical model, placing pairs of mutations with low Jaccard dissimilarity coefficients close together and mutations with high Jaccard dissimilarity coefficients far apart.<br>
<br>
<br>
Supporting Information<br>
Accession Numbers<br>
The 11,355 <database>GenBank</database> (http://www.ncbi.nlm.nih.gov/Genbank) accession numbers of the sequences used in this study are provided in Text S1.<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2475669</b><br>
Efficient Algorithms for Probing the RNA Mutation Landscape<br>
The diversity and importance of the role played by RNAs in the regulation and development of the cell are now well-known and well-documented. This broad range of functions is achieved through specific structures that have been (presumably) optimized through evolution. State-of-the-art methods, such as McCaskill's algorithm, use a statistical mechanics framework based on the computation of the partition function over the canonical ensemble of all possible secondary structures on a given sequence. Although secondary structure predictions from thermodynamics-based algorithms are not as accurate as methods employing comparative genomics, the former methods are the only available tools to investigate novel RNAs, such as the many RNAs of unknown function recently reported by the ENCODE consortium. In this paper, we generalize the McCaskill partition function algorithm to sum over the grand canonical ensemble of all secondary structures of all mutants of the given sequence. Specifically, our new program, <software>RNAmutants</software>, simultaneously computes for each integer k the minimum free energy structure MFE(k) and the partition function Z(k) over all secondary structures of all k-point mutants, even allowing the user to specify certain positions required not to mutate and certain positions required to base-pair or remain unpaired. This technically important extension allows us to study the resilience of an RNA molecule to pointwise mutations. By computing the mutation profile of a sequence, a novel graphical representation of the mutational tendency of nucleotide positions, we analyze the deleterious nature of mutating specific nucleotide positions or groups of positions. We have successfully applied <software>RNAmutants</software> to investigate deleterious mutations (mutations that radically modify the secondary structure) in the Hepatitis C virus cis-acting replication element and to evaluate the evolutionary pressure applied on different regions of the HIV trans-activation response element. In particular, we show qualitative agreement between published Hepatitis C and HIV experimental mutagenesis studies and our analysis of deleterious mutations using <software>RNAmutants</software>. Our work also predicts other deleterious mutations, which could be verified experimentally. Finally, we provide evidence that the 3? UTR of the GB RNA virus C has been optimized to preserve evolutionarily conserved stem regions from a deleterious effect of pointwise mutations. We hope that there will be long-term potential applications of <software>RNAmutants</software> in de novo RNA design and drug design against RNA viruses. This work also suggests potential applications for large-scale exploration of the RNA sequence-structure network. Binary distributions are available at http://RNAmutants.csail.mit.edu/.<br>
<br>
Introduction<br>
RNA's ubiquitous role in regulation and development is now understood to be much more important than previously believed. Apart from messenger RNA (mRNA), transfer RNA (tRNA) and ribosomal RNA (rRNA), there are many important enzymatic and regulatory functions of RNA, and it seems clear that we are far from having discovered all non-coding RNA (ncRNA) genes (Non-coding RNA [1],[2] is functional RNA that is transcribed, yet does not code for a protein). Indeed, according to the ENCODE Consortium [3], RNA is ?pervasively expressed? in the human genome, with approximately 15% of genomic DNA being transcribed, much of it into RNA of no known function.<br>
The functional diversity of non-coding RNA is enormous, ranging from translating mRNA into proteins via the genetic code (tRNA), to catalyzing the peptidyltransferase reaction in appending an amino acid to the growing peptide (rRNA [4]), to directing the chemical modifications of specific ribosomal nucleotides (snoRNA [5]), to the down-regulation of protein product (miRNA [6]), to gene up- or down-regulation by transcriptional and translational modification (riboswitches [7]), to the regulation of alternative splicing ([8]). To achieve their function, non-coding RNAs (except for small RNAs such as miRNA) require a structure well suited to their role. If we assume that ncRNA sequences have been adapted, or optimized, by evolution to fulfill a specific function, it is natural to believe that their structures have been also optimized or at least conserved. This observation is the basis for a family of methods for secondary structure determination using multiple sequence alignment and comparative sequence analysis [9]?[12].<br>
RNA is also a molecule governed by fundamental physical laws, and thus folds according to thermodynamic and kinetic principles. Algorithms using experimentally derived free energy parameters [13] for secondary structure prediction have been successfully designed, implemented and applied in <software>mfold</software> [14] and <software>RNAfold</software> [15]. As a consequence, a series of methods combining thermodynamic principles with evolutionary information [16]?[19] has appeared in the last few years.<br>
RNA molecules are not static, forever frozen in a native structure, but rather transition from one low energy structure to another (slightly different) low energy structure, due to thermal fluctuations. In his seminal work, J.S. McCaskill [20] introduced an algorithm to compute the partition function over all secondary structures as well as the base pairing probabilities. This approach has been significantly extended by Ding and Lawrence [21], who sampled secondary structures from the low energy ensemble.<br>
There is a growing interest in understanding which nucleotides of structurally important ncRNA are inessential, and may be modified with no phenotypic change, and which nucleotides play a critical role in structure, hence function. Indeed, mutagenesis studies are a popular technique for investigating the structure and function of both RNA and protein. In silico exploration of deleterious mutations in RNA secondary structure have thus far been carried out by exhaustive studies, where an available tool, such as <software>mfold</software> [22], <software>Vienna RNA Package</software> [23], or <software>Sfold</software> [24], etc. is applied successively to each 1-point mutant, then to each 2-point mutant, etc. depending on sequence length and available computational time; see, for instance Barash [25]. Clearly, this exhaustive technique cannot be used to study the effect of many pointwise mutations in a large sequence. In contrast, the current paper describes an efficient algorithm, <software>RNAmutants</software>, to investigate the minimum free energy structure MFEk and Boltzmann low energy ensemble ?k of all secondary structures of all k-point mutants, for each value of k. In addition to detecting deleterious mutations, <software>RNAmutants</software> could lead to a better understanding of fast-mutating RNA viruses. By understanding fundamental properties of functional RNAs and their robustness to mutation, there may be ultimate applications of our work to the areas of RNA gene discovery and RNA drug design.<br>
In this paper, we describe a new thermodynamics-based method for the investigation of the mutational secondary structure landscape of a given RNA sequence. State-of-the-art thermodynamics-based, single-molecule methods such as McCaskill's algorithm, use a statistical mechanics framework based on the computation of the partition function over the canonical ensemble of all possible secondary structures on a given sequence. Unfortunately, methods such as Zuker's algorithm for minimum free energy structure [14], McCaskill's algorithm for the partition function [20], and the sampling method of Ding and Lawrence [24], do not permit any modification of the input sequence during their execution and thus cannot investigate the mutation landscape of a sequence, except by exhaustive enumeration of all mutated sequences. Indeed, the highly original work on neutral networks due to Peter Schuster and the Vienna group [26]?[28] reposes on such experiments where <software>RNAfold</software> is applied to all 4n many RNA sequences of length n. The theory of neutral network is still an active area of research?see the recent review of Cowperthwaite and Meyers [29]. It follows that <software>RNAmutants</software> could be useful for further studies of these networks.<br>
Consequently, except for small exhaustive enumeration studies, such as in the work of Barash [25], no group has been able to answer questions like the following. What is energetically the most favorable secondary structure adopted by an arbitrary k-point mutant, possibly subject to preserving the location of specific binding sites and possibly constrained by requiring certain positions to be paired resp. unpaired? If an RNA molecule is under evolutionary pressure to adopt a low energy structure, subject to certain constraints (binding site, catalytic core), then which positions are most likely to be mutated and what is the consensus sequence and secondary structure of the low energy ensemble.<br>
There may be objections to what may seem to be yet another thermodynamics-based RNA structure algorithm that we present in this paper, since it is known that RNA secondary structure prediction algorithms that incorporate comparative genomics (multiple structural alignments) generally predict structure more accurately than do single-molecule, thermodynamics-based algorithms such as <software>mfold</software>, <software>RNAfold</software>, and <software>Sfold</software>. See work of Gardner and Giegerich [30], who show for instance the more accurate performance of <software>Pfold</software>, a program of Knudsen and Hein [18] that depends on an explicit evolutionary model and a probabilistic model for structures.<br>
There are two answers to this objection. First, our program <software>RNAmutants</software> performs computations and admits biological applications that no other software can realize, regardless of whether the software is based thermodynamics or comparative genomics. Second, recent findings of the encode project consortium [3] indicate that the human genome is ?pervasively expressed,? with many RNA transcripts of unknown function having no homology to known RNA families. While comparative genomics has successfully been used to investigate the structure and evolution of RNAs for which reliable multiple alignments exist, only thermodynamics-based methods can be applied to novel RNAs, such as those reported by the encode consortium. Given the existence of highly reliable, multiple structural alignments of RNAs of the same class, it makes sense to apply comparative genomics methods, such as <software>Pfold</software> of Knudsen and Hein [18], the phylogenetic stochastic context-free grammar (phylo-SCFG) program <software>EvoFold</software> of Pedersen et al. [11], or the Bayesian MCMC program <software>SimulFold</software> of Meyer and Mikl?s [12]. In the absence of highly reliable multiple alignments, such as with the raw data of the encode consortium, thermodynamics-based algorithms are not just the only alternative, but such algorithms in general perform rather well. Indeed, on average, the predicted MFE structure contains 73% of known base-pairs when tested on domains of fewer than 700 nt; cf. Mathews et al. [31].<br>
In previous work [32], we introduced a novel formal grammar framework (AMSAG) to compute the ?-superoptimal structure. By ?-superoptimal structure, we mean the minimum free energy (MFE) structure among all sequences ?? with a string edit cost of at most ? from the input sequence ? (i.e., ?? such that d(?,??)?? for a given edit distance d). Hence, in principle AMSAG can handle any edit operation (e.g., mutation, insertion, and deletion). However, in addition to the difficulty of estimating good edit costs, the time required to compute the ?-superoptimal structures can be prohibitive, even for small values of ?.<br>
To overcome these problems, in subsequent work [33], we refined the problem by restricting our sequence search space to k-mutants (i.e., sequences differing of exactly k mutations with the input sequence). This simplification allowed us to design and implement an efficient algorithm to compute the partition function over all secondary structures of all k -point mutants, with respect to the Nussinov energy model [34]. (The Nussinov energy model ascribes an energy of ?1 per base pair, while ignoring any destabilization due to loops. In contrast, the Turner energy model [13] ascribes experimentally measured, context-dependent free energies for base stacking, as well as positive, destabilizing free energies for various types of loops: hairpins, bulges, internal loops and multiloops. It is well-known that the Nussinov energy model is too simplistic to permit reasonable applications of the kind presented in this paper.)<br>
However, due to AMSAG's generality, it is technically difficult to incorporate the full Turner energy model [13] into the AMSAG framework. In order to circumvent these difficulties, we have designed new multiple recursions, allowing for a technical breakthrough to develop <software>RNAmutants</software>, a unified algorithm to compute the minimum free energy structure MFE(k) and partition function Z(k) over all k-point mutants of a given RNA sequence, even admitting constraints of two forms?sequence identity constraints (certain positions, such as those known to be important for protein binding are not allowed to mutate), and structural constraints (certain positions are required to pair or to be unpaired). <software>RNAmutants</software> uses the state-of-the-art Turner energy model [13] without dangles. (A dangle is a single-stranded nucleotide, occurring either 5? or 3? to a base pair. This energy model corresponds to <software>RNAfold</software> -d 0 in the <software>Vienna RNA Package</software>. In a first implementation with dangles, the computational overhead caused by including dangles was so prohibitive that we decided not to implement them in the final version of <software>RNAmutants</software>.)<br>
Using our partition function, we explore the mutation landscape of a given RNA sequence by sampling not from the uniform distribution of k-point mutants, but rather from the Boltzmann distribution of low energy k-point mutants.<br>
<software>RNAmutants</software> naturally extends the classical RNA secondary structure model. Instead of considering the set of secondary structures that can be built on the input sequence alone, as do mfold, <software>RNAfold</software>, and <software>Sfold</software>, we consider all secondary structures of all sequences with at most k mutations. In other words, given an RNA sequence of length n and an integer kmax?n, we compute the partition function Zk over all secondary structures of all k-point mutants, for all 0?k?kmax. When k?=?0, we obtain McCaskill's partition function. The approach is illustrated in Figure 1.<br>
We then extend the range of techniques developed in previous work [32],[33] for mutant RNAs and present a sampling algorithm allowing us to sample mutant sequences, together with their sampled secondary structure, from the low energy ensemble. A novelty of our algorithm is to sample mutations according to their weight in the Boltzmann ensemble. This result generalizes the RNA secondary structure sampling algorithm of Ding and Lawrence [21]. From sampling, we derive a novel method to predict mutations disrupting the secondary structure of the original sequence (a.k.a. deleterious mutations).<br>
Here, we provide a technical breakthrough far beyond brute force computational techniques in the work of Barash [25] and of Shu et al. [35]. Since there are , or roughly nk, many k-point mutants of an RNA sequence of length n, any method relying on exhaustive listing of all k-point mutants has only a limited range of applicability.<br>
We tested our algorithms on six different families of RNA sequences from Hepatitis C and Human Immunodeficiency viruses available in the <database>Rfam</database> database [9], as well as the 3? UTR of GB virus C. We then compared our results with experimental studies [36]?[39], to investigate the robustness of RNA structures and the nature of deleterious mutations. We performed five types of computational experiments, thus showing the range of possibilities afforded by RNAmutants. First, we demonstrate the computational efficiency of <software>RNAmutants</software> by computing the partition function over all possible mutants (i.e., all k-mutants, for 0?k?n, where n is sequence length), and by sampling we estimate the probability of mutation of each nucleotide of the given sequence. Second, we analyze the robustness of RNA structures to point-wise mutations of the wild-type sequence, over a collection of 2806 sequences taken from five different families of RNA elements from hepatitis C virus (HCV) and human immunodeficiency virus (HIV). From our analysis of HCV and HIV, we make some observations concerning possible application to RNA gene discovery and drug design. Third, using previously published experimental results [39], we evaluate the accuracy of our predictions of deleterious mutation predictions for the hepatitis C virus cis-acting replication element (HCV CRE). We suggest new possible mutation sites which have not been previously detected or tested. Fourth, we show how our techniques can be used to identify regions that have been constrained during evolution to conserve patterns preserving the (functional) structure of a given RNA. In this fashion we can predict nucleotide sites likely to be under purifying selective pressure. Taken altogether, our applications of <software>RNAmutants</software> provide a better identification and understanding of those critical areas of an RNA secondary structure. Finally, by scanning of the 3? UTR of the GB RNA virus C with a fixed size frame, we show how <software>RNAmutants</software> can be used to perform genome-scale analysis and offer a novel insight inside the genome structure that cannot be achieved through other approaches. More specifically, we provide evidence that the sequence has been optimized to preserve evolutionarily conserved stem regions from a deleterious effect of pointwise mutations.<br>
<br>
Methods<br>
We present in this section the theoretical results achieved in this paper.<br>
McCaskill's Partition Function<br>
We build our algorithms upon the seminal McCaskill's recursions [20]. Hence, for the benefit of the reader, we give a brief presentation of McCaskill's algorithm.<br>
Given RNA nucleotide sequence a1,?,an, we will use the standard notation  to denote the free energy of a hairpin,  to denote the free energy of an internal loop (combining the cases of stacked base pair, bulge, and proper internal loop), while the free energy for a multiloop containing Nb base pairs and Nu unpaired bases is given by the affine approximation a+b Nb+c Nu.<br>
For RNA sequence a1,?,an, for all 1?i?j?n, the McCaskill partition function Z(i,j) is defined by ?S e?E(S)/RT, where the sum is taken over all secondary structures S of a[i, j], E(S) is the free energy of secondary structure S, R is the universal gas constant, and I is absolute temperature.<br>
Definition 1 (McCaskill's partition function)<br>
Before continuing, we remark here that in our implementation of McCaskill's algorithm and its far-reaching extension, <software>RNAmutants</software>, we parse the free energy parameters from tables of mfold 2.3 for all temperatures 0 to 100 in degrees Celsius (for reliable free energies at temperatures other than 37 ?C). <software>RNAmutants</software> also allows the user to choose to apply the newer mfold 3.0 energy parameters at 37 ?C. Affine parameters a, b, and c for multiloops are taken from mfold tables as well.<br>
With this, we have the unconstrained partition function(1)The constrained partition function closed by base pair (i, j) is given by(2)The multiloop partition function with a single component and where position ii is required to base-pair in the interval [i, j] is given by(3)Finally, the multiloop partition function with one or more components, having no requirement that position i base-pair in the interval [i, j] is given by(4)See Figure 2 for a pictorial representation of the recursions of McCaskill's (original) algorithm [20]; note that the recursions are are not quite the same as those given in [15].<br>
<br>
Partition Function for Mutant RNA<br>
We now turn to our mutational partition function and show how to generalize the original McCaskill's recursions.<br>
In the following, a base pair between nucleotide ai and aj is denoted by the ordered pair (i, j). When we wish to consider the nucleotides of this base pair, we write ?x, y?, where x?=?ai, y?=?aj. In short, round brackets connote nucleotide positions, while angle brackets connote nucleotides.<br>
Since we consider mutations, we need to introduce energy parameters for hairpins, stacked base pairs, bulges, and internal loops, in which nucleotides and sometimes their neighboring nucleotides are explicitly given. Parameters for multiloops remain unchanged. This is done in the following definition.<br>
Definition 2 (Generalized free energy parameters)<br>
Let x,x?,y,y?,u,u?,v,v? denote nucleotides, and ?, ?1, ?2 denote lengths.<br>
Free energy parameters used in the functions in Definition 2 come from the most current nearest-neighbor model described in [13].<br>
Our recursions require the following notation. Let  denote the set of RNA nucleotides A, C, G, and U, and let  denote the set of Watson?Crick and wobble pairs AU, UA, GC, CG, GU, and UG. The number of k-point mutants of a given RNA sequence of length n is clearly equal toWe use the Kronecker delta function, defined byAs well, let ?x,y?=?1??x,y. As we will see in the following, these notations allow to keep the structure of the McCaskill algorithm [20] unchanged and thus generalize its principle. In consequence, we use the same partition function arrays given in definition 1, but extend them to keep track of the number of mutations k and the nucleotides x and y at the extremities of the sequence (i.e., at index i and j). In other words, we add the fields k, x, and y to the partition function arrays.<br>
We now begin the recursion equations. Given RNA sequence a1,?,an, the k-point mutant partition function for interval [i, j] with nucleotide x at position i (ai ?=? x) and nucleotide y at position j (aj ?=? y) is given by(5)where . In the sequel, we show how to compute ZB.<br>
To compute ZB, we need first to compute the partition functions for hairpins , for stacked base pairs , for bulges , for internal loops , for multiloops of exactly one component, and form multiloops of at least one component.<br>
The partition function for a hairpin is given by(6)where . The partition function for a stacked base pair is given by(7)The partition function for a bulge is computed by summing over all possible opening base pairs ?u, v??B at one extremity of the bulge, over all bulge sizes b, and over the number m of mutations in the bulge. The location of the bulge (left or right) must be distinguished. To simplify the notations we let ? denote j?i?3??.(8)where  and . The recursion associated with an internal loop is an extension of that for bulges. We sum over all possible base pairs ?u, v??B at the extremity of the internal loop and consider all possible nucleotides x?, u?, v?, y? adjacent to the base pairs defining the loop. All possible lengths for the left (?1) and right (?2) portions of the internal loop are considered, and we distribute 0?m?min(?1 + ?2,k) mutations within the loop, the remaining mutations left for the component closed by (u, v). Since there are special energy parameters for 1?1, 1?2, 2?1 (and 2?2) internal loops, these cases are treated independently; i.e., when x??=?u? or v??=?y?. For readability, we suppress these latter loop details, although they are handled correctly in the program <software>RNAmutants</software>. Denote ?? ?=? j?i?7??. The partition function for internal loops is given by(9)where .<br>
We now focus on the formation of multiloops, first considering the computation of ZM1 for multiloops having a single component. The definition of ZM1(k,i,j,x,y) requires that position i base-pair in the interval [i, j], so we consider all intermediate positions i&lt;r?j which might base-pair with i, and distribute the required k mutations among the component closed by (i, r) and the unpaired bases in the interval [r+1, j]. This yields(10)Now we consider the partition function ZM (k,i,j,x,y) for multiloops of one or more components, without the requirement that position i base-pair. There are two cases to consider. First, we determine an intermediate position i?r&lt;j for which there is a multiloop with exactly one component closed by base pair (r, s) for some r&lt;s?j, and all bases in the intervals [i, r?1] and [s+1, j] are unpaired. This case is handled by a recursive call to ZM1, where we distribute the k mutations among the intervals [i, r?1] and [r, j]. In the second case, we determine a multiloop of one component closed by a base pair of the form (r, s) where i&lt;r&lt;s&lt;j and recursively consider the multiloop on the interval [i, r?1]. Again, k many mutations must be distributed between the left and right multiloops. This yields the following(11)where  and .<br>
We can now formalize the recursion for the constrained partition function ZB(k,i,j,x,y) closed by base pair (i,j). This function is defined by(12)where .<br>
For a given RNA sequence of length n, we define the partition function for k-point mutants byFinally, given a length n RNA sequence, the (complete) partition function for mutants is given byFigure 2 illustrates these recursive equations using Feynman diagrams. Drawing on analogous notions from thermodynamics, we may consider McCaskill's partition function [20] to be over the canonical ensemble of all secondary structures of a given RNA sequence, while the (complete) mutant partition function is over the grand canonical ensemble of all secondary structures of all mutants of the given sequence.<br>
<br>
Computational Complexity<br>
The computation of the complete partition partition of the grand canonical ensemble of a sequence of length n is achieved in time O(n5) and space O(n3). Compared to the original complexity of the McCaskill partition function algorithm (O(n3) in time and O(n2) in space), the increase of the complexity in space can be imputed to the necessity to add a parameter in the dynamic array to memorize the exact number of mutations occurring between two index i and j. While the increase in the time complexity results from the enumeration of all configurations obtained from the concatenation of these two arrays in Equation 11.<br>
In practice the enumeration of the eight index at the extremities of the internal loops in Equation 9 generates a large constant overweighting this recursion. The growth of the weight of this phenomena in the time complexity saturates once more than eight mutations are performed since no more mutation can be performed in the configuration. However, the constant remains large and for usual RNA sequence lengths (few hundreds) the time complexity may be dominated by this term.<br>
Curves illustrating time performances of <software>RNAmutants</software> in function of the number of mutations performed for a fixed size input or of the length of the input sequence are given in Figure 3. Figure 3A shows the time required for each value of k for a 37 nucleotide sequence (Hepatitis C virus stem-loop IV). Statistics have been computed for the 110 sequences of the Rfam seed of the Hepatitis C virus stem-loop IV.<br>
Figure 3B shows the time required to compute the complete partition function over all mutants of a given length N. We computed the statistics over five random sequences of size 0?N?37. The experimental complexity progressively converges toward the theoretical bound of O(n5). The gap observed between the two curves for small values of N can be explained by (i) the combinatorial explosion of the internal loops configurations detailed above and (ii) the fact that the maximum length of internal loops is not reached. (This upper bound is usually set to 30 and is used to justify a time complexity of O(n5).)<br>
<br>
Sampling RNA k-Mutants<br>
The sampling procedure follows the classical stochastic backtracking method introduced by Ding and Lawrence [21]. Complexity improvements using the boustrophedon technique recently introduced by Ponty [40] may also be adapted, but for purposes of clarity, such improvements are not discussed here. (In work of Ding and Lawrence [21], sampling RNA secondary structures, given the McCaskill partition function, has worst-case run time O(n2), where n is RNA sequence length. In contrast, Ponty [40] shows how the boustrophedon sampling method requires run time O(n log n) in the worst case. In addition, Ponty proves an average-case run time improvement from O(n ?n) to O(n log n).)<br>
The main novelty of our sampling algorithm is that in addition to a sample secondary structure traditionally output by RNA sampling algorithms [21],[40],[41], it also outputs a sample k-mutant RNA sequence. Indeed, the algorithm will output of a series of sequences with k mutations, together with secondary structures for these sequences.<br>
Once the partition function is computed and the dynamic programming tables are filled, we proceed to a stochastic backtracking using the values stored in the arrays, together with the equations given in the previous section, to (randomly) decide which parameters will be used for each recursive calls.<br>
The algorithm uses three functions to sample each basic type of secondary structure motif (e.g., exterior loop, stem and multiloop). An overview of the complete procedure is given in Figure 4. The process starts by randomly choosing the initial parameters x (the leftmost nucleotide) and y (the rightmost nucleotide) and eventually k (number of mutations). In contrast, if desired, for fixed value of k, one can sample precisely the Boltzmann weighted k-point mutants. The probability of such a configuration is given by . Then, we sample the exterior loop with the function sampleExteriorLoop and recursively call the function sampleStem to build each type of loop (i.e. hairpin, stacked pair, bulge and internal loop). An exception is for multiloops which use the function sampleMultiLoop. The recursions end each time when a hairpin is created inside the function sampleStem.<br>
<br>
Using Sampling To Predict Deleterious Mutations<br>
A deleterious mutation in RNA is a nucleotide mutation which alters the structure or function of the molecule. For example, the catalytic core of the Tetrahymena thermophila group I intron contains a well-defined guanosine binding pocket, whose geometry depends on the secondary and tertiary structure adopted by the intron. Disruption of binding ability caused by a mutation leading to a different structure would be termed deleterious.<br>
The prediction of deleterious mutations has recently emerged as a useful and promising research direction [25],[35]. With the exception of the present paper, all current techniques rely on exhaustively enumerating all possible pointwise mutants, followed by the application of available software such as <software>mfold</software> [22], <software>RNAfold</software> [23], or <software>Sfold</software> [24]. Unlike the approach using <software>RNAmutants</software>, such approaches are limited and cannot be applied to long sequences and/or with more than one or two mutations. Consequently, such traditional approaches could well miss potentially critical mutations or groups of mutations.<br>
Our method is described as follows. Given a wild-type sequence and its native structure (by native structure, we mean either the secondary structure inferred from the X-ray crystal, or in the absence of crystal structures, the secondary structure inferred by comparative sequence analysis. Often we take the <database>Rfam</database> consensus structure as the native structure), we use <software>RNAmutants</software> to sample an ensemble of 1000 k-point mutant sequences and their structures, for each value of k, from 0 to the maximum number of mutation allowed, denoted by kmax. (If not stipulated as part of the input, then kmax?=?n.) To ensure the pertinence of our approach, we first verify that the centroid secondary structure at level 0 (i.e., no mutation) is close to the native structure. Here, by centroid structure, not to be confused with <database>Rfam</database> consensus structure, we mean the secondary structure consisting of those base pairs, whose frequency of occurrence in the sampled set is strictly greater than 0.5. Then, at each level 1?k?kmax, we probe the samples and extract the sequence and structure such that the base pair associated with the mutation does not belong to the native structure. Alternative experiments or more flexible criteria can be adopted, but the latter seemed to give the best compromise between the number of candidates and the relevance of the structural deterioration.<br>
We measured the deleterious effect of a base pair in the mutant structure, which does not occur in the native structure, by using a value called the break number. The break number is computed as the number of base pairs that must be removed from the native structure to prevent the formation of a pseudoknot or base triple, if we force the presence of the base pair created by the mutation. In this fashion we quantify the deleterious effect induced by the newly created base pair. A break number of 0 indicates that the new base pair is compatible with the native structure and does not create any pseudoknot or base triple. In lieu of measuring break number, we could have computed the base pair distance between mutant and native structure; however, two topologically very similar structures can have large base pair distance. For instance, both of the structures<br>
GGGGGGGGACCCCCCCC?GGGGGGGGACCCCCCCC<br>
((((((.....))))))?.((((((....))))))<br>
are very similar, and both have free energy of ?13.80 kcal/mol, yet their base pair distance is 12. For this reason, we introduce and use break number.<br>
Deleterious mutations extracted from the sample set are ranked according to their deleterious effect, i.e., in decreasing order, sorted by break number. A ranking based on the frequency of occurrence of the mutation would not have been necessarily a wise choice. Indeed, this approach would have highlighted those mutations that lower folding energy, since these would the largest weight in the Boltzmann ensemble. Deleterious mutations that break the native structure do not necessarily improve the MFE in the first steps and hence would appear with a lower frequency in the sample set.<br>
<br>
<br>
Results/Discussion<br>
We present here the results of our computational experiments, compare them with previously published experimental results, and discuss their significance.<br>
Evaluation of the Nucleotide Mutation Propensity and Exploration of the Complete Mutation Landscape<br>
We illustrate in this section the computational efficiency of <software>RNAmutants</software> by exploring the full mutation landscape of a family of RNA sequences (i.e., we compute the partition function  for all 0?k?n). By sampling, we estimate the probability of mutation of each nucleotide by evaluating its effect on the thermodynamic stability of the structure of all k-mutants. Additionally, we compute the MFE and the free ensemble energy for all k-mutants.<br>
We tested our software on 110 sequences of Hepatitis C virus stem-loop IV (HCV SLIV), each comprising 37 nucleotides, taken from the seed alignment of <database>Rfam</database> [9]. For each sequence, we compute the (complete) partition function over all possible mutants. In the case of the Hepatitis C virus stem loop IV, this represents a total of  (? 1.9?1013) sequences. Then, for each sequence and each value of 1?k?37, we sample 1,000 k-point mutants and structures. Per HCV SLIV sequence, this procedure requires about 3 h on a 2.6 GHz AMD 64 byte processor with 250 Mb. The same operation is of course impossible using any classical software such as <software>mfold</software> [14] or <software>RNAfold</software> [15].<br>
We show the results in Figure 5. Figure 5A depicts the mutation profile, which gives the probability of mutation of a residue at a level k (i.e., among all k-point mutants). Here, the profile is displayed as a 37 ? 37 matrix with position in the sequence (sequence index) on the x-axis and the level k on the y-axis. The probability of mutation observed over samples is represented as a gray level. A probability of 1 is displayed as a black entry while a probability of 0 is displayed as white. Below the matrix, we also give the sequence logo and the consensus secondary structure from the <database>Rfam</database> seed alignment.<br>
The mutation profile allows us to identify fragile and robust positions in the sequence. In the case of Hepatitis C virus stem-loop IV (HCV SLIV), the secondary structure given by the consensus <database>Rfam</database> seed alignment is a single stem with a tight hairpin loop, without any structural irregularity such as a bulge or internal loop. Such a secondary structure for HCV SLIV is energetically favorable and cannot be drastically improved. Thus, the mutations will tend to conserve the structure and improve the base stacking free energies, while preserving the same base-paired positions. Since the stacking of GC base pairs provides the lowest stacking free energy, all non-GC base pairs will tend to be substituted by GC in the first steps. The sequence logo in Figure 5A confirms this intuition, showing that positions with a clear preference for the nucleotide U, and base-paired with a G in the consensus structure are the first to mutate. Subsequently the nucleotide A tends to be affected, while C and G are relatively well conserved.<br>
All columns present a strictly monotone gradient of color from white to black, thus suggesting that preferred mutation sites are independent and ordered. In addition, the mutation profile shows an alternation of white columns (groups of residues which start to mutate with small value of k) and black columns (groups of residues which mutate late). Here, it appears that base-paired positions evolve simultaneously (see, for instance, the motif AU at index 13?14 and UA at index 22?23), presenting examples of compensatory mutations. This phenomenon reveals a stability in the base-pairing of the regions involved, certain to be of interest in RNA design.<br>
In Figure 5B we plot the superposed curves of k-superoptimal free energies and k-mutant ensemble free energies, as computed by <software>RNAmutants</software>; the x-axis represents the number of mutations and the y-axis the energy in kcal/mol. Here, k-superoptimal free energy is defined as the minimum free energy (MFE) over all mutants having k mutations [33], while k-mutant ensemble free energy is defined by ?RI log (Zk). (In work of Waldisp?hl et al. [32] and Clote et al. [33], the k-superoptimal structure is defined to be the MFE structure over all ?k-point mutants, while in the present paper, it is defined to be the MFE structure over all k-point mutants. The current usage seems more appropriate.) These results provide a novel insight into preferential mutation sites as well as structural impacts caused by mutations.<br>
We now analyze the curves of Figure 5B. While the ensemble free energy curve resembles a parabola, the superoptimal free energy curve shows three distinct regions (k?5, then 6?k?17, and 18?k), each having a linear appearance. Each region reflects the phenomenon described above. From k ?=? 0 to k ?=? 5, the GU base pairs are progressively substituted by GC and the slope is roughly equal to the difference of the stacking free energies associated with both base pairs. Then, the region from k?=?6 to k?=?17 is associated with the substitution of AU base pairs by GC, which now requires 2 mutations with a smaller gain of energy. Other optimizations, such as the reordering of nucleotides G and C inside the stem, only bring minor energy improvements and are then performed in the last region (18?k) which presents a flat free energy profile. The characteristic shape of the superoptimal energy curve may be of interest for characterizing sequences that require an optimal secondary structure.<br>
Interestingly, the 5-nucleotide hairpin is very well conserved over sample centroid structures (base pairs with a frequency &gt;0.5 in the sample set?data not shown), even for large values of k. Indeed, a tetraloop hairpin might have been expected, due to the energy bonuses assigned to GNRA-tetraloops. This suggests that evolutionary pressure might have designed the sequence to prevent any slippage in the formation of the helix.<br>
Since the secondary structure is conserved throughout the sampled ensemble, the following questions arise. What function is required by those structural motifs that are preserved in the sampled ensemble? Why did evolution not select a thermodynamically more stable secondary structure in such cases? Our ability to compute, for the first time, the complete mutation landscape for a given RNA sequence, makes <software>RNAmutants</software> a fundamental tool to address such questions. By using <software>RNAmutants</software> in computational experiments, such as those just described, we can determine putative functionally important motifs and structures that can be subsequently tested experimentally. RNAmutants could lead to important breakthroughs in our understanding of the remarkable combination of robustness and fragility of RNA structures [42].<br>
<br>
Evaluation of the Secondary Structure Robustness Highlights Differences between Families of RNAs in Hepatitis C and HIV Viruses<br>
Estimating how robust a secondary structure is to mutations can be of interest for the characterization of functional RNAs. Here, by sampling structures, we evaluate the conservation of the <database>Rfam</database> consensus structure in the k-mutant ensembles, and compare the results obtained from five different families of RNA from Hepatitis C and HIV viruses. These computational experiments highlight major differences between these RNA families and suggest potential application in RNA design.<br>
The method proposed here first samples 1,000 k-point mutant sequences and structures for 0?k?5. To quantify robustness, we compute two notions of distance. First, for each sampled structure S, we compute the base pair distance between S and the native secondary structure S0, and thus determine the average over all sampled structures, called average distance in the following. Second, we compute the base pair distance between S and the sample centroid Sc, where the latter is defined to consist of those base pairs occurring in strictly more than half the sampled structures. (In work of Ding et al. [43], the sample centroid is called the Boltzmann centroid, when sampling over all secondary structures using <software>Sfold</software> [24].) This distance is called the centroid distance in the following.<br>
A small average distance means that most sampled structures are identical to the native structure (this entails a small centroid distance as well). A large average distance with a small centroid distance indicates that the core of the native structure is conserved in the sampled structures, while most sampled structures differ from the native structure with respect to a number of base pairs. In this case, the nonnative base pairs in the samples are not well conserved over the ensemble of sampled structures, hence do not appear in the centroid structure. In contrast, a large centroid distance indicates that the same nonnative base pairs are present (or missing) in the majority of sampled structures.<br>
To benchmark robustness, we used (seed) multiple sequence alignments from <database>Rfam</database> [9]. We selected five RNA elements associated with Hepatitis C and human immunodeficiency viruses, each of which is reasonably well predicted by the nearest neighbors energy model, using <software>RNAmutants</software> with 0 mutations, or (equivalently) <software>mfold</software> [22] or <software>RNAfold</software> [23] without dangles. The resulting dataset contains a total of 2,806 sequences. By native secondary structure, we mean the Rfam consensus structure from the multiple sequence alignment. Results are given in Table 1.<br>
The structures sampled from the RNA elements of Hepatitis C virus are close to the native structure, while those of human immunodeficiency virus have more base pairs than the native structure. Nevertheless, the centroid structure for samples generated by <software>RNAmutants</software> is reasonably close to the native structure; i.e., centroid distance for RNA elements from HIV is small.<br>
The Hepatitis C virus stem-loop IV (HCV SLIV) is accurately predicted by minimum free energy methods, i.e., Zuker algorithm [14], and despite its small size (35 nucleotides) and large number of base pairs (15), HCV SLIV is also very well conserved in the ensemble of mutants generated by <software>RNAmutants</software>. These results suggest that the RNA nucleotide sequence of HCV SLIV has been thermodynamically optimized and is robust with respect to mutations. In contrast, the secondary structure of sampled mutants of Hepatitis C virus cis-acting replication element (HCV CRE) is increasingly divergent as the number of mutations increases. The secondary structure of wild-type HCV CRE sequence is very well predicted by energy minimization methods. The centroid structure of samples generated by <software>RNAmutants</software>, for one to three mutations, changes little and remains very close to the native structure, even if most of the sampled structures have more base pairs than that of the native structure. However, when four or more mutations are allowed, another structure, significantly different from the native one, emerges from the ensemble generated by <software>RNAmutants</software>. This result suggests that HCV CRE has been optimized to resist only a few mutations. This remark suggests the use of <software>RNAmutants</software> to detect those sequences whose structure is locally optimized.<br>
At level 0 (no mutation allowed), in spite of a higher average distance, the centroid structure of the ensemble of samples of HIV RNA elements remains close to the native structure. Interestingly, average distance remains approximately constant when the number of mutations increases. This number even decreases for human immunodeficiency virus primer binding site (HIV PBS). Here again, analysis of mutants generated by <software>RNAmutants</software> seems to confirm the optimization of these sequences to support some functional secondary structure. We note that with similar characteristics (length and number of base pairs) the human immunodeficiency virus frameshit signal (HIV FE) structure appears to be more robust with respect to mutations than is the Hepatitis C virus cis-acting replication element (HCV CRE). The centroid structure of human immunodeficiency virus primer binding site (HIV PBS) seems well conserved. In analogy to the phenomenon observed for the Hepatitis C virus cis-acting replication element (HIV CRE), the average distance increase suggests that an alternate structure will emerge as the number of mutations increases. Summarizing, we feel that the combination of average and centroid distance measurements is a reasonable tool to estimate the robustness of a structure under mutational variation of a sequence.<br>
Remarkably, the average and centroid distances are very well conserved in human immunodeficiency virus gag stem loop 3 (HIV GSL3), in spite of the huge hairpin loop (69 nucleotides) and very small stem (8 base pairs). One must bear in mind that <database>Rfam</database> consensus structures indicate only those base pairs that are inferred by covariation. It follows that many base pairs may not appear in the centroid structure, such as the 69-nt hairpin. Each of the 8 base pairs in HIV GSL3 is a GC pair, which means that this stem region is not optimized by <software>RNAmutants</software> for the small numbers of mutations k. This supports the idea that the mutation robustness of the hairpin loop sequence is optimized.<br>
<br>
Prediction of deleterious mutations in Hepatitis C virus cis-acting replication element<br>
In this section, we predict deleterious mutations in Hepatitis C virus cis-acting replication element using the method described in section Using sampling to predict deleterious mutations in Methods. We confirm our results by comparing our predictions with previously published experimental results [39]. Moreover, our computational experiments suggest new deleterious mutations which have not been predicted or tested before.<br>
We performed computational experiments with Hepatitis C virus cis-acting replication element (HCV CRE), known to be essential for viral replication. Figure 6 depicts the secondary structure of HCV CRE. To validate our predictive results, we used mutagenesis data from experiments of You et al. [39]. To simplify exposition and enhance clarity of results, we focus our investigation on the prediction of single point deleterious mutations (i.e., kmax?=?1), although of course <software>RNAmutants</software> can be used to infer deleterious noncontiguous groups of mutation sites. Results are given in Figure 7. The top line gives the native secondary structure of the RNA element, while the following lines contain 1-point mutants sampled by <software>RNAmutants</software>. For each pointwise mutant, we display the base pair associated with the mutation, the mutation type (index and nucleotide substitution), the index and the type of the nucleotide that can be associated with the concerned base pair, the frequency of this mutation and the break number.<br>
Here, the HCV CRE sequence has a length of 47 nucleotides, which is slightly shorter than those given in the <database>Rfam</database> multiple alignment. Also, we note a shift of 53 positions between the index of our sequence and those used in [39].<br>
Our results predict the mutation U33G (U86G according to the notation used in [39]) to be the most deleterious. This prediction is confirmed by [39]. In this study, You et al. observed that the mutant C84A/U86G is not viable, while C84A/U86A is still functional. Additionally, it was observed [39] that the mutation U86G is responsible for the alteration of the upper helix (subsequence from nucleotide 8 to 31 in Figure 7) and hence deleterious. However, their results also showed that the single point mutation U86G is still viable, suggesting that this mutation must be supported by C84A to be deleterious. In fact, C84A is suspected to alter the stability of the upper helix, amplifying the ability of U86G to disrupt the structure. The slight overestimation of the deleterious potential of U86G is due to the quality of the energy model used by RNAmutants. Without dangles, the centroid structure is effectively altered by U86G, while with dangles, the mutation C84A is required to disrupt the upper helix (data not shown). The difference is then due to the absence of dangles in the energy model of <software>RNAmutants</software>. However, the deleterious effect of U86G is correctly detected.<br>
The non-viability of other mutants studied in [39] (U71C, C74U, A75U/G76C/C77U, C77U, C90A/A92G, A92G, and C90A) is not attributed to a significant alteration of the native secondary structure. <software>RNAmutants</software> predicts a few other deleterious mutations (with a lower impact)?these are discussed in the following.<br>
The next four deleterious mutations can be grouped in a cluster involving the base pairs (11, 35) and (11, 36). When looking at the 348 sequences in the <database>Rfam</database> seed alignment, it appears that 30 sequences have the mutation C36U, 3 the mutation C35U and 1 the mutation A11G. EMBL accession numbers for the <database>Rfam</database> sequences and the mutations found are shown in Table 2.<br>
The 33 sequences mutating at index 35 and 36 have also several other significant mutations. Most of these mutations are similar. Assuming that these mutants are viable, this suggests that some of these additional mutations offset the deleterious effect of C35G or C36G. A complete analysis of all these sequences would be too demanding, but we can illustrate this phenomenon by looking at the 3 sequences associated with C35G.<br>
Three mutations (A15U, C25G, and A39G) are found simultaneously in all occurrences of C35U. The mutations C25G and A15U are located at the extremities of the hairpin loop in the native structure, and more specifically, C25G creates a potential base pair with nucleotide U at index 14 (and potentially also with nucleotide U at index 15). We conclude that these two mutations tend to stabilize the upper helix and counterbalance the deleterious effect of C35G. The role of A39G remains more obscure. In [39], You et al. observed that this mutation (A92G in the paper) is lethal. However, structure probing did not reveal any irregularity in the cleavage product of this RNA, suggesting that the sequence of the bulge is affected rather than the global structure. A potential structural use of this mutation would be to prevent the creation of base pairs supporting the disruption of the upper helix through C35G. An analysis using the thermodynamic model with <software>RNAmutants</software> tends to support this hypothesis.<br>
An interesting case is for A11G which occurs, in a single sequence (AF054264.1:326?376) from the <database>Rfam</database> seed alignment, together with A1G. This has been reported as one of the clones used in [44]. From this study, it remains unclear how replication is affected by these mutations; however, the possibility of a deleterious effect of A11G is potentiality (indirectly) supported by this work.<br>
The following group of predicted deleterious mutations involves the nucleotide C at position 29, either directly (C29G) or indirectly through a base pair (C19G, U16G, C12G). If no specific analysis has been performed for this mutation, it appears that C29G can be found in two nonviable mutants (5BSL3.2 mutA and 5BSL3.2 mutB) in [39]. The deleterious nature of C29G has not been validated, but the destabilization effect of this mutation in the upper helix is suggestive.<br>
Other minor mutations which do not disrupt the native structure are identified. With a break number of 0, these mutations cannot be considered as deleterious. However, some of them have been detected to alter replication (C74U and C77U or C21U and C24U in our notation) in [39]. One potential explanation suggested by our results is that the local structure of the hairpin loop is affected, rather than that of the global secondary structure.<br>
<br>
Scan of HIV trans-activation response reveals regions under evolutionary pressure<br>
In this section, we show how <software>RNAmutants</software> can be used to detect regions of the sequence which have been optimized during evolution. We restrict mutations to a 3-nucleotide frame and slide the latter on sequences. The frames associated with an alteration of the functional structure in 3-mutants are most likely optimized to preserve the structure, and are thus identify under a purifying selective pressure. Our results reveal critical regions in the trans-activation response element of the human immunodeficiency virus and suggest applications for RNA drug design.<br>
For this study, we use sequences of human immunodeficiency virus trans-activation response (HIV TAR) from the HIV-1 genome. The <database>Rfam</database> seed alignment contains 426 sequences of length 57 nt with an average identity of 91%. This RNA element is critical for the trans-activation of the viral protomer and virus replication. The TAR hairpin acts as a binding site for the Tat protein and this interaction stimulates the activity of the long terminal repeat promoter. Previous studies have shown that the 3 nt bulge from index 22 to 24 is essential for binding [36]. Moreover, the 3D structure of the 6 nt apical loop (index 29 to 34) is indispensable for trans-activation of the viral protomer and virus replication [37]. This RNA element is a potentially important drug target [38]. Its consensus secondary structure is shown in Figure 6.<br>
We are interested in detecting regions which have been selected during evolution to preserve a specific pattern, for structural or functional purposes. For each sequence in the dataset, we slide an open frame and allow mutations in this region only. Then, we sample structures from this model, and measure the centroid and average distances.<br>
Here, the size of the open frame is chosen to fit the length of the bulge (i.e., three nucleotides). Larger frame sizes would result in an attenuation of the signal (data not shown). For each starting position of the frame (1 to 55), we compute the mean centroid distance and mean average distance for each sequence in the dataset. These curves are displayed in Figure 8. The secondary structure annotation is given at the bottom of each of these three graphs (one for each number of mutations in the open frame).<br>
The secondary structure can be decomposed into four distinct patterns which are: (1) a pairing (denoted S1 for stem 1) between regions (17,21) and (39,43), (2) a bulge at index (22,24), (3) another pairing (denoted S2 for stem 2) between regions (25,28) and (35,38), and (3) a hairpin at index (29,34).<br>
We look first at the curves with a single mutation inside the frame?see Figure 8A. A clear signal appears at index 35?36 and 40?41: Both curves (average and centroid) show a clear peak at these positions. The regions associated with this signal correspond exactly to the 3?-end regions involved in the two stems S1 and S2. We observe a mirror effect when the frame matches the 5?-end regions; two other peaks emerge at index 18 and 25?26. Interestingly, the magnitude of these two peaks is significantly lower than those of the first ones, indicating that the 3? regions have been potentially under a higher selective pressure.<br>
When two mutations are allowed inside the frame (see Figure 8B), the phenomenon observed above is amplified. The asymmetry between the two paired regions of S2 is almost cancelled, but not for those of S1. In addition, a clear signal now appears when the frame matches the bulge. It may also be interpreted as a signal indicating that this region has been constrained along evolution.<br>
Finally, when three mutations are performed inside the frame (see Figure 8C), the signals mentioned before can still be identified, but tend to be washed out by the noise. Indeed, when all positions in the frame mutate, the sequence is so denatured that the conservation of the secondary structure would require an optimization of the surrounding sequence. This remark is related to the observation given below for the hairpin region.<br>
Additionally, two clear peaks now appear when the frame matches the paired region of the stem S2. This may be a correction of the weakness of the signal observed in the previous graph (Figure 8B). It also confirms that both these regions may have been optimized to base-pair.<br>
For these three graphs, it is remarkable to notice that mutations inside the subsequence of the hairpin never really affect the global structure of the RNA element. It may be suggested that the sequence outside the hairpin has been optimized to prohibit any stable interaction with the central region in order to stabilize the secondary structure and facilitate the formation of the complex 3D motifs observed in [37].<br>
According to these observations, four sequence optimizations may have been performed for these sequences. The first two are for the regions paired to each other through the stem S1 and S2. This may be justified by the need for these sequences to pair to each other in order to stabilize the bulge and the hairpin lying between them. It also appears that the sequence of the bulge cannot tolerate two mutations. Our analysis suggests that evolutionary pressure has selected these nucleotides to facilitate the formation of the bulge required for the binding. Finally, the global structure does not seem to be affected by mutations inside the hairpin loop. As it has been said before, this suggests an optimization of the surrounding sequence to stabilize this loop and allow the formation of a complex 3D motif inside the apical loop.<br>
These results suggest that a method combining RNA binding predictors [45],[46] and secondary structure prediction software [14],[15] with <software>RNAmutants</software> could be a successful and promising approach for the prediction and design of functional RNAs.<br>
Scan of the 3? UTR of GB virus C reveals how evolution shaped the sequence. We conclude the results section with a series of computational experiments on the 3? UTR of the GB virus C (GBV-C). By scanning this RNA sequence, we show how <software>RNAmutants</software> can provide evidence that different regions have been optimized to conserve RNA secondary structure even in the presence of pointwise mutations. In particular, we show that the sequence has been designed to prevent deleterious effects of mutations on the evolutionarily conserved stem-loops. This work suggests potential large-scale applications of <software>RNAmutants</software> for genome-wide scanning purposes.<br>
In recent years the structure of RNA viruses in the family of Flaviviridae has received particular attention [47]. Here, we focus on the 3? UTR of the Hepatitis G virus (GB virus), a single-stranded positive-strand RNA virus with <database>GenBank</database>/EMBL accession number AB013500, whose secondary structure has been determined using both thermodynamics and evolutionarily information [48]. This 311 nt sequence has the advantage of containing a balanced number of nucleotides located within regions having an evolutionarily conserved secondary structure (167 nt), as well as outside of any region having conserved secondary structure (144 nt). The conserved secondary structure is composed of seven stem-loops numbered from SLI to SLVII.<br>
We aim to study how evolution shaped this sequence, and to provide some evidence that certain regions have been thermodynamically optimized. In a manner similar to that of <software>Vienna Package</software> program <software>RNAplfold</software> [49], we scanned the 3? UTR GBV-C RNA sequence with a moving window of fixed size, and analyzed the distribution of mutations and base pairs in k-mutant ensembles of each window.<br>
Sliding a window of size L over this sequence, we extracted 311?L+1 subsequences and ran <software>RNAmutants</software> to sample mutated sequences and their secondary structures. Here, we use the following notation. Let ? denote the complete sequence of the 3? UTR of GBV-C (length N?=?311), and let WiL denote the subsequence of size L starting at index i. Let SWiL(k,ns) denote the set of ns many k-mutant sequences and secondary structures computed from WiL. The full set of sequences scanned by <software>RNAmutants</software> is denoted by , and the sample set of ns k-mutants and structures computed from  is denoted .<br>
The probability of a base pair (i, j) in  is defined as the number of occurrences of (i, j) in the secondary structure samples divided by the number of samples computed for a sequence that can potentially form a base pair between indices i and j (e.g., WkL such that j?i&lt;L). Formally(13)This measure, motivated by that from the <software>Vienna Package</software> program <software>RNAplfold</software> [49], averages the frequency of occurrence of base pair (i, j) in the ensemble of k-point mutants, over all size L windows containing both i, j.<br>
For this set of computational experiments we chose a frame size L?=?50 and chose the number ns of sampled k-mutant sequences and structures to be 1000, for each k from 0 to 8. These values were chosen to provide a good balance between the computation speed (a bounded, yet somewhat deep search in mutation depth) and maximal range j?i&lt;L of base pair (i, j). For comparison, the default value for window size used in <software>RNAplfold</software> is 70.<br>
The first analysis aims to estimate the density of base pairs in the different regions?regions of evolutionarily conserved stems, denoted by stem region or inside region, and regions having no evolutionarily conserved stems, denoted by non-stem region or outside region. We clustered the base pair density values in five cases according to the location of each index i, j of base pair (i, j): (1) i and j are two indices belonging to the same stem region, (2) i and j are in two different stem regions, (3) i is in a stem region and j in a non-stem region, (4) i is in a stem region and j is in a nonstem region, and (5) i and j do not belong to any stem region. Then, we plotted these base pair density values with respect to the number of mutations in samples. The results computed with the parameters given above (L ?=? 50, ns ?=? 100, and 0?k?8) are shown in Figure 9. Note that the count done in the denominator of Equation 13 respects the same classification constraints and ensures normalization of the estimator values.<br>
The figure shows very distinct behavior for base pairs occurring inside the same stem region (1) versus other possibilities (2?5). As expected when no mutation is allowed (i.e., k?=?0), he base pair density appears to be higher for base pairs in stem regions. This means that these regions are more structured than the others. (This argument does not suggest that nonstem regions are not structured but only that they are locally less optimized.) However, when the number of mutations increases, all curves tend to reach an equilibrium, with approximately equal density for each of the five cases. While density for base pairs in the same stem, case 1, decreases with an increasing number of mutations, density for the other four cases increases. This phenomenon suggests that selective pressure has been applied to ensure robustness of (local) structure in the 3? UTR GBV-C RNA with respect to mutation. Putatively, an inflection in the curve of stem regions appears at roughly 4 mutations in the figure. This remark will take its importance later in the discussion.<br>
The next study aims to analyze the base pairing preferences of mutations regarding their location in the sequence. Using the same set of computational experiments, we investigated the distribution of base pairs (i, j) involving a mutation at one of their extremities (i.e., index i or j mutates). We computed the base pair probability mkL(i,j) restricted to these specific base pairs and normalized the results (i.e., we divided the base pair density by the number of mutations allowed in the sample set)(14)Then, we clustered the results according to the same classification of base pairs as above and computed the base pair density in each cluster. Results are shown in Figure 10. For clarity of discussion, in the left panel of this figure we plotted the curves associated with a mutation occurring in the stem region (Figure 10A), while the right panel displays the curves associated with a mutation occurring outside the stem region (Figure 10B).<br>
Figure 10A reveals that in the close neighborhood (small number of mutations) of the wild sequence, the mutations occurring in a stem region base-pair preferentially inside the same stem region. An increase in the number of mutations has very different consequences on the density of base pairs in the different clusters. In agreement with our previous observations, the number of mutations created inside the same stem region decreases. In contrast, if the densities increase in the two other cases, we observe a clear preference for creating a base pair outside any other stem region. Indeed, while the behavior of the two curves (base-pairing in another distinct stem region, and outside) have similar behavior for small number of mutations, it turns out that roughly beyond 4 mutations, more mutations tend to base-pair outside and ?protect? as much as possible the cleavage between the stem regions.<br>
Symmetrically, when few mutations are performed outside the stem regions (cf. Figure 10B), we observe a clear preference for base pairings in the same region, thus preserving the stems from destabilization by mutations occurring in the nonstem regions. However, in agreement with previous observations, larger numbers of mutations tend to progressively equilibrate the distributions by increasing the base pair density of mutations base pairing in stem regions. This observation suggests that non-stem regions have been constrained to prevent mutations from interacting with stems to disrupt the structure.<br>
We now investigate the distribution of mutations that increase the base pairing probability (called base pair increasing mutations), versus those that decrease base pairing probability (called base pair decreasing mutations). To evaluate the evolution of these probabilities from one level of mutation k to the next k+1, we compare the local base pairing probabilities pk (i, j) computed from  (e.g., sample set with k mutations) with those computed from . Then, we estimate the difference pk+1 (i, j)?pk (i, j), subsequently called the differential probability. We show the corresponding curves in Figure 11, where the results have been once again classified into five clusters.<br>
The distribution of base pair increasing mutations (cf. Figure 11A) presents some interesting features. Indeed, when a single mutation is performed, we first observe a tendency to stabilize the structures already existing in and out the stem regions, thus conserving the existing structure of the full 3? UTR GBV-C RNA sequence. However, afterward, an increased number of mutations tends to be more favorable to mutations strengthening the base pairs between a stem and a non-stem region. Simultaneously the probability of mutations favoring base pairs inside stem regions increases to a lesser extent. Interestingly, if the probability of base pair increasing mutations for bases occurring between two distinct stem regions seems also to increase for small values of k, it turns out that these probabilities tend to remain identical afterwards (e.g., the differential values decrease).<br>
The case of base pair decreasing mutations is in fact much more interesting since essentially only base pairs inside stem regions seem significantly affected by such mutations, although single mutations appear not to have any significant effect (differential probability close to zero). The two next levels (K?=?2, 3) present a remarkable peak which completely collapses for a further increasing number of mutations (k?4). The negative values indicate that the probability of base pair decreasing mutations inside the stem are decreasing, and thus that stabilization occurs once a few mutations have been occurred to locally reorganize the structure. This clear signal could prove useful in detecting structured regions of a genome, and possibly help identify subsequences under evolutionary pressure. Interestingly, the change of sign of the differential base pairing probability in the same stem region happens for 4 mutations, which correlates with the putative inflection point in Figure 9 for the base pair density curve for the same cluster of base pairs.<br>
Finally, we study the distribution of mutations in the complete 3? UTR GBV-C RNA sequence. In complement to the previous experiments performed with a frame size of 50 nucleotides and thus restricted to local considerations, we now also provide an insight on the influence of mutations, sampled from the Boltzmann k-point mutant ensemble, on the medium and long range base pairing by including statistics computed with larger frame sizes. Using the equation 14, we estimate the mutation base pair probability in the sample set and derive the average mutation probability from these values.<br>
The average mutation probability at index i in a sample set of k-point mutants is defined as the sum of the mutation base pair probabilities mkL(i,j) (i.e., mkL(i)?=??j mkL(i,j)). Additionally, in order to investigate the influence of medium and long range base pairing on the mutation distribution, we also computed the values of mkL(i) restricted to base pairs (i, j) with |j?i|?25. Mutation profiles computed using this procedure are given in Figure 12.<br>
The distribution of the mutations inside and outside stem regions is evaluated as the sum of the mutation probabilities mkL(i) in both regions normalized by the number of nucleotides in these regions (166 in stem regions and 144 outside). The numerical results given in Table 3 summarize these statistics for the general case as well as the case of mutations involved in a medium to long range base pair, i.e., base pairs (i, j) whose extremities i, j are at a distance of at least 25 nucleotides. Average mutation rates for such medium to long range base pairs are depicted in Figure 12.<br>
Since the threshold used to filter short range base pairs may seem arbitrary, for the sake of clarity of discussion, we include graphs representing the values obtained for all possible threshold values together with the ratio of samples satisfying the cut-off in the sample set. Figure 13 illustrates these statistics. The x-axis represents the minimal base pair length while the y-coordinates give the fraction of mutations in non-stem regions (plain line) and the fraction of samples satisfying the threshold (dashed line).<br>
In this study, we used frame sizes of L ?=? 50, 100, and 150 nucleotides and computed 1,000 samples with k ?=? 1 mutation (results with 2 mutations were also computed and produced the same results). Frame sizes larger than 150 nucleotides have not been considered since only few base pairs distanced at more than 150 nt appeared in our sample sets. (See Figure 13. As shown in the supplementary Figure 1, the <software>RNAfold</software> dotplots of the full sequence confirmed this observation.)<br>
The distribution of mutations between structured (stem regions) and nonstructured regions presents a small but significant bias in the general case. When the requirement on the minimal length of base pairs is applied, this signal is strong and surprisingly clear. This observation suggests that in the fitness model [29], [50]?[52], evolution has constrained medium and long range base pairing to favor mutations outside evolutionarily conserved stem regions. This remark automatically suggests the potential usefulness of <software>RNAmutants</software> in gene discovery based on clustering of <software>RNAmutants</software> statistics. This hypothesis is the subject of current research on larger scale studies.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2658886</b><br>
Natural Image Coding in V1: How Much Use Is Orientation Selectivity?<br>
Orientation selectivity is the most striking feature of simple cell coding in V1 that has been shown to emerge from the reduction of higher-order correlations in natural images in a large variety of statistical image models. The most parsimonious one among these models is linear Independent Component Analysis (ICA), whereas second-order decorrelation transformations such as Principal Component Analysis (PCA) do not yield oriented filters. Because of this finding, it has been suggested that the emergence of orientation selectivity may be explained by higher-order redundancy reduction. To assess the tenability of this hypothesis, it is an important empirical question how much more redundancy can be removed with ICA in comparison to PCA or other second-order decorrelation methods. Although some previous studies have concluded that the amount of higher-order correlation in natural images is generally insignificant, other studies reported an extra gain for ICA of more than 100%. A consistent conclusion about the role of higher-order correlations in natural images can be reached only by the development of reliable quantitative evaluation methods. Here, we present a very careful and comprehensive analysis using three evaluation criteria related to redundancy reduction: In addition to the multi-information and the average log-loss, we compute complete rate?distortion curves for ICA in comparison with PCA. Without exception, we find that the advantage of the ICA filters is small. At the same time, we show that a simple spherically symmetric distribution with only two parameters can fit the data significantly better than the probabilistic model underlying ICA. This finding suggests that, although the amount of higher-order correlation in natural images can in fact be significant, the feature of orientation selectivity does not yield a large contribution to redundancy reduction within the linear filter bank models of V1 simple cells.<br>
<br>
Introduction<br>
It is a long standing hypothesis that neural representations in sensory systems are adapted to the statistical regularities of the environment [1],[2]. Despite widespread agreement that neural processing in the early visual system must be influenced by the statistics of natural images, there are many different viewpoints on how to precisely formulate the computational goal the system is trying to achieve. At the same time, different goals might be achieved by the same optimization criterion or learning principle. Redundancy reduction [2], the most prominent example of such a principle, can be beneficial in various ways: it can help to maximize the information to be sent through a channel of limited capacity [3],[4], it can be used to learn the statistics of the input [5] or to facilitate pattern recognition [6].<br>
Besides redundancy reduction, a variety of other interesting criteria such as sparseness [7],[8], temporal coherence [9], predictive information [10],[11] , or bottom-up saliency [12] have been formulated. An important commonality among all these ideas is the tight link to density estimation of the input signal.<br>
At the level of primary visual cortex there is a large increase in the number of neurons. Hence, at this stage the idea of redundancy reduction cannot be motivated by a need for compression. However, the redundancy reduction principle is not limited to be useful for compression only. More generally, it can be interpreted as a special form of density estimation where the goal is to model the statistics of the input by finding a mapping which transforms the data into a representation with statistically independent coefficients [5]. In statistics, this idea is known as projection pursuit density estimation [13] where density estimation is carried out by optimizing over a set of possible transformations in order to match the statistics of the transformed signal as good as possible to a pre-specified target distribution. Once the distribution has been matched, applying the inverse transformation effectively yields a density model for the original data. From a neurobiological point of view, we may think of the neural response properties as an implementation of such transformations. Accordingly, we here think of redundancy reduction mainly in terms of projection pursuit density estimation.<br>
A crucial aspect of this kind of approach is the class of transformations over which to optimize. From a statistician's point of view it is important to choose a regularized function space in order to avoid overfitting. On the other hand, if the class of possible transformations is too restricted, it may be impossible to find a good match to the target distribution. From a visual neuroscientist's point of view, the choice of transformations should be related to the class of possible computations in the early visual system. Here we assume the simplest case of linear transformations, optionally followed by a pointwise nonlinearity.<br>
Intriguingly, a number of response properties of visual neurons have been reproduced by optimizing over the class of linear transformations on natural images for redundancy reduction (for a review see [12],[14]). For instance, Buchsbaum and Gottschalk as well as Ruderman et al. revealed a link between the second-order statistics of color images and opponent color coding of retinal ganglion cells by demonstrating that decorrelating natural images in the trichromatic color space with Principal Component Analysis (PCA) yields the luminance, the red-green, and the blue-yellow channel [15],[16]. Atick and Redlich derived the center-surround receptive fields by optimizing a symmetric decorrelation transformation [17]. Later, also spatio-temporal correlations in natural images or sequences of natural images were linked to the receptive field properties in the retina and the lateral geniculate nucleus (LGN) [18]?[20].<br>
On the way from LGN to primary visual cortex, orientation selectivity emerges as a striking new receptive field property. A number of researchers (e.g., [21],[22]) have used the covariance properties of natural images to derive linear basis functions that exhibit similar properties. Decorrelation alone, however, was not sufficient to achieve this goal. Rather, additional constraints were necessary, such as spatial locality or symmetry.<br>
It was not until the reduction of higher-order correlations were taken into account that the derivation of localized and oriented band-pass filters?resembling orientation selective receptive fields in V1? was achieved without the necessity to assume any further constraints. Those filters were derived with Independent Component Analysis (ICA), a generalization of Principal Component Analysis (PCA), which aims at reducing higher-order correlations as well [8],[23].<br>
This finding suggests that within the linear filter model, orientation selectivity can serve as a further mechanism for redundancy reduction. The tenability of this hypothesis can be tested by measuring how large the advantage of orientation selective filters is over non-oriented filter shapes. The importance of such a quantitative assessment has first been pointed out by Li and Atick [22] and are the main focus of several publications [12], [22], [24]?[29]. Generally speaking, two different approaches have been taken in the past: In the first approach, nonparametric methods such as histograms or nearest neighbor statistics have been used with the goal to estimate the total redundancy of natural images [22],[27],[29]. While this approach seeks to answer the more difficult question how large the total redundancy of natural images is, the second approach compares the importance of orientation selectivity for redundancy reduction only within the class of models that are commonly used to describe V1 simple cell responses [24]?[26],[28].<br>
Using histogram estimators, Zhaoping and coworkers [22],[27] argued that the contribution of higher-order correlations to the redundancy of natural images is five times smaller than the amount of second-order correlations. They concluded that this amount is so small that higher-order redundancy minimization is unlikely to be the main principle in shaping the cortical receptive fields.<br>
Two objections may be raised against this conclusion: First, it is not clear how generally valid the result of [27] is. The study relies on the assumption that higher-order dependencies at distances beyond three pixels are negligible. More recent work based on nearest neighbor methods [29], however, finds a substantially larger amount of higher-order correlations when taking dependencies over longer distances into account. Secondly, even if the contribution of higher-order correlation was only 20% of the amount of second-order correlations, this contribution is not necessarily negligible. Several previous studies report that the redundancy reduction achieved with ICA for gray level images is at the same level at about 20% [24]?[26]. Taken together these two findings suggest that orientation selective ICA filters can account for virtually all higher-order correlations in natural images. If this was true, it would strongly support the idea that redundancy reduction could be the main principle in shaping the cortical receptive fields.<br>
In general, however, density estimation in high dimensions is a hard problem and the results reported in the literature do not fit into a consistent view. Therefore, the crucial challenge is to control for all technical issues in order to allow for safe conclusions about the effect of orientation selectivity on redundancy reduction. Here, we address many such issues that have not been addressed before. In our study, we take the second approach and focus on ?linear redundancy reduction??the removal of statistical dependencies that can be achieved by linear filtering. While most studies have been carried out for gray level images the two studies on color images find the advantage of ICA over PCA to be many times larger for color images than for gray level images with an improvement of more than 100% [25],[26]. Since it is not clear how to explain the large difference between color and gray value images, we reinvestigate the comparison between the orientation selective ICA filters and the PCA filters for color images using the same data set as in [25],[26].<br>
Our goal is to establish a reliable reference against which more sophisticated image models can be compared to in the future. We elaborate on our own previous work [28] by optimizing the ICA algorithm for the multi-information estimators used in the comparison. Additionally, we now test the advantage of the resulting orientation selective ICA filters comprehensively with three different types of analyses that are related to the notion of redundancy reduction, density estimation, and coding efficiency: (A) multi-information reduction, (B) average log-likelihood, and (C) rate-distortion curves.<br>
Our results show that orientation selective ICA filters do not excel in any of these measures: We find that the gain of ICA in redundancy reduction over a random decorrelation method is only about 3% for color and gray-value images. In terms of rate-distortion curves, ICA performs even worse than PCA. Furthermore, we demonstrate that a simple spherically symmetric model with only two parameters fits the filter responses significantly better than a model that assumes marginal independence . Since in this model the specific shape of the filters is ignored, we conclude that it is unlikely that orientation selectivity plays a critical role for redundancy reduction even if the class of transformations is extended to include contrast gain control mechanisms [30],[31]. While many of the previous studies do not provide enough detail in order to explain their different outcomes, we provide our code and the dataset online (http://www.kyb.tuebingen.mpg.de/bethge/code/QICA/) in order to ensure the reproducibility and verifiability of our results.<br>
<br>
Materials and Methods<br>
An important difficulty in setting up a quantitative comparison originates from the fact that it bears several issues that may be critical for the results. In particular, choices have to be made regarding the evaluation criteria, the image data, the estimation methods, which linear transformations to include in the comparison, and which particular implementation of ICA to use. The significance of the outcome of the comparison will depend on how careful these choices have been made. The most relevant issues will be addressed in the following.<br>
Notation and Nomenclature<br>
For both, color and gray-value data, we write  to refer to single vectors which contain the raw pixel intensities. Vectors are indicated by bold font while the same letter in normal font with a subindex denotes one of its components. Vectors without subindices usually denote random variables, while subindices indicate specific examples. In some cases it is convenient to define the corresponding data matrix  which holds single images patches in its columns. The letter  denotes the number of examples in the dataset, while  is used for the dimension of a single data point.<br>
Transformations are denoted by , oftentimes with a subindex to distinguish different types. The result of a transformation to either a vector  or a data matrix  will be written as  or , respectively.<br>
Probability densities are denoted with the letters  and , sometimes with a subindex to indicate differences between distributions whenever it seems necessary for clarity. In general, we use the hat symbol to distinguish between true entities and their empirical estimates. For instance,  is the true probability density of  after applying a fixed transformation , while  refers to the corresponding empirical estimate.<br>
A distribution  is called factorial, or marginally independent, if it can be written as a product of its marginals, i.e.,  where  is obtained by integrating  over all components but .<br>
Finally, the expectation over some entity  with respect to  is written as . Sometimes, we use the density instead of the random variable in the subindex to indicate the distribution, over which the expectation is taken. If there is no risk for confusion we drop the subindex. Just as above, the empirical expectation is marked with a hat symbol, i.e., .<br>
<br>
How to Compare Early Vision Models?<br>
A principal complicacy in low-level vision is the lack of a clearly defined task. Therefore, it is difficult to compare different image representations as it is not obvious a priori what measure should be used.<br>
Multi-information<br>
The first measure we consider is the multi-information [32], which is the original objective function that is minimized by ICA over the choice of filters . The multi-information assesses the total amount of statistical dependencies between the components  of a filtered patch :(1)The terms  and  denote the marginal and the joint entropies of the true distribution, respectively. The Kullback-Leibler-Divergence or Relative Entropyis an information theoretic dissimilarity measure between two distributions  and  [33]. It is always non-negative and zero if and only if  equals . If the redundancy reduction hypothesis is taken literally, the multi-information is the right measure to minimize, since it measures how close to factorial the true distribution of the image patches in the representation  really is.<br>
The application of linear ICA algorithms to ensembles of natural images reliably yields transformations consisting of localized and oriented bandpass filters similar to the receptive fields of neurons in V1. It is less clear, however, whether these filter properties also critical to the minimization of the multi-information? In order to assess the tenability of the idea that a V1 simple cell is adjusted to the purpose of redundancy reduction, it is important to know whether such a tuning can?in principle?result in a large reduction of the multi-information. One way to address this question is to measure how much more the multi-information is actually reduced by the ICA filters in comparison to others such as PCA filters. This approach has been taken in [28].<br>
One problem with estimating multi-information is that it involves the joint entropy  of the true distribution which is generally hard to estimate. In certain cases, however, the problem can be bypassed by evaluating the difference in the multi-information between two representations  and . In particular, if  is related to  by the linear transformation  it follows from definition (1) and the transformation theorem for probability densitiesthat difference in multi-information can be expressed asFor convenience, we chose a volume-conserving gauge [28] where all linear decorrelation transforms are of determinant one, and hence . This means that differences in multi-information are equal to differences of marginal entropies which can be estimated robustly. Thus, our empirical estimates of the multi-information differences are given by:(2)For estimating the entropy of the univariate marginal distributions, we employ the OPT estimator introduced in [28] which uses the exponential power family to fit the marginal distributions by OPTimizing over the shape parameter. This estimator has been shown to give highly reliable results for natural images. In particular, it is much more robust than entropy estimators based on the sample kurtosis which easily overestimate the multi-information.<br>
<br>
Average log loss (ALL)<br>
As mentioned earlier, redundancy reduction can be interpreted as a special form of density estimation where the goal is to find a mapping which transforms the data into a representation with statistically independent coefficients. This means that any given transformation specifies a density model over the data. Our second measure, the average log-loss (ALL), evaluates the agreement of this density model with the actual distribution of the data:(3)The average log-loss is a principled measure quantifying how different the model density  is from the true density  [34]. Since the KL-divergence is positive and zero if and only if  the ALL is minimal only if  matches the true density. Furthermore, differences in the average log-loss correspond to differences in the coding cost (i.e., information rate) in the case of sufficiently fine quantization. For natural images, different image representations have been compared with respect to this measure in [24]?[26].<br>
For the estimation of the average log-loss, we compute the empirical average(4)This estimator is equivalent to the first method in Lewicki et al. [24],[35] apart from an extra term  in their defining equation. This extra term is only necessary if one aims at relating the result to a discrete entropy obtained for a particular bin width .<br>
While the empirical average in Eq. 4 in principle can be prone to overfitting, we control for this risk by evaluating all estimates on an independent test set, whose data has not been used during the parameter fit. Furthermore, we compare the average log-loss to the parametric entropy estimates  that we use in (A) for estimating the multi-information changes (see Eq. 2). The difference between both quantities has been named differential log-likelihood [36] and can be used to assess the goodness of fit of a model distribution:<br>
The shape of the parametric model is well matched to the actual distribution if the differential log-likelihood converges to zero with increasing number of data points.<br>
<br>
Rate-distortion curves<br>
Finally, we consider efficient coding or minimum mean square error reconstruction as a third objective. In contrast to the previous objectives, it is now assumed that there is some limitation of the amount of information that can be transmitted, and the goal is to maximize the amount of relevant information transmitted about the image. In the context of neural coding, the redundancy reduction hypothesis has oftentimes been motivated in terms of coding efficiency. In fact, instead of minimizing the multi-information one can equivalently ask for the linear transformation  which maximizes the mutual information between its input  and its output  when additive noise  is added to the output [3],[37],[38]. It is important to note, however, that this minimalist approach of ?information maximization? is ignorant with respect to how useful or relevant the information is that has been transmitted [14].<br>
For natural images, the source signal  is a continuous random variable which requires infinitely many bits to be specified with unlimited precision. In reality, however, the precision is always limited so that only a finite amount of bits can be represented. Both, the multi-information and the average log-loss do not take into account the problem what information should be encoded and what information can be discarded. Therefore, it is interesting to compare the redundancy reduction of the linear transforms with respect to the relevant image information (while the irrelevant information can be discarded anyway). To this end, we here resort to the framework of linear transform coding as it has been developed in the field of image compression [39],[40], and which constitutes the theoretical foundation of the JPEG standard.<br>
It is clear that at the level of V1 the number of neurons, encoding the retinal image, is substantially larger than the number of fibers in the optic nerve. Therefore, it is not the need for compression that makes rate distortion theory interesting at this stage. However, Barlow's redundancy reduction hypothesis must not be equated with compression. In more recent work, Barlow introduced the term ?redundancy exploitation? instead of ?redundancy reduction? in order to avoid this misunderstanding [41]. But also if we think in terms of density estimation rather than compression, it is still important to take into account that not all possible changes in the image pixels may be of equal importance for inferring the content of an image. Therefore, we here want to combine the notion of redundancy reduction with a measure for the quality with which the image can be reconstructed from the information that is preserved by the representation. Following Lewicki and coworkers (method 2 in [24],[35]) we will consider the mean squared error reconstruction that can be achieved at a certain quantization level of the transformed representation. This objective is in fact very much related to the task of image compression.<br>
Clearly, we expect that the criteria for judging image compression algorithms may not provide a good proxy to an accurate judgement of what information is considered relevant in a biological vision system. In particular, the existence of selective attention suggests that different aspects of image information are transmitted at different times depending on the behavioral goals and circumstances [12]. That is, a biological organism can change the relevance criteria dynamically on demand while for still image compression algorithms it is rather necessary that this assessment is made once and forever in a fixed and static fashion.<br>
These issues are outside the scope of this paper. Instead we follow the common path in the past to use the mean squared reconstruction error for the pixel intensities. This is the measure of choice for high-rate still image compression [42]. In particular, it is common to report on the performance of a code by determining its rate?distortion curve which specifies the required information rate for a given reconstruction error (and vice versa) [40]. Consequently, we will ask for a given information rate, how do the image representations compare with respect to the reconstruction error. As result, we will obtain a so-called rate?distortion curve which displays the average reconstruction error as a function of the information rate or vice versa. The second method used in [24],[35] is an estimate of a single point on this curve for a particular fixed value of the reconstruction error.<br>
The estimation of the rate?distortion curve is clearly the most difficult task among the three criteria. The framework of transform coding [39], which is extensively used in still image compression, makes several simplifying assumptions that allow one to obtain a clear picture. The encoding task is divided into two steps: First, the image patches  are linearly transformed into . Then the coefficients  are quantized independently of each other. Using this framework, we can ask whether the use of an ICA image transformation leads to a smaller reconstruction error after coefficient quantization than PCA or any other transform.<br>
As for quantizing the coefficients, we resort to the framework of variable rate entropy coding [43]. In particular, we apply uniform quantization, which is close to optimal for high-rate compression [39],[44]. For uniform quantization, it is only required to specify the bin width of the coefficients. There is also the possibility to use a different number of quantization levels for the different coefficients. The question of how to set these numbers is known as the ?bit allocation problem? because the amount of bits needed to encode one coefficient will depend monotonically on the number of quantization levels. The number of quantization levels can be adjusted in two different but equivalent ways: One possibility is to use a different bin width for each individual coefficient. Alternatively, it is also possible to use the same bin width for all coefficients and multiply all coefficients with an appropriate scale factor before quantization. The larger the variance of an individual coefficient, the more bits will be allocated to represent it.<br>
Here, we will employ the latter approach, for which the bit allocation problem becomes an inherent part of the transformation: Any bit allocation scheme can be obtained via post-multiplication with a diagonal matrix. Thus, in contrast to the objective function of ICA, the rate?distortion criterion is not invariant against post-multiplication with a diagonal matrix. For ICA and PCA, we will determine the rate?distortion curve for both, normalized output variances (?white ICA? and ?white PCA?) and normalized basis functions (?normalized ICA? and ?orthonormal PCA?), respectively.<br>
<br>
<br>
Decorrelation Transforms<br>
The particular shape of the ICA basis functions is obtained by minimization of the multi-information over all invertible linear transforms . In contrast, the removal of second-order correlations alone generally does not yield localized, oriented, and bandpass image basis functions. ICA additionally removes higher-order correlations which are generated by linear mixing. In order to assess the importance of this type of higher-order correlations for redundancy reduction and coding efficiency we will compare ICA to other decorrelating image bases.<br>
Let  be the covariance matrix of the data and  its eigen-decomposition. Then, any linear second-order decorrelation transform can be written as(5)where  and  are defined as above,  is an arbitrary orthogonal matrix and  is an arbitrary diagonal matrix. It is easily verified that  has diagonal covariance for all choices of  and , i.e., all second-order correlations vanish. This means that any particular choice of  and  determines a specific decorrelation transform. Based on this observation we introduce a number of linear transformations for later reference. All matrices are square and are chosen to be of determinant , where  is the number of columns (or rows) of  (i.e.,  is the geometrical mean of the eigenvalues ).<br>
Orthogonal principal component analysis (oPCA)<br>
If the variances of the principle components (i.e., the diagonal elements of ) are all different, PCA is the only metric-preserving decorrelation transform and is heavily used in digital image coding. It corresponds to choosing  as the identity matrix and , such that .<br>
<br>
White principal component analysis (wPCA)<br>
Equalizing the output variances in the PCA representation sets the stage for the derivation of further decorrelation transforms different from PCA. In order to assess the effect of variance equalization for coding efficiency, we also include this ?white PCA? representation into our analysis: Choose  as for orthonormal PCA and then set  with  such that .<br>
<br>
Symmetric whitening (SYM)<br>
Among the non-orthogonal decorrelation transforms, symmetric whitening stays as close to the input representation as possible (in Frobenius norm) [45]. In terms of early vision this may be seen as an implementation of a wiring length minimization principle. Remarkably, the basis functions of symmetric whitening resemble the center-surround shape of retinal ganglion cell receptive fields when applied to the pixel representation of natural images [17]. The symmetric whitening transform is obtained by setting  and  such that .<br>
<br>
Random whitening (RND)<br>
As a baseline which neither exploits a special structure with respect to the input representation nor makes use of higher-order correlations we also consider a completely random transformation. To obtain a random orthogonal matrix we first draw a random matrix  from a Gaussian matrix-variate distribution and then we set . With  we obtain .<br>
<br>
White independent component analysis (wICA)<br>
Finally, ICA is the transformation which has been suggested to explain the orientation selectivity of V1 simple cells [8],[23]. Set  for which the multi-information  takes a minimum. With  we obtain .<br>
<br>
Normalized independent component analysis (nICA)<br>
Normalized independent component analysis (nICA) differs from white ICA () only by a different choice of the second diagonal matrix . Instead of having equal variance in each coefficient, we now choose  such that the corresponding basis vector of each coefficient has the same length in pixel space. It is easy to see that our first two criteria, the multi-information and the negative log-likelihood, are invariant under changes in . It makes a difference for the rate?distortion curves as in our setup the variance (or, more precisely, the standard deviation) determines the bit allocation. Practically,  can be determined by using  as follows: First, we compute the matrix inverse  and determine the Euclidean norm  of the column vectors of . With , we then obtain .<br>
<br>
<br>
ICA Algorithm<br>
If the true joint probability distribution is known, the minimization of the multi-information over all linear transformations can be formulated without any assumptions about the shape of the distribution. In practice, the multi-information has to be estimated from a finite amount of data which requires to make assumptions about the underlying density.<br>
There are many different ICA algorithms which differ in the assumptions made and also in the optimization technique employed. The choice of the particular ICA algorithm used here was guided by a set of requirements that arise from the specific problem setting. Although a wide variety of ICA algorithms has been published, none of them fits exactly all of our requirements.<br>
We would like to use an ICA algorithm, which gives the ICA image basis the best chance for the comparison with other image representations. For the comparison of the multi-information reduction, we are using the OPT estimator introduced in [28] which has been found to give the most reliable results. This estimator employs a parametric estimate of the coefficient distributions based on the exponential power family which is known to provide an excellent fit to the coefficient distributions of natural images [28],[46]. Our ICA algorithm should make the same assumptions about the data as we make for the final comparison of the multi-information reduction. Therefore, we are also using the exponential power family model for the marginal densities during the minimization of the multi-information. In addition, we want to have an ICA basis which is indistinguishable from the other image representations with respect to the second-order statistics. Therefore, we are using a pre-whitened ICA algorithm, whose search space is restricted to the subgroup of orthogonal matrices . One of the most efficient ICA methods in the public domain specialized to pre-whitened ICA is FastICA [47]. We use this fixed-point algorithm as an initialization. Subsequently, the solution is further refined by performing a gradient ascent over the manifold of orthogonal matrices on the likelihood of the data, when each marginal is modelled by a the exponential power distribution as in the case of the OPT estimator.<br>
In order to optimize the objective function over the subspace of orthogonal matrices, we adapted the algorithms for Stiefel manifolds proposed by Edelman et al. [48] to the simpler case of orthogonal groups and combined it with the line-search routine dbrent from [49] to achieve a rather straightforward gradient descent algorithm. For the initialization with FastICA, we use the Gaussian non-linearity, the symmetric approach and a tolerance level of 10?5.<br>
<br>
Spherically Symmetric Model<br>
A well known result by Maxwell [50] states that the only factorial distribution invariant against arbitrary orthogonal transformations is the isotropic Gaussian distribution. Natural images exhibit marginals which are significantly more peaked than Gaussian. Nevertheless, their distribution does share the spherical symmetry with the Gaussian as already found by [51] for gabor filter pairs and lately exploited by [31] for nonlinear image representations. Therefore, it makes sense to compare the performance of the ICA model with a spherically symmetric model of the whitened data . Note that any spherically symmetric model is still invariant under orthogonal transformations while only the Gaussian additionally exhibits marginal independence.<br>
While the radial distribution of a Gaussian (i.e., the distribution over the lengths of the random vectors) is a , whose shape and scale parameter is determined by the number of dimensions and the variance, respectively, the spherical symmetric model may be seen as a generalization of the Gaussian, for which the radial distribution  with  can be of arbitrary shape. The density of the spherically symmetric distribution (SSD) is defined as , where  is the surface area of a sphere in  with radius . For simplicity we will model the radial distribution with a member of the Gamma family(6)with shape parameter  and scale parameter , which can be easily matched to the mean and variance of the empirical distribution via  and .<br>
<br>
Dataset<br>
The difference in the performance between ICA and other linear transformations clearly depends on the data. For gray-scale images we observed in our previous study [28] that the difference in the multi-information between ICA and any other decorrelation transform is consistently smaller than 5%. In particular, we controlled for the use of different pictures and for the effect of different pre-processing steps.<br>
Here, we resort to the dataset used in a previous study [25],[26], which among all previous studies reported the largest advantage of ICA compared to PCA. This color image dataset is based on the <database>Bristol Hyperspectral Images Database</database> [52] that contains multi-spectral recordings of natural scenes taken in the surroundings of Bristol, UK and in the greenhouses of Bristol Botanical Gardens. The authors of [26] kindly provided to us a pre-processed version of the image data where spectral radiance vectors were already converted into LMS values. During subsequent processing the reflectance standard was cut out and images were converted to log intensities [26].<br>
All images come at a resolution of 256?256 pixels. From each image circa 5000 patches of size 7?7 pixels were drawn at random locations (circa 40000 patches in total). For chromatic images with three color channels (LMS) each patch is reshaped as a 7?7?3?=?147-dimensional vector. To estimate the contribution of color information, a comparison with monochromatic images was performed where gray-value intensities were computed as  and exactly the same patches were used for analysis. In the latter case, the dimensionality of a data sample is thus reduced to 49 dimensions. All experiments are carried out over ten different training and test sets sampled independently from the original images.<br>
Our motivation to chose 7?7 patches is to keep the same setting as in [26] for the sake of comparability. As this patch size is rather small, we performed the same analysis for patch sizes of 15?15 as well. All results in the paper refer to the case of 7?7 image patches. The results for 15?15 can be found in the supplementary material (Text S1).<br>
The statistics of the average illumation in the image patches, the DC component, differs significantly from image to image. Therefore, we first separated the DC component from the patches before further transforming them. In order to leave the entropy of the data unaffected, we used an orthogonal transformation. The projector  is computed such that the first (for each color channel) component of  corresponds to the DC component(s) of that patch. One such a possible choice is the matrixHowever, this is not an orthogonal transformation. Therefore, we decompose  into  where  is upper triangular and  is an orthogonal transform. Since , the first column of  must be a multiple of the vector with all coefficients equal to one (due to the upper triangluarity of ). Therefore, the first component of  is a multiple of the DC component. Since  is an orthonomal transform, using all but the first row of  for  projects out the DC component. In the case of color images  becomes a block-diagonal matrix with  as diagonal elements for each channel.<br>
By removing the DC component in that manner, all linear transformations are applied in  dimensions, if  denotes the number of pixels in the original image patch. In this case the marginal entropy of the DC-components has to be included in the computation of the multi-information in order to ensure a valid comparison with the original pixel basis. We use the same estimators as in [28] to estimate the marginal entropy of DC-component.<br>
<br>
<br>
Results<br>
Filter Shapes<br>
As in previous studies [8],[23] the filters derived with ICA exhibited orientation selective tuning properties similar to those observed for V1 simple cells (see Figure 1). For illustration, we also show the basis functions learned with PCA and RND in Figure 1. The basis functions  are obtained by inverting the filter matrix  (including the DC component). The result is displayed in the upper panel (Figure 1A?C). Following common practice, we also visualize the basis functions after symmetric whitening (Figure 1D?F).<br>
The basis functions of both PCA and ICA exhibit color opponent coding but the basis functions of ICA are additionally localized and orientation selective. The basis functions of the random decorrelation transform does not exhibit any regular structure besides the fact that they are bandpass. The following quantitative comparisons will show, however, that the distinct shape of the ICA basis functions does not yield a clear advantage for redundancy reduction and coding efficiency.<br>
<br>
Multi-Information<br>
The multi-information is the original objective function that is minimized by ICA over all possible linear decorrelation transforms. Figure 2 shows the reduction in multi-information achieved with different decorrelation transforms including ICA for chromatic and gray value images, respectively. For each representation, the results are reported in bits per component, i.e., as marginal entropies averaged over all dimensions:(7)<br>
Table 1 shows the corresponding values for the transformations RND, SYM, PCA and ICA. For both chromatic images and gray-value intensities, the lowest and highest reduction is achieved with RND or ICA, respectively. However, the additional gain in the multi-information reduction achieved with ICA on top of RND constitutes only 3.20% for chromatic images and 2.39% for achromatic in comparison with the total reduction relative to the pixel basis (PIX). This means that only a small fraction of redundancy reduction can actually be accounted to the removal of higher-order redundancies with ICA.<br>
One may argue that the relatively small patch size of 7?7 pixel may be responsible for the small advantage of ICA as all decorrelation functions already getting the benefit of localization. In order to address the question how the patch size affects the linear redundancy reduction, we repeated the same analysis on a whole range of different patch sizes. Figure 3 shows the multi-information reduction with respect to the pixel representation (PIX) achieved by the transformations RND and ICA. The achievable reduction quickly saturates with increasing patch size such that its value for 7?7 image patches is already at about 90% of its asymptote. In particular, one can see that the relative advantage of ICA over other transformations is still small (?3%) also for large patch sizes. All Tables and Figures for patch size 15?15 can be found in the additional material (Text S1).<br>
<br>
Average Log-Loss<br>
Since redundancy reduction can also be interpreted as a special form of density estimation we also look at the average log-loss which quantifies how well the underlying density model of the different transformations is matched to the statistics of the data. Table 2 shows the average log-loss (ALL) and Table 3 the differential log-likelihood (DLL) in bits per component. For the average log-loss, ICA achieved an ALL of 1.78 bits per component for chromatic images and 1.85 bits per component for achromatic images. Compared to the ALL in the RND representation of 1.9 bits and 1.94 bits, respectively, the gain achieved by ICA is again small. Additionally, the ALL values were very close to the differential entropies, resulting in small DLL values. This confirms that the exponential power distribution fits the shape of the individual marginal coefficient distributions well. Therefore, we can safely conclude that the advantage of ICA is small not only in terms of redundancy reduction as measured by the multi-information, but also in the sense of density estimation.<br>
Comparison to a Spherical Symmetric Model<br>
The fact that ICA fits the distribution of natural images only marginally better than a random decorrelation transform implies that the generative model underlying ICA does not apply to natural images. In order to assess the importance of the actual filter shape, we fitted a spherically symmetric model to the filter responses. The likelihood of such a model is invariant under post-multiplication of an orthogonal matrix, i.e., the actual shape of the filter. Therefore, a good fit of such a model provides strong evidence against a critical role of certain filter shapes.<br>
As shown in Table 2, the ALL of the SSD model is 1.67 bits per component for chromatic images and 1.65 bits per component for achromatic images. This is significantly smaller than the ALL of ICA indicating that it fits the distribution of natural images much better than ICA does. This result is particularly striking if one compares the number of parameters fitted in the ICA model compared to the SSD case: After whitening, the optimization in ICA is done over the manifold of orthogonal matrices which has  free parameters (where  denotes the number of dimensions without the DC components). The additional optimization of the shape parameters for the exponential power family fitted to each individual component adds another  parameters. For the case of 7?7 color image patches we thus have  parameters. In stark contrast, there are only two free parameters in the SSD model with a radial Gamma distribution, the shape parameter  and the scale parameter . Nevertheless, for chromatic images the gain of the SSD model relative to random whitening is almost twice as large as that of ICA and even three and a half times as large for achromatic images.<br>
Since the SSD model is completely independent of the choice of the orthogonal transformation after whitening, its superior performance compared with ICA provides a very strong argument against the hypothesis that orientation selectivity plays a critical role for redundancy reduction. In addition, it is also corroborates earlier arguments that has been given to show that the statistics of natural images does not conform to the generative model underlying ICA [51],[53].<br>
Besides the better fit of the data by the SSD model, there is also a more direct way of demonstrating the dependencies of the ICA coefficients: If  is data in the wICA representation, then the independence assumption of ICA can be simulated by applying independent random permutations to the rows of . Certainly, such a shuffling procedure does not alter the histograms of the individual coefficients but it is suited to destroy potential statistical dependencies among the coefficients. Subsequently, we can transform the shuffled data  back to the RND basis . If the ICA coefficients were independent, the shuffling procedure would not alter the joint statistics, and hence, one should find no difference in the multi-information between  and . But infact, we observe a large discrepancy between the two (Figure 4). The distributions of the sRND coefficients were very close to Gaussians and the average marginal entropy of sRND yielded  bits in contrast to  bits. In other words, the finding that for natural images the marginals of a random decorrelation transform have Laplacian shape () stands in clear contradiction to the generative model underlying ICA. If the ICA model was valid, one would expect that the sum over the ICA coefficients would yield Gaussian marginals due to the central limit theorem. In conclusion, we have very strong evidence that the ICA coefficients are not independent in case of natural images.<br>
<br>
<br>
Rate-Distortion Curves<br>
There are different ways to account for the limited precision that is imposed by neural noise and firing rate limitations. As mentioned above the advantage with respect to a plain information maximization criterion can equivalently be measured by the multi-information criterion considered above [37],[54]. In order to additionally account for the question which representation optimally encodes the relevant image information, we also present rate distortion curves which show the minimal reconstruction error as a function of the information rate.<br>
We compare the rate?distortion curves of wICA, nICA, wPCA and oPCA (see Figure 5). Despite the fact that ICA is optimal in terms of redundancy reduction (see Table 2), oPCA performs optimal with respect to the rate-distortion trade-off. wPCA in turn performes worst and remarkably similar to wICA. Since wPCA and wICA differ only by an orthogonal transformation, both representations are bound to the same metric. oPCA is the only transformation which has the same metric as the pixel representation according to which the reconstruction error is determined. By normalizing the length of the ICA basis vectors in the pixel space, the metric of nICA becomes more similar to the pixel basis and the performance with respect to the rate-distortion trade-off improved considerably. Nevertheless, for a fixed reconstrucion error the discrete entropy after quantization in the oPCA basis is up to 1 bit/component smaller than for the corresponding nICA-basis.<br>
In order to understand this result more precisely, we analyzed how the quantization of the coefficients affects the two variables of the rate?distortion function, discrete entropy and reconstruction error.<br>
Figure 6 shows an illustrative example in order to make the following analysis more intuitive. The example demonstrates that the quality of a transform code not only depends on the redundancy of the coefficients but also on the shape of the partition cells induced by the quantization. In particular, when the cells are small (i.e., the entropy rate is high), then the reconstruction error mainly depends on having cell shapes that minimize the average distance to the center of the cell. Linear transform codes can only produce partitions into parallelepipeds (Figure 6B). The best parallelepipeds are cubes (Figure 6A). This is why PCA yields the (close to) optimal trade-off between minimizing the redundancy and the distortion, as it is the only orthogonal transform that yields uncorrelated coefficients. For a more comprehensive introduction to transform coding we refer the reader to the excellent review by Goyal [39].<br>
Discrete entropy<br>
Given a uniform binning of width  the discrete entropy  of a probability density  is defined as(8)where  denotes the interval defined by the  bin. For small bin-sizes , there is a close relationship between discrete and differential entropy: Because of the mean value theorem we can approximate  with , and henceThus, we have the relationship  for sufficiently small  (i.e., high-rate quantization). In other words,  asymptotically grows linearly with . Therefore, we can fit a linear function to the asymptotic branch of the function  which is plotted in Figure 7A (more precisely we are plotting the average over all dimensions). If we take the ordinate intercept of the linear approximation, we obtain a nonparametric estimate of the differential entropy which can be compared to the entropy estimates reported above (Those estimates were determined with the OPT estimator). Equivalently, one can consider the function  which gives a better visualization of the error of the linear approximation (Figure 7, left, dashed line). For  the differential entropy is obtained in the limit .<br>
This analysis shows that differences in differential entropy in fact translate into differences in discrete entropy after uniform quantization with sufficiently small bins. Accordingly, the minimization of the multi-information as proposed by the redundancy reduction hypothesis does in fact also minimize the discrete entropy of a uniformly quantized code. In particular, if we look at the discrete entropy of the four different transforms, oPCA, wPCA, wICA, nICA (Figure 7B), we find that asymptotically the two PCA transforms require slightly more entropy than the two ICA transforms, and there is no difference anymore between oPCA and wPCA or wICA and nICA. This close relationship between discrete and differential entropy for high-rate quantization, however, is not sufficient to determine the coding performance evaluated by the rate?distortion curve. The latter requires to compare also the reconstruction error for the given quantization.<br>
<br>
Reconstruction error<br>
The reconstruction error is defined as the mean squared distance in the pixel basis between the original image and the image obtained by reconstruction from the quantized coefficients of the considered transformation. For the reconstruction, we simply use the inverse of the considered transformation, which is optimal in the limit of high-rate quantization.<br>
When looking at the reconstruction error as a function of the bin width (Figure 8) we can observe much more pronounced differences between the different transformations than it was the case for the entropy. As a consequence, the differences in the reconstruction error turn out to be much more important for the rate-distortion trade-off than the differences in the entropy. Only the two transformations with exactly the same metric, wPCA and wICA, exhibit no difference in the reconstruction error. This suggests that minimization of the multi-information is strictly related to efficient coding if and only if the transformation with respect to the pixel basis is orthogonal. As we have seen that the potential effect of higher-order redundancy reduction is rather small, we expect that the PCA transform constitutes a close approximation to the minimizer of the multi-information among all orthogonal transforms because PCA is the only orthogonal transform which removes all second-order correlations.<br>
<br>
<br>
<br>
Discussion<br>
The structural organization of orientation selectivity in the primary visual cortex has been associated with self-organization since the early seventies [55], and much progress has been made to narrow down the range of possible models compatible with the empirical findings (e.g., [56]?[58]). The link to visual information processing, however, still remains elusive [59]?[61].<br>
More abstract unsupervised learning models which obtain orientation selective filters using sparse coding [8] or ICA [23] try to address this link between image processing and the self-organization of neural structure. In particular, these models not only seek to reproduce the orientation tuning properties of V1 simple cells but they additionally address the question of how the simple cell responses collectively can instantiate a representation for arbitrary images. Furthermore, these image representations are learned from an information theoretic principle assuming that the learned filters exhibit advantageous coding properties.<br>
The goal of this study is to quantitatively test this assumption in the simple linear transform coding framework. To this end, we investigated three criteria, the multi-information?i.e., the objective function of ICA?the average log-loss, and rate-distortion curves. There are a number of previous studies which also aimed at quantifying how large the advantage of the orientation selective ICA filters is relative to second-order decorrelation transformations. In particular, four papers [24]?[26],[28], are most closely related to this study as all of them compare the average log-loss of different transformations. However, they did not provide a coherent answer to the question how large the advantage of ICA is compared to other decorrelation transforms.<br>
Lewicki and Olshausen [24] found that their learned bases show a 15?20% improvement over traditional bases. However, their result cannot be used to compare second-order and higher-order redundancy reduction because the entire analysis is based on a dataset in which all images have been preprocessed with a bandpass filter as in olshausen:1996. Since bandpass filtering already removes a substantial fraction of second-order correlations in natural images, their study is likely to systematically underestimate the total amount of second-order correlations in natural images.<br>
Lee et al. [25],[26] reported an advantage of over 100% percent for ICA in the case of color images and a more moderate but substantial gain of about 20% for gray-value images. In order to avoid possible differences due to the choice of data set we here used exactly the same data as in [25],[26]. Very consistently, we find only a small advantage for ICA of less than five percent for both multi-information and the average log-loss. In particular, we are not able to reproduce the very large difference between color and gray-value images that they reported. Unfortunately, we cannot pinpoint where the differences in the numbers ultimately come from because it is not clear which estimation procedure was used in [25],[26].<br>
The estimators used for the measurements in the present study have been shown previously to give correct results on artificial data [28] and we provide our code online for verification. Furthermore, Weiss and Freeman showed for an undirected probabilistic image model that whitening already yields 98% of the total performance [62]. Finally, the superior performance of the simple SSD model with only two free parameters provides a very strong explanation for why the gain achieved with ICA is so small relative to a random decorrelation transform: Since a spherically symmetric model is invariant under orthogonal transformations and provides a better fit to the data, the actual shape of the filter does not seem to be critical. It also shows that the fundamental assumption underlying ICA?the data are well described by a linear generative model with independent sources?is not justified in the case of natural images.<br>
From all these results, we can safely conclude that the actual gain of ICA compared to PCA is smaller than 5% for both gray level images and color images.<br>
Is Smaller Than 5% Really Small?<br>
A valid question to ask is whether comparing the amount of higher-order correlations to the amount of second-order correlations is the right thing to do. Even if the amount of higher-order correlations may be small in comparison to the amount of second-order correlations, we still know that higher-order correlations can be a critical signature of the content of an image. For example, textures are very useful to demonstrate how changes in higher-order correlations can change the perceptual meaning of an image.<br>
Our results on the rate-distortion trade-off can be taken as an indication that the fraction of higher-order correlations captured by ICA is perceptually less relevant. This interpretation is further corroborated by a psychophysical comparison of the perceptual redundancy of the ICA and the PCA basis [63]. Another confirmation of this interpretation can be obtained if we use the learned image representations as generative models. Perceptually image patches sampled from the ICA model do not look more similar to natural image patches than those sampled from the random decorrelation basis (Figure 9). Currently, we are running psychophysical experiments which also show quantitatively that there is no significant difference between the ICA model and the PCA model if the subjects have to discriminate between textures that are generated by these models.<br>
In summary, we were not able thus far to come up with a meaningful interpretation for which the improvement of ICA would be recognized as being large. On the basis of the present study it seems rather unlikely that such a measure can be found for linear ICA. Instead, we believe that more sophisticated, nonlinear image models are necessary to demonstrate a clear advantage of orientation selectivity.<br>
<br>
What about Nonparametric Approaches?<br>
The focus on linear redundancy reduction models in this study is motivated by the goal to first establish a solid and reproducible result for the simplest possible case before moving on to more involved nonlinear transformations. Nevertheless, it is important to discuss what we can expect if the restriction to linear transformations is dropped. From a nonparametric analysis [27], Petrov and Zhaoping concluded that higher-order correlations in general contribute only very little to the redundancy in natural images and, hence, are probably not the main cause for the receptive field properties in V1. The empirical support for this claim, however, is limited by the fact that their comparison is based on mutual information estimates within a very small neighborhood of five pixels only. This is problematic as it is known that many kinds of higher-order correlations in natural images become apparent only in much higher-dimensional statistics [64]. Furthermore, their estimate of the amount of second-order correlations is not invariant against pointwise nonlinear transformations of the pixel intensities.<br>
In a more recent non-parametric study, Chandler and Field arrived at a very different result regarding the relative contribution of second-order and higher-order dependencies [29]. They use nearest-neighbor based methods to estimate the joint entropy of natural images in comparison to ?spectrum-equalized? noise and white noise, where ?spectrum-equalized? noise denotes Gaussian noise with exactly the same spectrum as that of natural images. As shown in Figure 18 of [29] they find a smaller difference between spectrum-equalized noise and white noise than between natural images and spectrum-equalized noise. Hence, from their finding, it seems that the amount of higher-order correlations in natural images is even larger than the amount of second-order correlations. Also this result has to be taken with care: Reliable non-parametric estimates in high-dimensions are difficult to obtain even if one resorts to nearest-neighbor based methods, and the estimate of the amount of second-order correlations in [29] is not invariant against pointwise nonlinear transformations of the pixel intensities, too.<br>
In summary, the present nonparametric studies do not give a unique answer regarding the total amount of higher-order correlations in natural images. Since estimating the absolute amount of multi-information is an extremely difficult task in high dimensions, the differences in the results can easily originate from the different assumptions and approximations made in these studies. Consequently, it remains an open question how large the true total redundancy of natural images is. In any case, it is clear that there are many higher-order redundancies in natural images that play a crucial role for visual perception. No matter how large these redundancies are in comparison to the second-order correlations, we need to develop better image models that have the right structure to capture these regularities.<br>
<br>
What about Nonlinear Image Models?<br>
Apart from the non-parametric approaches, a large number of nonlinear image models has been proposed over the years which are capable to capture significantly more statistical regularities of natural images than linear ICA can do (e.g., [62], [65]?[72]). In fact, Olshausen and Field [8] already used a more general model than linear ICA when they originally derived the orientation selective filters from higher-order redundancy reduction. In contrast to plain ICA, they used an overcomplete generative model which assumes more source signals than pixel dimensions. In addition, the sources are modeled as latent variables like in a factor analysis model. That is the data is assumed to be generated according to  where  denotes the overcomplete dictionary,  is distributed according to a sparse factorial distribution, and  is a Gaussian random variable. The early quantitative study by Lewicki and Olshausen [24] could not demonstrate an advantage of overcomplete coding in terms of the rate-distortion trade-off and also the more recent work by Seeger [70] seems to confirm this conclusion. The addition of a Gaussian random variable  to , however is likely to be advantageous as it may help to interpolate betweem the plain ICA model on the one hand and the spherically symmetric model on the other hand. A comparison of the average log-loss between this model and plain ICA has not been done yet but we can expect that this model can achieve a similar or even better match to the natural image statistics as the spherically symmetric model.<br>
The spherical symmetric model can also be modeled by a redundancy reduction transformation which changes the radial component such that the output distribution is sought to match a Gaussian distribution [31]. Hence, the redundancy reduction of this model is very similar to the average log-loss of the spherically symmetric distribution. From a biological vision point of view, this type of model is particularly interesting as it allows one to draw a link to divisive normalization, a prominent contrast gain control mechanism observed for virtually all neurons in the early visual system. Our own ongoing work [30] shows that this idea can be generalized to a larger class of  symmetric distributions [67]. In this way, it is possible to find an optimal interpolation between ICA and the spherically symmetric case [73]. That is, one can combine orientation selectivity with divisive normalization in a joint model. Our preliminary results suggests that optimal divisive normalization together with orientation selectivity allows for about 10% improvement while divisive normalization alone (i.e., the spherical symmetric model) is only 2% worse [30].<br>
<br>
Concluding Remarks<br>
Taken together, the effect of orientation selectivity on redundancy reduction is very limited within the common linear filter bank model of V1 simple cells. In contrast to Zhaoping and coworkers, we do not claim that higher-order redundancy minimization is unlikely to be the main constraint in shaping the cortical receptive fields [22],[27]. Our conclusion is that although there are significant higher-order correlations in natural images, orientation selective filtering turns out to be not very effective for capturing these. Nevertheless, we do expect that visual representations in the brain aim to model those higher-order correlations, because they are perceptually relevant. Therefore, we think it is important to further explore which type of nonlinear transformations would be suitable to capture more pronounced higher-order correlations. The objective functions studied in this paper are related to factorial coding, density estimation and minimization of the pixel mean square reconstruction error. Of course, there are also other alternatives that are interesting, too. For example, Zhaoping proposed that one possible goal of V1 is to explicitly represent bottom-up saliency in its neural responses for visual attentional selection [12]. As a further alternative, we are currently trying to extend the efficient coding framework to deal with other loss functions. Obviously, the goal of the visual system is not to preserve the pixel representation of the visual input. Instead, seeing serves the purpose to make successful predictions about behaviorally relevant aspects of the environment [74]. Since 3D shape inference is necessary to almost any naturally relevant task, it seems particularly interesting to explore the role of orientation selectivity in the context of 3D shape inference [75]. For a quantitative account of this problem one can seek to minimize the reconstruction error for the 3D shape rather than for its 2D image. Certainly, this task is much more involved than image reconstruction. Nevertheless, we need to think more about how to tackle the problem of visual inference within the framework of unsupervised learning in order to unravel the principles of neural processing in the brain that are ultimately responsible for our ability to see.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2947980</b><br>
Ten Simple Rules for Editing Wikipedia<br>
<br>
<br>
Wikipedia is the world's most successful online encyclopedia, now containing over 3.3 million English language articles. It is probably the largest collection of knowledge ever assembled, and is certainly the most widely accessible. Wikipedia can be edited by anyone with Internet access that chooses to, but does it provide reliable information? A 2005 study by Nature found that a selection of Wikipedia articles on scientific subjects were comparable to a professionally edited encyclopedia [1], suggesting a community of volunteers can generate and sustain surprisingly accurate content.<br>
For better or worse, people are guided to Wikipedia when searching the Web for biomedical information [2]. So there is an increasing need for the scientific community to engage with Wikipedia to ensure that the information it contains is accurate and current. For scientists, contributing to Wikipedia is an excellent way of fulfilling public engagement responsibilities and sharing expertise. For example, some Wikipedian scientists have successfully integrated biological data with Wikipedia to promote community annotation [3], [4]. This, in turn, encourages wider access to the linked data via Wikipedia. Others have used the wiki model to develop their own specialist, collaborative databases [5]?[8]. Taking your first steps into Wikipedia can be daunting, but here we provide some tips that should make the editing process go smoothly.<br>
Rule 1: Register an Account<br>
Although any visitor can edit Wikipedia, creating a user account offers a number of benefits. Firstly, it offers you privacy and security. Though counterintuitive, editors registered under a pseudonymous username actually have greater anonymity than those who edit ?anonymously?. A few of us have chosen to associate our accounts with our real identities. Should you choose to forgo pseudonymity on Wikipedia, your entire editing history will be open to indefinite scrutiny by curious Web searchers, including future colleagues, students, or employers. Do not forget this.<br>
As in academic circles, a good reputation helps your wiki career. By logging in you can build a record of good edits, and it is easier to communicate and collaborate with others if you have a fixed, reputable identity. Finally, registering an account provides access to enhanced editing features, including a ?watchlist? for monitoring articles you have edited previously.<br>
<br>
Rule 2: Learn the Five Pillars<br>
There are some broad principles?known as the ?five pillars??all editors are expected to adhere to when contributing to Wikipedia. Perhaps most important for scientists is the appreciation that Wikipedia is not a publisher of original thought or research [9]. Accordingly, it is not an appropriate venue to promote your pet theory or share unpublished results. It is also not a soapbox on which to expound your personal theories or a battleground to debate controversial issues. In this respect, Wikipedia fundamentally differs from other types of new media, such as blogs, that encourage editorializing.<br>
Contributing to Wikipedia is something to enjoy; a natural extension of your enthusiasm for science. But differences of opinion inevitably arise, particularly on pages provided for discussion on how to improve articles. Treat other editors as collaborators and maintain a respectful and civil manner, even in disagreement [10]. If you begin to find a particular interaction stressful, simply log off and come back another time. Unlike most scientific enterprises, Wikipedia has no deadlines.<br>
<br>
Rule 3: Be Bold, but Not Reckless<br>
The survival and growth of any wiki requires participation. Wikipedia is unmatched in size, but its continuing success depends on the regular contributions of tens of thousands of volunteers. Therefore, Wikipedia urges all users to be bold: if you spot an error, correct it. If you can improve an article, please do so. It is important, however, to distinguish boldness from recklessness. Start off small. Begin by making minor modifications to existing articles before attempting a complete rewrite of History of science.<br>
Many new editors feel intimidated about contributing to Wikipedia at first, fearing they may a mistake. Such reticence is understandable but unfounded. The worst that can happen is your first edits are deemed not to be an improvement and they get reverted. If this does occur, treat it as a positive learning experience and ask the reverting editor for advice.<br>
<br>
Rule 4: Know Your Audience<br>
Wikipedia is not primarily aimed at experts; therefore, the level of technical detail in its articles must be balanced against the ability of non-experts to understand those details. When contributing scientific content, imagine you have been tasked with writing a comprehensive scientific review for a high school audience. It can be surprisingly challenging explaining complex ideas in an accessible, jargon-free manner. But it is worth the perseverance. You will reap the benefits when it comes to writing your next manuscript or teaching an undergraduate class.<br>
<br>
Rule 5: Do Not Infringe Copyright<br>
With certain conditions, almost all of Wikipedia's content is free for anyone to reuse, adapt, and distribute. Consequently, it does not accept non-free material under copyright restriction. Some journals, including those from the Public Library of Science, publish material under an open-access license that is compatible with use in Wikipedia if properly attributed. Most do not. Therefore, although it may be tempting, avoid copying text or figures from your latest review article (or anyone else's) into Wikipedia. It will quickly be identified as a copyright violation and flagged for immediate deletion.<br>
You can give Wikipedia permission to use material you own, but this process is non-reversible and can be time consuming. It is often better to rewrite the text in simpler language or redraw the figure to make it more accessible. This will also ensure it is more suitable for Wikipedia's non-expert readership (see Rule 4).<br>
<br>
Rule 6: Cite, Cite, Cite<br>
To maintain the highest standards possible, Wikipedia has a strict inclusion policy that demands verifiability [11]. This is best established by attributing each statement in Wikipedia to a reliable, published source (but see Rules 7 and 8 on excessive self-citing). Most scientists are in the fortunate position of having access to a wide body of literature, and experience in using inline citations to support their writing. Since unverified content may be removed from Wikipedia at any time, provide supporting citations for every statement that might be challenged by another editor at some point in the future. Whenever possible, give preference to secondary sources (such as reviews or book chapters) that survey the relevant primary research over research articles themselves.<br>
Wikipedia's accessibility makes each of its scientific articles an excellent entry point for laypeople seeking specialist information. By also providing direct hyperlinks to reliable, freely accessible online resources with your citations (biological databases or open-access journals, for example), other editors can quickly verify your content and readers have immediate access to authoritative sources that address the subject in greater detail.<br>
<br>
Rule 7: Avoid Shameless Self-Promotion<br>
Many people are tempted to write or edit Wikipedia articles about themselves. Resist that urge. If you are sufficiently notable to merit inclusion in an encyclopedia, eventually someone else will write an article about you. Remember that unlike a personal Web page, your Wikipedia biography is not yours to control. A lovingly crafted hagiography extolling your many virtues can rapidly accumulate information you would rather not be publicized. You may already have a Wikipedia biography, but it contains factual inaccuracies that you wish to correct. How do you do this without breaking the rules? Wikipedia's guidelines encourage you to provide information about yourself on the associated discussion page, but please permit other editors to add it to the article itself.<br>
Think twice, also, before writing about your mentors, colleagues, competitors, inventions, or projects. Doing so places you in a conflict of interest and inclines you towards unintentional bias [12]. If you have a personal or financial interest in the subject of any article you choose to edit, declare it on the associated discussion page and heed the advice of other editors who can offer a more objective perspective.<br>
<br>
Rule 8: Share Your Expertise, but Don't Argue from Authority<br>
Writing about a subject about which you have academic expertise is not a conflict of interest [12]; indeed, this is where we can contribute to Wikipedia most effectively. Jimmy Wales, co-founder of Wikipedia, told Nature that experts have the ability to ?write specifics in a nuanced way?, thereby significantly improving article quality [1]. When writing in your area of expertise, referencing material you have published in peer-reviewed journals is permitted if it is genuinely notable, but use common sense (and revisit Rule 7). For example, if you have an obscure, never-been-cited article in the Journal of New Zealand Dairy Research discussing the RNA content of cow milk, then referencing this in the introductory paragraph of the Wikipedia articles on ?RNA?, ?Milk?, ?Cow?, and ?Evolution of mammals? is not a good idea.<br>
Occasionally you may interact with another editor who clearly does not share your expertise on the subject of an article. This can often prove frustrating for experts and is the basis of much academic angst on Wikipedia [1]. On such occasions, remember that you are assessed only on your contributions to Wikipedia, not who you are, your qualifications, or what you have achieved in your career. Your specialist knowledge should enable you to write in a neutral manner and produce reliable, independent sources to support each assertion you make. If you do not provide verification, your contributions will be rightly challenged irrespective of how many degrees you hold.<br>
<br>
Rule 9: Write Neutrally and with Due Weight<br>
All articles in Wikipedia should be impartial in tone and content [13]. When writing, do state facts and facts about notable opinions, but do not offer your opinion as fact. Many newcomers to Wikipedia gravitate to articles on controversial issues about which people hold strong opposing viewpoints. Avoid these until familiar with Wikipedia's policies (see Rule 3), and instead focus on articles that are much easier to remain dispassionate about.<br>
Many scientists who contribute to Wikipedia fail to appreciate that a neutral point of view is not the same as the mainstream scientific point of view. When writing about complex issues, try to cover all significant viewpoints and afford each with due weight, but not equal weight. For example, an article on a scientific controversy should describe both the scientific consensus and significant fringe theories, but not in the same depth or in a manner suggesting these viewpoints are equally held.<br>
<br>
Rule 10: Ask for Help<br>
Wikipedia can be a confusing place for the inexperienced editor. Learning Wiki markup?the syntax that instructs the software how to render the page?may appear daunting at first, though the recent implementation of a new editing toolbar has made this easier, and usability development is ongoing. The intersecting guidelines and policies (and the annoying tendency of experienced editors to use an alphabet soup of acronyms to reference them) can also be tricky to comprehend. Thankfully, the Wikipedia community puts great stock in welcoming new editors. Guidance is available through a number of avenues, including help desks, a specific IRC channel, and an Adopt-a-User mentorship program. You can even summon help using a special template?{{helpme}}?and, as if by magic, a friendly Wikipedian will appear to offer one-on-one assistance.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2947985</b><br>
HIV Promoter Integration Site Primarily Modulates Transcriptional Burst Size Rather Than Frequency<br>
Mammalian gene expression patterns, and their variability across populations of cells, are regulated by factors specific to each gene in concert with its surrounding cellular and genomic environment. Lentiviruses such as HIV integrate their genomes into semi-random genomic locations in the cells they infect, and the resulting viral gene expression provides a natural system to dissect the contributions of genomic environment to transcriptional regulation. Previously, we showed that expression heterogeneity and its modulation by specific host factors at HIV integration sites are key determinants of infected-cell fate and a possible source of latent infections. Here, we assess the integration context dependence of expression heterogeneity from diverse single integrations of a HIV-promoter/GFP-reporter cassette in Jurkat T-cells. Systematically fitting a stochastic model of gene expression to our data reveals an underlying transcriptional dynamic, by which multiple transcripts are produced during short, infrequent bursts, that quantitatively accounts for the wide, highly skewed protein expression distributions observed in each of our clonal cell populations. Interestingly, we find that the size of transcriptional bursts is the primary systematic covariate over integration sites, varying from a few to tens of transcripts across integration sites, and correlating well with mean expression. In contrast, burst frequencies are scattered about a typical value of several per cell-division time and demonstrate little correlation with the clonal means. This pattern of modulation generates consistently noisy distributions over the sampled integration positions, with large expression variability relative to the mean maintained even for the most productive integrations, and could contribute to specifying heterogeneous, integration-site-dependent viral production patterns in HIV-infected cells. Genomic environment thus emerges as a significant control parameter for gene expression variation that may contribute to structuring mammalian genomes, as well as be exploited for survival by integrating viruses.<br>
<br>
Introduction<br>
The life cycle dynamics of HIV-1 within a host are shaped by numerous apparently stochastic processes, from the statistics of immune cell infection in humans, to mutation during reverse transcription, semi-random integration of the proviral DNA into the host-cell chromosome, and stochastic viral gene expression thereafter [1]?[7]. We and others have experimentally shown that expression from the HIV-1 promoter is indeed stochastic and shaped by host factors at the viral integration site [4], [8], [9], [10], and we have argued as well that the resultant expression heterogeneities are important in the genesis of viral latency [4], a ubiquitous feature of infection that currently confounds our ability to cure HIV in patients [7], [11], [12], [13]. Gaining a deeper understanding of the factors that influence cell-cell variability in viral gene expression may thus shed light on how to ameliorate the effects of latency, and more generally on the processes that affect the expression of any gene.<br>
The semi-random integration of HIV-1 into the host genome provides a particularly ideal opportunity to dissect the relative contribution of genomic environment as a fundamental element of expression regulation that may contribute importantly to expression dynamics and heterogeneities in eukaryotes. It is now well established that HIV-1 integration is biased towards actively transcribed chromosomal locations [3], [14], and it has been demonstrated that mean expression levels from model HIV-1 viruses correlate with specific epigenetic features at their integrations [9] and of their surrounding genomic regions [3]. Prior studies in other systems focused on how the population average expression of genetic constructs depends on integration context, and have found correlations with the expression levels of surrounding genes and with the local 3-D chromatin structure [15], as well as with DNA methylation, nucleolar association, and DNA diffusional mobility [16]. Importantly, these studies inform us about the features of genomic environment that might affect mean expression levels. However, the effects of genomic environment, or integration site, on stochastic expression and heterogeneity have not yet been explored.<br>
The discrete and stochastic nature of gene expression has been appreciated for some time [17], [18], [19], and it has become increasingly recognized that the resulting expression variability may significantly impact diverse biological functions, shaping the outcomes of cellular decisions, being exploited as a tool for survival in changing environments, and often inducing qualitatively different behaviors than would be predicted from a deterministic understanding [20]?[26]. Theoretical and computational analyses have explored the relative contributions of key processes to heterogeneity in gene expression, including open-complex formation, transcriptional elongation, translation, post-transcriptional and translational processing/modification, as well as chromatin regulation [27]?[32]. Importantly for this study, the latter, an integral element of epigenetic control over gene expression, yields potentially slow and probabilistic dynamics that have been postulated as a significant source of expression heterogeneity in eukaryotes. In parallel with these theoretical studies, experimental approaches have been developed to characterize expression noise arising both ?intrinsically? from the biochemical processes directly involved in the expression of any individual gene, as well as ?extrinsically? from variability in other cellular processes that more homogeneously affect the expression dynamics of groups of genes simultaneously, such as cell cycle or concentration fluctuations of upstream transcription factors [33]?[37]. Interestingly, genome-scale measurements of expression heterogeneity demonstrate correlations with gene functional class, implying that perhaps noise is a ?selected? feature of a gene's expression pattern [38]?[43].<br>
Despite the apparent complexity of cellular transcriptional regulation, for many genes across a broad range of cell types, the patterns of cell-to-cell expression variability within isogenic populations are remarkably well described by simple stochastic models that represent the gene ? including the associated genomic environment, chromatin structure, transcriptional regulators, and transcriptional machinery ? as existing in a small number of discrete configurations, or states, with expression heterogeneities depending on probabilistic transitions between states and on probabilistic transcript and protein production and degradation [27], [44]. These models are often necessarily abstract, yet they parsimoniously capture many essential features of transcriptional biology. Model fits to clonal single-cell experimental data, primarily in Saccharomyces cerevisiae for eukaryotic studies, has allowed the inference of underlying gene-state and transcriptional dynamics that largely account for observed expression heterogeneities in a number of instances [39], [45]?[49].<br>
A wide range of transcriptional dynamics have been revealed by such analyses, from continuous [38], [48], to ?pulsatile? [45], [47], to ?bursty? [46], [48]. These diverse dynamics are effectively characterized by the frequency of gene-activation events, their duration, and the number of transcripts produced during each event [29], [30], [48], [50], and contrasting results have emerged concerning the relative contributions of cellular regulation of each of these quantities to specifying the expression pattern of any given gene. For example, several pioneering studies, of single, targeted integrations of inducible, synthetic constructs, in yeast have suggested that the concentration of inducer largely controls the frequency of gene-activation events rather than the number of transcripts produced by each event [45], [47]. A subsequent study in yeast ? which considered three targeted integrations of similar constructs into 1) adjacent locations on a single chromosome, 2) homologous locations on sister chromosomes, or 3) non-homologous chromosomal locations ? similarly found that transcriptional activation frequency varied between locations [36]. Genome-scale studies of stochastic gene expression in yeast suggest as well that the primary feature of transcriptional dynamics that varies between genes, over a wide range of genes, is the frequency of transcriptional activation events [38], [39]. In contrast, an elegant study in mammalian cells quantified expression heterogeneities ? from a single, random integration of a Tet-inducible construct into one locus in the genome ? by using fluorescent in-situ hybridization (FISH) to directly visualize single transcripts [46]. The authors concluded that transcripts are produced in bursts, and that the typical number of transcripts produced during a burst (referred to as the ?transcriptional burst size?), rather than the frequency of bursting, was the primary measure that varied with tetracycline induction level.<br>
While the above studies have begun to characterize the dependence of gene-expression dynamics on a number of cellular inputs, a systematic, quantitative investigation of the contribution of genomic environment over a broad range of genomic integration positions remains to be conducted. Furthermore, the contrasting observations as to whether transcriptional activation frequency, transcriptional burst size, or some other feature of transcriptional dynamics represents the primary variable that cells modulate to control expression patterns in these diverse systems raise key questions of how important features of genetic, epigenetic, and regulatory architecture may differ in yeast and mammalian cells.<br>
Here we explore the fundamental relationship between genomic environment and expression heterogeneity from a diverse set of semi-random single integrations of a model HIV-1-promoter/GFP-reporter construct in cultured Jurkat T-cells. Systematically and rigorously fitting a model of stochastic gene expression allows us to infer the underlying expression dynamics that account for the single-gene expression distributions that we measure from single-integration clonal populations. Our analysis reveals that transcript production in bursts accounts for the wide, highly skewed, expression profiles that we observe, and importantly that transcriptional burst size is the primary feature that varies across viral integrations. These results interestingly suggest that the virus samples a particularly ?noisy? range of possible expression profiles across cellular integrations, and open a number of important questions for further study. We propose several qualitative models that may explain this inferred variation of transcriptional dynamics with genomic environment and discuss the implications of our findings for HIV dynamics, and for cellular gene expression in general.<br>
<br>
Results<br>
HIV-1 LTR distributions are wide and highly skewed<br>
Although HIV-1 requires transactivation by the virally-encoded protein Tat to amplify its expression [51], the HIV-1 promoter still supports basal transcription in the absence of Tat [9], which occurs initially after viral infection but before significant viral protein is produced. The dynamics of this basal expression, and the associated expression heterogeneities that result, may play an important role in affecting the cellular ?decision? between lytic viral production and latency [4]. To study heterogeneities in basal expression from the HIV promoter, we infected Jurkat T-cells, at a low multiplicity of infection (MOI), with a model HIV-1 virus that contains the full-length LTR driving expression of a GFP reporter but no viral genes. Cells with single integrations were isolated by fluorescence activated cell sorting (FACS) and expanded into clonal populations. The resulting clonal GFP expression profiles were quantified by flow cytometry and smoothed for comparison to model distributions in the analysis that follows. Thirty-one such clones, with average florescence levels ranging over an order of magnitude, and expression profiles clearly distinguishable from a measured autofluorescence profile, were selected for analysis (Fig. 1A). Integrations whose mean fluorescence was less than twice the autofluorescence mean were not included in our analysis.<br>
The shape features of our experimental distributions (such as mean, variance, skewness, etc.) are diagnostic of the underlying expression dynamics that generate them ? and of the regulatory role of various molecular ?inputs? such as integration position (as well as promoter structure and concentrations of transcription factors, which were the ?inputs? considered in several other elegant studies: [38], [45], [46], [47]). For example, a simple model assuming transcript number fluctuations as the primary source of expression heterogeneity, with only the rate of constant transcript production varying with a given ?input?, predicts a Poisson-like distribution shape variation whereby distribution variances ( considered as a measure of distribution width and expression heterogeneity) vary proportionately to the mean (, for mean ). Such a variation, illustrated by the lower dashed line in Fig. 1B (?Poisson?), has been observed over a large set of yeast promoters under multiple experimental conditions [38], [39]. Alternatively, a model in which distribution shape variations are effectively described by a simple scaling of single-cell fluorescent values by an ?input-controlled? constant value (, where  is the probability of observing fluorescence , for a normalized value of the ?input-controlled? parameter ) would predict distribution variances to vary in proportion to the mean squared (, Fig. 1B upper dashed line, ?Scaling?). Such a shape variation might arise if heterogeneities are instead due primarily to probabilistic transitions between promoter configurations that specify different transcription rates, with only these transcription rates varying (proportionately) with the ?input? from one clonal distribution to the next. In contrast to these possibilities, we find that the trend in distribution-shape variation over our set of clonal populations is best described by a relationship where the distribution variance changes proportionately to the mean raised to the 1.7?0.2 power (Fig. 1B, solid regression). This characteristic trend differs significantly from either of the above simple models (P&lt;0.025), suggesting that neither is sufficient, and that integration-site variation may specify a more complex modulation of promoter and transcriptional dynamics in our system.<br>
To visualize additional features of the expression distributions over the set of clones, we translated each to a common mean fluorescence, and correspondingly scaled its fluorescence values about that mean based on the variance regression in Fig. 1B, revealing a ?typical? distribution shape that is wide and highly skewed (Fig. 1C). These features are signatures of a bursty underlying transcriptional dynamic [27], [50], as we discuss in further depth below.<br>
<br>
A two-state gene model of transcriptional bursting qualitatively captures characteristic HIV-LTR distribution shapes and variation over viral integrations<br>
A simple stochastic model that captures a number of essential features of transcriptional biology, and that can reproduce a range of single-gene expression profiles, assumes that the promoter may exist in either an activated state (?a) that produces mRNA probabilistically at a fixed rate (?t+), or repressed state (?r) that is unproductive (Fig. 2A). These model states may represent different characteristic configurations of chromatin and/or transcriptional complexes, with transitions between them occurring at rates ?a and ?r. Together with the active-state transcription rate, these lumped parameters represent contributions from diverse modes of genetic and epigenetic transcriptional regulation that may depend essentially on features of the genomic environment at the viral integration sites. Variants of this model have been used in other studies as well to analyze single-gene expression data [45]?[49] and have also been studied theoretically [27], [52], [53], [54].<br>
In the analysis that follows, we always consider steady-state model distributions, since longitudinal measurements over the course of a week on several clones indicate that distribution shapes are relatively stable over our time scale of interest (see Fig. S3 and  Sec. S.VII for further discussion). Furthermore, we determine all rate constants relative to the transcript degradation rate (, estimated to be approximately 0.2 h?1, see Text S1 Sec. S.V), as their relative rather than absolute values determine expression profiles at steady state. In addition, we adopt the working hypothesis that our experimental distribution shapes are determined by the intrinsic processes represented in our model at fixed values of its rates ? possible contributions of extrinsic sources of variability have been considered in earlier work on this system [4] and are discussed further in the Supporting Information (Text S1, Sec. S.VIII).<br>
The qualitative expression regimes of the two-state gene model fundamentally depend on the relative values of the gene-state transition rate constants (Fig. 2B), with different dynamics corresponding to different potential underlying transcriptional regulatory mechanisms. ?Fast? gene-state dynamics (, perhaps specified by fast binding and unbinding of transcription factors) approximate continuous transcription from a single fixed gene state and can generate relatively narrow Poisson-like expression profiles, which widen for ?Intermediate? dynamics (). ?Slow? gene-state dynamics (, due perhaps to slower changes in chromatin configuration) may generate multiple transcripts after each transition to a relatively stable active state, and the dynamics can be described as ?pulsatile? [45], [47]. Distributions become bimodal in the extreme case.<br>
Another dynamic regime that has received considerable attention can be termed the transcriptional ?bursting? regime, in which the gene inactivation rate is fast (), and the transcription rate is sufficiently large ( not small) that transcriptional bursts of average size  are produced during short excursions of frequency  to a relatively unstable active gene-state (see Text S1, Sec. S.V for further discussion, and Refs. [29], [30], [48], [50], [55]). Distributions in the ?bursting? regime are wide and highly skewed, in qualitative agreement with the ?typical? HIV-LTR distributions from our measurements (compare Fig. 1C and 2B, solid curves), with both the protein and transcript distribution means and variances approximately demonstrating a relatively simple dependence on transcriptional burst size and frequency: ;  (see Text S1, Secs. S.III ? S.V). Indeed, by assuming a model solution in the ?bursting? regime, one can analytically calculate a unique transcriptional burst size and burst frequency that reproduce the mean and variance of each of our experimental distributions, with good qualitative agreement in distribution shape (see Text S1, Secs. S.III and S.VI for further discussion). Furthermore, variation in transcriptional burst size and frequency, individually or in combination, can account for the range of distribution-shape variation discussed in Fig. 1B, with the best agreement to our experimental observations occurring if burst size and burst frequency typically vary simultaneously, but with the dominant effect coming from burst-size variation (Fig. 2C).<br>
Though the relatively slow time scale of protein degradation in our system () effectively ?filters? some of the dynamic information propagated from model transcript to protein distributions, we emphasize that the calculated protein distribution shapes still reflect the underlying transcript distribution shapes and demonstrate distinctive features in each expression regime (Fig. 2B). Below, through careful analysis, we will make use of this observation, building on the qualitative analysis developed here, to quantitatively infer the underlying heterogeneity-generating gene-state and transcriptional dynamics within our system from measured protein expression distributions, and to determine quantitative bounds on our ability to distinguish between different dynamic regimes. Of considerable benefit for this analysis, cytometry-based protein measurements can be acquired rapidly (e.g. compared to microscopy-based transcript-counting measurements), allowing good resolution of the probability distributions that underlie the expression histograms collected over populations of cells, and enabling measurements on sufficient numbers of clones to identify trends in the variation of single-gene expression distributions over integration sites.<br>
<br>
Bursting gene expression quantitatively accounts for HIV-1 LTR integration-clonal distributions<br>
While the analysis above provides intuition as to the dynamics and regulation that may underlie our experimental observations of the HIV LTR, it is solely a qualitative assessment based on the assumption of ?burstiness? and comparisons to ?stereotypical? model distributions. In reality, model distributions vary continuously between regimes, and means and variances provide an incomplete characterization of the actual distribution shapes. Therefore, to better determine the degree to which transcriptional ?bursting? best accounts for our experimental distributions, and the degree to which it can be distinguished from other possible dynamic regimes, we used a systematic fitting routine to identify the best-fit combination of transcription rate and gene-state transition rates for each distribution. Transcript degradation, protein production, and protein degradation rates (?t?, ?p+, and ?p?, Fig. 2A) were fixed at values that were separately measured.<br>
An important indicator of the dynamic regime of our system is the average time that the promoter remains in the active configuration following a gene activation event, relative to the average life time of a transcript (see Fig. 2B), specified by ??=??t?/?r, which we refer to as the ?active duration? (?). We therefore began by identifying best-fit sets of model parameters for each clone over a range of fixed values for ?. We arrived at a robust estimate of the range of parameters for which the model quantitatively accounts for our measured distributions by considering the ratio, Devr, of each best-fit deviation at a given ?, to a bootstrap-estimated 95% upper bound on the deviation expected due to uncertainty in our measurements, which served as a metric for identifying model fits whose quality was statistically comparable. Fits for which the values of Devr differ by less than 1 for a given clone were considered to be effectively indistinguishable, since their differences may be accounted for by uncertainty in our experimental data, and these fits were thus considered to identify a range of parameters for which the model gives a statistically comparable account. The work-flow for our analysis is summarized in Table S1; the definitions that we used to quantify fit deviations, as well as the error model used for our bootstrap error calculation, are discussed briefly in the Materials and Methods, and in more depth, Text S1, Secs. S.I and S.VII, together with Fig. S1.<br>
We find that the optimal agreement between model and experiment always occurs at short active-state durations (sample fits given in Fig. 3A), with deviations increasing for larger values of ? (Fig. 3B), past a distinguishability cut-off (where Devr has increased by 1) that effectively marks the resolution limit of our analysis, which we call ?Max for each clone (Fig. 3C). The value of ?Max defines a range of active durations (bounded below by ??=?0), for which the quality of model fits for a given clone is comparable, and acts as a measure of how well our analysis can distinguish a ?bursty? underlying dynamic from other regime possibilities. Small values of ?Max indicate model fits where short-lived gene activation events, which are a hallmark of transcript production in bursts, provide a significantly better account of our experimentally measured distributions than a less noisy dynamic (i.e. one specified by longer active durations). Because we do find that the best model fits always occur at the shortest active durations (where the relative deviation Devr?=?DevrOpt, Fig. 3B), we conclude that a transcriptional dynamic in the ?bursting? regime does indeed always give the best quantitative account of our data, and we further note that larger predicted transcriptional bursts (generally associated with brighter clones) are correlated with better regime resolution (Fig. 3C). Finally, we note that our systematic distribution fitting procedure always resulted in improved fits over those obtained by only considering the first two distribution moments, with the improvement often statistically significant. Nevertheless, small systematic deviations remained, which are discussed further in Fig. S2.<br>
<br>
Transcriptional burst size is the primary feature that varies across viral integrations<br>
From the optimal fits above we identified best-fit transcriptional burst sizes and frequencies that specify the predicted transcriptional dynamics for each integration clone. Importantly, we find that the transcriptional burst size is the primary feature that varies over the set of genomic environments sampled by our 31 viral integrations, increasing from a few transcripts in very dim clones to tens of transcripts in very bright clones (Fig. 4A, with ?/??=?3.5 for the distribution of log10(b)). Consistent with the qualitative analysis in Fig. 2C, we find that transcriptional burst size varies approximately sub-linearly with expression-distribution mean (, R2?=?0.66). In contrast, the transcriptional burst frequencies inferred through our analysis are scattered about a characteristic value of one burst per several transcript degradation times, corresponding to several transcriptional bursts per cell-division time (Fig. 4B). In addition, these frequency values vary no more than several-fold (?/??=?2.2 for log10(?a)), and they demonstrate little correlation with distribution mean (, R2?=?0.2). These results were maintained, to within the accuracy of our analysis, when the scattering gate used to control for cell-size variability in our experimental distributions was decreased by a factor of 6 from the value that was found to be optimal for our analysis (see Text S1, Sec. I and Fig. S4), indicating a robustness to this source of uncertainty, which had been found to significantly impact results from other cytometry-based analyses of expression variability [38]. Further, we find no significant correlation between the inferred transcriptional burst sizes and burst frequencies that might influence the interpretation of their trends with expression mean (see Fig. S5).<br>
Our findings thus indicate that burst-size variation makes the dominant contribution in controlling single-gene expression profiles and represents the primary feature of transcriptional dynamics whose modulation distinguishes typical bright from dim clones. Importantly, the trends noted in Fig. 4 characterize the modulation of a ?typical? LTR integration by the sampled genomic environments. However, we must emphasize that the significant scatter of both the burst sizes and burst frequencies inferred for each individual clone about these ?typical? variations, as well as the possibility that a different trend may exist for very dim integrations (which were not considered in this study), suggest a potentially richer behavior that may still be uncovered through further study.<br>
Another recent study has also considered a two state model to analyze expression variability from the HIV LTR [56]. This study similarly suggests that transcript production occurs in bursts and that both burst size and frequency vary with LTR integration position, though the analysis is qualitative, based only on consideration of distribution moments. In contrast to our findings, they emphasize burst frequency modulation as structuring distribution-shape variation over integration positions, as well as in response to pharmacological perturbation, though the later finding is difficult to interpret, as a steady-state model is considered to analyze data that are clearly varying in time. Additional quantitative analysis, including systematic model fitting, would be necessary to characterize the relative contributions of burst-size and burst-frequency modulation in this study, and to determine whether its findings are consistent with our own.<br>
<br>
Distinguishing modes of integration-site regulation of transcriptional dynamics<br>
A correlate of our findings ? that transcription in short bursts underlies basal expression heterogeneities from the HIV LTR in the absence of Tat ? is that the active promoter configuration is short-lived. This implies that the promoter would be observed in the active configuration for only a small fraction of cells in a clonal population at any given time at steady state. The value of this fraction in the two-state model, which we refer to as the ?active fraction?, f, is related to the activation frequency (?a, whose value is relatively well resolved for each clone by our analysis, Fig. 4B) and active duration (?, for which our analysis only provides an upper bound ?Max, Fig. 3C), as . Our analysis provides a predicted upper bound on this fraction for each clone as  (Fig. 5, bars), where any value of f below  is consistent with our analysis. Small values of  specify clones for which the active fraction is indeed predicted to be small, while larger values indicate clones for which its value is less well resolved. In particular, our analysis predicts that although the brightest and dimmest integration clones considered in our study differ in mean expression by a factor of approximately 30, the brightest clones will nevertheless only be observed with the integrated LTR in the ?active? transcript-producing configuration less than 20% of the time.<br>
Transcriptional burst size ? defined by the ratio of the transcription rate to promoter-inactivation rate (or the product of transcription rate and the active duration ) ? can be modulated by two qualitatively difference ?Modes? of regulation. First, integration position could affect the dynamics of promoter inactivation (), reflecting integration-position effects on the stability of the active configuration, possibly due to direct effects of the surrounding chromatin configurations on the energetics of the active configuration and/or the stabilizing effects of regulatory factor recruitment by surrounding regions (Mode 1). Alternately, integration position could affect transcriptional productivity in the active state (), which may also be affected by modulation of chromatin configuration and/or recruitment of regulatory factors by surrounding genomic regions (Mode 2). We have seen in our analysis to this point, that model fits of our cytometry data cannot separate the two constituent parameters that define transcriptional burst size, and therefore they cannot resolve these two possible ?Modes? of its regulation (e.g. Fig. 3; a similar parametric indeterminacy has been noted by [46], [48]). Furthermore, the potentially overlapping effects of many molecular regulatory mechanisms on transcriptional dynamics may make it difficult to define experiments that directly distinguish these ?Modes?, and to decouple their regulatory contributions.<br>
However, our analysis predicts that each ?Mode? of control leads to a distinct pattern of active-fraction variation over the set of integration clones (Fig. 5, symbols): for Mode 1 the active-fraction varies proportionately to the clonal expression mean, whereas for Mode 2 the scatter in active fraction predicted over our set of integration clones reflects scatter in the predicted burst frequencies. We thus suggest that future experimental analysis of the active fraction may provide a means of distinguishing between these two key ?Modes? of integration-site modulation of gene expression.<br>
<br>
<br>
Discussion<br>
Our findings, that expression from the HIV promoter is characterized by transcript production in bursts and that the site of viral integration primarily modulates transcriptional burst size, contribute to an emerging paradigm for transcriptional regulation that emphasizes the importance of stochastic/probabilistic dynamics [20], [27], [50], [57]. In particular, the expression patterns that we observe from single integrations of the HIV promoter cannot be accounted for by transcription from a single, fixed state of promoter activation, which would involve a single transcription rate that specifies a comparatively narrow single-gene expression profile with little variation over a population of cells. Rather, our analysis predicts that the large expression heterogeneities observed in this system (Fig. 1) are shaped by probabilistic transitions between (at least) two distinct configurations (Fig. 2A), with the promoter spending only the minority of time in the transcriptionally active configuration even for the most productive integrations (Fig. 2B, Figs. 3B,C and Fig. 5). Furthermore, our analysis suggests that an essential component of the regulatory effect of genomic environment at the viral integration site is to modulate the dynamics of transitions between states of differing transcriptional activity, in addition to possible effects on the transcriptional activity of each state (Figs. 4 and 5). Of note, it is only by systematically fitting a quantitative model to our measurements that these underlying dynamics were revealed, as quantitative single-cell measurements of protein expression only provide an indirect measure, and it is only by applying our systematic analysis to observations across a diverse sampling of integration-modulated expression patterns that we succeeded in extracting a characteristic effect of integration position on transcriptional dynamics.<br>
What features of the HIV-LTR determine ?bursty? transcription?<br>
Transcript production in bursts is a particularly ?noisy? transcriptional dynamic that can generate significant cell-to-cell expression variability, which is reflected in wide and highly skewed single-gene expression distributions across clonal populations (Fig. 1C, 2B). In particular, the ?typical? distribution identified in Fig. 1C demonstrates a coefficient of variation (standard deviation/mean, or relative width), corresponding to 60% variability. This value is significantly larger than the values observed for most eukaryotic promoters in several large-scale studies (compare to data in: [38], [39], [58]), and we anticipate that a number of features of the HIV promoter, some of which are common in mammalian promoters, may conspire to account for this ?noisy? expression pattern.<br>
Similar to HIV expression shortly after infection, our system lacks the viral transcriptional activator Tat. In the absence of Tat the LTR has been observed to bind repressive factors that maintain a non-conducive chromatin configuration [14], , and the likely greater stability of this ?inactive? configuration may limit the fraction of time that a transcriptionally ?active? configuration can be maintained. On the other hand, like many mammalian promoters, the HIV LTR contains multiple binding sites for repressing and activating elements (which still bind in the absence of Tat), several of which affect chromatin state and bind competitively and/or cooperatively. For example, the histone-acetyltransferase (HAT) p300 and the activating NF-?B component RelA are thought to bind their respective HIV-1 Sp1 and NF-?B sites cooperatively to activate transcription, and in competition with the histone deacetylase (HDAC) recruiting activity of SP1 and the p50/p50 homo-dimer that bind the same sites respectively to inhibit transcription [10], [62]?[66]. One may hypothesize that this competition could thus lead to an infrequent all-or-none binding of activating factors that directly remodel promoter-bound nucleosomes to establish a transcriptionally conducive chromatin configuration [10]. In addition, the LTR includes a number of other cis-regulatory elements that bind transcriptional activators such as NFAT and AP-1 [67], [68], as well a TATA motif that contributes core transcriptional complexes [69], [70]. These elements may enable more efficient recruitment, assembly, and stabilization of a productive transcription complex, with transcriptional reinitiation potentially yielding multiple transcripts from each gene-activation event (the presence of a TATA box has been linked to increased expression noise in other studies as well, see for example: [41], [47], [71]). In combination, the above features may specify transcript production during short, infrequent bursts, consistent with the results of our analysis.<br>
Intriguingly, a recent mammalian genome-wide mapping of HAT and HDAC association found them simultaneously bound to a large number of active promoters, suggesting that simultaneous regulation by competitive epigenetic regulators may be more common than previously thought [72]. It is therefore possible that transcript production in bursts represents a more general feature of mammalian expression regulation, and it will be interesting to discover how properties of the HIV promoter shape its transcriptional dynamics, and whether similar promoter architectures specify ?bursty? dynamics for other genes.<br>
<br>
Significance of burst-size variation over LTR genomic integrations<br>
Our findings suggest that transcriptional burst size is a more ?locally? determined property, more sensitive to those features of genomic environment that vary significantly between integration sites, whereas transcriptional burst frequency is, by comparison, a more ?globally? determined feature, specified by interactions with the cellular environment that may be more promoter-specific but less significantly integration-site dependent. Burst frequency reflects the statistics of assembling the more active promoter configuration from an inactive one, and we might speculate that this transition depends in part on large-scale chromatin reorganization and dynamics that are coordinated globally across the nucleus [73], [74], [75]. Burst size, on the other hand, is a property of the transcriptionally ?active? configuration, and we may conjecture that some of the reorganization that accompanies its establishment also may provide opportunities for important ?local? features to exert their regulatory influences. For example, chromatin remodeling may expose new binding sites for transcriptional regulators [31], [76], and the initiation of transcriptional activity could contribute to association with ?nearby? transcription factories where additional transcriptional regulators are localized, and where interactions with surrounding (and possibly distant) genomic regions may be enhanced [73].<br>
At a more basic level, a feature of transcriptional burst size that could more generally account for a greater sensitivity to genomic environment is its complimentary dependence on transcriptional productivity and the stability of the active promoter configuration. We had noted earlier that this complimentary dependence specifies two distinct ?Modes? by which surrounding genomic regions may differentially affect the resulting transcriptional dynamics (see Fig. 5), both of which might be effected by recruitment of transcriptional regulators by surrounding genomic regions, epigenetic features of the surrounding regions, and the transcriptional activity of neighboring genes [15], [75], [77]. If we assume that a ?typical? more productive integration increases , ?, and  all proportionately (i.e. without assuming a weaker dependence for burst frequency), then the dual dependence of burst size would dictate that it vary as burst frequency squared (), and the scalings  and  would result, which fall within the 95% confidence interval of our regression analysis in Fig. 4. This possibility is consistent with our suggestion in the previous subsection that the architecture of the HIV LTR may effectively couple the control of gene activation and inactivation, and with the hypothesis that the chromatin regulators that may control these dynamics could also modulate the active-state transcription rate either directly or indirectly. Such a combined ?Mode? of modulation would specify an active-fraction variation intermediate between that predicted for the two pure ?Modes? of modulation considered in Fig. 5, and might be used to distinguish it experimentally.<br>
Burst-size variation with promoter induction level from a tetracycline-inducible construct at a single genomic position has been noted in another study using mammalian cells [46], though this result contrasts with a number of yeast studies that have identified the frequency of gene-activation events as the primary feature that varies with genetic-construct induction level [45], [47], over a single set of three targeted genomic loci [36], and over a large set of endogenous promoters [38], [39]. It thus remains to be determined whether our observation of burst-size variation represents a mode of transcriptional regulation particular to mammalian cells or to transcriptional control by genomic environment, or whether it is determined by any specific features of the HIV promoter that dictate a unique coupling to mammalian genomic environments that might be shared by other ?bursty? promoters and cell types. Future studies investigating greater numbers of genomic integrations, in our and other systems, that correlate expression variability with promoter and surrounding genomic sequences, may provide important answers to such questions.<br>
<br>
Basal transcription as a determinant of HIV-infected cell fate<br>
The observation that integration site primarily modulates transcriptional burst size from the HIV promoter implies that viral integrants sample a ?noisy? set of basal expression distributions by semi-randomly integrating in the genome. Specifically, relative distribution widths (i.e. the coefficient of variation) are approximately maintained and comparable between ?dim? and ?bright? integrations. This contrasts with the naive expectation that dimmer integrations should demonstrate greater relative expression heterogeneity due to larger relative fluctuations typically generated by smaller numbers of molecules, as would be the case if burst frequency were the primary covariate over viral integrations (see Fig. 2C), and as was found to be the case over a large sampling of yeast promoters [38], [39].<br>
The basal expression patterns, and their associated expression noise, that were measured here reflect the range of expression dynamics that may be generated initially from an HIV infection after its semi-random integration into the host genome but prior to significant production of viral proteins [4], [9]. Productive viral replication depends on subsequent production of the HIV protein Tat, which mediates expression transactivation by enhancing both transcript elongation from the LTR as well as the binding of other transcriptional activators [51], [78]?[81]. In an intact virus, this positive feedback would act to amplify the basal expression fluctuations observed here.<br>
We anticipate that certain ranges of parameters representing integration-site dependent basal fluctuations in promoter activity may act to specify distinct infected-cell fates, as illustrated in Figure 6 where the drawn region boundaries are hypothetical and the insets depict representative expression phenotypes that result when Tat is expressed from the HIV LTR in a minimal viral system that we had studied in earlier work [4], [10]. Promoter integrations with smaller basal transcriptional burst sizes, and with frequencies that do not effectively couple one burst to the next, will never produce sufficient Tat for transactivation and may represent unproductive infections (Region I). On the other hand, promoter integrations specifying larger basal burst sizes and sufficient frequencies will quickly and stably transactivate after a small number of initial transcriptional bursts and may represent a productive infection (Region II). In contrast, those integrations with small to intermediate basal burst sizes and frequencies will only infrequently (stochastically) generate sufficient Tat for positive feedback activation. Moreover, the transactivated state may be subsequently destabilized by the infrequent occurrence of consecutive smaller and more widely spaced bursts, to generate a bimodal expression pattern (Region III). We have hypothesized that the dynamics of this phenotype, which include significant delays in switching between non-productive and productive expression phenotypes, may create a sufficient time window for the establishment of latent infections in vivo [4], [10]. Future experimental and computational analysis may provide additional insights into the role of Tat in amplifying basal, integration-modulated, expression fluctuations, as well as their hypothesized role in fate determination of HIV-infected cells.<br>
<br>
Implications for investigations of genomic architectures in health and disease<br>
While other studies have considered the effects of genomic environment on mean expression, we have analyzed its effects on expression heterogeneities. By applying an integrated computational and experimental approach, we have characterized the modulation of underlying transcriptional dynamics by genomic environment in human cells. Since classes of human promoters often share common enhancer and repressor motifs, it is possible that two such promoters at different genomic loci would demonstrate significantly different transcriptional dynamics, as we have observed from different integrations of a single promoter in our system. In this way, genomic architecture would provide an additional axis of expression regulation complementary to that specified by individual promoter sequence architectures, and promoter and genomic architectures might evolve in parallel to optimize their coupled contributions to transcriptional control [72], [82]?[85]. Similarly, integrating viruses such as HIV, whose host-cell specificity determines the range of possible genomic environments that could be selectively sampled, may evolve promoter architectures that best exploit this host-regulatory axis to adopt a set of expression patterns that enhance, or even optimize, viral replication fate.<br>
<br>
<br>
Materials and Methods<br>
Harvesting and infection of lentivirus<br>
The HIV-1 based lentiviral plasmid, pCLG, (encoding the HIV-1 LTR and GFP) was packaged and harvested in HEK 293T cells using 10 mg of vector, 5 mg pMDLg/pRRE, 3.5 mg pVSV-G, and 1.5 mg pRSV-Rev, as previously detailed [4], [86]. Harvested lentivirus was concentrated by ultracentrifugation to yield between 107 and 108 infectious units/ml. Approximately 103?106 infectious units of concentrated virus were used to infect 3?105 Jurkat cells. Six days after infection, gene expression of infected cells was transactivated by incubating Jurkats with a combination of 20 ng/ml TNF?, 400 nM TSA, and 12.5 mg exogenous Tat protein [10]. After stimulation for 18 hours, GFP expression was measured by flow cytometry, and titering curves were constructed by determining the percentages of cells that exhibited GFP fluorescence greater than background levels. This titering curve was used to attain the desired MOI (?0.05?0.10).<br>
<br>
Clone selection and FACS analysis<br>
Forty-eight single GFP+ LTR-GFP (LG) Jurkats (clones) were sorted on a DAKO-Cytomation MoFlo Sorter into 96-well plates and cultured for at least 4 weeks to allow for clonal expansion. Infected cultures were analyzed via flow cytometry on a Beckman-Coulter EPICS XL-MCL cytometer (http://biology.berkeley.edu/crl/cell_sorters_analysers.html). Thirty-one single-integration clones, whose expression histograms were sufficiently distinguishable from an autofluorescence profile for model fitting (with mean fluorescence exceeding twice the autofluorescence mean), were selected for further analysis.<br>
<br>
Distribution processing<br>
Cytometry measurements on 104 cells for each clone quantified GFP fluorescence as well as forward and side scatter (FSC and SSC). Live cells were selected by standard gating of FSC and SSC, and further gated to select the mid 60% of FSC and SSC values. This gating was optimized using a bootstrap approach to resolve the GFP profile at the mean scattering measure, while eliminating significant correlation between GFP distribution and scattering (see Text S1 Sec. S.I for further discussion, Fig. S1, and Table S1). GFP histograms were smoothed using an optimized low-pass Fourier filter, and normalized to obtain probability distributions, that were used for model fitting. Distribution deconvolution, for the transformation applied in Fig. 2B, was accomplished using a Weiner filter. Model fits were also obtained for distributions resulting from a 10% scattering gate, and indicate no significant effect on our parameter inferences (Fig. S4).<br>
<br>
Model solution<br>
The model in Fig. 2A represents a continuous-time discrete-state Markov process described by a chemical master equation [87], which was solved using an in-house <software>Matlab</software> routine (The MathWorks, Inc.; code available upon request) for steady-state protein distributions, which were then convolved with a separately measured autoflorescence profile and converted to cytometer-based RFU (Relative Fluorescence Units) values for comparison with smoothed experimental distributions. Briefly, the master equation was truncated at large protein and transcript numbers to specify a finite system. A graded coarse-graining approach was applied, whereby neighboring states at higher transcript and/or protein number, where distributions admit a continuum approximation, were binned together (probabilities summed), and transition rates between binned states were approximated by interpolation to estimate probability fluxes at the boundaries. The coarse graining scheme reproduces the master equation at small transcript and protein numbers (where no coarse-graining is applied), and specifies a second-order approximation to the corresponding Fokker-Planck equation at large protein and transcript numbers. The resulting linear system was then integrated in time until an effectively stationary distribution was achieved by using a forward/backwards Euler method that alternates treating transcript and protein transitions implicitly or explicitly; this represents a fast and stable method, appropriate to stiff systems and multi-dimensional PDEs [88]. Marginal coarse-grained protein distributions were then calculated by summing calculated probabilities over transcript numbers and gene states, and the resulting distributions were interpolated. Solution accuracy was established by comparing the first three moments of the calculated distributions to their theoretical values (calculated analytically, see Text S1, Sec. S.III), by varying the coarse graining and the time step for the integrator, and by comparing our solutions to those calculated using the Finite State Projection algorithm developed by Munsky and Khummash [89], which allows a rigorous calculation of numerical error for finite times, for several test cases. Further details may be found in Text S1 Sec. S.II.<br>
<br>
Fitting procedure<br>
Fit parameters (?a, b?=??t+/?r, and ?r) were varied using the <software>MATLAB</software> minimization function ?fmincon? to identify the combination that minimized the fit deviation, defined as , where  is the predicted/measured probability of counting a cell in cytometer bin i for the data/fit.<br>
<br>
Specifying non-fit model parameters<br>
A number of model parameters quantify processes occurring at spatially separate locations from the integrated LTR. These were assumed to be the same for all integrations, and were specified separately. Methods developed independently from this study allowed us to calibrate relevant non-fit model parameters via comparison between the measured transcript distribution for a single clone, and the corresponding cytometry-based GFP distributions (Foley, et al. manuscript in preparation). A conversion factor between transcript number and RFU could be estimated from the measured ratio of means, as ?=?2.5 (?=?measured cytometry-based RFU/transcript mean). By assuming transcriptional bursting, the ratio of transcript to protein degradation rates could be calculated as , yielding a value of  for our measurement. These constitute the remaining quantities necessary to specify our model. While uncertainties in these quantities would affect the values inferred for model fit parameters, they would approximately affect the inferred fit parameters for each clone by the same scale factor, preserving inferred trends in parameter variation over the set of integration clones. These uncertainties were therefore not explicitly considered in our analysis (see Text S.1, Sec. S.VII for further discussion). Quantifying the dilution of a synthetic non-degraded fluorescence marker allowed us to estimate a cell-division rate of 0.05 h?1, which served as an effective protein degradation rate () in our model, and thus specified ; the absolute values of these degradation rates were not essential to specifying our model because steady-state distributions only depend on ratios of rate constants, and all rates were therefore scaled relative to the transcript degradation rate in our analysis. The relatively large protein numbers in our system dictate that fluctuations in protein production and degradation do not significantly influence distribution shapes, and as long as the ratio  was chosen to be a sufficiently large value, its specific value did not affect our analysis; we chose . See Text S1, Sec. S.V for further discussion of non-fit model parameters.<br>
<br>
Quantifying experimental uncertainties and model-fit discrimination<br>
A bootstrap procedure was used to estimate a 95% upper-bound on the value of Dev for our processed experimental distributions (Devdata) that included uncertainties due to the finite number of cells sampled and to specifying distributions at a single scattering measure. Other sources of uncertainty, such as cytometer PSF and distribution variability over time, were found not to significantly affect our determination of trends in model-parameter variations over the set of integration clones and were not included (see Text S1, Sec. S.VIII, and Fig. S3). Model fits whose deviations (Devfit) differed from each other by less than Devdata were considered effectively indistinguishable, as the differences in their quantified deviations might be accounted for by uncertainty in our experimental distributions. 95% confidence intervals about best-fit model parameters were calculated as maximum variations for which the increase in Devr?=?Devfit/Devdata was less than 1 (assuming simultaneous parameter variations), as estimated using a Hessian-based quadratic approximation for variation of Devr with respect to burst parameters and based on the parametric sampling in Fig. 3 for ?r.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

</body></html>
