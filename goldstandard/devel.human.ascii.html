<html><body link="black" alink="black" vlink="black" hlink="black">
<b>PMC272927</b><br>
Trustworthiness and metrics in visualizing similarity of gene expression<br>
<br>
<br>
Background<br>
Statistical data analysis usually consists of two successive phases: exploratory and confirmatory. In the first phase, the data is inspected and explored to form hypotheses that are then verified in the second, confirmatory phase.<br>
For data sets measured with microarrays, the exploratory phase is particularly important for two reasons. First, if the number of plausible research hypotheses is very large, it is advisable to narrow them down with thorough exploration. A search for correlates of cancer types is one example. Second, all microarray studies generate a large amount of data as a side product. The database can be explored later for other purposes.<br>
In this paper, we study one of the main tasks of exploratory data analysis: visualization of similarity relationships among high-dimensional data samples. We will focus particularly on the similarities, although the methods may additionally reveal clusters (groups of mutually similar data) and their similarity relationships. Visualizing similarities in high-dimensional (from a few to hundreds of dimensions) data items is a difficult task since the displays can be at most three-dimensional in practice. In particular, it is impossible to project the samples in such a way that all similarity relationships are preserved. Hence, the methods need to make compromises regarding which kinds of relationships to visualize.<br>
On one side of the coin, the visualizations should be trustworthy, in the sense that samples appearing similar (proximate) in the visualization can be trusted to be similar in actuality. The other side of the coin is whether all original proximities become visualized. This dualism is analogous to precision and recall in information retrieval and classification.<br>
We argue that, for data exploration, it is more important that the initial visualizations are trustworthy. The other side of the coin is important but not equally so. The proximities that are visible on the display are salient, and if they are not trustworthy the whole display is misleading. In contrast, if all similar samples cannot be placed proximate, the consequence is only that potentially useful discoveries may be overlooked. Since both goals cannot be achieved simultaneously, we argue that the compromise should initially be made in favor of trustworthiness, which will guarantee that at least a portion of the similarities will be perceived correctly. Afterwards, the potentially overlooked similarities may be hunted for by alternative visualizations.<br>
To our knowledge, studying trustworthiness of visualizations of similarity is a new idea. Projection methods have been compared earlier for other kinds of data [1,2] but the criterion has been the capability of preserving (all) the actual distances instead of the proximities (neighborhoods). This option biases the comparison in favor of methods that directly aim at optimizing the distances. An additional problem is that, in our opinion, the trustworthiness of proximate samples is more important than accurate preservation of all distances, as argued above.<br>
We have designed a measure of how trustworthy the proximate points on a display are. We use it to compare the trustworthiness of three unsupervised methods, hierarchical clustering [3], self-organizing map (SOM) [4], and multidimensional scaling (MDS) [5]. Of these, hierarchical clustering is an extremely popular tool in the bioinformatics community [6-8], and self-organizing maps have been applied as well [9-12].<br>
In the first part of the paper, these unsupervised tools will be applied to functional genomics data measured by DNA microarrays in gene knock-out mutation experiments [8] and in different tissues [13]. Functionally similar genes are sought by visualizing the similarity of the expression profiles of 1410 (after preprocessing) yeast genes, measured in 179 knock-out mutations. Likewise, the similarity of 1600 mouse genes will be studied based on their expression profile over 45 tissues.<br>
In the second part of the paper, we address another major question in visualization of similarity, and as a side note in clustering in general: how to measure similarity. Gene expression measurements in a variety of treatments potentially include valuable information about the function and co-regulation of genes. The important variation is, however, hidden within all the biological and measurement noise in the high-dimensional expression space.<br>
For the knock-out mutation data, the question is which mutations to select, and how to weight the mutations so that the functionally meaningful variation is emphasized and irrelevant variation suppressed. Moreover, the weighting should be different for different genes, that is, at different locations of the expression space.<br>
The learning metrics principle [14,15] is a new approach to finding important aspects of data, and expressing them in a way usable by standard data analysis and data mining methods. In general, the learning metrics principle refers to using certain differential-geometric methods for deriving metrics to data spaces, based on the interrelationship between the (primary) data set and auxiliary data. The metrics are called "learning metrics" because they are learned from the two data sets.<br>
In this paper, metrics will be learned in two case studies to measure differences between gene expression profiles, and used in visualizing similarities of the profiles in the yeast data. In the first experiment, the auxiliary data is selected to be functional classes of the genes, and in a second experiment the activity of the genes in the tissues of another organism. The crucial assumption underlying the learning is that differences in primary data (gene expression) are assumed important if they cause changes in the auxiliary data. The metric of the primary data space is adjusted locally to measure only the important differences, as illustrated in Figure 1. The adjustment may be different in different locations of the space.<br>
Formally, in the first case study with the knock-out mutation data [8], we have an expression data matrix containing measurements in n = 179 different treatments (columns) for N = 1400 different genes (rows). We know the functional class for each of the N genes that we will analyze. The method is generally applicable in such set-ups, assuming n is not too large compared to N, to avoid overfitting of the metric. Technical details of how to estimate the metric are given in the Methods section.<br>
We will use the new metric to find the set of genes for which the new metric is the most different from the usual similarity measure (correlation or Euclidean); their visualizations and clusterings with the usual metric are possibly misleading. Finally, the similarities of all genes in the new metric will be visualized.<br>
The methods are, additionally, briefly validated with another data set. The primary data are the gene expression profiles of human genes measured in different tissues [13], and the auxiliary data are the activities of the homologous mouse genes in a set of tissues. The abstract setting is almost identical to that in the yeast study: each human gene belongs to one or multiple classes that correspond to the mouse tissues. If the homologous mouse gene is expressed in the tissue number i, the human gene belongs to the class number i.<br>
The necessary condition for applying the learning metrics principle is that a suitable auxiliary data set is available. This is the case when learning metrics-based exploratory analysis of the primary data is used to complement supervised learning (regression, classification). When classifying genes to different functional groups or tissues to disease types, for example, learning metrics-based visualizations can reveal relationships among the groups, highlight outliers, or even help the discovery of new groups. These kinds of auxiliary data are typically constructed manually, and hence are costly. Alternatively, the auxiliary data can be constructed automatically to summarize another data set, for example the mouse gene expressions in this paper. Then the aspects of the primary data that are related to the auxiliary data will become emphasized in the visualizations.<br>
In summary, this paper (i) compares visualizations generated by a set of commonly used methods with a new criterion, trustworthiness; and (ii) presents a method for adjusting the metric to further improve the visualizations.<br>
<br>
Results<br>
Trustworthiness of the visualizations<br>
We consider a projection onto a display trustworthy if all samples close to each other after the projection can be trusted to have been proximate in the original space as well. Measuring such trustworthiness requires specifying what is meant by 'proximate', and how to quantify possible non-trustworthiness of the proximate samples.<br>
The details of the measures and their motivation are given in the Methods Section. In summary, we use simple non-parametric definitions to avoid biases in favor of any of the projection methods. The k nearest samples will be regarded 'proximate', and results will be reported for several values of k. If the proximate samples are not also neighbors in the original space, their rank distance from the neighborhood will be measured to quantify the magnitude of error. Our trustworthiness measure M1 (Eq. 3) is essentially the average trustworthiness over all data.<br>
We compared the trustworthiness of four visualization methods: Sammon's projection, non-metric MDS, SOM, and hierarchical clustering (see the Methods Section). All were applied in the standard textbook way. Sammon's mapping and non-metric MDS were selected to represent MDS methods since they have beneficial properties; Sammon's mapping emphasizes the preservation of short distances which are the focus of our trustworthiness measure as well. Non-metric MDS tries to preserve rank orders of distances, which is the error measure we use. For hierarchical clustering, there are lots of variants; we compared all variants available in the <software>Cluster</software> program by Eisen [16]: centroid linkage, complete linkage, and single linkage. Complete linkage gave clearly better results than the other variants and is the only one included in the results below.<br>
All methods used the same inner product (correlation) metric, which is the most commonly used metric for gene expression data sets. Additional justification for the choice is that correlation metric works well for classification of the specific yeast dataset (preliminary studies). It is imperative to use the same metric for all methods to keep the results comparable. In principle, the whole study could be repeated for different metrics. However, it is unlikely that the conclusions would change; in an earlier experiment [17] on Euclidean metrics for non-biological data sets, the conclusions were the same.<br>
Trustworthiness<br>
The results are shown in Figure 2. We focus on trustworthiness of relatively small neighborhoods, of the order of some tens of genes, which are perceived to be most saliently proximate in displays such as Figure 8. In this range, hierarchical clustering is the best for the smallest neighborhoods (k &lt; 10), and SOM after that. The excellent performance of hierarchical clustering at very small neighborhood sizes was to be expected as it explicitly connects the closest points first.<br>
<br>
Preservation of the original neighborhoods<br>
As discussed in the Background Section, all methods make a compromise between trustworthiness and preservation of the original proximities. The latter kinds of errors result from discontinuities in the projection; we measured them by how well neighborhoods of data points in the original data space were preserved. Non-parametric measures were again used to avoid biases. The neighborhood of size k of an expression profile is defined as those k profiles that have the smallest distance (here, strongest correlation) from the profile. If a profile becomes projected away from the neighborhood, the error is quantified by rank distances on the display. The measure M2 (Eq. 4; for details, see the Methods Section) summarizes the errors for all expression profiles. For these data sets, the SOM and multidimensional scaling (Sammon and non-metric MDS) are the best for preserving small (k &lt; 50) original neighborhoods (Fig. 3). Hierarchical clustering is by far the worst.<br>
<br>
Improving the trustworthiness<br>
Trustworthiness can be improved by discarding the least trustworthy data samples and analyzing them separately. Figure 4 shows the increase of trustworthiness as the number of discarded samples is increased. It is striking that although the performance of most of the other methods increases rapidly, they do not reach even the starting point of the SOM before nearly one third of the data set has been discarded. The ultrametric measure (see the Methods Section) of similarity for hierarchical clustering has the smallest improvement rate.<br>
<br>
<br>
Visualization of functional similarity by learning metrics<br>
A main problem in comparing gene expression profiles is to choose which properties to compare, that is, how to define the similarity measure or, equivalently, the metric. When comparing knock-out mutation profiles of genes, the relevant mutations need to be selected and scaled suitably for each gene.<br>
There is not enough prior knowledge to do this manually, and our goal is to learn automatically the proper metric from interrelationships between the expression data set and another data set that is known to be relevant to gene function: the functional classification of the genes. In an additional study, the primary data are the gene expression profiles of human genes measured in different tissues, and the auxiliary data used to guide the learning are the activities of the homologous mouse genes in a set of tissues [13].<br>
Details on how to learn the metrics are described in the Methods Section [14,15]. In summary, the metric is such that functional classes change uniformly in the new metric. If some of the knock-out mutations have only a weak correlation with the functional classes, they contribute only weakly in the measured similarity among expression profiles. The similarity measure focuses on those differences that are relevant for the functional classes.<br>
The metric is defined as a local scaling of the expression space, which makes it very general; the contributions of the knock-out profiles to the similarities may be different for different genes.<br>
We applied the new metric to one of the visualization methods, the SOM, and compared the results with the same method in the standard correlation metric. For technical details of combining of the SOM and the learning metrics, see the Methods Section.<br>
We began by measuring quantitatively whether SOMs in learning metrics represented the functional classes better than those in the standard inner product metric. In short, a standard estimator is used to predict the (probability) distribution of functional classes for each SOM unit, and when a new expression profile is projected to the SOM, the accuracy of the prediction is computed. A standard accuracy measure, the log-likelihood, was used. The prediction is derived from the same probability estimator that is used for computing the learning metrics (cf. the Methods Section). The estimator has a free parameter called 'kernel width'; the value that produced the best results was selected for the subsequent experiments. The results shown in Figures 5 and 6 confirm that the new metric yielded more accurate results for the two data sets for a wide parameter range.<br>
We finally used the SOM in learning metrics to visualize similarity relationships of the knock-out expression profiles of yeast genes, and picked up sample findings as demonstrations. To make the display as trustworthy as possible, 10% of the least trustworthy genes were discarded. If desired, the genes most similar to them can be later sought by directly comparing expression profiles. The entire analysis process is summarized in diagram form in Figure 7.<br>
The learning metrics SOM is shown in Figure 8. The display visualizes similarity relationships; if two genes are proximate in it, they can be reasonably well trusted to behave similarly. Clusteredness of the data is shown by the U-matrix visualization of the SOM (Figure 8. for details see the Methods Section), revealing several lighter areas with mutually relatively similar genes, and darker areas in between, where the genes are relatively more different.<br>
The novelty in the display, compared with standard SOM displays of gene expression data, is in the metric. Proximate genes both behave similarly in the mutation experiments, and are likely to have similar functional classes. Knowledge about the functional classes has been incorporated in the theoretically justified method described in the Methods Section, such that the display still shows correctly similarities among the expression profiles. The main thing that has changed is that the mutation experiments are weighted to bring forth better the differences related to gene function.<br>
We will next analyze as demonstrations three sample findings from the SOM. There is an interesting small group or subcluster of nine genes (number 1 in Fig. 8) associated with mitochondria. Four of the genes are additionally associated with branched-chain amino acid biosynthesis (YJR016C, YHR208W, YLR355C, YCL009C). These were grouped with three genes related to fermentation and carbohydrate utilization (YOL059W, YER073W, YKL120W), and genes involved in the threonine and lysine, as well as leusine biosynthesis (YDR234W, YER086W). All the genes with a known function in this cluster were located to mitochondria.<br>
Assuming the new metric is more informative than the standard correlation metric, it is particularly interesting to know for which genes the metric has changed the most. All old analyzes with the standard correlation metric have potentially yielded misleading results. Such genes were sought by comparing how many of the closest neighbors were different in the two metrics, and emphasized by underlining the gene names in Figure 8.<br>
The area number 3 is an example where the metric has changed. The analysis of functional classifications and annotations of the area revealed that 8 out of the 17 genes were associated with transcription and DNA repair (YMR179W, YAR007C, YBR088C, YDR501W, YDL101C) or cell cycle (YAL024C, YHR153C, YMR198W). Two genes are protein tyrosine kinases (YDL101C, YGL179C), and as the former is involved in DNA damage response, it is possible that also YGL179C mediates similar kind of function. The gene YJL196C may be an outlier in this area; it is associated with fatty acid elongation. The rest of the genes in this cluster have an unknown function.<br>
Finally, we sought the display for groups of genes belonging to known pathways. Some of these groups occurred proximate on the SOM. The genes involved in purine biosynthesis occurred together on the area number 2. These and other proximate genes had been classified under nucleotide metabolism. In addition, the gene YDL241W, whose function is unknown, was located nearby. Hence, it might be worthwhile to examine closer whether this gene is also related to nucleotide metabolism or the purine biosynthesis pathway.<br>
<br>
<br>
Conclusions<br>
Comparison of unsupervised methods for visualizing similarity of gene expression profiles revealed that the self-organizing map (SOM) was the most trustworthy except for the most similar gene expression profiles, where hierarchical clustering was the best. The less important other side of the coin is whether there are discontinuities in the mapping. In this latter regard, the relative goodness of the methods depends on the data (see also [17]). In the comparisons with gene expression data, the SOM has hitherto performed well.<br>
The learning metrics principle was then applied to derive a new location-specific metric based on functional classes of the genes. The resulting metric measures changes in gene expression but weights the changes according to how much they contribute to changes in the functional classes. The metric is a step toward a more comprehensive picture of the functional similarity of the genes, incorporating prior biological knowledge in the measurements.<br>
The basic learning metrics method considers the auxiliary data as a classification of the primary data into mutually exclusive classes. This restriction can be easily relaxed by considering multi-class data as samples from the class distribution at the point. Generalizations to hierarchical classifications and other more general types of auxiliary data are also possible and will be considered in later work.<br>
<br>
Methods<br>
Data<br>
Three different gene expression data sets were used in the experiments.<br>
The first data set was provided by Hughes et al. [8]. It consists of expression measurements for all yeast (Saccharomyces cerevisiae) genes in 300 knock-out mutations. They had derived error estimates based on replicated measurements, and tested whether the expression of the genes differed significantly from noise. We selected a subset of the data containing saliently expressed genes and mutations that induced expressions. Only genes and mutations with at least two measurements that differed significantly from noise (P &lt; 0.01) and were expressed over 2-fold when compared to the control were selected, resulting in a data set of the size of 1410 genes measured for 179 mutations.<br>
We have (similarly as in [18]) compared different preprocessing methods (normalization of measurement error, standard deviation, length and/or mean), with the classification error of a k-nearest neighbor classifier as the performance measure. For this data, the following alternative gave the best performance: the data was preprocessed by dividing each measurement by its estimated measurement error, and then the standard deviation of each mutation was normalized. Finally, all gene expression profiles were normalized to unit length.<br>
The auxiliary data for the first gene expression data set was selected from the <database>MIPS functional classification</database> [19] for yeast. The classification consists of over two hundred classes at different levels of hierarchy. Many of the functional classes are known to correlate with gene expressions, although some classes undoubtedly are very heterogeneous at the level of gene expression. A set of 46 classes were selected from the various levels of functional classification, in order to obtain non-hierarchical and a priori as coherently behaving classes as possible.<br>
The second data set [13] was used to confirm the findings on trustworthiness. Expression of over 13000 mouse genes had been measured in 45 tissues. We selected an extremely simple filtering method, similar to that originally used in [13]. Of the mouse genes significantly (average difference in Affymetrix chips, AD &gt; 200) expressed in at least one of the 45 tissues, a random sample of 1600 genes was selected, preprocessed as described above, and visualized based on their profile of expression in the tissues. The variance in each tissue was normalized to unity.<br>
The third data set was created for an additional validation of the learning metrics. The data were taken from the same publication as the second one [13], but now consisted of over 13000 human genes measured in 46 tissues. From these genes, a set of genes with known homologues in mouse and expressed (AD &gt; 200) at least in one human tissue, were selected, resulting in 3724 genes. The comparison of different preprocessing methods (logarithm, normalized tissue variances, none) and distance metrics (Euclidean, inner product) for the third data set by k-nearest neighbors method resulted in the use of inner product as a similarity metric, and with no normalizations.<br>
The auxiliary data for the third, human gene expression data was derived from the expression level of homologous mouse genes. Each class corresponded to one mouse tissue, and human gene was assigned to the class, if the homologous mouse gene was clearly expressed in that tissue. The limit was that it must belong to the fourth quartile of that gene's expression over all mouse tissues. Only those mouse tissues (21) for which there was an equivalent in human tissues were considered as classes.<br>
<br>
Methods for visualizing similarity<br>
Hierarchical clustering constructs a tree or dendrogram that visualizes similarity and clusteredness of data. For an example tree of gene expression data, see [7]. Data samples are located in the leaves of the tree, and similar samples occur in proximate branches. There are several variant methods for constructing the trees [3]. Here we will use the <software>Cluster</software> program by Eisen [16] that progressively agglomerates pairs of most similar clusters together. The program offers three variants of the clustering algorithm that differ on how the distance between clusters is defined. The first variant is centroid linkage, where the distance between clusters is defined as the distance between the means of the clusters. The second variant is complete linkage, where the distance between clusters is defined as the maximum distance between points in the clusters to be joined, and the third variant is single linkage, where the distance between clusters is defined as the minimum distance between points in the two clusters.<br>
The tree produced by the clustering algorithm can be cut at any level to obtain disjoint clusters. Here, we are not interested in clusters per se, however, but in the visualization of similarity. The hierarchical clustering algorithms do not directly define such similarity, so we have devised two different definitions that are the best we could think of. The simple method is to order the leaves into a linear order according to how far from each other they are in the tree. The ordering is not unique; we have fixed it by using the method recommended by Eisen: in non-unique cases, use the order provided by a one-dimensional SOM.<br>
Since it can be argued that ordering by the one-dimensional SOM is somewhat arbitrary, we additionally include an alternative that is in a way more justified. Distance between leafs is the distance measure directly induced by the dendrogram, that is, the ultrametric distance [3].<br>
The self-organizing map (SOM) [4] is an algorithm that maps high-dimensional data nonlinearly onto a low-dimensional lattice in a topology-preserving manner. As with hierarchical clustering, the SOM can be used as both a nonlinear projection and a clustering method; clusters can be extracted from the computed SOM (see e.g. [20]).<br>
The SOM is a discrete lattice of map nodes (marked by the dots and labels in Fig. 8). There is a model vector  mi attached to each map unit i. A data sample x is projected onto the SOM display to the node having the closest model vector mw, defined in the basic SOM by<br>
<br>
Here d is the distance measure, which in this paper is the inner product (correlation).<br>
The SOM algorithm computes such values for the model vectors that (i) the projection becomes ordered: proximate samples on the SOM display are similarly proximate in the data space, and (ii) the projection models the data distribution; each model vector becomes the centroid of all data samples mapped to it and to its neighborhood on the map display.<br>
There are several variants of the SOM algorithm; here we describe the original sequential one that we will later complement with new metrics. We used this 'vanilla' version of the algorithm in its basic form and without any tricks not found in basic textbooks, to avoid biasing the study in favor of SOMs.<br>
During the iterative computation of the SOM, at step t a data sample x(t) is selected randomly, and the model vectors are updated toward the data sample according to<br>
<br>
If inner product is used as the distance measure, the model vectors should be normalized after the adaptation step. Here, hw(x), i = ?(t) exp (-d(w,i) / 2?(t)2) is the neighborhood function, where d(w,i) is the distance of the units w and i on the SOM lattice, and ?(t) and ?(t) are piecewise-linearly decreasing coefficients.<br>
Clustering can be visualized on a SOM display using the U-matrix [[21], see Fig. 8]. In the gray-shade display, the light areas contain genes that are mutually more similar than on the dark areas. Hence, very light areas are clear clusters and dark stripes are gaps in between them. Technically, a hexagon is added in between each pair of SOM units and shaded according to the distance between their model vectors. The shade of the hexagons of the map units is the median of the neighboring hexagons.<br>
On a SOM display, the similarity can be defined simply as the distance on the display plane. This measure does not, however, take into account the density of the model vectors that is visualized by the U-matrix. Hence, we have used distances along minimal paths on the map lattice, with weights equal to the distances between the model vectors. On light areas, such distances are shorter and on dark areas they are longer.<br>
The SOMs of the yeast gene expression data were of the size of 20 ? 25 map units (about 3 genes in a unit on the average), and were computed in two phases for a conservative number of iterations. In the first, organizing phase ?(t) decreased from 11 to 3, and ?(t) from 0.2 to 0.02. In the second, fine-tuning phase ?(t) decreased from 3 to 1, and ?(t) from 0.02 to zero. The best map of 4 randomly initialized SOMs was selected according to the (local) cost function [4].<br>
Similarly, the SOMs of the mouse gene expression data were of the size of 22 ? 27 map units (about 2.7 genes in a unit on the average). In the first, organizing phase ?(t) decreased from 11 to 3, and ?(t) from 0.2 to 0.02. In the second, fine-tuning phase ?(t) decreased from 3 to 1, and ?(t) from 0.02 to zero. The best map of 7 randomly initialized SOMs was selected according to the (local) cost function [4].<br>
Multidimensional scaling (MDS) attempts to represent the data as points in a small-dimensional space such that all pairwise distances of data points are preserved. It can be used for constructing a non-linear projection from the high-dimensional expression space to a two-dimensional display plane.<br>
There are several variants of multidimensional scaling that differ in the details of the cost function. We will compare two of them, Sammon's projection and non-metric multidimensional scaling (NMDS), that have favorable properties for the used trustworthiness measure. Sammon's projection [22] minimizes the mean-square error in the pairwise distances, normalized by the original distances. Hence, it emphasizes the preservation of short distances, which is important for trustworthiness. Non-metric multidimensional scaling [23] attempts to preserve the rank order of the distances; the rank order is used to measure errors in the trustworthiness measure.<br>
Sammon's projection and non-metric multidimensional scaling do not have parameters to select, but the optimization can get caught in local minima, depending on the initialization. We computed the Sammon's projection with 10 different random initializations and selected the one with the smallest cost. Non-metric multidimensional scaling was computed only once because of the very long computational time.<br>
<br>
Measuring trustworthiness and detecting genes for which the visualization is suspect<br>
When visualizing similarities of data samples, the local similarities are the most salient: the first perceptions are which samples are proximate, and which proximate samples form groups. Hence, to measure how trustworthy a visualization is, we should focus on the preservation of local similarities, i.e., the proximities. To avoid biases in the comparison studies, we will use a simple non-parametric measure of whether samples within a set of closest samples on the display are in fact closest in the expression space as well.<br>
Let us first consider some alternative measures. The most straightforward way of defining the neighborhood would be to fix a radius and include all samples within the ball with the fixed radius. The problem would be that, since the density of data varies, the amount of data within the ball would similarly vary considerably as well. Additionally, selecting a good neighborhood radius would require prior knowledge of the data density. These problems can be solved by defining the neighborhood to consist of the k nearest neighbors, where k is selected based on the number of nearby samples we are interested in analyzing.<br>
The second decision that needs to be made is how to measure similarity preservation within the neighborhood of proximate samples. In principle, preservation of all distances could be directly measured, but we discounted this possibility because it would bias the study in favor of MDS methods that try to directly preserve all distances. We considered it enough that the samples are within the neighborhood. In any case, preservation of distances within the neighborhoods is taken into account when one varies the size of the neighborhood, which we did in the experiments.<br>
The third decision that needs to be made is how to measure errors in trustworthiness for the samples that are visualized proximate but are in fact different. The simplest measure would be counts of erroneous data samples. This would, however, be only a rough measure and hence not very discriminative between the methods. Hence, we decided to measure the distance from the neighborhood even though it might bias the results slightly towards favoring MDS methods. We made the (arbitrary) decision to measure the distances in a rank scale.<br>
Based on the above reasoning, we ended up in a measure of assessing the trustworthiness of visualizations. We consider a projection onto a display trustworthy if the set of k closest neighbors of a sample on the display are also close by in the original space. This is measured for all data samples.<br>
More formally, let N be the number of data samples and r(xi, xj) be the rank of the data sample xj in the ordering according to distance from xi in the original data space. Denote by Uk(xi) the set of those data samples that are in the neighborhood of sample xi in the visualization display but not in the original data space. Our measure of trustworthiness of the visualization, M1, is defined by<br>
<br>
where A(k) = 2/(Nk (2N - 3k - 1)) scales the values between zero and one. The worst attainable values of M1 may, at least in principle, vary with k, and were estimated in Figures 2 and 3 with random projections and with random neighborhoods.<br>
Trustworthiness is one side of the coin; the other is that some neighborhoods of k points in the original space may not be preserved because of discontinuities in the projection. As a result of the latter kinds of errors, not all proximities existing in the original data are visible in the visualization.<br>
The errors caused by discontinuities may be quantified as follows, analogously to the errors in trustworthiness. Let Vk(xi) be the set of those data samples that are in the neighborhood of the data sample xi in the original space but not in the visualization, and let (xi, xj) be the rank of the data sample xj in the ordering according to distance from xi in the visualization display. The effects of discontinuities of the projection are quantified by how well the original neighborhoods are preserved, measured by<br>
<br>
In case of ties in rank ordering, all compatible rank orders are assumed equally likely, and averages of the error measures are computed.<br>
There exists a simple way of increasing the trustworthiness of a display: discarding the samples for which the display is the least trustworthy, and analyzing them separately. We will do this by iteratively finding the data sample that reduces most the trustworthiness, and removing it from the visualization. The process is continued until a suitable number of the most untrustworthy data samples have been removed, or a desirable level of trustworthiness has been attained. A similar method can be used to find where the visualization has broken the continuity of a neighborhood. The idea is to study which neighborhood sample pairs reduce M2 the most. Locating the sample representing the center of the neighborhood and the sample missing from the neighborhood on the visualization will reveal if separate areas on the display are in fact close by in the data space.<br>
<br>
Learning metrics<br>
The learning metrics principle<br>
The learning metrics [14,15] are based on the assumption that changes in the primary data space are important if they cause changes in another (auxiliary) data space.<br>
Formally, denote a primary data sample by x and its functional class by c. During learning, the data occurs in pairs (x, c). The squared distance measure of the data space is changed locally to measure the important differences, that is, the differences among the distributions of the functional classes p(c|x). When the differences are measured by the Kullback-Leibler divergence DKL, the distances become locally<br>
 (x, x + dx) ? DKL (p(c|x)||p(c|x + dx)) = dxT J(x) dx, ??? (5)<br>
where J(x) is the Fisher information matrix with parameters x. The conditional distribution p(c|x) can be computed using the Bayes rule from a standard estimator for the joint distribution, such as Mixture Discriminant Analysis (MDA2) [24], or obtained directly from a "mixture of experts" [25]. For more details see [14]. The metric can in principle be extended to non-local distances by computing (approximate) path integrals, but for computational reasons we resort to the local approximations. The approximation has worked satisfactorily for nearest-neighbor searches in empirical tests, particularly when complemented with a kind of regularization: in practice the metric will often be singular for very high-dimensional spaces, and hence we will add to it a portion of the Euclidean distance,<br>
 (x, x + dx) ? dxT [?I + (1 - ?) J(x)] dx, ??? (6)<br>
where I is the identity matrix. The coefficient ? is selected using a validation set. The regularization makes the local approximations more feasible for non-local distances as well.<br>
A difference in this paper, compared to the earlier works on learning metrics, is that the yeast data set lies on the surface of a hypersphere. For such data, the density estimators should also be defined on the hypersphere. Technically, instead of using Gaussian kernels, we used the so-called von Mises-Fisher kernels that are analogs of Gaussians on the hypersphere [26]. (We used 30 kernels.) The local distances (6) are still Euclidean on the hypersphere, but in practice non-local distances also need to be computed. When computing distances from x, we have projected all vectors to the tangent plane of the hypersphere at x, with the distance from x scaled to be equal to the arc length from x. Distances on the tangent plane are then computed with (6).<br>
<br>
Self-organizing map in the new metric<br>
In the first step of a SOM iteration, the best matching unit is sought in the new metric dL (Eq. 6; cf. also the discussion after the equation). The steepest descent update rule for learning metrics turns out [14] to be the same as in the Euclidean metric. Here, the update is applied in the tangent plane, and the results are transformed back to the hypersphere. It can be shown that the resulting update rule moves mi toward x along the shortest route on the hypersphere, such that their angle reduces by the fraction given by hwi(t).<br>
The underlined genes in Figure 8, for which the metric had changed the most, were found as follows. We sought 20 nearest neighbors of each gene, in both the old inner product metrics and the learning metric. The two sets were compared, and the proportion of neighbors that had remained the same was computed. The 202 genes with the smallest proportion (at most 13 neighbors remained the same) were selected.<br>
<br>
<br>
<br>
Authors' contributions<br>
SK, EC, JN, and PT developed the overall plan. SK, JN and MO were responsible for the results with learning metrics (MO did the actual simulation). SK and JV were responsible for measuring the trustworthiness of the visualization methods (JV did the simulations). PT and EC carried out the biological analysis of the results. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1373603</b><br>
A high level interface to <database>SCOP</database> and <database>ASTRAL</database> implemented in Python<br>
<br>
<br>
Background<br>
Bioinformatics tools often attempt to automatically predict the unknown properties of a given dataset. Manually curated data is therefore important both for training and for benchmarking new approaches to prediction. One database providing such a manual curation of data is <database>SCOP</database> (the <database>Structural Classification Of Proteins</database>) [1]. <database>SCOP</database> categorises all known protein domains in a hierarchy based upon the domain structure. This hierarchy is principally described by Class, Fold, Superfamily and Family. Crucially, the relationships between proteins grouped at the superfamily level may not be apparent from sequence considerations alone. This makes <database>SCOP</database> a valuable resource when examining the performance of algorithms that detect remote sequence relationships [2].<br>
The <database>ASTRAL</database> [3] Compendium for Sequence and Structure Analysis complements <database>SCOP</database>. <database>ASTRAL</database> provides sequences and structures for each domain in <database>SCOP</database>, and also provides non-redundant subsets of <database>SCOP</database> with preference given to higher quality structures.<br>
The <database>SCOP</database> and <database>ASTRAL</database> databases provide their data in structured files available from the relevant websites. Using this data requires parsing and handling of these files. We present a small and intuitive application programming interface (API) to the <database>SCOP</database> and <database>ASTRAL</database> datasets which allows these databases to be used with a minimum of programming overhead. In the past we have successfully used this API to develop a web-based database of <database>SCOP</database> alignments, <database>S4</database> [4]. The API described is now distributed as part of the Biopython suite for bioinformatics [5].<br>
<br>
Implementation<br>
The API provides methods that allow the <database>SCOP</database> tree to be queried. Nodes in the <database>SCOP</database> tree can be found using their identification or their position in the tree. In addition, given a particular node, nodes lying on a different level on the tree (ascendents or descendents) can be found.<br>
Each leaf of the tree corresponds to a domain in the <database>SCOP</database> hierarchy. The API uses <database>ASTRAL</database> to provide information on the leaves of the tree, corresponding to domains. For each domain, the API provides its sequence and its membership of non-redundant subsets and sequence as defined by <database>ASTRAL</database>.<br>
<br>
Usage<br>
Figure 1 shows a Unified Modelling Language (UML) diagram of the classes and methods involved in the API. Once a Scop object has been instantiated with either a reference to the file or a database, Node objects can be returned by specifying sids (<database>SCOP</database> identifiers that show the <database>pdb</database> identity plus a code that identifies the chain, such as "dlilk--") or sunids (<database>SCOP</database> unique identifiers, numerical identifiers assigned to each node in the tree that are guaranteed to be identical across releases, such as "63336"). These Node objects represent all nodes in the tree, including classes, folds, superfamilies and families.<br>
Node objects can be queried for their parent or child objects, and can be queried for relatives further up or down the tree using the getAscendent or getDescendents methods. These methods accept as an argument a string describing the level required, either as the human readable name of the node (e.g. getAscendent ('superfamily')) or using the <database>SCOP</database> conventions for the levels ('cf, 'sf', etc.). Domains are leaves of the <database>SCOP</database> tree and have a special class Domain which stores the sid (e.g. dlh32a2) as well. In addition, each Domain object has a Residues object storing the <database>pdb</database> chain that the domain corresponds to, as well the list of residues from the chain that have been determined to be part of the domain.<br>
The Astral class is an abstraction of the <database>ASTRAL</database> database. <database>ASTRAL</database> provides a FASTA formatted file of all domains in the <database>SCOP</database> database based on <database>PDB</database> records. Using the Biopython framework for handling FASTA files, sequences for <database>SCOP</database> domains can be quickly returned. So, by calling getSeqRecord on a domain with an instance of the Astral class we can retrieve the relevant sequence. <database>ASTRAL</database> also provides FASTA files containing <database>SCOP</database> domains clustered at percent id of residues shared between sequences, or <software>BLAST</software> expect values. The Astral class can parse these files and return Domain objects for each domain in the file. Furthermore, a list of domains for a given percent id (e.g. 10%) or E-value (e.g -10) can be returned using getDomainsClusteredById or getDomainsClusteredByEv.<br>
<br>
Examples<br>
Having downloaded the <database>SCOP</database> parsable files and the <database>ASTRAL</database> scopseq resources, the Astral and Scop objects are instantiated:<br>
&gt;&gt;&gt; from Bio.SCOP import *<br>
&gt;&gt;&gt; scop = Scop(dir_path="...", version=1.67)<br>
&gt;&gt;&gt; astral = Astral (dir_path="...", version=l.67, scop=scop)<br>
Where the ellipses are replaced by suitable paths. We could then find all domains with the same fold as a given domain:<br>
&gt;&gt;&gt; dom = scop.getDomainBySid("dlh32a2")<br>
&gt;&gt;&gt; print dom<br>
dlh32a2 a.3.1.8 (1h32 A:151-261) Di-heme...<br>
&gt;&gt;&gt; fold = dom.getAscendent('fold')<br>
&gt;&gt;&gt; related = fold.getDescendents('domain')<br>
We then use <database>ASTRAL</database> to retrieve a subset of related domains with less than 10% sequence identity.<br>
&gt;&gt;&gt; for r in related:<br>
...???if astral.isDomainInId(r, 10):<br>
...??????print astral.getSeq(r)<br>
...<br>
Seq('vdaeavvqqkcischggdltgasapa...<br>
Seq('eadlalgkavfdgncaachagggnnv...<br>
Seq('qadgakiyaqcagchqqngqgipgaf...<br>
A more complex example would be to create a novel dataset to benchmark homology recognition [6]. The authors wished to create a dataset of highly populated sequence diverse superfamilies: those with more than twenty members at less than ten percent sequence identity. Using these modules, such a dataset could be generated in a few lines of code:<br>
&gt;&gt;&gt; superfamilies = scop.getRoot(). getDescendents('superfamily')<br>
&gt;&gt;&gt; dataset = []<br>
&gt;&gt;&gt; for sf in superfamilies:<br>
??????desc = sf.getDescendents('px')<br>
??????desc = [x for x in desc if astral.isDomainInId(x,10)]<br>
??????if len(desc) &gt; 20:<br>
???????????????dataset.append(sf)<br>
<br>
Using <database>MySQL</database><br>
The database objects provide methods for serialising to a <database>MySQL</database> database handle.<br>
&gt;&gt;&gt; import MySQLdb<br>
&gt;&gt;&gt; db_handle = MySQLdb. connect (...)<br>
&gt;&gt;&gt; scop.write_cla_sql(db_handle)<br>
&gt;&gt;&gt; scop.write_hie_sql(db_handle)<br>
&gt;&gt;&gt; scop.write_des_sql(db_handle)<br>
&gt;&gt;&gt; astral.writeToSQL(db_handle)<br>
This creates the necessary tables and entries; it can then be used to construct Scop and Astral objects.<br>
&gt;&gt;&gt; scop_sql = Scop(db_handle=db_handle)<br>
&gt;&gt;&gt; astral_sql = Astral (db_handle=db_handle, scop=scop_sql)<br>
The advantage of using an SQL approach is that it avoids constructing the entire <database>SCOP</database> tree in memory when the Scop object is created. Instead, database queries are made as and when nodes from <database>SCOP</database> are requested. This avoids the time consuming process of parsing the entire tree, and allows an application using these modules to start quickly.<br>
<br>
Evaluation<br>
The classes have been tested using a unit testing framework, and can correctly parse version 1.61 to 1.67 of the <database>SCOP</database> and <database>ASTRAL</database> databases. Loading and building the <database>SCOP</database> tree from flat files typically takes a few seconds on a modern workstation, although this wait can be avoided by using the <database>MySQL</database> backend.<br>
<br>
Availability<br>
The API is distributed as part of the Biopython suite for bioinformatics .<br>
<br>
Authors' contributions<br>
GEC wrote the original package. JC updated the package with the Astral class and other enhancements with suggestions from GEC and MASS. All authors were involved in the preparation of the manuscript. All authors read and approved the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1482722</b><br>
Combining evidence, biomedical literature and statistical dependence: new insights for functional annotation of gene sets<br>
<br>
<br>
Background<br>
The numerous gene clusters identified thus far in molecular biology by high throughput analyses such as transcriptomic or proteomic technologies need to be understood according to the biological conditions under study. However, often only highly specialized individual biologists have an in-depth knowledge about a gene or gene product and therefore this knowledge is limited to relatively narrow research fields. The functional annotation of groups of gene products identified by genomic studies is a large challenge and new tools are needed to help in this task.<br>
Ontologies are widely used in informatics and are now becoming important in bioinformatics. They can make the large amounts of biological knowledge found in textbooks and research papers generally accessible in a structured way [1]. Although the definition of an ontology can be very technical [2], it can be considered as a formalised area of knowledge, represented by facts (or concepts, or terms) and their logical connections (or relationships). The most important current uses of bio-ontologies are for representing knowledge in a way that is understandable by computers, for cross databases interoperability, and for annotating and analysing large-scale data. The <database>Gene Ontology</database> (<database>GO</database>) [3] is the de facto standard for formalising our knowledge about biological processes, molecular functions and cellular components, in three independent hierarchies [4]. It contains over 18,000 defined terms and the nodes within each hierarchy are connected by is_a or part_of relationships. As the defined terms can have more than one parent, the structure of this ontology is called a Directed Acyclic Graph (DAG). Furthermore, each <database>GO</database> term is associated with a unique identifier (<database>GO</database> ID) in order to allow a biological database to link to the <database>GO</database> and to ensure interoperability between different biological databases. These <database>GO</database> IDs are used in several biological databases ? from almost 20 experimental organisms such as animals, plants, fungi, bacteria and viruses ? to tag gene products and assign functions, biological roles and sub-cellular locations to them. Therefore, a user can identify the gene products associated with a specific <database>GO</database> term as well as all of the <database>GO</database> terms associated with this gene product by using an appropriate browser, such as <software>AmiGo</software> [5] or <software>GenNav</software> [6]. The investigation of gene function is therefore an important application of bio-ontologies and this has been extended to include the exploration of gene clusters. There are many dedicated analysis tools, such as <software>FatiGO</software> [7], <software>GoMiner</software> [8], <software>MAPPFinder</software> [9], <software>GOTree Machine</software> [10], <software>Onto-Tools</software> [11] or <software>GOToolBox</software> [12], that offer automated, practical and efficient solutions for retrieving <database>GO</database> terms associated to a gene cluster. Most are able to find statistically over-represented <database>GO</database> terms in a set when compared to a reference set ? this can be the complete genome or the entire microarray used in the experiment [12,13].<br>
Nevertheless, annotating genes with a controlled vocabulary is laborious and needs an expert to inspect carefully the literature associated with each gene to determine the appropriate terms. As our knowledge of biology increases, becomes more refined, and expands into new areas such a process will no longer be sufficient [14]. It is therefore obvious that the annotation databases are incomplete: the number of gene products and associated data are increasing faster than they can be annotated, and there are genes for which attributes are not yet well known and for which the literature has not yet been investigated by curators. It is thus undeniable that most of the information about gene functions is primarily contained in the records of the <database>MEDLINE</database> database, which is considered the richest and most accurate source of functional information related to genes [10,14,15]. However, this information is not easily understood by computers and is not easily interpretable on a large scale for both humans and computers. However, Natural Language Processing (NLP) methods allow gene-reference relationships to be identified within a scientific text, and many studies have been done to transfer named entity recognition systems to the biological domain. However, their success has been limited because of the highly dynamic nature of research and the complexity of entity names in the biological domain [16-18]. Only a few groups have tried to associate gene symbols with <database>GO</database> terms: <software>MedMiner</software> [19] and <software>PubGene</software> [20] can find gene-term associations based on co-occurrences although other methods have been considered, such as clustering [15], maximum entropy analysis [14] and keyword mapping [21].<br>
We report here an original method for the functional annotation of gene clusters based on both evidence and literature profiles that aims to overcome the weaknesses and the limitations of each approach (annotation based on evidence and literature mining). We can functionally annotate a gene cluster by retrieving associated <database>GO</database> terms from two different sources of information. The first is an annotation database built on evidence: the <database>Gene Ontology Annotation</database> (<database>GOA</database>) database [22], and the second is a gene-term association database built on automated knowledge extraction from the biomedical literature: the <database>PubGene</database> index [23]. The PubGene method uses a probabilistic score to reflect the gene-term association strength. This score takes into account the frequencies of both gene and term in the 14 million article records of the database. We discard weak associations (i.e., score &gt; 0.01) to improve the precision of the PubGene method. The two sets of <database>GO</database> terms are then merged and <database>GO</database> terms having statistically enriched gene numbers are identified to aid the biological interpretation of the cluster.<br>
We evaluated the precision of each method and the overlap between them. We also evaluated the relevance of the bibliographic references associated with the gene cluster by the literature mining method. As we were seeking the biological meaning of a gene set, we focused on identifying metabolic pathways. This therefore limited the annotations to the Process hierarchy of <database>GO</database>.<br>
Although subsumption (is_a relationship) and meronymy (part_of relationship) are the backbone of <database>GO</database> and make it a proper ontology in the computational sense, it however lacks associative relations within and especiallly across its three hierarchies. These relations would be very helpful and informative. For example, they could show that a certain molecular function is involved in a certain biological process and that a certain cellular component is the location of a certain biological process. We previously investigated three non-lexical approaches for identifying associative relations between <database>GO</database> terms [24]. We have used these dependences here to strengthen the previous annotations and to build a network of inter-dependent terms. This network highlights relationships that could exist between annotated pathways and functions.<br>
This paper is organised as follows. The Results section describes how we compared evidence (<database>GOA</database>) and literature (<database>PUB</database>) using an exhaustive reference set of 7397 genes annotated by both methods. We then explain how we evaluated the contribution of statistical dependences (DEP) on the same reference set. The methodology was quantitatively evaluated in a multi-cluster analysis concerning 14 clusters chosen from 7 independent studies (Table 1). The biological consistency of our methodology was assessed for a minimal cluster of one single gene (cdc2) and for two clusters from a study by Bedrine-Ferran et al. [25] related to transcriptomic variations in human CaCo-2 cells used as an in vitro model of enterocyte differentiation (Up and Down clusters). A qualitative evaluation was also performed for two oncogenomic studies: a glioblastoma cluster (glioGBM) and a leukemia cluster (bcr-abl). The Discussion section describes the benefits and limitations of the evidence and literature annotations. We then comment on the major contributions of the bibliographic aspects of our method and that of the statistical dependence between <database>GO</database> terms. The Methods section details the technical and statistical aspects of our methodology.<br>
<br>
Result<br>
Reference gene set<br>
The evidence annotation of the reference set provided 1625 Process terms whereas the literature annotation provided 3226. The two methods shared 1079 terms (24.9%). Although the reference set represented only 49.6% of the overall Process hierarchy of <database>GO</database> (8730 terms), we checked its relevance by evaluating its representativeness compared to all the <database>GO</database> Process terms available in the <database>GOA Human</database> database [22] and in the <database>PubGene</database> database. In both cases, the set covered more than 80% of the available terms and was therefore appropriate as a reference set.<br>
Evidence codes (<database>GOA</database>)<br>
In the evidence annotation, many gene-term associations were based on electronic inference: 38.5% of the terms retrieved in <database>GOA</database> were associated with the "Inferred from Electronic Annotation" evidence code (IEA) [26]. The remaining 61.5% corresponded to annotations made or reviewed by curators: 39% "Traceable Author Statement" (TAS), 10.8% "Non-traceable Author Statement" (NAS), 7.3% in the "Inferred from ..." family (IC: Curator, IDA: Direct Assay, IEP: Expression Pattern, IGI: Genetic Interaction, IMP: Mutant Phenotype, IPI: Physical Interaction and ISS: Structural Similarity), 3.4% "Not Recorded" (NR) and 1% "No biological Data available" (ND).<br>
<br>
Probabilistic score (<database>PubGene</database>)<br>
<database>PubGene</database> retrieved 269172 gene-term associations of which 38703 (14.4%) had a scored below 0.01. Among the 3236 Process terms associated with the set, 10 (0.3%) were obsolete and 3075 (95.3%) had a score below 0.01.<br>
<br>
Number of genes per term<br>
The generalised estimating equations (gee) showed that the annotation method did not affect the number of genes associated with a given term (estimated regression coefficient = 0.0035, standard error = 0.0893, p = 0.968). Terms with significantly enriched gene numbers could then be compared between methods.<br>
<br>
Number of terms per gene<br>
The literature annotation provided more than twice as many terms for a given gene than the evidence annotation (<database>PUB</database> 5,559 &gt; <database>GOA</database> 2,485, Kruskal-Wallis ?2 = 1886.863, df = 1, p-value &lt; 2.2e-16).<br>
<br>
Number of references per gene<br>
There were many more references associated with one gene in the literature annotation than in the evidence annotation (<database>PUB</database> 73,520 &gt; <database>GOA</database> 1,666, Kruskal-Wallis ?2 = 4445.078, df = 1, p-value &lt; 2.2e-16).<br>
<br>
Depth (granularity)<br>
We found no significant difference in the granularities of the two annotations (Kruskal-Wallis ?2 = 1.5565, df = 1, p-value = 0.2122). The median depth was seven and was consistent with the overall granularity of the <database>GO</database> Process hierarchy.<br>
As the two annotation methods have similar representativeness, number of genes per term and granularity, we added the associative relations to the two previous term sets. Figure 1 shows the overlap between the three sources of <database>GO</database> terms. The core of common terms remained large with 21.8% of the terms. In the evidence annotation, 84.2% of the terms were also retrieved by dependence. By contrast, the literature method was more specific, with only 45.7% of the terms being found by dependence and 50.1% being original terms.<br>
<br>
Path quality index (PQI)<br>
For a <database>GO</database> term, the Path Quality Index (PQI) is a measure of its relative number of annotated parents and children terms. PQIs for the combination of both evidence and literature were significantly different from evidence alone (Kruskal-Wallis ?2 = 922.5441, df = 1, p-value &lt; 2.2e-16) or literature alone (Kruskal-Wallis ?2 = 2302.722, df = 1, p-value &lt; 2.2e-16, Figure 2A). The PQIs naturally increased when dependent or random terms were added to the combination of evidence and literature annotations. This increase was significantly different (Kruskal-Wallis ?2 = 40.065, df = 1, p-value = 2.457e-10) for the dependent set versus the random set (Figure 2B).<br>
<br>
<br>
Minimal cluster: a single gene (cdc2)<br>
The number of terms obtained by each method for cdc2 in the Process hierarchy is shown in Figure 3. We limited the literature profile to significant terms (p &lt;= 0.01) and discarded associative relations having a PQI of zero. The terms in each Venn category are graphically presented in Figure 4. The terms were sorted into five categories for clarity: (A) cell cycle, mitotis and meiosis, (B) DNA events occuring during the cell cycle, including generic DNA replication and controls, (C) cellular events occuring during the cell cycle, mainly related to the cytoskeleton, (D) post-translational events and kinase-associated processing, (E) apoptosis and proliferation.<br>
Evidence annotation retrieved only four Process terms and, with the exception of "traversing start control point of mitotic cell cycle" (GO:0007089), all were found by literature profiling and/or associative relations. The literature annotation retrieved 154 Process terms, 23 of which had scores below 0.01. These significant terms were associated with 266 <database>MEDLINE</database> references. A systematic reading of the title and abstract of these references showed that these were relevant for the associations brought out and related all the important steps of the cdc2 characterisation (discovery of the cell cycle mutants for the yeast, biochemical purification of the Mitosis Phase Factor (MPF) in several species, descriptions of the various substrates and inhibitors of cdc2, etc.). Furthermore, half the references provided by the literature annotation were less than 5 years old.<br>
Terms from the evidence and literature annotations were also associated with 153 terms in the associative relation database. Only 38 of these had a non-zero PQI. A selective part of the network of terms associated by dependence is presented in Figure 5.<br>
<br>
Down cluster<br>
A Venn diagram for the 37 down-regulated genes during CaCo-2 cells differentiation is shown in Figure 6. We limited the literature profile to significant terms (p &lt;= 0.01) and discarded associative relations having a PQI of zero.<br>
Evidence<br>
As for the reference set, genes from the Down cluster were primarily annotated with three evidence codes: IEA (47.1%), TAS (33.3%) and NAS (12.6%). TAS, NAS and IDA evidence codes were associated with 28 <database>MEDLINE</database> references. Manual inspection of the 87 gene-term associations confirmed the accuracy of the evidence annotation and the robustness of the inference methods used in building annotation databases. Less than 2% of the terms were unexploitable. These were either misassociated, for example "perception of sound" (GO:0007605) with ITM2B, or not very biologically informative, for example "biological_process unknown" (GO:0000004) for TRIP6.<br>
<br>
Literature<br>
The Down cluster was annotated with 259 significant <database>GO</database> terms associated with 3377 <database>MEDLINE</database> references. Manual inspection of all the 626 significant gene-term associations retrieved by <database>PubGene</database> showed that 81.5% had a direct link between the gene and the term, e.g., "copper ion transport" (GO:0006825) with ATP7B, "DNA replication initiation" (GO:0006270) with MCM3, "chromatin silencing" (GO:0006342) and "DNA packaging" (GO:0006323) with CBX1, or "ornithine catabolism" (GO:0006593) and "putrescine catabolism" (GO:0009447) with ODC1. There were very few false positives associations (1.2%). The remaining 17.3% of the associations were correct but imprecise. The gene symbol and the term were both found effectively in the title/abstract but there was either: (i) no biological relationship between them, for example, ATP7B associated with "mRNA metabolism" (GO:0016071) in a study of mRNA expression levels (and thus transcription) of ATP7B itself [27], or (ii) the biological relationship between them was indirect. For example, the relationship between "cell cycle checkpoint" (GO:000075) and EIF3S2 (eukaryotic translation initiation factor 3, subunit 2) from a study by Humphrey and Enoch [28] on sum1+ (suppressor of uncontrolled mitosis) is indirect because this protein presents a "striking sequence similarity" with EIF3S2. Similarly, "regulation of cell cycle" is indirectly related to TOP2A as shown by Pasion et al. [29] in a study demonstrating the "negative regulation of TOP2A mRNA during the cell cycle".<br>
<br>
Combination of evidence and literature<br>
PQIs for the combination of both methods were significantly different from evidence alone (Kruskal-Wallis ?2 = 48.9203, df = 1, p-value = 2.666e-12) or literature alone (Kruskal-Wallis ?2 = 41.2014, df = 1, p-value = 1.373e-10) (Figure 7A).<br>
<br>
Terms with significantly enriched gene numbers<br>
Enriched <database>GO</database> terms for the Down cluster are shown in Figure 8. At a threshold of 0.01, the cluster is characterised by processes also described in the Bedrine-Ferran study: cell cycle, transport, signal transduction, nucleic acid and polyamine metabolism. These metabolic pathways underlie the proliferative state of undifferentiated CaCo-2 cells. Among the additional pathways retrieved by our method, two were strongly annotated and relevant: apoptosis and growth. At a threshold of 0.05, the enrichment either supplies additional terms to the annotated pathes, specifically cell death, cell proliferation and nucleic acid metabolism, or identifies new functional areas, such as DNA metabolism. See Additional file 1 for the complete annotation of the Down cluster.<br>
<br>
Associative relations<br>
The addition of dependent or random terms to the evidence and literature annotations naturally increased the PQIs. This increase was significantly different (Kruskal-Wallis ?2 = 255.7346, df = 1, p-value &lt; 2.2e-16) for the dependent set versus the random set (Figure 7B). We found a higher, although not unreasonable, proportion of bad associations (about 10%) from the systematic inspection of all the 256 gene-term-term associations retrieved in the associative relations database for the Down cluster. We identified three different types of errors: (i) Term-term misassociations, for example, the process "positive regulation of smooth muscle contraction" (GO:0045987) and the function "G-protein-coupled receptor binding" (GO:0001664); (ii) term-gene misassociations, such as the process "response to pheromone" (GO:0019236) for HSPA9B; and (iii) gene-term-term misassociations, for example, the function "calcium ion binding" (GO:0005509) and the processes "synaptic transmission" (GO:0007268) and "neuropeptide signaling pathway" (GO:0007218) for ANXA5. Relevant associations comprised within hierarchy (WH) dependences (35%) and across hierarchies (AH) dependences (65%). A selective part of the inter-dependent terms network for the Down cluster is shown in Figure 9.<br>
<br>
<br>
Multi-cluster analysis<br>
The 14 clusters used in the multi-cluster analysis are presented in Table 1. The number of terms in each Venn categories for all these clusters can be found in Table 2. Results concerning the PQIs comparisons can be found in Table 3 and Additional files 2, 3, 4, 5, 6, 7 and 8 for the corresponding boxplots. The annotation of the glioGBM cluster is consistent with the conclusions of Tso et al. [50], reflecting characteristics of hyper-proliferation, hypervascularity, and apoptotic resistance in glioblastoma clusters. A DAG of the enriched annotated terms associated with at least 4 genes is provided in the Additional file 9. For the bcr-abl cluster, the retrieved processes are in full accordance with the pathogenesis of the BCR/ABL Acute Lymphoblastic Leukemia (ALL) [51]. Indeed, the high proliferation rate of the blast cells is highlighted by numerous cell cycle processes including cytokinesis and chromosome segregation, but also by the activation of the MAPKKK cascade and its links with cell cycle checkpoints and anti-apoptosis processes that lead to cell survival [52]. It correlates secondly with the angiogenesis process, linking the BCR/ABL fusion protein to VEGF (vascular endo-thelial growth factor) gene expression [53] which is a hallmark of tumor aggressiveness. A DAG of the enriched annotated terms associated with a least 4 genes is provided in the Additional file 10.<br>
<br>
<br>
Discussion<br>
Evidence<br>
Our study shows that <database>GOA</database> provides high-quality <database>GO</database> annotations with a 98% precision despite there being a large number of electronically inferred gene-term associations (about 50%). Bad associations are mostly indirect rather than entirely false. For example, the "perception of sound" (GO:0007605) associated with ITM2B comes from a spkw2go mapping [30] as this gene was implicated in causing deafness. These results are consistent with a recent evaluation carried out by Camon et al. [31]. The obvious limitation of such an annotation method is that manual processing capability could rapidly become overloaded and would no longer be able to deal with the increasing amount of scientific data. Therefore, semi-automatic methods are often used to speed up the curation process. Although these semi-automatic methods are primarily used to assist biologist curators [31] and have proven to be useful [32], they are very rarely used as automatic annotation tools. This is especially true when based on text mining of <database>MEDLINE</database> references [33].<br>
<br>
Literature<br>
Text mining of biomedical literature combined with probabilistic scoring of the gene-term associations is also a powerful annotation technique. For a given gene set, it retrieved more terms per gene than evidence annotation and with a similar precision. Although common terms highlighted the major pathways, supplementary terms were a valuable source of information for reinforcing those pathways, by adding parent, sibling and child nodes. For example, in the annotation of cdc2 (Figure 4), the literature profile retrieved "regulation of cell cycle" (GO:0000074) and "cell cycle checkpoint" (GO:0000075). These are respectively the parent and sibling of "traversing start control point of mitotic cell cycle" (GO:0007089) found by evidence. Likewise, the literature term "histone phosphorylation" (GO:0016572) is the child of the evidence term "protein amino acid phosphorylation" (GO:0006468). Literature terms are thus valuable for improving the coherence of the annotation, but they also retrieve recent biologically characterised path. For example, in the annotation of the Down cluster, "paclitaxel metabolism" (GO:0042616) and "paclitaxel biosynthesis" (GO:0042617) were associated with three gene products: CDKN1A, CDC2 and TOP2A. Although the inhibitory effect of paclitaxel (taxol) on cdc2 was identified ten years ago, its action on CDKN1A and TOP2A was only recently characterised [34].<br>
<br>
Bibliographic insights<br>
Scientific literature is the optimal resource for validating a functional annotation. However, <database>GOA</database> provides few <database>MEDLINE</database> references to support its annotation. Despite there being abundant literature on cdc2 only one article was retrieved: a general study by Laronga et al. [35] on cyclin-dependent kinases in which cdc2 was only used for an in vitro kinase assay. Moreover, the annotators linked this article to the Cellular Component "nucleus" (GO:0005634) whereas it would be better associated to "negative regulation of cell cycle" (GO:0045786) and "negative regulation of cyclin-dependent protein kinase activity" (GO:0045736). Likewise, HMGA2 was TAS-associated with "development" (GO:0007275) [36] whereas we would expect the <database>GO</database> terms to describe this protein as an architectural factor involved in adipogenesis ? "fat cell differentiation" (GO:0045444) for example ? and mesenchyme differentiation, as suggested in the article abstract and in more recent studies [37]. Most of the references found in <database>GOA</database> are used to justify the annotation and that is their purpose. Therefore, they are often referent references, such as the discovery of the gene product or its first characterisation, and are less informative when searching for recent advances in the field. Given the limitations of the manual processing, there is no reference at all for many genes in <database>GOA Human</database> (e,g., ODC1, CBX1, LAMB1).<br>
The significant increase in the associated <database>MEDLINE</database> references in the literature annotation corroborates the enrichment of the <database>GO</database> terms. The considerably higher number of associated references and their accuracy (up to 90% precision) make <database>PubGene</database> an excellent bibliographic tool for validating the biological interpretation of a cluster. In the Down cluster, <database>PubGene</database> was able to retrieve very informative references highlighting the main biological implications of one gene. For example, the study by Chen et al. [38] was associated with eight <database>GO</database> terms for ODC1 and described its involvement in the polyamine and derivative (ornithine and putrescine) metabolisms. The literature approach can also retrieve gene sub-clusters based on their common references. These sub-clusters can group genes by family, for example, KRT8 and KRT18 co-annotated in four references and NME1 and NME2 co-annotated in two references, or by shared processes, for example, HSPCA with HMGB1 [39], CDKN1A with TAGLN [40], and CDKN1A with cdc2 and TOP2A [35].<br>
As expected, the primary source of errors found in the literature approach was linked to the ambiguity of the gene symbols: CBX1 was associated with "secretion" (GO:0046903) whereas the abstract of Dodic et al. [41] referred to CBX (carbenoxo-lone), SON (SON DNA binding protein) was confused with SON (SupraOptic Nucleus) in the paper by Eguchi et al. [42] and was therefore inappropriately associated with "vasopressin secretion" (GO:0030103). Other errors may be more difficult to resolve: ANXA5 was annotated with "prostanoid biosynthesis" (GO:0046457) in a reference to a study by McGinty et al. [43] on Cyclooxygenase-2 (Cox-2) ? an enzyme responsible for catalyzing the committed step in prostanoid biosynthesis ? in which ANXA5 was only stained to assess the trophic withdrawal apoptosis level in pheochromocytoma cells. These problems stress the importance of the formalism provided by the HUGO Gene Nomenclature Committee (HGNC) [44].<br>
<br>
Statistical dependence<br>
Our data strongly suggest that networks of statistically inter-dependent <database>GO</database> terms highlight the leading features of a gene or gene cluster: a synthetic and simplified interpretation of its annotation. Most of the main processes identified in the functional annotation of the Down cluster (Figure 8: cell cycle and cell proliferation, growth, cellular communication, cell death) were also part of the inter-dependent terms network (Figure 9: cell cycle and cell proliferation, growth, signal transduction, apoptosis). This network also emphasised the most specific mechanisms involved in this cluster: cellular proliferation and growth is correlated with transcription phenomena required for the cell cycle and mitosis. The regulation of these processes involves specific kinase activities and can be either positive (cytokinesis) or negative (apoptosis).<br>
The associative relations primarily provided dependences within and especially across the <database>GO</database> hierarchies and linked functions to processes. With statistical dependence, biologically meaningful relations were found: (i) between <database>GO</database> terms across hierarchies, such as the "signal transduction" (GO:0007165) process with the "receptor binding" (GO:0005102) function in the Down cluster annotation, or the "mitosis" (GO:0007067) process and the "cyclin-dependent protein kinase activity" (GO:0004693) function in the cdc2 annotation; and (ii) between <database>GO</database> terms belonging to different sub-DAGs of the same hierarchy, such as the "regulation of cell cycle" (GO:0000074) and "apoptosis" (GO:0006915) processes in both the Down cluster and cdc2 annotations.<br>
<br>
Future directions<br>
We have presented here an application of the associative relations to the functional interpretation of experimental results. We deliberately restricted their contribution to reinforce the evidence or literature annotated pathways and to identify between annotated terms the relationships across hierarchies. This improvement needs to be evaluated in terms of the precision and specificity of each non-lexical approach and the term-term associations could also be filtered with respect to their similarity coefficient.<br>
The biological interpretation of a gene cluster will surely be facilitated by the identification of the <database>GO</database> sub-DAGs having a high number of annotated nodes. Each term in the gene cluster annotation has a PQI that measures its annotation degree: its relative number of co-annotated kinship terms. Using the distribution of the PQI within the DAG, it is therefore possible to identify statistically over-annotated sub-DAGs ? possibly biological pathways ? linked to a specific biological condition. Nevertheless, this measure needs to be normalised in order to be independent from the size of the gene cluster and, consequently, from the number of <database>GO</database> terms in the annotation.<br>
We used the associative relations to identify possible interactions between processes and functions but this method is general to <database>GO</database> and not specific to a gene cluster. At least two other approaches could be explored at the cluster level. The first and most obvious one is to link terms that share one or more genes (co-annotated genes). These terms are likely to be the enriched terms from the annotation. Such approach can yet be elusive as most of the terms are only annotated with one gene. A second approach is to link sub-DAGs with high PQI terms (co-annotated terms) if we consider the PQI as a quantification of a sub-DAG (pathway) relevance for the gene cluster (biological condition).<br>
<br>
<br>
Conclusion<br>
Despite their obvious differences, semi-automatic annotation based on evidence and literature mining combined with statistical scoring of the gene-term associations are both efficient methods for associating relevant <database>GO</database> Process terms to a gene cluster. The significantly higher PQIs obtained using a combination of both methods is an indication of their synergy: they do not contain the same information. We achieved a more robust and complete annotation by combining the coherence of <database>GOA</database> with <database>PubGene</database>'s exploratory and bibliographic qualities. Eventually, <database>GO</database> terms networks can be built with associative relations in order to highlight cooperative and competitive pathways and their connected molecular functions. Our methodology is an effort to improve the actual situation which is clearly suboptimal. It is, however, not demonstrated to what precise degree this improvement goes. This remains to be determined but is outside the scope of the present paper.<br>
<br>
Methods<br>
Sources of <database>GO</database> terms<br>
<database>GOA</database>: annotation based on evidence (<database>GOA Human</database>)<br>
The <database>GOA</database> database aims to provide high-quality supplementary <database>GO</database> annotation to proteins in the <database>UniProt</database> (<database>SWISS-PROT</database>/<database>TrEMBL</database>) databases. Most of the <database>GOA</database> content comes from the manual curation of scientific literature, with semi-automatic and electronic techniques being used to support the annotation process. Therefore, an evidence code assesses the reliability of the gene-term association. These codes are established by the GO Consortium [26] and range from electronically inferred to experimental evidence. As all the associations in <database>GOA</database> are gene-term associations, the only way to rank terms in clusters is to use the number of genes associated with each term. We limited our study to the annotation of human genes and gene products in the <database>GOA Human</database> database (93136 associations for 22720 distinct proteins and 10085 <database>MEDLINE</database> references in the December 2004 release).<br>
<br>
<database>PUB</database>: annotation based on literature (<database>PubGene</database>)<br>
<database>PubGene</database> is a web-based database of gene-gene and gene-term associations based on co-occurrences in biomedical literature. It provides a full-scale literature network for 25,000 human genes extracted from the titles and abstracts of over 14 million article records from the <database>MEDLINE</database> citation database of the National Library of Medecine (NLM). The method assumes that if two genes are mentioned in the same <database>MEDLINE</database> record there should be an underlying biological relationship. Genes are linked to terms from the <database>Gene Ontology</database> and a probabilistic score is computed that reflects the gene-term association strength which can be used to assess the relevance of each individual term. The computation of this probabilistic score assumes that occurrences of the gene and the term are independent. Therefore, a binomial formula can be used to estimate the probability of finding the gene and the term together in an article based on their respective frequencies in the whole database. Assuming a normal distribution, the expected number of articles mentioning the gene and the term is then compared to the number of times they actually occur together (see [46] for details). In clusters, the reliability of each term is a multiplication of its probabilistic scores. The literature annotation was carried out with the 2.4 release of the <database>PubGene</database> database (December 2004). Obsolete terms were replaced by updated ones if present in the term_definition table of the <database>GO</database> database (i.e., specified in the term_comment attribute) and obsolete terms with no updated term were discarded from the literature annotation. Terms being poorly associated with the gene cluster (i.e., probabilistic score greater than 0.01) were also discarded.<br>
<br>
DEP: statistical dependences (associative relations)<br>
The lack of representation in <database>GO</database> of the relations existing among functions, processes and components severely limits the power of reasoning based on <database>GO</database>. In a previous work, we investigated three non-lexical approaches for identifying associative relations between <database>GO</database> terms: the vector space model, statistical analysis of co-occurrences and association rules mining [24]. Here, we used the associative relations database we built (term-term associations) to strengthen the previous annotations: we queried this database with evidence terms and significant literature terms and retrieved a list of dependent terms (gene-term-term associations). The link between the Process "oxygen transport" (GO:0015671) and the Component "hemoglobin complex" (GO:0005833) is an example of such associative relation.<br>
<br>
<br>
Gene sets<br>
Reference gene set<br>
We built a reference set of 7397 human genes that we used for a quantitative evaluation of our approach and to identify the statistically significant enriched <database>GO</database> terms in the functional annotation of a given experimental gene set. We downloaded the <database>Human Gene Nomenclature Database</database> [44] from the HUGO Gene Nomenclature Committee on the 9 th of December 2004. It contained 20056 different mapped <database>LocusLink</database> IDs. We preferred mapped <database>LocusLink</database> IDs because these are subjected to a peer-review process. We annotated 13505 IDs in <database>GOA Human</database> and found that 10969 of them presented either an approved symbol or an alias or older symbol that could be used to query the <database>PubGene</database> database. These queries gave 7397 effective annotations. The <database>GOA Human</database> annotation was then restricted to this subset to carry out comparison.<br>
<br>
Minimal cluster: a single gene (cdc2)<br>
We wanted to determine what the method was able to retrieve for a minimal cluster, that is, for a single gene. We chose the cell division cycle 2 (cdc2) product, involved in the G2/M transition of the cell cycle [47], because its functions and regulations are well-known and fully documented.<br>
<br>
Multi-cluster analysis<br>
The methodology was quantitatively evaluated in a multi-cluster analysis concerning 14 clusters chosen from 7 independent studies. Detailed informations on these clusters can be found in Table 1. The qualitative evaluation and the biological relevance of our methodology was assessed with the two clusters from the study by Be-drine-Ferran et al. [25] related to transcriptomic variations in human CaCo-2 cells used as an in vitro model of enterocyte differentiation. These clusters are differentially expressed genes through the differentiation process: 30 up-regulated genes (Up cluster) and 37 down-regulated genes (Down cluster). Evaluations for both Up and Down clusters were quantitatively and qualitatively similar. We will therefore only detail here the results obtained for the down-regulated cluster (Down cluster). See Additional file 11 for <database>LocusLink</database> IDs, symbols, aliases and names of these down-regulated genes.<br>
<br>
<br>
Identification of <database>GO</database> terms with enriched gene numbers<br>
When annotating a gene set with an hypothetical biological meaning the challenge is to find the <database>GO</database> terms that best characterise this set. These terms will be among those relevant to a high number of genes. We used the hypergeometric distribution to identify statistically significant enrichments (see [11] for a comparison of statistical methods). As a reference set, we used here the 7397 gene set built for evaluating our method.<br>
<br>
Terms attributes<br>
The various parameters measured for each term and used to evaluate the contribution of each annotation method, compare them and bring forward their specificity are called attributes.<br>
Attributes related to the methods<br>
We measured, for each <database>GO</database> term, and for each annotation method, the number of occurrences and the number of associated genes. We carried out statistical analyses only on the number of genes per term because these two variables were strongly correlated (r = 0.997, p &lt; 0.001). For the terms only found by both methods, we tested the number of genes per term against the annotation method (i.e., evidence annotation versus literature annotation). We carried out analyses with a generalized estimating equation (gee) model to estimate parameters for correlated data, assuming a Poisson error and a log-link function. We used the <software>R</software> package '<package>geepack</package>' [48].<br>
Evidence and literature verbosities were compared using a Kruskal-Wallis rank sum test on the number of <database>GO</database> terms per gene. Likewise, we used the Kruskal-Wallis rank sum test to compare the number of <database>MEDLINE</database> references per gene and to assess the bibliographic wealth in the evidence and literature methods.<br>
<br>
Attributes related to the DAG<br>
A Directed Acyclic Graph (DAG) is a hierarchy in which a node can have multiple parents and children. The highest node, the one having no parents is called the root node and the deepest nodes, those with no children, are the leaves. Thus, a node can be characterised by its position within the DAG. The depth or granularity of the node is its minimum distance from the root node [49]. We compared the granularities of the evidence and literature methods using the Kruskal-Wallis rank sum test. A <database>GO</database> term is thus part of a sub-DAG that includes all its parents in every path up to the root node and all its children and their descendants down to the leaves of the DAG. A term will be relevant if it has an enriched gene number. However, it will also be interesting if many of its parents and children are annotated. For a term, the Path Quality Index (PQI) is a measure of its relative number of annotated parents and children nodes: PQI = (NPa + NCa)/N, where N is the total number of parents and children nodes in the sub-DAG, NPa is the number of annotated parent nodes and NCa is the number of annotated child nodes. We used the PQI to compare the evidence and literature annotations to a combination of both annotations. We also used it to evaluate the global relevance of the <database>GO</database> terms found only by dependence. In this case, we calculated the PQIs for the combination of the evidence and literature and compared it to the PQIs obtained after the addition of: (i) the terms found only by dependence, and (ii) a random term set of equal size. We compared PQIs using the Kruskal-Wallis Rank Sum Test with a Bonferroni correction for multiple comparisons. We eventually used the PQI to filter out the associative relations and to limit them to reinforcing the annotation: dependent terms with a zero PQI (i.e., terms with no annotated term in their sub-DAG) were discarded.<br>
<br>
<br>
Biological relevance<br>
Evidence annotation<br>
We carried out a systematic inspection of each gene-term association retrieved from the <database>GOA Human</database> database. Associations were sorted into three categories depending on their relevance: good associations in which the <database>GO</database> term was directly linked to the gene product, bad associations in which the <database>GO</database> term was misassociated with the gene product or non-informative, and doubtful associations in which the <database>GO</database> term could be indirectly linked to the gene product (e.g., a molecular mechanism implied by or associated with the gene activity but not the gene activity itself) or, inferred by a sequence similarity, etc.<br>
<br>
Literature profile<br>
We manually inspected each of the <database>MEDLINE</database> references retrieved by <database>PubGene</database>. We read the title and abstract and classified the relevance of each gene-term association into the same three categories as used for the evidence annotation.<br>
<br>
Associative relations<br>
For the Down cluster, we manually investigated each associative relation found only by dependence and having a non-zero PQI. We determined whether the associated terms were biologically related or not and wether the association between the dependent term and the gene was appropriate. Associative relations were also sorted into Within Hierarchy (WH) relations and Across Hierarchies (AH) relations.<br>
<br>
<br>
Databases and tools<br>
The version of <database>GO</database> used throughout this study is the February 2005 monthly release, available from the <database>GO</database> website. DAG graphical representations were achieved using <software>dot</software> v1.10 and <software>Graphviz</software> 1.13(v16). All other graphics and statistical analyses were done using the <software>R</software> language version 2.1.0.<br>
<br>
<br>
Authors' contributions<br>
All computational tasks and statistical analyses were carried out by MA. The biological relevance of the method was primarily evaluated by AM and assisted by CC, JM and MA. Expertises on glioblastomas and leukemias were respectively supplied by MdT and MG. AB and JM supervised this study and contributed to continuous discussions about its shortcomings. All authors have read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1513259</b><br>
Predicting population coverage of T-cell epitope-based diagnostics and vaccines<br>
<br>
<br>
Background<br>
T lymphocytes recognize a complex between a specific major histocompatibility complex (MHC) molecule and a particular pathogen-derived epitope. Thus, a given epitope will elicit a response only in individuals that express an MHC molecule capable of binding that particular epitope, explaining to a large extent the phenomenon known as "MHC restriction" [1]. In humans, MHC molecules are known as human leukocyte antigen (HLA) molecules and two different types exist: class I and class II. HLA class I molecules mostly bind peptides derived from the endogenous processing pathway, and their recognition is primarily associated with cytotoxic T lymphocytes (CTL), which are most important for antiviral and anticancer immunity responses. By contrast, HLA class II molecules bind peptides typically derived from the extracellular milieu, and they are important for helper T lymphocyte (HTL) responses, which regulate antibody and cytotoxic responses.<br>
HLA molecules are extremely polymorphic. Over a thousand different HLA allelic variants have been defined to date [2]. Specific HLA alleles are expressed at dramatically different frequencies in different ethnicities [3,4]. Therefore, in the design and development of T-cell epitope-based diagnostics or vaccines, selecting multiple epitopes with different HLA binding specificities will afford increased coverage of the patient population. A pertinent goal, in this context, might be to identify optimal sets of HLA alleles with maximal coverages for different populations [5,6]. Extensive analyses by Longmate and coworkers [7] suggested that 90% population coverage of several ethnic groups can be achieved by targeting eleven different HLA molecules. However, 90% coverage of African and Asian ethnicities required four or more additional molecules. Dawson et al. also analyzed the problem [8] and concluded that to reach 80% coverage, 3 to 5 HLA molecules were required in a given ethnicity, but the actual HLA specificities required were different in different ethnic groups.<br>
An important consideration in the process of epitope selection for a T-cell epitope-based diagnostic or vaccine is that the patient population coverage afforded by a given epitope set does not simply correspond to the sum of the coverage of the individual components. To calculate the coverage afforded by a given set of epitopes with multiple and/or overlapped HLA binding specificities, a more comprehensive approach, taking into account MHC binding and T cell recognition patterns, is required for this purpose. A suitable algorithm was previously utilized [9-11] but not described in detail. This method calculates the fraction of individuals predicted to respond to a given epitope or epitope set on the basis of HLA genotypic frequencies and on the basis of MHC binding and/or T cell restriction data. In this paper, we describe the algorithm and its implementation as a web application available to the public. We believe this is a useful tool to aid in the design and development of T-cell epitope-based diagnostics and vaccines intended to be effective across diverse populations.<br>
<br>
Implementation<br>
For a given HLA gene locus, let {m1, m2, ..., mN} denote a set of MHC alleles, with each allele associated with a genotypic frequency G(mi) for a population or ethnic group. To account for 100% of alleles of a given locus, the total genotypic frequency (?G(mi)) should add up to 1. If ?G(mi) is less than 1, an unidentified HLA allele with a genotypic frequency equal to the residual (1 - ?G(mi)) is added to the locus. If ?G(mi) is greater than 1, the genotypic frequency of each mi allele of the locus is scaled down proportionately by dividing the frequency by ?G(mi). Next, let {e1, e2, ..., eK} denote a set of epitopes with known MHC binding or restriction data. For each epitope ek, its restriction to an MHC allele mi, ek(mi), is defined as followed:<br>
ek(mi)={0ifek?is?not?restricted?to?mi1if?ek?is?restricted?to?mi?????(1).<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGLbqzdaWgaaWcbaGaem4AaSgabeaakiabcIcaOiabd2gaTnaaBaaaleaacqWGPbqAaeqaaOGaeiykaKIaeyypa0ZaaiqaaeaafaqaaeGacaaabaGaeGimaadabaWexLMBbXgBcf2CPn2qVrwzqf2zLnharyGvLjhzH5wyaGabaiaa=LgacaWFMbGaa8hiaiabdwgaLnaaBaaaleaacqWGRbWAaeqaaOGaeeiiaaIaeeyAaKMaee4CamNaeeiiaaIaeeOBa4Maee4Ba8MaeeiDaqNaeeiiaaIaeeOCaiNaeeyzauMaee4CamNaeeiDaqNaeeOCaiNaeeyAaKMaee4yamMaeeiDaqNaeeyzauMaeeizaqMaeeiiaaIaeeiDaqNaee4Ba8MaeeiiaaIaemyBa02aaSbaaSqaaiabdMgaPbqabaaakeaacqaIXaqmaeaacqqGPbqAcqqGMbGzcqqGGaaicqWGLbqzdaWgaaWcbaGaem4AaSgabeaakiabbccaGiabbMgaPjabbohaZjabbccaGiabbkhaYjabbwgaLjabbohaZjabbsha0jabbkhaYjabbMgaPjabbogaJjabbsha0jabbwgaLjabbsgaKjabbccaGiabbsha0jabb+gaVjabbccaGiabd2gaTnaaBaaaleaacqWGPbqAaeqaaaaaaOGaay5EaaGaaCzcaiaaxMaadaqadaqaaiabigdaXaGaayjkaiaawMcaaiabc6caUaaa@8AEA@<br>
First, for each MHC allele (mi), a total number of epitope "hits", H(mi), was tabulated by adding the number of epitopes that are restricted to (or bound by) mi:<br>
H(mi)=?k=1Kek(mi)??(i=1,?,N)?????(2).<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGibascqGGOaakcqWGTbqBdaWgaaWcbaGaemyAaKgabeaakiabcMcaPiabg2da9maaqahabaGaemyzau2aaSbaaSqaaiabdUgaRbqabaaabaGaem4AaSMaeyypa0JaeGymaedabaGaem4saSeaniabggHiLdGccqGGOaakcqWGTbqBdaWgaaWcbaGaemyAaKgabeaakiabcMcaPiabbccaGiabbccaGiabcIcaOiabdMgaPjabg2da9iabigdaXiabcYcaSiabl+UimjabcYcaSiabd6eaojabcMcaPiaaxMaacaWLjaWaaeWaaeaacqaIYaGmaiaawIcacaGLPaaacqGGUaGlaaa@51B1@<br>
Next, for each possible diploid MHC combination (mi, mj), a phenotypic frequency F(mi, mj) was calculated based on individual allele genotypic frequency:<br>
F(mi, mj) = G(mi) ? G(mj) ??? (3)<br>
For n MHC types, this corresponds to an n ? n tabulation of the phenotypic frequency at which each specific pair of MHCs will be found in the population from which the MHC frequencies were derived. A similar table was also generated to contain the number of epitope hits per each of the MHC combinations H(mi, mj). In the case of heterozygous combinations, H(mi, mj) was calculated as the sum of the number of epitope hits associated with each of the two alleles, H(mi) + H(mj). This is because mi and mj are two different alleles, and therefore the number of epitope hits recognized by each allele in the combination is independent of each other. However, in the case of homozygous combinations which contain two identical alleles, the number of epitope hits was the same as the number of epitope hits of the given allele:<br>
H(mi,mj)={H(mi)+H(mj)if?i?jH(mi)if?i=j?????(4).<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGibasdaqadaqaaiabd2gaTnaaBaaaleaacqWGPbqAaeqaaOGaeiilaWIaemyBa02aaSbaaSqaaiabdQgaQbqabaaakiaawIcacaGLPaaacqGH9aqpdaGabaqaauaabaqaciaaaeaacqWGibasdaqadaqaaiabd2gaTnaaBaaaleaacqWGPbqAaeqaaaGccaGLOaGaayzkaaGaey4kaSIaemisaG0aaeWaaeaacqWGTbqBdaWgaaWcbaGaemOAaOgabeaaaOGaayjkaiaawMcaaaqaaiabbMgaPjabbAgaMjabbccaGiabdMgaPjabgcMi5kabdQgaQbqaaiabdIeainaabmaabaGaemyBa02aaSbaaSqaaiabdMgaPbqabaaakiaawIcacaGLPaaaaeaacqqGPbqAcqqGMbGzcqqGGaaicqWGPbqAcqGH9aqpcqWGQbGAaaaacaGL7baacaWLjaGaaCzcamaabmaabaGaeGinaqdacaGLOaGaayzkaaGaeiOla4caaa@5DB7@<br>
Based on the calculated F(mi, mj) and H(mi, mj) tables, a frequency distribution was assembled by tabulating the phenotypic frequencies of all MHC combinations associated with a certain number of epitope/HLA combination hits (h):<br>
F(h)=?i=1N?j=1NF(mi,mj)I{H(mi,mj)=h}?????(5),<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGgbGrcqGGOaakcqWGObaAcqGGPaqkcqGH9aqpdaaeWbqaamaaqahabaGaemOrayKaeiikaGIaemyBa02aaSbaaSqaaiabdMgaPbqabaGccqGGSaalcqWGTbqBdaWgaaWcbaGaemOAaOgabeaakiabcMcaPiabdMeajnaaBaaaleaacqGG7bWEcqWGibascqGGOaakcqWGTbqBdaWgaaadbaGaemyAaKgabeaaliabcYcaSiabd2gaTnaaBaaameaacqWGQbGAaeqaaSGaeiykaKIaeyypa0JaemiAaGMaeiyFa0habeaaaeaacqWGQbGAcqGH9aqpcqaIXaqmaeaacqWGobGta0GaeyyeIuoaaSqaaiabdMgaPjabg2da9iabigdaXaqaaiabd6eaobqdcqGHris5aOGaaCzcaiaaxMaadaqadaqaaiabiwda1aGaayjkaiaawMcaaiabcYcaSaaa@5DB8@<br>
where I{H(mi,mj)=h}={1if?H(mi,mj)=h0if?H(mi,mj)?h<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGjbqsdaWgaaWcbaGaei4EaSNaemisaGKaeiikaGIaemyBa02aaSbaaWqaaiabdMgaPbqabaWccqGGSaalcqWGTbqBdaWgaaadbaGaemOAaOgabeaaliabcMcaPiabg2da9iabdIgaOjabc2ha9bqabaGccqGH9aqpdaGabaqaauaabeqaciaaaeaacqaIXaqmaeaacqqGPbqAcqqGMbGzcqqGGaaicqWGibascqGGOaakcqWGTbqBdaWgaaWcbaGaemyAaKgabeaakiabcYcaSiabd2gaTnaaBaaaleaacqWGQbGAaeqaaOGaeiykaKIaeyypa0JaemiAaGgabaGaeGimaadabaGaeeyAaKMaeeOzayMaeeiiaaIaemisaGKaeiikaGIaemyBa02aaSbaaSqaaiabdMgaPbqabaGccqGGSaalcqWGTbqBdaWgaaWcbaGaemOAaOgabeaakiabcMcaPiabgcMi5kabdIgaObaaaiaawUhaaaaa@6092@ is an indicator function.<br>
For calculation of coverage by epitope sets restricted to MHC alleles of multiple k different loci, a combined frequency distribution (P) as a function of epitope/HLA combination hits (n) was generated by merging k separate frequency distributions. This merging procedure is based on the assumption that linkages between MHC loci are in equilibrium, and was done as follows:<br>
P(n)=?h1?1??hk?1(?i=1kFi(hi)I{?i=1khi=n})?????(6),<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGqbaucqGGOaakcqWGUbGBcqGGPaqkcqGH9aqpdaaeqbqaaiabl+UimnaaqafabaWaaeWaaeaadaqeWbqaaiabdAeagnaaBaaaleaacqWGPbqAaeqaaOGaeiikaGIaemiAaG2aaSbaaSqaaiabdMgaPbqabaGccqGGPaqkcqWGjbqsdaWgaaWcbaWaaiWaaeaadaaeWbqaaiabdIgaOnaaBaaameaacqWGPbqAaeqaaSGaeyypa0JaemOBa4gameaacqWGPbqAcqGH9aqpcqaIXaqmaeaacqWGRbWAa4GaeyyeIuoaaSGaay5Eaiaaw2haaaqabaaabaGaemyAaKMaeyypa0JaeGymaedabaGaem4AaSganiabg+GivdaakiaawIcacaGLPaaaaSqaaiabdIgaOnaaBaaameaacqWGRbWAaeqaaSGaeyyzImRaeGymaedabeqdcqGHris5aaWcbaGaemiAaG2aaSbaaWqaaiabigdaXaqabaWccqGHLjYScqaIXaqmaeqaniabggHiLdGccaWLjaGaaCzcamaabmaabaGaeGOnaydacaGLOaGaayzkaaGaeiilaWcaaa@672D@<br>
where I{?i=1khi=n}={1if??i=1khi=n0if??i=1khi?n<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGjbqsdaWgaaWcbaWaaiWaaeaadaaeWbqaaiabdIgaOnaaBaaameaacqWGPbqAaeqaaSGaeyypa0JaemOBa4gameaacqWGPbqAcqGH9aqpcqaIXaqmaeaacqWGRbWAa4GaeyyeIuoaaSGaay5Eaiaaw2haaaqabaGccqGH9aqpdaGabaqaauaabeqaciaaaeaacqaIXaqmaeaacqqGPbqAcqqGMbGzcqqGGaaidaaeWbqaaiabdIgaOnaaBaaaleaacqWGPbqAaeqaaOGaeyypa0JaemOBa4galeaacqWGPbqAcqGH9aqpcqaIXaqmaeaacqWGRbWAa0GaeyyeIuoaaOqaaiabicdaWaqaaiabbMgaPjabbAgaMjabbccaGmaaqahabaGaemiAaG2aaSbaaSqaaiabdMgaPbqabaGccqGHGjsUcqWGUbGBaSqaaiabdMgaPjabg2da9iabigdaXaqaaiabdUgaRbqdcqGHris5aaaaaOGaay5Eaaaaaa@60DB@ is an indicator function, and Fi(hi) is a phenotypic frequency associated with hi epitope/HLA combination hits of locus i calculated from equation 5.<br>
The population coverage (C) or fraction of individuals projected to respond to the epitope set was then calculated as the sum of the combined phenotypic frequencies associated with at least one epitope hit/HLA combination:<br>
C=?n?1P(n)?????(7).<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqGH9aqpdaaeqbqaaiabdcfaqjabcIcaOiabd6gaUjabcMcaPaWcbaGaemOBa4MaeyyzImRaeGymaedabeqdcqGHris5aOGaaCzcaiaaxMaadaqadaqaaiabiEda3aGaayjkaiaawMcaaiabc6caUaaa@3DF6@<br>
Based on equation 6, a histogram was generated to summarize the fraction of population coverage (P) as a function of the number of HLA/epitope combinations (n) recognized. A cumulative population coverage distribution frequency (Y) as a function of the number of HLA/epitope combinations (n) was also calculated:<br>
Y(n)=?x?nP(x)?????(8).<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGzbqwcqGGOaakcqWGUbGBcqGGPaqkcqGH9aqpdaaeqbqaaiabdcfaqjabcIcaOiabdIha4jabcMcaPaWcbaGaemiEaGNaeyyzImRaemOBa4gabeqdcqGHris5aOGaaCzcaiaaxMaadaqadaqaaiabiIda4aGaayjkaiaawMcaaiabc6caUaaa@41D8@<br>
From this cumulative population coverage distribution of the whole epitope set, PC90, defined as the minimum number of epitope/HLA combination hits (n) recognized by 90% of the population, was determined as follow:<br>
PC90=n+Y(n)?0.9Y(n)?Y(n+1)?????(9),<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGqbaucqWGdbWqcqaI5aqocqaIWaamcqGH9aqpcqWGUbGBcqGHRaWkdaWcaaqaaiabdMfazjabcIcaOiabd6gaUjabcMcaPiabgkHiTiabicdaWiabc6caUiabiMda5aqaaiabdMfazjabcIcaOiabd6gaUjabcMcaPiabgkHiTiabdMfazjabcIcaOiabd6gaUjabgUcaRiabigdaXiabcMcaPaaacaWLjaGaaCzcamaabmaabaGaeGyoaKdacaGLOaGaayzkaaGaeiilaWcaaa@4C50@<br>
where Y(n) ? 0.9 &gt; Y(n + 1). Because) PC90 was determined by data interpolation, it can be of any positive decimal value. Based on equation 9, if the population coverage is less than 90% or C=?n?1P(n)?Y(1)&lt;0.9<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqGH9aqpdaaeqbqaaiabdcfaqjabcIcaOiabd6gaUjabcMcaPaWcbaGaemOBa4MaeyyzImRaeGymaedabeqdcqGHris5aOGaeyyyIORaemywaKLaeiikaGIaeGymaeJaeiykaKIaeyipaWJaeGimaaJaeiOla4IaeGyoaKdaaa@42C5@, PC90 will be less than 1.<br>
Additionally, the average number of epitope/HLA combination hits (A) recognized by the population is a weighted average and was calculated as follow:<br>
A=?n?1n?P(n)?????(10).<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGbbqqcqGH9aqpdaaeqbqaaiabd6gaUjabgEna0kabdcfaqjabcIcaOiabd6gaUjabcMcaPaWcbaGaemOBa4MaeyyzImRaeGymaedabeqdcqGHris5aOGaaCzcaiaaxMaadaqadaqaaiabigdaXiabicdaWaGaayjkaiaawMcaaiabc6caUaaa@4250@<br>
<br>
Results and discussions<br>
The <software>Population Coverage Calculation</software> program was implemented as a Java servlet public web-application (see Availability and Requirements section). HLA allele (genotypic) frequencies were obtained from <database>dbMHC</database> database [12]. At present, <database>dbMHC</database> database provides allele frequencies for 78 populations grouped into 11 different geographical areas. In addition to the allele frequencies obtained from the <database>dbMHC</database> database, the <software>Population Coverage Calculation</software> program also accepts custom populations with allele frequencies defined by users. Multiple population coverages can be simultaneously calculated and an average population coverage is generated. Since MHC class I and MHC class II restricted T cell epitopes elicit immune responses from two different T cell populations (CTL and HTL, respectively), the program provides three calculation options to accommodate different coverage modes ? (1) class I separate, (2) class II separate, and (3) class I and class II combined. For each population coverage, a histogram is generated to summarize the percentage distribution of individuals as a function of the number of epitope/HLA combinations recognized. A cumulative coverage distribution plot is also generated to determine the minimum number of epitope/HLA combinations recognized by 90% of the population (PC90). Finally, the average number of epitope/HLA combinations recognized by the population and coverages of individual epitope are also calculated.<br>
It should be noted that when population coverages are projected from an epitope set restricted to alleles from multiple HLA loci, linkages between loci are taken into account. The overall population (phenotypic frequency), (Ptotal), is mathematically derived as the sum of the individual locus' coverage corrected for the overlaps: S=?k=1nn!k!(n?k)!<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWucqGH9aqpdaaeWbqaamaalaaabaGaemOBa4MaeiyiaecabaGaem4AaSMaeiyiaeIaeiikaGIaemOBa4MaeyOeI0Iaem4AaSMaeiykaKIaeiyiaecaaaWcbaGaem4AaSMaeyypa0JaeGymaedabaGaemOBa4ganiabggHiLdaaaa@4072@, where Pij is the frequency of the ij haplotype, Pijk is the frequency of the ijk haplotype, etc... If gene linkage equilibrium is assumed, Pij can be calculated as the product of the individual allele phenotypic frequencies (Pi ? Pj), and Pijk = Pi ? Pj ? Pk, etc... This calculation is implicitly incorporated in our current algorithm (equation 6). However, if gene linkage is in disequilibrium, the frequency of a given haplotype is usually not equal to the product of their individual allele phenotypic frequencies, (Pij ? Pi ? Pj, Pijk ? Pi ? Pj ? Pk, ...). As a result, to account for linkage disequilibrium between HLA loci, complete data on haplotype frequencies must be known. Therefore, it would be difficult to factor in linkage disequilibrium at this time because linkage disequilibrium is known to be different in different ethnicities, and data regarding the specific disequilibrium in different ethnicities in general is not available or incomplete. As more comprehensive MHC linkage disequilibrium data becomes available, our method can be modified to incorporate this type of calculation.<br>
Although the present program assumes linkage equilibrium between HLA loci, the impact of linkage disequilibrium, which is known to occur in the MHC region, on the calculated coverage is expected, in most contexts, to be minimal. For example, in the North American Caucasian population, the A1 and B8 antigens of HLA-A and -B loci, respectively, are known to be the strongest linked antigen pair with an observed haplotype frequency of 7.95% [13]. The genotypic frequencies of the A1 and B8 antigens are 15.18% and 9.41%, respectively [13]. Assuming the linkage between A1 and B8 antigens is in equilibrium, the overall population coverage calculated by the present program is 40.97%, and the individual population coverages by A1 and B8 antigens are 28.06% and 17.93%, respectively. The expected equilibrium frequency for the A1/B8 haplotype, in this case, is 5.03% (28.06% ? 17.93%) which is 2.92% less than the observed frequency of 7.95%. Therefore, if linkage disequilibrium is considered, the overall population coverage will be 38.04% (28.06% + 17.93% - 7.95%). Thus, even for the most tightly linked A1/B8 haplotype in the Caucasian population, linkage disequilibrium, in this specific example, only accounted for less than 3% difference in the population coverage calculated by the present program. Furthermore, we have also investigated the deviations between the observed and expected equilibrium frequencies of 1012 HLA-A/-B haplotypes in the North American Caucasian population, based on available antigen- and haplotype-frequencies published by Mori et al. [14,15]. On average, the observed haplotype frequencies deviated from the expected equilibrium frequencies by approximately 0.58%. As a result, linkage disequilibrium is expected to impact the calculated population coverage, but the degree of the impact is expected to be negligible.<br>
It should be pointed out that the calculations described herein can also be performed on data <fileFormat>spreadsheets</fileFormat>, but the process is laborious, error prone and also requires extensive immunological expertise. In our experience, a single calculation without the aid of this tool requires several hours to complete. To the best of our knowledge, at this time, there is no existing program that is publicly accessible as a web-resource that can offer the flexibility and range of utility similar to the <software>Population Coverage Calculation</software> program that we have developed. The present application represents a significant enhancement of the <database>dbMHC</database> database's utility by incorporating its compiled data of world-wide ethnic population frequencies to calculate HLA coverage for user-defined population subsets. The program is flexible by allowing the user to specify groups of related or unrelated ethnicities as well as specify the HLA alleles under consideration. Additional flexibility features include the implementation of separate calculations for both MHC Class I and Class II demarcated recognitions as they involve immune responses from two different populations of T cells ? CTL and HTL, respectively. The output of the program was also specifically designed to be accessible to both specialists and neophytes in the field of MHC research. Therefore, having this tool publicly available is highly desirable. Additionally, in our future works, we plan to incorporate in the tool the ability to search for minimal epitope subset(s) within the given epitope set that will afford a specified population coverage level. This is not a trivial task due to a large number of possible epitope subsets (S) that has to be considered, S=?k=1nn!k!(n?k)!<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWucqGH9aqpdaaeWbqaamaalaaabaGaemOBa4MaeiyiaecabaGaem4AaSMaeiyiaeIaeiikaGIaemOBa4MaeyOeI0Iaem4AaSMaeiykaKIaeiyiaecaaaWcbaGaem4AaSMaeyypa0JaeGymaedabaGaemOBa4ganiabggHiLdaaaa@4072@ where n is the total number of epitopes and k is the number of epitopes in a subset. For example, for a set of 20 epitopes, there will be a total of 1,048,575 combinations of epitope subsets that needs to be evaluated. Therefore, a strategic searching approach must be devised to computationally accomplish this task. In summary, with the help of this <software>Population Coverage Calculation</software> program, epitope-based vaccines or diagnostics can be designed to maximize population coverage while minimizing complexity (that is, the number of different epitopes included in the diagnostic or vaccine), and also minimizing the variability of coverage obtained or projected in different ethnic groups.<br>
<br>
Conclusion<br>
Herein, we have implemented a method to calculate projected population coverage of a T-cell epitope-based diagnostic or vaccine using MHC binding or T cell restriction data and HLA gene frequencies. The <software>Population Coverage Calculation</software> program was designed to be user friendly and flexible. Besides the compiled HLA gene frequencies currently provided, users can also supply their own tabulated HLA gene frequencies for calculation. Therefore, researchers can use this tool to perform coverage analyses on their specific patient populations. We plan to continuously update the compiled HLA gene frequencies as more data are available, and thus to provide researchers with a useful tool to aid in the design and development of effective T-cell epitope-based diagnostics and vaccines.<br>
<br>
Availability and requirements<br>
Project name: Population Coverage Calculation<br>
Project home page: <br>
Programming language: Java<br>
Operating system: Fedora Linux<br>
Other requirements: Apache Tomcat 5.5.12, <software>MySQL</software> 4.1<br>
Web browser: <software>Population Coverage Calculation</software> program has been tested and shown to work with the following browsers: <software>Firefox</software> version 1.5 (PC and Mac OS X), <software>Netscape</software> version 8.0.4 (PC), <software>Netscape</software> version 7.2 (Mac OS X), <software>Internet Explorer</software> version 6.0 (PC), <software>Internet Explorer</software> version 5.2 for Mac (Mac OS X). Default security settings were used.<br>
<br>
Authors' contributions<br>
HHB developed the computer algorithm and designed the web-resource. AS and JS contributed the calculation approaches. KD helped with programming and collecting HLA frequency data. SS and MN were involved in conceptualizing the calculation approaches. HHB wrote the manuscript, AS and JS edited the final version. All authors read and approved the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1660556</b><br>
<software>CoXpress</software>: differential co-expression in gene expression data<br>
<br>
<br>
Background<br>
Microarrays have become a standard tool for the exploration of global gene expression changes at the cellular level [1]. Data analysis often includes the use of a statistical test, such as a t-test or analysis of variance, to find genes differentially expressed in one set of conditions when compared to another, or the use of clustering algorithms in order to find groups of genes which behave similarly over a number of experiments [2]. However, these techniques may not detect differential co-expression patterns that exist between two biological states.<br>
Statistical tests, such as the t-test or ANOVA, identify genes that are differentially expressed under one or more conditions. The output of such tests is a simple list of genes, with an associated test statistic and p-value [3]. There is no indication of which genes may be interacting with one another. Alternatively, clustering algorithms are often used to find groups of genes which display similar expression profiles across a dataset, and these clusters are subsequently analyzed visually for patterns of interest [4,5]. Eisen et al used hierarchical cluster analysis to determine groups of co-expressed genes, and found that genes within those groups were functionally related [4], and the use of hierarchical cluster analysis is now a standard technique for analysing microarray data [2,6]. Yeung et al [7] assessed the use of hierarchical clustering to find groups of co-regulated genes. Various clustering algorithms were used on a number of datasets, and the results evaluated by determining those genes that share a common transcription factor. Of the algorithms tested, MCLUST [8] and two hierarchical methods (based on the pearson correlation coefficient) showed the highest coincidence of correlated and co-regulated genes.<br>
However, genes which show highly correlated patterns of expression in one biological state, but not in another, may not be highly correlated across the entire dataset, and therefore would not be associated with one another if a clustering algorithm is used. Variation may exist in the expression of a gene in different groups of individuals due to the presence of sub-populations, and this may lead to that gene being grouped incorrectly. Furthermore, clustering algorithms do not provide methods to identify groups that are behaving differently in different biological conditions.<br>
Recent work has concentrated on alternative approaches to the discovery of co-expressed genes. Li [9] describes a method whereby genes whose expression is associated with differential co-expression patterns in other pairs of genes may be discovered, and Lai et al [10] describe a conceptually similar method whereby pairs of genes that display differential co-expression patterns between the normal and cancerous state may be discovered. Other approaches have centred on the construction of large gene co-expression networks. Lee et al [11] analysed 60 human microarray data sets to construct gene co-expression networks conserved across multiple data sets, and Stuart et al [12] constructed a gene co-expression network across different organisms, indicating that such relationships are evolutionarily conserved. However, neither of the above attempted to find group of genes differentially co-expressed between different conditions. Choi et al [13] tackled this problem by constructing normal and tumour co-expression networks from a variety of public datasets, comparing the results to find differences in co-expression patterns associated with cancer. In all of these cases, the networks were built by comparing genes pairwise, using some variation of the pearson correlation coefficient, to determine if a co-expression link exists between the two genes. These links were then joined to form a co-expression network.<br>
Cluster analysis and network construction can be thought of as alternative methods for finding co-expressed genes. Whereas networks concentrate on conserved, pairwise comparisons, there is no guarantee that genes that are close in the network, but are not directly linked, have correlated expression profiles. Alternatively, cluster analysis produces groups of genes that are correlated above a certain level, defined by where the tree is cut and the clustering algorithm, but there is no indication of which particular pairs of genes are interacting. Kostka and Spang [14] described the first method to investigate differentially co-expressed groups of genes, using an additive model for scoring gene-gene co-expression and then a stochastic search algorithm to find groups of genes showing differential co-expression patterns. Jen et al [15] have produced <software>ACT</software>, the <software>Arabidopsis Co-expression Tool</software>, which allows users to calculate co-expression across user-defined data sets and uses a correlation cut-off to define groups of genes.<br>
Here we describe <software>coXpress</software>, a simple and easy to use package that allows users to explore differential co-expression in an intuitive way. The package is aimed at biologists who want to analyse differential co-expression in their data set, which can be achieved in just 4 simple commands once the data has been loaded. <software>CoXpress</software> uses hierarchical cluster analysis to explore the relationship between genes, cutting the tree to form groups of genes that are co-expressed. This is an intuitive approach that many biologists are familiar with. <software>CoXpress</software> then uses a resampling approach to find those groups that are co-expressed in one set of experiments and not in another. The package should be used as first step in the analysis of co-expression, and is designed to complement the approaches described above.<br>
<br>
Implementation<br>
<software>CoXpress</software> is released as a package for <software>R</software>. <software>R</software> is a freely available, open-source statistical package [16] that is widely used in the biological community. <software>R</software> has very powerful statistical and graphical capabilities, and many add-on packages are freely available. The <database>bioconductor</database> project [17,18] provides a huge number of add-on packages for <software>R</software>, covering a wide range of biological data analysis applications, and the implementation of <software>coXpress</software> in <software>R</software> provides seamless integration with many of these packages. <software>CoXpress</software> is written in the native R language and has been fully tested on both windows and linux. <software>R</software> is available for windows, linux, unix and MacOS (including MacOS X).<br>
The input for <software>coXpress</software> is a matrix of data, with rows representing genes and columns representing microarrays. The R data.frame object is most convenient, and can be created by reading in a text file (using the read.table function), an <fileFormat>Excel spreadsheet</fileFormat> (using the <database>RODBC</database> library) or from existing R objects, created by the packages from the <database>bioconductor</database> project such as <package>affy</package> [19], <package>limma</package> [20] or <package>marray</package> [18].<br>
The genes are first clustered based on their expression values in a subset of experiments (termed subset 1), using the cluster.gene function. This function wraps the dist, cor and hclust functions that are built in to <software>R</software>, and thus provides a simple interface to hierarchical clustering. When a correlation coefficient is used as the distance measure, the distance measure is calculated as 1 - r, where r is the pearson correlation coefficient. The resulting tree is cut at a user-defined value, using the cutree function, to form groups of genes that are co-expressed in subset 1. These groups are then examined in both subset 1 and a second set of experiments, defined by the user, which we will term subset 2.<br>
Groups of size 1 are ignored as there can be no co-expression. Groups of size two are handled by the cox.pairs function. The cox.pairs function uses the cor.test function in <software>R</software> to test if the genes are significantly correlated in subset 1 and subset 2. Thus, a pair of genes significantly correlated in subset 1 and not significantly correlated in subset 2 can be described as differentially co-expressed.<br>
Groups with more than two members are handled by the coXpress function. The flow of analysis in <software>coXpress</software> is represented in figure 1. For each group of size n, where n ? 3, the pairwise correlation coefficients of the group in subset 1 are calculated. These are then summarised using the t-statistic, the use of which is discussed below. Then, m random groups of size n are created by randomly re-sampling the data matrix. For each of these random groups, the pairwise correlation coefficients of the group in subset 1 are calculated and again summarised using the t statistic. Thus, a distribution of t statistics is created, of size m, from randomly assigned groups of size n. The observed t statistic is then compared to the distribution of random t statistics. The proportion of random statistics greater than the observed is used as a "probability of randomness" for the group in subset 1. This process is then repeated for subset 2. A group which is found to be non-random in group 1 and random in group 2 is said to be differentially co-expressed. These groups will be highly correlated in subset 1 but show little or no correlation in subset 2. To find the reverse, the process must be repeated, but basing the original groups on a cluster analysis of the data based on subset 2.<br>
The t-statistic is used here not as a test of significance, but as a means of summarising a set of pairwise correlation coefficients into a single value. Correlation coefficients are on the scale:<br>
-1 ? r ? +1<br>
where 1 represents positive correlation, -1 represents negative correlation and 0 represents no relationship. The t-statistic is used here to summarise the "difference from zero" of a group of pairwise correlation coefficients. The exact formula for this is:<br>
t=x?se(x)<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG0baDcqGH9aqpdaWcaaqaaiqbdIha4zaaraaabaGaem4CamNaemyzauMaeiikaGIaemiEaGNaeiykaKcaaaaa@36B1@<br>
where x is the vector of unique, pairwise correlation coefficients, x?<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWG4baEgaqeaaaa@2E3D@ is the mean of x and se(x) denotes the standard error of x. A group of highly correlated genes will have a mean correlation close to 1 and a small standard error, resulting in a large value for t. However, a group of uncorrelated genes will have a mean close to 0 and a relatively large standard error, resulting in a small value for t. The observed t statistic is compared against m random t statistics in order to calculate a probability of randomness.<br>
<br>
Results<br>
The AML/ALL leukaemia dataset<br>
The utility of <software>coXpress</software> is demonstrated using gene expression data from the leukaemia microarray study of Golub et al [21]. This dataset represents gene expression measurements from 38 tumour mRNA samples, 27 acute lymphoblastic leukaemia (ALL) cases and 11 acute myeloid leukaemia (AML) cases. The HU6800 Affymetrix array was used, which contains 6800 probesets. The dataset has been filtered such that genes with negative values in any sample have been removed, resulting in 2568 genes.<br>
Using coXpress, the genes were first clustered according to their expression levels in the 27 ALL samples, using the cluster.gene function. The distance measure used was 1 - r, where r is the pearson correlation coefficient. The resulting tree was cut at a distance of 0.4, representing a correlation coefficient of 0.6, using the cutree function.<br>
These groups were then examined in both the ALL and AML cases using the coXpress function. The observed t statistics in all cases were compared with the t statistics generated by randomly resampling the dataset 10,000 times for each group size. The resulting table contains one row for each group.<br>
To test the robustness of the method to outliers, a bootstrapping approach was used. Each group was re-tested 1000 times, each time randomly selecting 75% of the observations for each leukaemia subtype (20 AML cases and 8 AML cases). The number of times each group was found to be differentially co-expressed by the coXpress method was recorded.<br>
Table 1 shows the results filtered for groups that are non-random in the ALL subset, random in the AML subset, and with more than 6 members. As can be seen, there are 10 groups, varying in size from 7 to 34 members. The mean pairwise correlations for the groups are all above 0.6 in the ALL cases, yet show little or no correlation in the AML cases, with mean values ranging from -0.093 to 0.144. The robustness resampling method provides evidence that the groups found are robust to outliers, with nine out of ten groups being found in over 90% of the resampled data sets, and the other being found in 76%.<br>
Figure 2 demonstrates the method of coXpress. These graphs show data from the largest of the groups, group 3, which has 34 members. Fig. 2A compares the distribution of pairwise correlation coefficients in the ALL subset with two random distributions. The blue graph is the distribution of observed correlation coefficients in the ALL subset for group 3, the red graph is the distribution of pairwise correlation coefficients from data generated by the random uniform distribution, and the green graph is the distribution of pairwise correlation coefficients from a group of genes randomly selected from the dataset. As can be seen, the observed distribution for this group in the ALL subset is very different from the two random distributions. Fig. 2B is an identical graph for the group based on the AML subset. This time, the observed distribution shows no difference compared to the two random distributions. The t-statistics for each distribution are shown on these graphs. Fig. 2C shows the observed t-statistic for group 3 in the ALL subset compared to the distribution of 10,000 randomly generated t-statistics, and Fig. 2D is the equivalent graph for the AML subset. Again, it is clear that this group in the ALL subset is non-random, yet is no different to random in the AML subset.<br>
Figure 3 shows the top 3 groups in table 1 graphically. Fig. 3A is the largest of the groups, with 34 members. These 34 genes have a mean pairwise correlation of 0.70 in the ALL subset, but only 0.003 in the AML subset. Fig. 3B shows a smaller group, with 7 members, with a mean pairwise correlation of 0.72 in the ALL subset and -0.09 in the AML subset. Finally, fig. 3C shows a group with 11 members, with a mean pairwise correlation coefficient of 0.679 in the ALL subset and only 0.086 in the AML subset. These graphs were produced using the plot.compare.group and plot.cluster.genes functions.<br>
Figure 4 shows the same three groups in a different way. Here, each plot is a representation of the correlation matrix of the group of genes in either the ALL or the AML subsets. Each coefficient in the correlation matrix is represented as a square, with the colour of the square representing the amount of correlation. The colour scale used is green to red, with green representing -1 (negative correlation), red representing +1 (positive correlation) and black representing 0 (no correlation). In all three groups, the correlation matrices are red for the ALL subset, yet are a mixture of black, green and red in the AML subset. This view of the data is more useful than simply considering the average pairwise correlation, as it shows all of the values in an intuitive way. These graphs were produced using the show.cor.matrices function.<br>
In each of the differentially co-expressed groups, not all pairwise correlation coefficients will have decreased or changed. To examine which pairs of genes have changed, the inspect.group function should be used. Table 2 shows the ten pairwise correlation coefficients that have changed the most between the ALL and AML subsets in group 62. As can be seen, these pairs of genes are all positively correlated in the ALL subset but are negatively correlated in the AML subset. Table 3 shows the ten pairwise correlation coefficients that have changed the least between the ALL and AML subsets in group 62. Many of these pairs of genes are still positively correlated in the AML subset, but not to the same extent. It is important that each differentially co-expressed group is examined in this way to determine which of the pairs of correlated genes have changed and which have not.<br>
The GOHyperG function of the <software>GOstats</software> package [22] was used to find <database>GO</database> terms over-represented in the differentially co-expressed groups. Group 3, with 34 members, is enriched for <database>GO</database> terms for lymph node development, cell organisation and biogenesis, and protein biosynthesis and transport. Group 62, which has 7 members, is enriched for <database>GO</database> terms for methyltransferase activity, DNA modification, protein transport and DNA and protein methylation. Group 121, with 11 members, is enriched for <database>GO</database> terms for nucleotidase activity, and RNA splicing, processing and metabolism.<br>
<br>
The ALL subtype dataset<br>
This dataset is from the Acute Lymphoblastic Leukaemia study by Yeoh et al [23]. Six subtypes of ALL leukaemias are represented in 248 cases. The six subtypes are T-ALL, E2A-PBX1, BCR-ABL, TEL-AML1, MLL rearrangement, and hyperdiploid &gt;50. The HG_U95Av2 Affymetrix microarray was used which contains 12,600 probesets. The dataset has been filtered such that genes with negative values in any sample have been removed, resulting in 1516 genes present in the dataset.<br>
Using <software>coXpress</software>, the genes were first clustered according to their expression levels in the BCR-ABL samples, using the cluster.gene function. The distance measure used was 1 - r, where r is the pearson correlation coefficient. The resulting tree was cut at a distance of 0.5, representing a correlation coefficient of 0.5, using the cutree function. These groups were then examined in both the BCR-ABL and T-ALL subsets.<br>
Those groups of size two were analysed using the cox.pairs function. Table 4 shows three pairs of genes that are significantly positively correlated in the BCR-ABL subset, and significantly negatively correlated in the T-ALL subset.<br>
Groups of N ? 3 were analysed in the BCR-ABL and T-ALL subsets using the coXpress function. The observed t statistics in all cases were compared with the t statistics generated by randomly resampling the dataset 10,000 times for each group size. The resulting table contains one row for each group.<br>
To test the robustness of the method to outliers, a bootstrapping approach was used. Each group was re-tested 1000 times, each time randomly selecting 75% of the observations for each leukaemia subtype. The number of times each group was found to be differentially co-expressed by the coXpress method was recorded.<br>
Table 5 shows the results filtered for groups that are non-random in the BCR-ABL cases, random in the T-ALL cases, and with more than 10 members. Figure 5 shows the top 3 groups in table 1 graphically. Figure 5A shows a group of 16 genes that have a mean pairwise correlation coefficient of 0.669 in the BCR-ABL subset, yet only 0.06 in the T-ALL subset. Figure 5B shows a group of 10 genes that have a mean correlation of 0.65 in the BCR-ABL subset and only 0.08 in the T-ALL data. Finally, Figure 5C shows a group of 13 genes that have an average correlation of 0.64 in the BCR-ABL data, yet only 0.04 in the T-ALL data. The robustness resampling method provides evidence that the groups found are robust to outliers, with twelve out of thirteen groups being found in over 80% of the resampled data sets, and the other being found in 68.6%.<br>
The GOHyperG function of the <software>GOstats</software> package [22] was used to find <database>GO</database> terms over-represented in the differentially co-expressed groups. Group 47 with 16 members, is enriched for <database>GO</database> terms for hormone catabolism, glucocorticoid receptor signalling and glucocorticoid catabolism. Group 31 with 10 members contains two probes for a gene in the RAS oncogene family, and is enriched for <database>GO</database> terms for oxidoreductase activity and ubiquitin activating enzyme activity. Finally, group 89 with 13 members contains genes annotated as B-cell lymphoma and cancer susceptibility genes, as well as genes enriched for <database>GO</database> terms for endothelial cell migration, regulation of cell motility and migration, angiostatin binding and regulation of blood vessel endothelial cell migration.<br>
<br>
<br>
Discussion<br>
It is clear that <software>coXpress</software> is capable of finding differentially co-expressed groups of genes in both data sets. The groups presented above are extremely highly correlated in one subset of experiments, yet show little correlation in another subset. Furthermore, these patterns of correlation are shown to be non-random in the first subset, and no different from random in the second subset. The results show that it is the overall correlation structures of these groups that have changed significantly and some pairs of genes are still highly correlated in the second subset. It is important that each group is examined using the inspect.groups function in order to determine which of the pairs of genes are still correlated and which are not. The groups found by <software>coXpress</software> could also feed into the network construction technique described by Choi et al [13] to determine which pairwise relationships are conserved and which are not. One would expect the differences between ALL and AML leukaemia in the Golub dataset to be larger than those between different ALL subtypes in the Yeoh dataset, and the fact that coXpress can still find groups with such different correlation structures demonstrates the power of the method.<br>
The use of hierarchical cluster analysis, followed by cutting the tree, is an intuitive approach and one that is familiar to biologists. However, it has limitations. For example, each gene may only be in one group, which does not ring true for biological systems, where many genes have multiple functions. Also, the choice of where to cut the tree is arbitrary. A high cut-off will produce many small groups of genes that are very highly correlated, whereas a lower cut-off will produce fewer groups, of larger size, which are not as highly correlated. In reality the user must use a range of different cut-offs to see which performs best with their dataset. Other clustering algorithms, such as MCLUST [7,8], have been shown to out-perform hierarchical cluster analysis, however, there is no reason why these algorithms could not be used to define the groups of genes prior to running the <software>coXpress</software> function.<br>
There are several directions in which the software can be developed. At present, the user defines which subsets of experiments are analysed, however it is possible that <software>coXpress</software> could suggest, or improve, these groupings using an approach such as random forests or genetic algorithms. This may allow researchers to discover sub-populations in the system under study. Integration of other clustering algorithms with coXpress, such as MCLUST, may also improve the performance of the software. In particular, clustering or grouping algorithms that allow genes to be present in more than one group may be advantageous. Finally, the integration of network construction algorithms would allow researchers to further analyse and visualise the differentially co-expressed groups discovered by <software>coXpress</software>.<br>
<br>
Conclusion<br>
We describe <software>coXpress</software>, an open-source <software>R</software> package that allows researchers to analyse differential co-expression patterns in DNA microarray data. <software>CoXpress</software> contains several methods for the discovery and visualisation of differentially co-expressed genes. We show how <software>coXpress</software> can be used to find groups of differentially co-expressed genes in two publicly available microarray datasets. The groups found are shown to be highly correlated in one subset of experiments, yet show little or no correlation in a second subset of experiments. A comparison against random distributions is used to obtain a p-value for the co-expression of the genes in different subsets.<br>
<br>
Availability and requirements<br>
? Project Name: coXpress<br>
? Project Home Page: <br>
? Operating Systems: Windows, Linux<br>
? Programming Language: R<br>
? Other Requirements: <software>R</software>, gplots, gtools, gdata (for heatmaps), hu6800, hgu95av2, plotrix (for examples)<br>
? License: GNU GPL<br>
<br>
Abbreviations<br>
ALL: acute lymphoblastic leukaemia<br>
AML: acute myeloid leukemia<br>
T-ALL: T lineage leukaemias<br>
E2A-PBX1: B lineage leukemias that contain t(1;19)<br>
BCR-ABL: B lineage leukemias that contain t(9;22)<br>
TEL-AML1: B lineage leukemias that contain t(12;21)<br>
MLL rearrangement: B lineage leukemias that contain rearrangements in the MLL gene on chromosome 11, band q23<br>
Hyperdiploid &gt;50: hyperdiploid karyotype (i.e., &gt;50 chromosomes)<br>
<br>
Authors' contributions<br>
MW developed and tested the software in full.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2242808</b><br>
<software>TABASCO</software>: A single molecule, base-pair resolved gene expression simulator<br>
<br>
<br>
Background<br>
Mechanistic models of the individual biochemical events that comprise gene expression are quite detailed and are continuously improving. For example, experiments studying the kinetics of promoter initiation and single molecule studies of processive protein movements along DNA have revealed intricate regulatory processes that control gene expression [1-4]. As these details accrue, understanding how individual processes work together to determine system-level behavior becomes increasingly difficult to intuit [5]. The ability to simulate events on and along DNA at base-pair resolution would benefit those interested in studying or attempting to control the consequences of molecular processes on system behavior. However, while currently available simulation techniques are sufficient to study the expression from a single operon at base-pair resolution, they become computational expensive if the system is of much greater scale or complexity [6]. Also, existing simulators cannot formally account for intermolecular events along DNA, such as collisions between polymerases.<br>
There are several mathematical approaches for computing the dynamics of systems of biochemical reactions. For example, the chemical species that define a system can be modeled as continuous variables that change over time. In this "continuous" approach, reactions between chemical species are modeled as a set of coupled ordinary differential equations (ODEs) [7,8]. The set of ODEs are often numerically integrated using established algorithms to compute the dynamics of the system. As a second example, a system's chemical species can be treated as discrete variables that change over time. In this "discrete" approach, reactions between species are treated as individual events that update the system and can be combined into the chemical master equation [9]. The chemical master equation is usually computationally integrated to determine the time evolution of the system using stochastic simulation algorithms (SSAs). SSAs have some theoretical advantages over continuous formulations [10]. For instance, in some systems, an individual reaction event can cause a large difference in the likelihood that other reactions will occur, and so the precise order and timing of individual reaction events can influence overall system behavior. This situation may occur when the numbers of any particular reactant in a system are low, as in the case of a single copy of DNA bound by a protein. In these cases, SSAs provide exact calculations of the system dynamics, while the continuous approximations break down.<br>
However, challenges exist in using either approach to study high-resolution models of gene expression. Both discrete and continuous approaches to solving the time evolution of biochemical reactions share the so-called "combinatorial explosion" problem, in which the number of possible states that need to be enumerated in a system becomes exceedingly large [11]. For example, the spatial and temporal control of gene endo16 expression during S. purpuratu development is controlled by a 2,300 base-pair (bp) sequence [12]. This control sequence contains 33 sites that bind 15 distinct proteins [13]. Developing a model that fully enumerates all the possible states of the endo16 gene regulatory region requires stating over 1013 distinct protein:DNA species. Not only is this computational expensive, it seems unnecessary; the number of possible species far exceeds the copy numbers of the relevant DNA and protein molecules in the cell.<br>
Two approaches have addressed the combinatorial explosion problem with respect to protein complex formation and modification during signaling [11,14,15]. The first approach, single-molecule simulation, taken by the <software>StochSim</software> simulator, tracks individual molecules and their state (e.g., what other molecules they are bound to) so that only the complexes formed at any given time are enumerated (and not all possible complexes) [11]. <software>StochSim</software> allows individual molecules to transition between states (e.g., chemical modification of a protein or complex formation), which in turn modulates the reaction rates of processes that the molecules participate in. These transition and state changes are specified at the start of simulation. Single-molecule simulators are computationally advantageous when the number of tracked molecules is much smaller than the number of possible states that the system can potentially achieve. A second approach, taken by the <software>Molecularizer</software> simulator, is to dynamically generate reactions as they occur based on automatic adjustments to reaction rates based on diffusion considerations [15]. <software>Moleculizer</software> can construct the set of reactants and reactions that are actually executed during simulation and can use this more manageable set of reactions as input into more traditional simulators. Dynamic reaction generation simulators such as <software>Moleculizer</software> work well when many of the possible states of the system are never realized, and thus the simulators need only ever represent a fraction of the total possible system.<br>
<software>StochSim</software> and <software>Moleculizer</software> were designed to model the binding and unbinding of proteins within complexes. However, these simulators are not well suited for modeling systems in which components within a complex are processive (i.e., components transitioning among many binding states) or when the number of binding states is large. For example, simulating a single RNA polymerase transcribing a 3,000 bp gene at single-base resolution using a single-molecule simulator would require a model for a DNA molecule comprised of over 3,000 different states with enumerated rules for the transitions from one base to the next. Using a dynamic reaction generation simulator, such as <software>Moleculizer</software>, would not lead to significant speed enhancements because most of the states of the system are reached during a simulation, albeit rarely.<br>
Recent efforts to stochastically simulate gene expression have focused on reducing the number of states and complexity of the system being studied by approximating the kinetic delays that occur during transcriptional and translational elongation processes. For example, the stochastic simulator <software>Dizzy</software> allows for modeling transcription elongation as a series of equivalent steps that can be sampled in total from the gamma distribution, as Gibson and Bruck suggest [16,17]. However, these simplifications cannot account for the effects that intermolecular interactions on the DNA may have on system dynamics. In addition, Roussel and Zhu approximated the delays in transcription that can take into account polymerase interactions using a reduced site-oriented Markov model that is accurate in cases where there are limited polymerase interactions [18,19]. However, without a priori knowledge for how polymerase interactions affect the expression of any particular gene, especially in the case of multi-promoter, multi-gene, multi-polymerase systems, we must use a more detailed model of gene expression.<br>
Thus, in order to efficiently and more realistically simulate base-pair resolved models of gene expression, we developed a single-molecule stochastic simulator, <software>Tabasco</software>, optimized to handle molecular events specific to gene expression such as the initiation, elongation and termination of transcription and translation as well as interactions among protein-DNA complexes. <software>Tabasco</software> tracks individual molecules and reactions on the DNA at single base resolution (e.g., RNA polymerase transcribing DNA) in order to avoid the specification of many unoccupied system states. <software>Tabasco</software> also uses dynamic reaction generation based on encoded rules for gene expression in order to avoid specification of all possible states and transitions within the system (e.g., all RNA polymerase molecules transcribe DNA to produce RNA). This framework provides the accurate descriptions of gene expression dynamics while allowing analysis of phenomena such as how intermolecular events between DNA-protein complexes affect system-wide gene expression.<br>
Our motivations for creating <software>Tabasco</software> started from our interest in simulating the dynamics of bacteriophage T7 gene expression during phage infection. For example, past models of T7 infection used ODEs to simulate T7 gene expression dynamics. The use of ODEs limited our ability to accurately simulate the kinetics of T7 gene expression and study how intermolecular events may impact the genome-wide allocation of expression resources [20,21]. For example, the E. coli RNA polymerase initiates expression of early T7 genes, including the gene encoding T7 RNA polymerase. Thus, newly synthesized T7 RNA polymerase first initiates transcription behind already transcribing E. coli RNA polymerase molecules. E. coli RNA polymerase has a ~45 nucleotide per second elongation rate; T7 RNA polymerase has a ~250 nucleotide per second elongation rate. What happens when a T7 RNA polymerase molecule overtakes an E. coli RNA polymerase is not well understood. However, the transition from E. coli to T7 RNA polymerase mediated genome entry will impact the timing of expression across the genome; cell-cell variation in the entry transition will limit the precision by which T7 can control infection. Thus, simulating possible polymerase-polymerase interaction models is an interesting research question. Using <software>Tabasco</software>, we can explore the dynamics of gene expression during T7 development at single-base resolution. In the case of T7, the increased modeling resolution afforded by <software>Tabasco</software> allows for direct evaluation of assumptions concerning polymerase interactions, transcriptional coupling to genome entry, and stochastic fluctuations on phage development. Although motivated by our interest in T7, <software>Tabasco</software> can also be used to represent other genetic systems and, as such, is generally useful to those interested in understanding how detailed molecular processes affect genome-wide gene expression.<br>
<br>
Results<br>
Algorithm<br>
<software>Tabasco</software> is a stochastic simulator that tracks individual molecules of DNA and associated proteins at single base-pair resolution. <software>Tabasco</software> makes use of a Gibson-accelerated Gillespie SSA to compute the reaction event timing and the resultant time-evolution of the genetic system [17]. <software>Tabasco</software> uses predefined rules of transcription and translation such as initiation, elongation, termination, and protein interactions of polymerases, ribosomes, and other DNA/RNA-associated proteins. Based on these rules, <software>Tabasco</software> automatically updates the states of molecules and reaction events (Methods). For example, if a transcribing RNA polymerase temporarily occupies the DNA-binding site for a second protein, the simulator makes the site unavailable for binding until the polymerase is no longer occluding access to the binding site.<br>
<software>Tabasco</software> transitions between two levels of resolution while simulating gene expression: "single-molecule" and "species level" (Figure 1). Reactants and events that occur on the DNA are tracked at single-molecule resolution?each copy of DNA and proteins associated with them are tracked individually by the simulator. In this regime, events such as binding/un-binding and polymerase movements are dynamically generated based on the current state of the DNA molecules and proteins on the DNA. On the other hand, the species-level resolution is akin to traditional SSAs in which reactants are tracked as groups of equivalent species. For example, <software>Tabasco</software> tracks all cellular protein-protein interactions at the species level.<br>
The transition between the single-molecule level and the species level occurs during the tracking of RNA abundances (Figure 1). Only those RNA molecules that are still attached to transcribing RNA polymerases are tracked as single molecules. As each coding domain on an RNA molecule is completed, it becomes part of the species-level simulation. At the species-level, each of the coding domains is treated separately as a species, and ribosomes that initiate translation on a coding domain are assumed not to interfere with one another. At both the single-molecule and species-level, translation is treated as a series of single amino acid polymerization steps, with the number of steps depending on the length of the coding domain. This gives a more accurate distribution of times for protein production than treating the whole elongation process as a single step. Based on the work of Gibson and Bruck, we compute translation steps in aggregate using the gamma distribution (Figure 2) [17].<br>
The structure of <software>Tabasco</software> confers at least four advantages. First, treating gene expression at base-pair resolution allows for more accurate representation of the kinetics of gene expression. For example, traditional SSAs often lump multi-step reactions as single steps causing inaccurate estimates on pre-steady state kinetics. Second, tracking the state of individual proteins on DNA and allowing internal logic to automatically generate reactions eliminates the need to enumerate all the possible states of polymerases and proteins associated with the DNA. For example, transcribing polymerases and processes such as genome entry into a cell can cause certain protein binding sites to be inaccessible (Figure 3a, b). This feature also allows us to consider and integrate many factors that may influence the rate of RNA polymerization for any particular gene, such as the binding of multiple transcription factors or the contribution of RNA polymerases that initiated transcription at a promoter connected to an upstream gene (Figure 3d). Third, protein-protein interactions that may occur on DNA, such as collisions between different polymerases, can be accounted for and simulated based on simple and explicit rules (Figure 3c). Fourth, <software>Tabasco</software> can be used to graphically depict the location and dynamics of individual RNA polymerase molecules transcribing DNA, providing a useful visual tool for considering genome-scale gene expression dynamics.<br>
<br>
Testing<br>
Simple gene expression models<br>
As an initial test, we simulated the expression of a 1,000 amino acid protein using both <software>Tabasco</software> and a standard species-level SSA (Figure 4). Both simulators used identical gene expression models except for one difference. In the standard SSA, processive transcription and translation elongation reactions along the DNA are treated as a lumped, single-step reaction sampled from an exponential distribution; in <software>Tabasco</software> these reactions are treated as a series of individual base-pair elongation steps. In addition, due to internal structural differences between the two simulators, the two underlying models had slightly different rate constants for RNA polymerase clearance from the promoter region (Methods). Thus, the clearance rate was adjusted in the species-level SSA to produce equal steady state levels of protein to the <software>Tabasco</software> simulation (Table 1). At steady state, both simulators produce statistically equivalent results (Figure 4a, b). However, an expected difference arises in the pre-steady state dynamics of the system (Figure 4c). The species-level SSA simulation first produces protein by ~30 seconds (Figure 4d). This is unrealistic; E. coli RNA polymerase transcribes at an average elongation rate of 40 bp per second, production of a 3,000 nucleotide messenger RNA should take ~75 seconds. Thus, even if a ribosome directly followed the first transcribing RNA polymerase, protein production should not take less time than RNA production. <software>Tabasco</software> produces protein only after a more accurate ~100 seconds (Figure 4d).<br>
The discrepancy in the time for synthesis of a first protein product occurs for two reasons [17]. First, since the RNA polymerase and ribosome elongation steps in the species-level simulation are treated as single exponential elementary reactions, 63% of the reactions occur prior to the average reaction time (Figure 2). The second and more substantive reason is that SSAs assume that reactions are Markovian (i.e., reaction event timing only depends on the current state of the system, and not system history). In an SSA that treats elongation as a single step, a ribosome that has initiated translation has some non-zero chance of completing protein synthesis; since the SSA treats the reaction as Markovian, the greater the number of ribosomes that have initiated translation, the greater the chance that any single protein synthesis event will be completed. Thus, if many ribosomes initiate translation before any one ribosome has enough time to complete translation of a coding sequence, then then protein synthesis will be computed at completing in an unrealistically short time. This is physically unrealistic; lumped reactions representing processive reactions should not be modeled as Markovian. In other words, a ribosome that begins translation should not affect the speed at which downstream ribosomes will complete protein synthesis. The fact that <software>Tabasco</software> treats each elongation reaction as a series of elongation steps alleviates both of these problems. As a result, <software>Tabasco</software> provides a more accurate estimate of pre-steady state gene expression kinetics than simulators and models that lump transcription and translation processes into a single reaction. Accurate calculation of the pre-steady state kinetics is important for systems in which a steady state is never reached, such as during phage infection, cell cycle, or animal development, and is the reason that many groups have incorporated approximate delays into their simulation frameworks [18,19,22-27].<br>
<br>
Polymerase Interactions<br>
The first natural biological system that we studied using <software>Tabasco</software> is bacteriophage T7. During bacteriophage T7 infection, the E. coli RNA polymerase initiates expression of early T7 genes, including T7 RNA polymerase [28]. T7 RNA polymerase first initiates transcription behind already transcribing E. coli RNA polymerase molecules. At 37?C, E. coli RNA polymerase transcribes at ~45 bp per second; T7 RNA polymerase transcribes at ~250 bp per second [29]. Thus, T7 RNA polymerases will overtake E. coli RNA polymerases. How transcribing RNA polymerase molecules interact mechanistically is just beginning to be understood, but experimental studies show that these interactions are important [30-33].<br>
In order to test how different models of polymerase-polymerase interactions impact gene expression, we first used <software>Tabasco</software> to simulate gene expression from a reduced genetic system that captures key features from the layout of the T7 genome. Briefly, this two-gene system has an E. coli promoter expressing two hypothetical genes encoded on a polycistronic mRNA (encoding Proteins 1 and 2), and a T7 promoter expressing a monocistronic mRNA that only encodes Protein 2 (Figure 5). We used <software>Tabasco</software> to test three distinct models for how co-transcribing RNA polymerases may interact: (1) the downstream polymerase terminates transcription allowing the upstream polymerase to continue (downstream falloff model), (2) the upstream polymerase terminates transcription while the downstream polymerase continues (upstream falloff model), or (3) the upstream polymerase follows at the speed of the downstream polymerase (traffic jam model). Each polymerase collision model leads to different levels of steady state protein production (Figure 5). For example, in the upstream falloff model, most of the T7 polymerases will prematurely terminate because they overtake slower E. coli RNA polymerases (Figure 5d). As a result, gene expression levels are similar to the situation where there is no T7 polymerase at all (Figure 5b).<br>
<br>
<br>
Application<br>
Genome-scale models and simulation<br>
In order to test the effectiveness of <software>Tabasco</software> on genome-scale systems, we simulated gene expression during bacteriophage T7 infection. The 39,937 base pair T7 genome, as represented in our test simulation, is comprised of 52 coding domains, 22 host and phage promoters, two transcriptional terminators, and three distinct transcriptional feedback loops; four T7 coding domains were not represented as they are the result of translational frameshifts or alternative start sites for other genes [28,34]. Because there are relatively few transcriptional terminators and many promoters, the rates of transcription for individual T7 genes depends on the combined levels of transcription initiation from all upstream promoters, as well as the presence of different types of RNA polymerases. In the case of T7, there is a further complication in that not all genes are available for transcription at once, since the entry of the T7 genome is a relatively slow process that is itself mediated by transcribing RNA polymerases.<br>
Using realistic parameter sets, we simulated a single cell being simultaneously infected by three phage particles (Methods). The protein-DNA interactions on the DNA were tracked independently for each infecting phage, but use the same pools of soluble proteins in the cell; thus, these simulations track a combined 119,811 base pairs of DNA. We incorporated known mechanisms of cellular entry of T7 DNA into our simulations. We also developed a method to visualize the results of our simulations that allows for the graphical display of dynamic <software>Tabasco</software> output (Methods). Simulation of a full-scale genomic model of T7 gene expression on a single AMD Athlon MP 2100+ 1.8 GHz processor takes between 1?2 hours for a model that represents 30 minutes of real-time infection. Visualization of <software>Tabasco</software> output displays both T7 and E. coli RNA polymerases transcribing the DNA, the extent of genome entry, and the resulting mRNA and protein levels from the 52 encoded genes (Additional file 1). Over the course of the simulation, the phage DNA enters and is transcribed by the E. coli RNA polymerase. As the T7 RNA polymerase is produced, host transcription is attenuated while the T7 RNA polymerase takes over transcription and entry of the remaining T7 genes. For the first time, we are able to simulate gene expression at single base resolution for all gene expression in a particular organism during its development. In turn this allows us to test specific hypotheses of polymerase interactions, stochastic gene expression, and coupling of entry and transcription. An experimental analysis of the T7 gene expression program, using <software>Tabasco</software>, will be presented in a forthcoming paper [Keller H, Endy D, Kosuri S, in preparation].<br>
<br>
<br>
<br>
Discussion &amp; Conclusion<br>
We designed and developed <software>Tabasco</software> to revisit approximations encoded within previously available gene expression simulation algorithms, and to develop a method for efficiently computing single-molecule, base-pair resolved models of gene expression. Previous tools required either modeling smaller systems at increased resolution, supercomputers, or simplifications and assumptions of the biophysical models that result in reduced simulation accuracy. For example, <software>Tabasco</software>'s implementation of a Gibson-accelerated Gillespie SSA allowed us to bypass simplifications that are often made for promoter binding by RNA polymerase and transcription initiation [18-20,35]. We have shown here that the increased resolution of <software>Tabasco</software> provides more accurate pre-steady state kinetics of gene expression, gives us the ability to test models of polymerase and protein interactions on the DNA, and allows us to fully simulate the dynamics of gene expression during development of bacteriophage T7 at base-pair resolution.<br>
Like <software>Moleculizer</software>, <software>Tabasco</software> dynamically generates reactions based on rules. Specifically, <software>Tabasco</software> uses the mechanics of gene expression as rules in order to avoid specification of all the states and transitions prior to simulation [15]. Like <software>Stochsim</software>, <software>Tabasco</software> is a single-molecule simulator, which allows <software>Tabasco</software> to track the state of individual molecules of DNA, rather than tracking individually all states of a system that could be reached [11]. As a result, <software>Tabasco</software> differs from previous gene expression simulators in that it is able to automatically represent the effects of molecular collisions, such as polymerase-polymerase interactions and the occlusions of DNA elements by transiting molecules.<br>
Since <software>Tabasco</software> tracks individual molecules of DNA, the processing power required for <software>Tabasco</software> simulation scales with the copy number and length of the template DNA. While requiring significant processing power, <software>Tabasco</software> is computationally tractable in regimes for which stochastic simulations are often needed to produce accurate results (for example, when the number of DNA molecules in the system is low). Those interested in using simulators similar to <software>Tabasco</software> to study detailed interactions at the RNA level would face challenges due to the higher numbers of RNA compared to DNA in the cell. Such challenges may be overcome in the future by incorporating the work of others on increasing the efficiency of SSAs without significantly sacrificing accuracy [36-39]. In particular, if the probabilistic simplifications of Russell and Zhu can be extended to simulations in larger, more complex systems such as T7, we would expect to see increases in simulation efficiency [19].<br>
While <software>Tabasco</software> is an exact SSA, it only exactly simulates the kinetics of the already simplified models that we give it. For example, we model transcription elongation as a first-order reaction, even though we know that this process is more complicated [40]. In addition, the general use of physics models based on well-mixed elementary chemical reactions may sometimes be an inappropriate approximation of the inside of a cell. For example, it is known that the binding of proteins interacting with DNA often involves one-dimensional diffusion along a DNA template [41,42]. The effects of these simplifications on simulation accuracy are not well studied, and are common across currently available simulators of gene expression dynamics. Finally, other processes that affect transcription such as mRNA secondary structure, sequence-specific kinetics, regulated pausing and backtracking, and many other known biophysical phenomena are not considered here. Such processes can and should be incorporated into gene expression simulation frameworks as the particular biological system being studied or engineered warrants.<br>
Executables, source code, documentation, and usage notes for <software>Tabasco</software> are freely available, and should facilitate future extensions on the current design (Availability and Requirements). We have already found <software>Tabasco</software> to be useful in constraining models of T7 gene expression by comparing simulator output to new experimental measurements of absolute copies of mRNA abundance during infection; these results will be presented in a forthcoming paper [Keller S, Endy D, Kosuri S, in preparation]. In addition, <software>Tabasco</software> should be a good base to further study interactions on DNA that lead to transcription and translation. For example, <software>Tabasco</software> provides a platform to explicitly simulate hypotheses how many transcription factors can interact to direct eukaryotic gene expression, such as in control of endo16 expression [13]. Finally, the general approach of using simulator-encoded logic and tracking of one-dimensional reaction systems should be useful for studying other biological phenomena ? for example, oligosaccharide modifications of proteins.<br>
<br>
Methods<br>
Overall Simulator Structure<br>
<software>Tabasco</software> implements a modified version of the Gibson Next Reaction Method (NRM) [17]. Gibson's NRM is an exact SSA that extends Gillespie's original First Reaction method by (1) updating only the minimum number of reactions through the use of a dependency graph and using absolute tentative reaction times, and (2) using an efficient data structure, the indexed priority queue, to store and sort reactions. At the start of the NRM, all reactions are defined and their tentative time of next execution ("tentative reaction time") is calculated and stored within an indexed priority queue. In addition, a dependency graph, which allows an executed reaction to call an update on only those reactions that are affected, is generated. The use of absolute times when calculating tentative reaction times allows reactions that are not affected by the execution of the last reaction to remain valid for the next iteration. The indexed priority queue sorts these reactions efficiently to allow quick searches for the minimum tentative time as well as quick lookups for any particular reaction. The reaction with the next tentative reaction time is executed, any tentative reaction times that are affected by the execution of the current reaction are updated, and the indexed priority queue structure is reordered to reflect the new times. The NRM does not change the tentative reaction times of reactions that are not affected by the currently executing reaction. The process is repeated to compute the time evolution of the entire system. Gibson and Bruck proved that the NRM is equivalent to the exact SSA algorithms developed by Gillespie.<br>
The NRM uses a dependency graph to determine which reactions are affected by any particular reaction's execution. In the NRM, the dependency graph is constructed only once at the start of simulation and remains unchanged afterwards. However, since <software>Tabasco</software> creates reactions and complexes at the single-molecule level dynamically during simulation, a static dependency graph and indexed priority queue will not work. In order to solve this problem, <software>Tabasco</software> contains two specialized classes per DNA molecule within the overall indexed priority queue that are used to track a set of dynamically generated reactions. Each class contains a dynamic priority queue that stores the dynamically generated transcriptional and translational reactions and their tentative reaction times, as well the dependencies of any particular reaction (these specialized priority queues and the overall indexed priority queue are easy to confuse, and thus we will refer to the prior as the dynamic priority queue). The minimum tentative reaction time for all the dynamic reactions is set as the tentative reaction time of that dynamic priority queue with respect to the overall priority queue. Since, as in the NRM, <software>Tabasco</software> uses absolute times for determining the next reaction, the particular choice of the data structures containing the reactions does not affect the simulation results. The reason to separate the dynamic queues from the main indexed priority queue is to allow the size of the dynamic queues to change over time. As long as all dependencies are accurately updated upon reaction execution in the dynamic priority queues, the results are equivalent to the NRM. The structure of these dynamic priority queues will now be discussed in more detail.<br>
<br>
Transcription<br>
A special class tracks the transcriptional processes for each DNA molecule. This class contains a dynamic priority queue that keeps track of all reactions related to that DNA molecule, such as transcriptional elongation processes. All reactions that are stored in this dynamic priority queue have tentative reaction times (as calculated by the NRM), and the minimum tentative reaction time is used to set when this class should be called to execute within the indexed priority queue. In order to determine what the effects of a particular binding or elongation reaction are, the class contains two arrays where each element represents one DNA base. Each element of the first array contains pointers to transcriptional elements encoded at that location such as promoters and terminators. The second array contains pointers to all DNA-protein complexes that reside on the DNA such as elongating RNA polymerase. Only one transcriptional control element or DNA complex can occupy any particular position at any time.<br>
The framework we use here to simulate protein-DNA binding is shown in Figure 6A. Interactions from the species level to this specialized reaction occur when proteins bind the DNA. For example, RNA polymerase or other proteins can bind free promoters to form a protein-DNA complex, which causes the DNA complex array to be updated, the number of available promoters and RNA polymerase to decrement, and finally places two reactions into the dynamic priority queue. The two reactions compete to either have the complex form an initiation complex or fall off the DNA. If an initiation complex is formed, a stochastic decision is made as to whether the complex will recycle back to an initiation complex with some characteristic time, or moves on to be become an elongation complex. This stochastic decision is instantaneous in reaction time and will be discussed below. The polymerase can undergo abortive initiation step, recycling to the initiation complex, or form an elongation complex. A promoter is not made available for rebinding until the footprint of an initiating polymerase has cleared the entire promoter region.<br>
Once an elongation complex is formed, each elongation reaction causes the complex to move one base pair, and the elongation reaction is updated with a new time for the next elongation step. When an elongation reaction executes, the algorithm also checks whether another complex's footprint prevents the current polymerase from moving forward by checking the array containing all complex positions on the DNA. If there is another polymerase blocking the current polymerase's path, the polymerases will behave according to the polymerase interaction model chosen at the start of simulation. For example, the upstream polymerase can terminate, or signal the downstream polymerase to terminate the next time the polymerase is set to elongate, or simply hold its position until the next opportunity to elongate. The elongation reaction also does a check for transcriptional elements on the DNA such as promoters and transcriptional terminators. Upon arriving at a transcriptional terminator, a polymerase will either continue transcribing or terminate depending on a stochastic decision that will also be discussed below. If a transcribing polymerase occludes an open promoter, then the promoter will be decremented and unavailable for binding.<br>
The "stochastic decisions" described above for transcriptional termination and abortive initiation on the promoter are used as methods to simplify parameterization in the models, so as to be consistent with how experimental data for such events is typically reported. In the case of termination, a uniform random number is chosen, and if that number is less than the termination efficiency, the polymerase will terminate transcription and fall off the DNA. Analogously, in the case of abortive initiation, if the uniform random number is less than the abortive initiation percentage, then the next reaction will be an abortive initiation step rather than formation of an elongation complex. These decisions take no simulator time, but cause changes in the downstream actions the complexes may take. This simplification is not done for computational efficiency, but more for model simplicity. <software>Tabasco</software> could be modified, with little computational load, to replace the stochastic decisions into competing rates of different reactions. However, these rates are not well understood, and thus we chose to incorporate the measurable models of termination efficiencies and abortive initiation frequencies into the simulator.<br>
<br>
Translation<br>
The transcribing RNA polymerase complexes also produce mRNA, which are tracked by a separate class. Since there are many more copies of RNA than DNA during simulation, and because we were uncertain as to the importance of protein-protein interactions on the mRNA, we chose to treat the majority of translation at the species level. However, if, for example, an RNA polymerase prematurely terminates before reaching the translation stop site, the mRNA and the ribosomes translating it will not produce function proteins. Thus, so long as the coding sequence is still being transcribed, we must treat all translation events at the single-molecule level as well. However, as soon as the entire coding sequence of an open reading frame is transcribed, <software>Tabasco</software> transitions to tracking species of mRNA molecules.<br>
We used two classes to model the formation of RBSs and their respective start sites, the Nascent RBSs and Mature RBSs, in order to differentiate when mRNA should be tracked at the single-molecule level or the species-level, respectively. At the start of simulation, one more array of genome length is created that contains the positions of translation start and stop sites on the DNA. As RNA polymerase elongates (as described above), if the polymerase transcribes past a translational start site, a Nascent RBS is made. This Nascent RBS is available for binding by free ribosomes, and a reaction is automatically created and placed into the dynamic priority queue for translation processes. Once the polymerase arrives at the corresponding translation termination site, the Nascent RBS is converted to a Mature RBS, which is tracked at the species-level in the overall Indexed Priority Queue.<br>
Translation, both at the single-molecule level and species level, occurs in three steps. First, ribosomes can bind either Nascent or Mature RBSs; there is an initial reaction to form the (Nascent or Mature) Initiation Complex. Second, these initiation complexes are converted to (Nascent or Mature) Elongation Complexes and an RBS at a rate that depends on the speed of the ribosome and the length the ribosome must travel to clear the ribosome binding site. This reaction is treated as a single step, however the distribution of times is chosen from a gamma distribution, in order to better represent a series of individual elongation steps. Third, the Elongation Complex then forms a finished protein and a free ribosome at a rate proportional to the remaining length of the open reading frame. This reaction also is computed via a gamma distribution.<br>
At the single molecule level, two additional mechanisms allow for the state of the transcribing RNA polymerase to affect translations. First, if the transcribing RNA polymerase terminates before reaching a corresponding translation stop site, all bound ribosomes are immediately released, the Nascent RBS is removed, and no protein product is formed. Also, for any particular Nascent RNA, the maximal number of Nascent Elongation Complexes that can be formed is capped at the length of the currently transcribed portion of the open reading frame divided by the footprint of the ribosome.<br>
Finally, to note, all transitions between single-molecule tracking and species-level tracking occur when a reaction, such as RNA polymerase termination or Initiation Complex conversion into an Elongation Complex. Thus, the transition from single-molecule tracking back to the species level tracking should also not affect the validity of using the NRM.<br>
<br>
DNA entry<br>
We developed multiple models to represent DNA entry into a cell or compartment (such a step can be useful in starting a simulation of infection or transformation). First, DNA can enter the cell via a zero-order constant reaction rate. Second, RNA polymerases have themselves been implicated as molecular motors that can drive DNA entry [29,43]. Thus, in <software>Tabasco</software>, RNA polymerases that reach the end of a DNA molecule that has not yet fully entered the cell can cause DNA internalization at the rate of transcription elongation. Both of these mechanisms were used during our simulation of T7 gene expression.<br>
<br>
Simulation, Data Output, Visualization, Code<br>
<software>Tabasco</software> is written in Java? 1.4. The input file to the simulator is an XML file that describes and parameterizes the relevant genetic elements, initial conditions, and any other reactions that occur. The visualization is created by producing images that are then merged using <software>Quicktime</software>? to create a movie. The source code, executables, along with documentation and instructions for use are available within Additional Files 2, 3 and 4 and via the <software>TABASCO</software> website (Availability &amp; Requirements).<br>
<br>
Parameterization<br>
All constants used are provided for completeness. Please see the Supplementary Materials for input files and exact constants used.<br>
Gamma versus Exponential distribution<br>
The distribution of expected times for a reaction to occur in a stochastic simulator depends on the underlying model (Figure 2). Elementary chemical reactions will follow an exponential distribution in arrival times. However, this is not true of non-elementary reactions. Treating an imaginary elongation process as one step versus 50 individual steps has significant consequences. To obtain the distribution time for the two cases, we used uniformly-distributed pseudorandom numbers and transformed them into exponential- or gamma-distributed random numbers. The exponential distribution is used for the single step representation. Exponentially distributed numbers are calculated by simply taking the negative natural log of a uniformly-distributed pseudorandom number. A series of exponentially distributed arrival times, as in the case of the multi-step elongation process, is given exactly by the gamma distribution. Gamma distributed numbers are calculated from uniformly-distributed pseudorandom numbers by an implementation of the rejection method [44]. Pseudorandom numbers are generated from Java's implementation (java.util.Random) of a linear congruential pseudorandom number generator with a 48-bit seed [45].<br>
<br>
Simple Gene Expression model<br>
We simulated two models of a promoter driving expression of a coding domain. The first model, termed single-molecule simulation, used the described <software>Tabasco</software> simulator to account for each reaction step during transcription and translation. The model uses the schemes shown in Figures 1 and 6a. The second model, termed the species-level simulation, treats transcriptional and translational elongation as a single step as shown in Figure 6b. The main parameters used in both models are shown in Table 1. Constants were adjusted slightly to account for small differences in model structure to give equal steady state values of mRNA and protein levels. Finally, the input files for the simulations can be found in the Supplementary Materials; the input files can be used to either run the simulation using <software>Tabasco</software>, or check all parameters used for the simulation.<br>
<br>
Polymerase Interactions &amp; Bacteriophage T7<br>
The simulations for the polymerase interactions and bacteriophage T7 development used parameters that can be found in the input files in the Supplementary Materials. Table S1 details the meaning of each of the parameters in the input files. The parameters used were based on empirical measurements where possible. However, in general, the exact values of the constants are ancillary to this section, which is to show that <software>Tabasco</software> is able to simulate processes such as polymerase interactions and entire genetic systems such as T7. The parameter derivations are detailed are detailed elsewhere [46].<br>
<br>
<br>
<br>
Availability and Requirements<br>
Project name: TABASCO;<br>
Project home page: ;<br>
Operating system(s): Platform independent;<br>
Programming language: Java;<br>
Other requirements: J2SE 1.4.2 or higher<br>
License: Public domain;<br>
Any restrictions to use by non-academics: n/a.<br>
<br>
Authors' contributions<br>
SK and DE developed the idea of <software>Tabasco</software>. SK did all work on <software>Tabasco</software> design, code development, modeling and simulation, and drafted the manuscript. SK and JRK developed the <software>Tabasco</software> visualization software. SK and DE edited the manuscript. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2262056</b><br>
ProLoc-GO: Utilizing informative <database>Gene Ontology</database> terms for sequence-based prediction of protein subcellular localization<br>
<br>
<br>
Background<br>
<database>Gene Ontology</database> (<database>GO</database>) [1] annotation, which describes the function of genes and gene products across species, has recently been utilized to predict protein subcellular and subnuclear localization. The prediction of protein localization is important for elucidating protein functions involved in various cellular processes. Additionally, the accomplishment of the various genome sequencing projects causes the accumulation of massive amount of gene sequence information. For example, the percentage of large-scale eukaryotic proteins with subcellular locations annotated in the <database>Swiss-Prot</database> database increased rapidly from 52.4% (version 49.5, released on April 18, 2006) [2] to 69.4% (version 50.7, released Sep. 11, 2006) [3]. Meanwhile, the percentage of proteins with subcellular locations annotated in the <database>GO</database> database increased from 44.9% [2] to 65.5% [3]. The growth of the <database>GO</database> database in size and popularity increases the effectiveness of <database>GO</database>-based features.<br>
Some existing computation methods in literature for predicting protein localization are described below according to the used classifiers and features.<br>
a) Mining informative features. The prediction methods in this group focus on mining informative features consisting of <database>GO</database> terms [2-5], sorting signals [6,7], amino acid composition (AAC) [8-10], k-peptide encoding vector [7,11-14], physicochemical properties of amino acids [15-17], and fusing AAC and physicochemical properties [2,4,18,19].<br>
b) Designing efficient classifiers. Most of the following prediction methods use effective classifiers based on support vector machine (SVM) [5,10-12,14,16,17,20] or the k nearest neighbour (k-NN) classifiers [2,4,5,13,19,21].<br>
c) Integrating informative features with efficient classifier. Methods in this group include pSLIP [17], ProLoc [18], Euk-OET-PLoc [2] and Hum-PLoc [4]. The pSLIP system utilizes five top-rank features of physicochemical properties according to the prediction accuracy of SVM using a single feature [17]. The ProLoc system uses SVM with automatic selection from physicochemical properties to predict protein subnuclear localization [18]. The two ensemble classifiers Euk-OET-PLoc [2] and Hum-PLoc [4] fuse many basic individual classifiers operated by the engine of k-NN rules, where protein sequences are represented by hybridizing the <database>GO</database> annotation and amphiphilic pseudo amino acid (Pse-AA) composition.<br>
Additionally, these two efficient <database>GO</database>-based systems Euk-OET-PLoc [2] and Hum-PLoc [4] predict subcellular localization of proteins using their known accession numbers. However, they cannot work for novel proteins without known accession numbers. The GO-AA method [5], which uses <database>GO</database> terms of homologies retrieved by <software>BLAST</software> to assess protein similarity, can deal with novel proteins without known accession numbers for subnuclear localization prediction. Besides, some SVM-based methods using only the features derived from input sequences, such as ProtLock with AAC [8], Ploc with AAC and acid pairs [10], and HSLPred with AAC and dipeptide composition [11], predict subcellular localization inaccurately [4]. Therefore, this study would develop an accurate SVM-based method for predicting subcellular localization of novel proteins by using input sequences with <software>BLAST</software>.<br>
The <database>Gene Ontology</database> provided by the GO Consortium [1] has quickly grown in size and popularity. The newest version (<database>UniProt</database> 52.0 released in September 2007) of <database>GO</database> [22] contained 29,383 terms in the three branches, molecular function, biological process and cellular component. The terms and relationships among them are represented by a directed acyclic graph in which vertices represent the <database>GO</database> terms, and edges represent the relationships among these terms. Genes can be annotated with <database>GO</database> terms creating gene associations that can be used for whole genome analyses [23].<br>
<database>GO</database> annotation has been successfully used in various sequence-based applications, which can be classified into two groups. 1) The first group uses the <database>GO</database> terms and their corresponding structure information of <database>GO</database> graph, such as grouping <database>GO</database> terms to improve the assessment of gene set enrichment [24]; using <database>GO</database> with probabilistic chain graphs for protein classification [25,26], and prediction of subnuclear localization [5]. 2) The second group uses <database>GO</database> terms only without structure information, such as predicting transcription factor DNA binding preference [27] and various predictions of subcellular and subnuclear localization [2,4,5]. In the second group, protein sequences are often represented as high dimensional vectors of n binary features, where n is the total number of terms in the complete annotation set (a component of 1 if the annotation is hit, and 0 otherwise) [28]. This representation is valuable in well-known vector space clustering algorithms such as k-NN [2,4,13,19,21] and fuzzy k-NN [13,29,30]. However, because n is often large, and each gene product is generally annotated by few <database>GO</database> terms, the vectors became long and sparse, making the clustering rather problematic [28].<br>
This study proposes an efficient method, named GOmining, based on an intelligent genetic algorithm (IGA) [31,32] incorporating an SVM classifier to simultaneously identify a small number m out of a large number n of <database>GO</database> terms as input features, where m &lt;&lt;n. Some <database>GO</database> annotations corresponding to subcellular compartments are called essential <database>GO</database> terms for subcellular localization prediction, such as GO:0005634 (Nucleus), GO:0005737 (Cytoplasm) and GO:0005856 (Cytoskeleton), shown in Table 1. These essential <database>GO</database> terms are regarded as domain knowledge to be included in the feature set of m informative <database>GO</database> terms for subcellular localization prediction. A prediction method ProLoc-GO based on GOmining was implemented using the feature set of informative <database>GO</database> terms. This method performed well in predicting protein subcellular localization from input sequences only.<br>
<br>
Results<br>
Data sets<br>
Two existing data sets SCL12 [4] and SCL16 [2] obtained from <database>UniProtKB</database>/<database>Swiss-Prot</database> database [33] were used to evaluate the proposed method ProLoc-GO. The SCL12 and SCL16 have 2041 human proteins localized in 12 human subcellular compartments and 4150 eukaryotic proteins in 16 subcellular compartments, respectively. The two data sets were operated by a culling program [34] so that those sequences had &lt; 25% sequence identity.<br>
The proteins in SCL12 were screened strictly using the following rules: 1) only those sequences annotated with "human" in the ID (identification) field were collected; 2) sequences annotated with ambiguous or uncertain terms, such as "potential", "probable", "probably", "maybe", or "by similarity", were excluded; 3) sequences annotated by two or more locations were excluded, and 4) sequences with less than 50 amino acid residues were removed [4]. The data set SCL12 was divided into two parts, SCL12L and SCL12T, with 919 and 1122 proteins, respectively. The SCL12L set was used for training and the SCL12T was used for independent testing, as shown in Table 2[4].<br>
The proteins of SCL16 were screened according to four criteria. The first criterion is to exclude sequences annotated with "prokaryotic", because this study focused only on eukaryotic proteins. The other three criteria were the same as criteria 2?4 for SCL12 above. Table 3 shows the numbers of proteins within each compartment, where the SCL16 consists of two parts, SCL16L for training and SCL16T for independent testing. The sequences in the training and test data sets were obtained from the web servers of <software>Euk-OET-PLoc</software> [2] and <software>Hum-PLoc</software> [4].<br>
<br>
<database>GO</database> annotation<br>
This study applied the <database>Gene Ontology Annotation</database> (<database>GOA</database>) database [35], which includes <database>GO</database> annotations for non-redundant proteins from many species in the <database>UniProtKB</database>/<database>Swiss-Prot</database> database [33]. The <database>GOA</database> database was downloaded directly from [36] (<database>UniProt</database> 45.0 released in Jan. 2007). The accession numbers of proteins are required for querying the <database>GOA</database> database to obtain <database>GO</database> terms. <software>BLAST</software> [37,38] was used to obtain a homology with a known accession number to the protein for retrieving the <database>GO</database> terms. The corresponding accession numbers of all protein sequences in SCL12 and SCL16 were obtained by using <software>BLAST</software> with h = 1 and e = 10-9.<br>
Table 4 shows the <database>GO</database> annotation results of all proteins in the training data sets SCL12L and SCL16L. For SCL12L, the size of the complete set of all <database>GO</database> terms that appeared was n = 1714 from the 919 human proteins. The smallest, largest and mean numbers of <database>GO</database> terms annotated for individual proteins were 0, 35 and 8.3, respectively. The percentage of training proteins whose homologies were not annotated by any <database>GO</database> term (that is, the number of <database>GO</database> terms annotated is zero) was 1.31%. For SCL16L, n = 2870 <database>GO</database> terms were obtained from 2423 eukaryotic proteins. The smallest, largest and mean numbers of <database>GO</database> terms annotated were 0, 50 and 7.7, respectively. The percentage of training proteins whose homologies were not annotated was 3.96%. The proteins annotated by <database>GO</database> are often represented as an n-dimensional binary feature vector, where the attribute value is 1 if the corresponding <database>GO</database> term is annotated, and 0 otherwise.<br>
To know the prediction performance according to only the essential <database>GO</database> terms annotated, we calculated the numbers of sequences annotated by g essential <database>GO</database> terms. Table 4 shows that 453 out of 919 (49.3%) sequences are annotated by only one essential <database>GO</database> term (g = 1) for SCL12L, where 425 sequences are correctly annotated and 28 sequences are incorrectly annotated. The other 466 sequences annotated by zero (g = 0) or more than one (g &gt; 1) essential <database>GO</database> term can not be effectively predicted. Table 2 lists the numbers of sequences which are correctly annotated by only one essential <database>GO</database> term for every compartment. The two <database>GO</database> terms, GO:0005634 (Nucleus) and GO:0005739 (Mitochondrion), made a great contribution to the prediction accuracy of 46.2% (= 425/919), which correctly annotate a large number of sequences, 179 and 111, respectively.<br>
As for SCL16L, the number of sequences annotated by only one essential <database>GO</database> term is 1247 out of 2423 (51.5%). Table 3 lists the numbers of sequences which are correctly annotated by only one essential <database>GO</database> term for every compartment. Only 48.0% (= 1162/2423) of the sequences with known accession numbers can be correctly predicted by using only the annotation of essential <database>GO</database> terms. According to Table 3, the three essential <database>GO</database> terms, GO:0005634 (Nucleus, 395 out of 474), GO:0009507 (Chloroplast, 192 out of 207) and GO:0005739 (Mitochondrion, 173 out of 183), made a great contribution to prediction accuracy.<br>
The analytic results reveal that it is not sufficient to use only essential <database>GO</database> terms for accurately predicting protein subcellular localization. However, the essential <database>GO</database> terms play an important role in designing <database>GO</database>-based prediction methods.<br>
<br>
Selected informative <database>GO</database> terms<br>
Selecting a set of m informative <database>GO</database> terms out of n candidate <database>GO</database> terms is a combinatorial optimization problem C(n, m), which can be solved by using the intelligent genetic algorithm with an inheritance mechanism (IGA) [31,32]. IGA can efficiently search for the solution Sr+1 to C(n, r+1) by inheriting a good solution Sr to C(n, r). This study proposes an efficient algorithm based on IGA, called GOmining, to identify a small set of m informative <database>GO</database> terms including the essential <database>GO</database> terms as features to SVM. The GOmining algorithm incorporates <software>LIBSVM</software> [39] using series of binary classifiers. GOmining aims to maximize the training accuracy of prediction using 10-fold cross-validation (10-CV) when identifying the m informative <database>GO</database> terms.<br>
The SVM classifier based on the selected informative <database>GO</database> terms as features is called SVM-IGO. To evaluate a candidate set of r informative <database>GO</database> terms accompanied with the SVM parameters, the prediction accuracy of 10-CV serves as a fitness function of IGA. Figure 1 shows the results of SVM-IGO from r = 40, 41,..., 70. Table 5 lists the m = 44 informative <database>GO</database> terms for SCL12L obtained from the highest accuracy of 89.8% (r = 44), where the SVM parameters (C, ?) = (23, 2-4). Table 6 lists the m = 60 informative <database>GO</database> terms for SCL16L, where the highest accuracy was 86.5%, and (C, ?) = (25, 2-3).<br>
The orthogonal experimental design with orthogonal array and factor analysis used in IGA is an efficient method for simultaneously examining the individual effect of several factors on the evaluative function [40,41]. The factors are the parameters (<database>GO</database> terms) that manipulate the evaluation function, and a setting of a parameter is regarded as a level of the factor. In this study, the two levels of one factor are the inclusion and exclusion of the ith <database>GO</database> term in the feature selection using IGA. The factor analysis can quantify the effects of individual factors on the evaluation function, rank the most effective factors and determine the best level for each factor to optimize the evaluation function. The most effective factor has the largest main effect difference (MED). Tables 5 and 6 show that the essential <database>GO</database> term GO:0005634 (Nucleus) having the largest values of MED is the most effective feature of discrimination. The only essential <database>GO</database> term GO:0030198 (Extracellular matrix organization and biogenesis) belongs to biologic process branch and the other essential <database>GO</database> terms belong to cellular component branch. The abbreviations M, B and C represent the three branches molecular function, biological process, and cellular component, respectively.<br>
<br>
Evaluation of feature selection<br>
SVM-IGO was implemented by using the m informative <database>GO</database> terms and the SVM classifier using (C, ?) = (23, 2-4) and (C, ?) = (25, 2-3) for SCL12L and SCL16L, respectively. To evaluate the effectiveness of SVM-IGO, four additional classifiers were implemented for comparison. Three classifiers SVM-GO, k-NN-GO and fuzzy k-NN-GO in order based on SVM, k-NN and fuzzy k-NN were implemented by using all the n <database>GO</database> terms as features without <database>GO</database> term selection. The classifier SVM-RBS used SVM with a subset of n <database>GO</database> terms selected by the rank-based selection (RBS) method [17,42]. The best values of parameters C and ? determined using a step-wise approach were employed to the SVM-based methods SVM-GO and SVM-RBS, where ? ? {2-7, 2-6,..., 28} and C ? {2-7, 2-6,..., 28}. The best values (C, ?) = (23, 2-4) and (24, 2-6) were applied to SVM-GO for SCL12L and SCL16L, respectively. As for SVM-RBS, (C, ?) = (21, 2-3) and (22, 2-2) were used for SCL12L and SCL16L, respectively. The Methods section describes SVM-RBS, k-NN-GO and fuzzy k-NN-GO in detail. Table 7 lists all prediction accuracies using 10-CV for both data sets SCL12L and SCL16L.<br>
The highest accuracies of SVM-RBS are 86.5% and 83.5% using 65 and 68 selected <database>GO</database> terms for SCL12L and SCL16L, respectively, shown in Fig. 1. Table 7 shows that the three SVM-based classifiers (SVM-GO, SVM-RBS and SVM-IGO), with accuracies &gt;80%, were better than the two k-NN based classifiers (k-NN-GO and fuzzy k-NN-GO), with accuracies &lt;75%, for both data sets. SVM-IGO had the highest accuracies 89.8% and 86.5% for SCL12L and SCL16L, respectively. The <database>GO</database> term selection method based on GOmining was more effective than RBS and the method without selection of <database>GO</database> terms. Furthermore, SVM uses the selected <database>GO</database> terms as features, making it better than the k-NN classifier.<br>
<br>
Performance comparison<br>
The proposed ProLoc-GO method predicts the subcellular localization of an input sequence using either SVM-IGO or SVM-GO, depending on its annotation on the informative <database>GO</database> terms (see Methods for detail). Tables 8, 9, 10, 11 list the results of ProLoc-GO using SCL12 and SCL16. Some existing AAC-based prediction methods, such as ProtLock [8], Least Euclidean distance [9], Ploc [10] and HSLPred [11], use only the query sequence as input data for their classifiers. Hum-PLoc [4] and Euk-OET-PLoc [2] use both the sequence and its accession number as input data. For comparison with these predictors, the method ProLoc-GO was performed using the two kinds of input data separately. The first test used only the sequence and used <software>BLAST</software> to obtain annotated <database>GO</database> terms. The second test used the known accession number of proteins directly. For the accuracy on both SCL12L and SCL16L, ProLoc-GO used leave-one-out cross-validation (LOOCV) for comparison with the other methods (see Methods section).<br>
The test accuracies for ProLoc-GO performed on the human protein data set SCL12L and SCL12T were 90.0% and 88.1%, respectively, where m = 44, and (C, ?) = (23, 2-4) for SVM-IGO and SVM-GO. These results were much better than those (&lt;35%) of the four sequence-based prediction methods [8-11] using only input sequences, shown in Table 8. Ploc [10] had the highest test accuracy of 34.3% among the four AAC-based methods.<br>
The Matthews correlation coefficient (MCC) [5,12,18] values are usually employed while evaluating the performance on unbalanced datasets. In addition to the overall accuracy, the MCC values were also recorded due to the unbalance of numbers of proteins localized in the compartments, such as 196 of Nucleus vs. 7 of Microsome (Table 2). The MCC is defined as follows [5]:<br>
(1)MCCc=pcsc?ucoc(pc+uc)(pc+oc)(sc+uc)(sc+oc),c=1,2,?,Nc,<br>
where pc is the number of correctly predicted proteins of the location c, sc is the number of correctly predicted proteins not in the location c, uc is the number of under-predicted proteins, oc is the number of over-predicted proteins, and Nc is the number of locations. The test MCC performances of ProLoc-GO were 0.822 and 0.661 for SCL12L and SCL12T, respectively. Table 10 presents the detailed results for individual compartments. The results of the five sequence-based methods reveal that the set of informative <database>GO</database> terms is more useful for protein subcellular localization than the AAC-based features.<br>
<br>
Performance of using known accession numbers<br>
The accession number of each protein sequence in SCL12 and SCL16 was available in querying the <database>GOA</database> database. For comparison with the methods [2,4] based on the proteins with known accession numbers, ProLoc-GO using the known accession numbers of proteins as input data obtained test accuracies of 91.1% and 90.6% (MCC = 0.724) performed on SCL12L and SCL12T, respectively, where m = 56, (C, ?) = (22, 2-1) for SVM-IGO and (C, ?) = (22, 2-4) for SVM-GO. Hum-PLoc [4] using hybridization of <database>GO</database> terms and Pse-AA composition obtained training and test accuracies of 81.1% and 85.0% for SCL12L and SCL12T, respectively. The performance of ProLoc-GO using sequences or accession numbers as the input data was better than that of Hum-PLoc [4] using the ensemble classifiers with features of both sequence and accession number.<br>
Tables 9 and 11 show the performance results of ProLoc-GO and Euk-OET-PLoc [2] using SCL16. ProLoc-GO using input sequences yielded test accuracies 86.6% (MCC = 0.799) and 83.3% (MCC = 0.706) for SCL16L and SCL16T, respectively, where m = 60, (C, ?) = (25, 2-3) for SVM-IGO, and (C, ?) = (24, 2-6) for SVM-GO. ProLoc-GO is significantly better than all the AAC-based methods with test accuracies smaller than 35%. ProLoc-GO yields the test accuracies 89.0% and 85.7% (MCC = 0.710) for SCL16L and SCL16T, respectively, using the known accession numbers of proteins, where m = 60, (C, ?) = (22, 2-3) for SVM-IGO and (C, ?) = (23, 2-5) for SVM-GO. Euk-OET-PLoc [2] using the ensemble classifiers with features of both sequence and accession number obtains training and test accuracies of 81.6% and 83.7%, respectively. ProLoc-GO performed better than Euk-OET-PLoc on SCL16 using either sequences or accession numbers as the input data [2].<br>
<br>
Analysis of informative <database>GO</database> terms<br>
The GOmining method identifies a feature set of m effective <database>GO</database> terms, called informative <database>GO</database> terms, to design an accurate SVM-based prediction method. Table 12 shows the distribution of the m informative <database>GO</database> terms in the <database>GO</database> graph. For SCL12L with m = 44, GOmining selected 12 essential <database>GO</database> terms and 32 instructive <database>GO</database> terms. The 32 instructive <database>GO</database> terms consist of 7 <database>GO</database> terms from the molecular function branch, 14 terms from the biological process branch, and 11 terms from the cellular component branch, denoted as 7(M), 14(B) and 11(C), respectively. Analytical results reveal that all the three branches contain instructive <database>GO</database> terms.<br>
Due to the high correlation among <database>GO</database> terms in the <database>GO</database> graph, the feature selection of SVM should consider simultaneously a set of informative <database>GO</database> terms, rather than individual <database>GO</database> terms. Since the essential <database>GO</database> terms are always included, GOmining benefits from a confined search space of candidate instructive <database>GO</database> terms. Considering the position relationships between instructive and essential <database>GO</database> terms in the <database>GO</database> graph, instructive <database>GO</database> terms belonged to one of the three classes: (a) offspring but not ancestor of some essential <database>GO</database> term; (b) between two essential <database>GO</database> terms, and (c) not offspring of any essential <database>GO</database> term. Of the 32 instructive <database>GO</database> terms, 4, 2 and 26 <database>GO</database> terms belonged to the classes (a), (b) and (c), respectively. The 26 <database>GO</database> terms consist of 7(M), 14(B) and 5(C). The <database>GO</database> terms near the root of the <database>GO</database> graphs are considered to be more generic while terms near the leaves are more specific [23]. Of the instructive <database>GO</database> terms, 81.2% (26/32) were not offspring of any essential <database>GO</database> term. These analytical results reveal that the essential <database>GO</database> terms are informative enough in predicting subcellular localization, and are effective in confining the space of searching instructive <database>GO</database> terms. The other six instructive <database>GO</database> terms from the cellular component branch have more specific functions than the essential <database>GO</database> terms in discrimination of the subcellular localization.<br>
Figures 2, 3, 4 illustrate some of the instructive <database>GO</database> terms belonging to the three classes. Three instructive <database>GO</database> terms were found to belong to class (a), namely SCL12L: GO:0031227 (Intrinsic to endoplasmic reticulum membrane, rank 11), GO:30662 (Coated vesicle membrane, rank 41) and GO:0017119 (Golgi transport complex, rank 21), according to Fig. 2. The two terms belonging to class (b), namely GO:0005815 (Microtubule organizing center, rank 25) and GO:0005813 (Centrosome, rank 36), were found between the essential <database>GO</database> terms GO:0005856 (Cytoskeleton) and GO:0005814 (Centriole), as shown in Fig. 3. According to Fig. 4, five instructive <database>GO</database> terms belonging to the class (c) were not offspring of essential <database>GO</database> terms, GO:0016021 (Integral to membrane, rank 3), GO:0005576 (Extracellular region, rank 4), GO:0005622 (intracellular, rank 18), GO:0005578 (Proteinaceous extracellular matrix, rank 37) and GO:0005615 (Extracellular space, rank 38).<br>
The m = 60 informative <database>GO</database> terms for SCL16L comprises 15 essential <database>GO</database> terms and 45 instructive <database>GO</database> terms. The 45 instructive <database>GO</database> terms consisted of 18(M), 13(B) and 14(C). The numbers of instructive <database>GO</database> terms coming from each branch were not significantly different. However, the numbers of instructive <database>GO</database> terms belonging to the three classes (a), (b) and (c) are 9, 0 and 36, respectively, which are very different. 80% (36/45) of the instructive <database>GO</database> terms were not offspring of any essential <database>GO</database> term. The 9 instructive <database>GO</database> terms belonging to the class (a) had 5, 2 and 2 terms, respectively, as shown in Figs. 2, 3 and 4. Class (c) has five <database>GO</database> terms with a dot-pattern box: GO:0005622 (intracellular), GO:0005615 (Extracellular space), GO:0020015 (Glycosome), GO:0016020 (Membrane) and GO:0045261 (Proton-transporting ATP synthase complex, catalytic core F(1)), as revealed by Fig. 4.<br>
The statistical results of instructive <database>GO</database> terms distributed in the three classes for both SCL12L and SCL16L reveal that the inclusion of essential <database>GO</database> terms can be regarded as using domain knowledge for GOmining to mine a feature set of informative <database>GO</database> terms. The heuristic approach (using domain knowledge) of GOmining is efficient when the <database>GO</database> database grows fast. Therefore, GOmining can be easily applied to other applications of sequence-based predictions using SVM with the features of informative <database>GO</database> terms.<br>
<br>
<br>
Discussion<br>
The <database>GO</database> database has grown in size recently, increasing the effectiveness of <database>GO</database>-based features. Meanwhile, the percentage of proteins with subcellular locations annotated in the <database>GO</database> database increased from 44.9% [2] to 65.5% [3] fast. It is indicated that there is a linkage in the <database>GO</database> annotation process between molecular function annotation and subcellular localization annotation [43]. Therefore, the <database>GO</database>-based prediction method for protein subcellular localization is increasingly efficient. Because the accession number of proteins is necessary for retrieving <database>GO</database> terms from <database>GO</database> databases, existing efficient <database>GO</database>-based systems Euk-OET-PLoc [2] and Hum-PLoc [4] directly utilize the accession numbers of proteins and a large number n of <database>GO</database> terms annotated in a complete set where n = 9918 for SCL12L [4] and n = 9567 for SCL16L [2].<br>
To predict subcellular localizations for novel proteins, <software>ProLoc-GO</software> uses a good homology, rather than the query protein itself, to retrieve annotated <database>GO</database> terms using <software>BLAST</software>. To use <database>GO</database> term features effectively, <software>ProLoc-GO</software> uses only a homology with annotated <database>GO</database> terms to reduce n. Thus, n = 1714 for SCL12L and n = 2870 for SCL16L. Furthermore, a small set of m informative <database>GO</database> terms is selected simultaneously by GOmining. GOmining can consider internal relevant-feature correlation, instead of individual features by using an efficient global optimization method. The distribution analysis of informative <database>GO</database> terms in the <database>GO</database> graph is consistent with the properties of <database>GO</database> annotation. Additionally, <software>ProLoc-GO</software> using input sequences is slightly worse than using the accession numbers of proteins, with accuracies of 88.1% vs. 90.6% for SCL12T, and 83.3% vs. 85.7% for SCL16T, as shown in Tables 10 and 11.<br>
<br>
Conclusion<br>
Computational prediction methods from primary protein sequences are fairly economical in terms of identifying large-scale eukaryotic proteins with unknown functions. The <database>GO</database> annotation, which describes the function of genes and gene products across species, has been used to improve the prediction of protein subcellular localization. The accession numbers of proteins are necessary to query the <database>GOA</database> database to obtain <database>GO</database> terms. Since novel proteins have no known accession numbers, <software>BLAST</software> was used to obtain homologies with known accession numbers to the proteins for the retrieval of <database>GO</database> terms.<br>
<database>GO</database> annotation has grown in size and popularity. However, few studies have explored informative <database>GO</database> terms from the over 20,000 annotations available at present for sequence-based prediction problems. This study proposes a genetic algorithm based method, GOmining, which combines SVM to simultaneously identify a small number m out of the n <database>GO</database> terms as features to SVM, where m &lt;&lt;n. The m <database>GO</database> terms include the essential <database>GO</database> terms annotating subcellular compartments such as GO:0005634 (Nucleus), GO:0005737 (Cytoplasm) and GO:0005856 (Cytoskeleton). <software>ProLoc-GO</software> was evaluated using SVM with the <database>GO</database>-based features from two kinds of input data, sequence and known accession numbers of proteins.<br>
<software>ProLoc-GO</software> yields test accuracies of 88.1% and 83.3% from SCL12 and SCL16, respectively, when using only input sequences. These results are significantly superior to those of the other SVM-based methods, which have accuracies &lt;35% using AAC with acid pairs, and using AAC with dipedtide composition. ProLoc-GO using known accession numbers of proteins has accuracies 90.6% and 85.7% for SCL12 and SCL16, which is also slightly better than Hum-PLoc and Euk-OET-PLoc, which have 85.0% and 83.7%, respectively.<br>
Analysis of m informative <database>GO</database> terms in the <database>GO</database> graph reveals that GOmining can consider internal relevant-feature correlation, rather than individual features, by using an efficient global optimization method. GOmining can serve as an efficient tool for mining informative <database>GO</database> terms for various sequence-based predictions of proteins, especially when the <database>GO</database> database grows fast. The prediction system using <software>ProLoc-GO</software> with protein sequence as input data for protein subcellular localization has been implemented (see Availability).<br>
<br>
Methods<br>
Proposed GOmining algorithm<br>
An efficient genetic-algorithm-based method, called GOmining, is proposed for selecting informative <database>GO</database> terms. GOmining uses an intelligent genetic algorithm with an inheritable mechanism (IGA) [31,32], combined with an SVM classifier, to simultaneously identify a small number m out of a large number n of <database>GO</database> terms as input features, where m &lt;&lt;n. The exploration of the m informative <database>GO</database> terms from n candidate <database>GO</database> terms is a combinatorial optimization problem C(n, m) with a huge search space of size C(n, m) = n!/(m!(n-m)!)). An IGA based on orthogonal experimental design using a divide-and-conquer strategy and systematic reasoning method can efficiently solve this large combinatorial optimization problem.<br>
The leave-one-out cross-validation (LOOCV) is considered to be the most rigorous and objective test. Although bias-free, this test is very computationally demanding and is often impractical for large data sets. The N-fold cross-validation not only provides a bias-free estimation of the accuracy at a much reduced computational cost, but is also considered as an acceptable test for evaluating prediction performance of an algorithm [44]. Therefore, GOmining uses the prediction accuracy of 10-CV as the fitness function to perform IGA on the entire training sets of proteins under considering the computation cost.<br>
The input of the algorithm GOmining is composed of 1) a training set of protein sequences categorized into a number of compartments (classes), and 2) the essential <database>GO</database> terms corresponding to the compartments. The output comprises a set of m informative <database>GO</database> terms and the associated parameter settings of an SVM classifier. Since the novel sequences without known accession numbers use <software>BLAST</software> to obtain annotated <database>GO</database> terms, all training sequences use the same <software>BLAST</software> to obtain <database>GO</database> terms for consistence.<br>
Step 1: (preparation of SVM) The multi-classification problem is solved by using a series of binary classifiers of <software>LIBSVM</software> [39]. In this study, the kernel parameter ? and cost parameter C are tuned where ? ? {2-7, 2-6,..., 28} and C ? {2-7, 2-6,..., 28}.<br>
Step 2: (sequence representation) Obtain annotated <database>GO</database> terms from the <database>GOA</database> database for all training proteins using <software>BLAST</software> with h = 1 and e = 10-9. Let n be the total number of <database>GO</database> terms that appear among all proteins in the training data set. For example, n = 1714 and n = 2870 were derived for SCL12L and SCL16L, respectively. The protein is represented as an n-dimensional binary feature vector.<br>
Step 3: (inclusion of essential <database>GO</database> terms) Identify d essential <database>GO</database> terms out of n <database>GO</database> terms and number them from 1 to d. For example, d = 12 and d = 15 were found from SCL12L and SCL16L, respectively.<br>
Step 4: (chromosome encoding) The IGA-chromosome comprises n binary IGA-genes fi for selecting informative <database>GO</database> terms and two 4-bit IGA-genes for encoding ? and C, where fi = 1, i = 1,..., d. The ith <database>GO</database> term is included in the feature set of the SVM classifier if fi = 1; otherwise, the ith <database>GO</database> term is excluded (fi = 0). Figure 5 shows the sequence representation and IGA-chromosome encoding method.<br>
Step 5: (initial solution) Perform IGA to select rstart out of n <database>GO</database> terms, i.e., the solution to C(n, rstart), where the d <database>GO</database> terms are always selected. Table 13 shows the parameter settings of IGA, such as crossover probability pc = 0.8. The procedure of IGA is described in detail in the work [18].<br>
Step 6: (inheritance mechanism) The inheritance mechanism of IGA can efficiently search for the solution to C(n, r+1) by inheriting a good solution Sr to C(n, r). Obtain all solutions Sr from r = rstart+1,..., rend one by one using IGA [31,32]. For example, rstart = 40 and rend = 70 according to former experience.<br>
Step 7: (decoding chromosome) Let Sm be the most accurate solution with m selected <database>GO</database> terms among all solutions Sr. Obtain the m informative <database>GO</database> terms and parameter values of ? and C.<br>
Step 8: (robust performance) Perform Steps 5?7 for N independent runs to obtain the best one of N solutions Sm and the associated parameter settings of the SVM parameters. The best solution considers both high prediction accuracy and high mean frequency of the m selected <database>GO</database> terms appeared in the N runs. In this study, N = 30.<br>
<br>
ProLoc-GO<br>
As shown in Fig. 6, each query protein is first <software>BLAST</software>ed with h = 1 and e = 10-9 against the <database>Swiss-Prot</database> database to obtain a homology with a known accession number. If no such homology exists, then adjust the threshold value e of <software>BLAST</software> until the desired homology is obtained, where h = 1 and e ? {10-9, 10-8,..., 10-1}. The accession number of the homology of each protein sequence in SCL12 and SCL16 was obtained by using <software>BLAST</software> with h = 1 and e = 10-9. This accession number is used as input to the <database>GOA</database> database for retrieving the corresponding k (&gt;1) <database>GO</database> terms: GO:1, GO:2,... GO:k. If none of the k <database>GO</database> terms belongs to the set of m informative <database>GO</database> terms, then the sequence is represented using an n-dimensional binary vector and is predicted by the SVM-GO classifier. Otherwise, the sequence is represented as an m-dimensional binary vector and is predicted by the SVM-IGO classifier. Notably, the SVM-GO classifier predicts only a very small percentage of input sequences. ProLoc-GO is derived from the two major classifiers SVM-GO and SVM-IGO for subcellular localization prediction.<br>
<br>
Fuzzy k-NN<br>
The protein is represented as an n-dimensional binary vector and the generalized distance between two proteins P and Pi [2] is denoted as :<br>
(2)D(P, Pi) = 1 - P?Pi/||P|| ||Pi||,<br>
where P?Pi is the dot product of vectors P and Pi, and ||P|| and ||Pi|| are their moduli.<br>
This study determined the best value of k by using a step-wise approach where k ? {1, 2,..., 10}.<br>
The fuzzy k-NN classifier [13,29,30] is a variation of k-NN, which assign fuzzy membership values rc(P) of a query sequence P to each class c as follows:<br>
(3)rc(P)=?j=1krc(Pj)|P?Pj|?2/(w?1)?j=1k|P?Pj|?2(w?1),c=1,2,...,NC,<br>
where the distance is calculated by according to (1). In this study, the best values of parameters (k, w) are tuned iteratively from k ? {1, 2,..., 10} and w ? {1.05, 1.10,..., 1.95} for the fuzzy k-NN classifier.<br>
<br>
SVM-RBS<br>
To evaluate the proposed IGA-based feature selection method GOmining, this study implements a classifier SVM-RBS by using SVM with a subset of the n <database>GO</database> terms by the rank-based selection (RBS) method [17,42]. One previous work on ProLoc [18] showed that this univariate method RBS is inferior to the multivariate feature selection by IGA for selecting physicochemical properties. First, each of all n <database>GO</database> terms (for example, n = 1714 for SCL12L) is ranked according to the accuracy of SVM with the evaluated single feature, where the best values of parameters (C, ?) were determined using a step-wise approach where ? ? {2-7, 2-6,..., 28} and C ? {2-7, 2-6,..., 28}. The top-ranking 70 features ai, i = 1,..., 70 are then picked, and the top-ranking 40 features with r = 40 are used as an initial feature set {b1,..., b40}. Consequently, the feature set with size r+1 is incrementally established by adding the best feature br+1 (having the highest accuracy of SVM using 10-CV) from the remaining 70-r features into the current feature set.<br>
<br>
<br>
Authors' contributions<br>
WLH designed the system, implemented programs, participated in manuscript preparation and carried out the detail study. CWT designed the system and implemented programs. SWH, SFH and SYH conceived the idea of this work. Additionally, SYH supervised the whole project and participated in manuscript preparation. All authors have read and approved the final manuscript.<br>
<br>
Availability<br>
The prediction system using ProLoc-GO with input sequences of query proteins for protein subcellular localization has been implemented at .<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2267704</b><br>
<software>t2prhd</software>: a tool to study the patterns of repeat evolution<br>
<br>
<br>
Background<br>
The conceptual models developed to explain the evolution of multigene families [1,2] have also been applied to sequence repeats inside a gene/protein. Studies of repeat evolution [3-9] have often revealed complex patterns: some repeats may evolve in concert, subject to homogenization, while other repeats may maintain their sequence identity, which is more consistent with birth-and-death and/or divergent evolution. Phylogenetic reconstruction is an established approach with which to study the mode of evolution of multigene families [10-14] and repeats [3-9]. In the analysis of closely related species, the association of genes on the same locus (orthologues) on phylogenetic trees suggests independent evolution of the respective members, as expected for the birth-and-death model. On the contrary, the association of paralogues from the same species suggests homogenization, which is consistent with the concerted model. The application of the same logic to repeats poses a number of problems. Studies on the evolution of multigene families by phylogenetic methods make use of a prior concept of homology defined by the locus. As the term "locus" denotes genomic position, it follows that the exact repeat position in the full sequence (starting and ending positions) should be used to identify them. This approach has seldom been used in the previous studies, probably because it complicates the interpretation of the phylogenetic trees. Instead, a simple numbering according to repeat order is often applied, although this can be misleading: if a repeat is not detected by the selected method (for example the profile Hidden Markov Model (HMM) method) in one of the compared sequences, the ordinal numbers of the following repeats will be shifted by one and hence the analysis will become laborious.<br>
<br>
Results and Discussion<br>
To simplify the analyses of repeat evolution, a novel rapid visual method has been developed to highlight the relationships of repeats detected in a pair of sequences by the profile HMM method. The primary aim of the application is the analysis of tandem amino acid sequence repeats but DNA sequences can also be used with a profile HMM trained with DNA sequences. When DNA sequences are used, the script does not consider the reverse complement of the sequences, so the usefulness of this application in a genomic context (for example in the case of transportable elements) might be limited, though the implementation can be easily extended to handle this kind of analysis.<br>
The "pairwise repeat homology diagram" visualizes repeats in a simple scheme, together with their homology relations (orthology and paralogy) inferred by a phylogenetic tree, providing an intuitive way to analyse the patterns of repeat evolution. This visualization also facilitates a survey of the sequence structure (size, linker sequences and non-repetitive regions). Regions in which consecutive repeats are connected with those in the other sequence forming a ladder-like pattern are most likely to have evolved independently following the birth-and-death and/or divergent process. On the contrary, in regions where repeats have only internal or no identified connections, repeats possibly evolve in concert. The patterns demonstrated by internal connections can reveal units of concerted evolution or recent internal duplications. It should be taken into account, however, that if there are clades formed by more than two identical sequences, the branching pattern of such clades, and hence the identified relation, is arbitrary and uninformative in this kind of analysis.<br>
Implementation<br>
The method is implemented in a Perl script with command line interface (<software>t2prhd</software>, standing for "tree to pairwise repeat homology diagram"). The repeats are first identified with hmmsearch from the <package>HMMER</package> package [15] and a profile HMM specified by the user. Raw search results are parsed by using BioPerl [16] modules and the extracted repeats are aligned with hmmalign. The resulting alignment is converted into <fileFormat>Fasta</fileFormat> and sequencial <fileFormat>PHYLIP</fileFormat> formats and a phylogenetic tree is built by using <software>CLUSTAL W</software> [17] (with default parameters) or by using <software>PhyML</software> [18]. The script reads in the resulting tree as a Bio::Tree::TreeI object. After getting the list of all leaf nodes it finds the "sister leaf nodes" (leaf node pairs having the same ancestor) by an algorithm that for n leaf nodes (repeats) needs in the best case n2 and in the worst case n iterations. So, the asymptotic upper bound for the time complexity of this algorithm is O(n) and the asymptotic lower bound is ?(n2). These sister leaf nodes have a most recent common ancestor as indicated by the tree and accordingly we regard them as unambiguously identified homologues.<br>
The script creates an SVG (Scalable Vector Graphics) file containing the diagram by using the XML::Writer module and saves the outputs and logs produced by the external applications. Optionally, it can also generate a LaTeX output. The identified homology relations are represented by lines or arcs connecting the respective repeats: blue lines are drawn between repeats from different sequences (orthology), and brown arcs in cases of internal relations (paralogy). The colour intensity of the connecting lines is a function of the patristic distance between the respective leaf nodes (d) divided by the total tree length (T): (1?dT)w. The default value of the "colour gradient parameter" w is 1 (linear colour scale), but by setting this parameter the colour scale can be tuned so that one can discriminate between close distance values. To make the interpretation of the colour scale easier a legend is drawn.<br>
The generated SVG file can be viewed by <software>Firefox</software> 1.5 [19] or higher as an example and can be rasterized (and also viewed) using the <software>Batik</software> toolkit [20] or any other image editor capable to handle the SVG format. The generated LaTeX file should be processed by <software>pdflatex</software>, the pgf/TikZ, fancyhdr, xcolor and fullpage packages are required. The script has a manual page embedded in POD format.<br>
The script is also accessible as an online tool [21] (with <software>CLUSTAL W</software> back-end only) through a web interface created by using the <software>Pise</software> form generator [22]. The real power of this visual method is manifested in studies with large data sets, where the analysis of numerous or large trees would be highly laborious. In these cases, it is very advantageous that the command line interface enables the use of the scripts to automate the diagram generation.<br>
<br>
Example<br>
Tenascins are extracellular matrix glycoproteins containing regions of repeated EGF-like and fibronectin-III-like (Fn-III) domains. The evolution of these repeated domains in mammalian Tenascins has been studied in detail by means of phylogenetic and other methods [8]. To illustrate the power of the method, we generated pairwise repeat homology diagrams with three selected protein sequences. The protein sequences corresponding to the DNA sequences studied by Hughes [8] were used (abbreviations and <database>GenBank</database> accession numbers in parentheses): human Tenascin X (TXH, [GenBank: AAB47488.1]), human Tenascin C (TCH, [GenBank: CAA39628.1]) and murine Tenascin X (TXM, [GenBank: AAB82015.1]). The profile HMM files were downloaded from <database>Pfam</database> (EGF-like: PF00008.17, Fn-III: PF00041.11).<br>
Hughes [8] concluded that EGF domain repeats underwent homogenization within each Tenascin gene after duplication, but remained conserved after the divergence of rodents and primates. The same conclusions can be drawn after the evaluation of the diagrams generated with paralogous (Figure 1A and 1B) and orthologous (Figure 1C) sequence pairs. Our diagrams are also in accordance with the conclusions of Hughes regarding the evolution of Fn-III type domain repeats. These repeats can be divided into three categories. The last three C-terminal repeats demonstrate conservation since the duplication of the Tenascin X and Tenascin C genes (Figure 1A', B' and 1C'). Other repeats became homogenized within each gene subsequent to gene duplication, but have remained conserved following the divergence of primates and rodents (Figure 1C'). The repeats of the third category have evolved in a concerted fashion in rodent and primate lineages since their divergence (Figure 1C').<br>
<br>
<br>
Conclusion<br>
Although pairwise repeat homology diagrams do not carry all the information about the phylogenetic tree on which they are based, by visualizing the exact positions of the repeats and the homology relations, they permit a rapid and intuitive assessment of the patterns of repeat evolution (compare Figure 1C' with Figure 2). These features make <software>t2prhd</software> a powerful tool, especially in cases of massive datasets, as in studies of repeat evolution in large gene families.<br>
<br>
Availability and requirements<br>
? Project name: t2prhd<br>
? Project home page: <br>
? Online access: <br>
? Operating system(s): OS Independent (Written in an interpreted language)<br>
? Programming language: Perl<br>
? Other requirements: Perl version 5.8.8 or higher with the standard modules, BioPerl modules (version 1.4.0 or higher), XML::Writer module, <package>HMMER</package> package 2.3.2 or higher, <software>CLUSTAL W</software> version 1.83 and/or <software>PhyML</software> version 2.4.4 or higher<br>
? License: GNU General Public License<br>
? Any restrictions to use by non-academics: none<br>
<br>
Authors' contributions<br>
BS, KS and ZP developed the approach. BS wrote the program and the manual and created the website. ZP reviewed the code and the manual and tested the program functionality. BS and KS wrote the manuscript. IA and ZP conceived and coordinated the project and refined the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2483287</b><br>
Estimation and testing for the effect of a genetic pathway on a disease outcome using logistic kernel machine regression via logistic mixed models<br>
<br>
<br>
Background<br>
The rapid progress in gene expression array technology in the past decade has greatly facilitated our understanding of the genetic aspect of various diseases. Knowledge-based approaches, such as gene set or pathway analysis, have become increasingly popular. In such gene sets/pathways, groups of genes act in concert to accomplish tasks related to a cellular process and the resulting genetic pathway effects may manifest themselves through phenotypic changes, such as occurrence of disease. Thus it is potentially more meaningful to study the overall effect of a group of genes rather than a single gene, as single-gene analysis may miss important effects on pathways and difficult to reproduce from studies to studies [1]. Researchers have made significant progress in identifying metabolic or signaling pathways based on expression array data [2,3]. Meanwhile, new tools for identification of pathways, such as <software>GenMAPP</software> [4], <software>Pathway Processor</software> [5], <software>MAPPFinder</software> [6], have made pathway data more widely available. However, It is a challenging task to model the pathway data and test for a potentially complex pathway effect on a disease outcome.<br>
One way to model pathway data is through the linear model approach, where the pathway effect is represented by a linear combination of individual gene effects. This approach has several limitations. Activities of genes within a pathway are often complicated, thus a linear model is often insufficient to capture the relationship between these genes. Furthermore, genes within a pathway tend to interact with each other. Such interactions are not taken into account by the linear model approach.<br>
In this paper we propose a nonparametric approach, the kernel machine regression, to model a pathway effect. The kernel machine method, with the support vector machine (SVM) as a most popular example, has emerged in the last decade as a powerful machine learning technique in high-dimensional settings [7,8]. This method provides a flexible way to model linear and nonlinear effects of variables and gene-gene interactions, unifies the model building procedure in both one- and multi-dimensional settings, and shows attractive performance compared to other nonparametric methods such as splines.<br>
Liu et al. [9] proposed a kernel machine-based regression model for continuous outcomes. In this paper, we propose a logistic kernel machine regression model for binary outcomes, where covariate effects are modeled parametrically and the genetic pathway effect is modeled parametrically or nonparametrically using the kernel machine method. A main contribution of this paper is to establish a connection between logistic kernel machine regression and the logistic mixed model. We show that the kernel machine estimator of the genetic pathway effect can be obtained from the estimator of the random effects in the corresponding logistic mixed model. This connection provides a convenient vehicle to connect the powerful kernel machine method with the popular mixed model method in the statistical literature. This mixed model connection also provides an unified framework for statistical inference for model parameters, including the regression coefficients, the nonparametric genetic pathway function, and the regularization and kernel parameters. Based on the proposed logistic kernel machine regression model, we develop a new test for the nonlinear pathway effect on disease risk. An appealing feature of the proposed test is that it performs well without the need to correctly specify the functional form of the effects of each gene or their interactions. This feature has a significant practical implication when analyzing genetic pathway data, as the true relationship between the pathway and the disease outcome is often unknown. We extend the results to generalized kernel machine regression for a class of continuous and discrete outcomes and discuss its connection with generalized linear mixed models [10].<br>
Recently, Wei and Li [11] proposed a nonparametric pathway-based regression (NPR) to model pathway data. NPR is a pathway-based gradient boosting procedure, where the base learner is usually a regression or classification tree. It provides a flexible approach in modeling pathways and interactions among genes within a pathway. Michalowski et al. [12] proposed a Bayesian Belief Network approach for pathway data. Neither method is likelihood-based. Thus parameter estimation and inference cannot be casted within a unified likelihood framework. It is hence difficult to estimate and quantify the overall pathway effect on disease risk and assess its statistical uncertainty. Secondly, a primary interest in this paper is to test for the statistical significance of the overall pathway effect on the risk of a disease. Both NPR and Bayesian belief network do not provide such a statistical test for the pathway effect. For example, NPR uses an importance score to rank the relative importance of each pathway. It lacks formal inferential procedure for assessing the statistical significance of a pathway. Further, when considering a single pathway, the importance score loses its meaning in assessing the importance of a pathway. Our method, on the other hand, is based on penalized likelihood, and estimation and inference can be conducted in a systematic manner within the likelihood framework. We also propose a formal statistical test for the significance of a pathway effect on the risk of a disease.<br>
Goeman et al. [13] proposed a linear mixed model to relate the pathway effect with a continuous outcome. They modeled the pathway effect using a linear function with each gene entering into the model as a regressor. They assumed the regression coefficients of the gene as random from a common distribution with mean 0 and an unknown variance. The pathway effect can then be tested through a variance component test for random effects. Our approach is different from theirs in the following aspects. First, we model the pathway effect by allowing for a nonparametric model rather than a parametric one. As we commented earlier, the highly complicated nature of activities of genes within a pathway makes the linear model assumption untenable. Secondly, the kernel function used in kernel machine regression usually contains unknown tuning parameters. The parameter is present under the alternative hypothesis but disappears under null hypothesis. This makes tests as proposed in [13,14] not applicable. Our proposed test, on the other hand, works quite well under this scenario. Third, Goeman et al. [14] extended their linear model results to discrete outcomes using basis functions. A key advantage of the kernel machine approach over this basis approach for modeling multi-gene effects is that one does not need to specify bases explicitly, which is often difficult for high-dimensional data especially when interactions are modeled.<br>
<br>
Results<br>
Analysis of prostate cancer data<br>
In this section, we apply the proposed logistic kernel machine regression model (3) as described in the Methods section to the analysis of a prostate cancer data set. The data came from the Michigan prostate cancer study [15]. This study involved 81 patients with 22 diagnosed as non-cancerous and 59 diagnosed with local or advanced prostate cancer. Besides the clinical and demographic covariates such as age, cDNA microarray gene expressions were also available for each patient. The early results of Dhanasekaran et al. [15] indicate that certain functional genetic pathways seemed dys-regulated in prostate cancer relative to non-cancerous tissues. We are interested in studying how a genetic pathway is related to the prostate cancer risk, controlling for the covariates. We focus in this analysis on the cell growth pathway, which contains 5 genes. The pathway we describe was annotated by the investigator (A. Chinnaiyan) and is simply used to illustrate the methodology. Of course, one could take the pathways stored in commercial databases such as <database>Ingenuity Pathway Analysis</database> (<database>IPA</database>) and use the proposed methodology based on those gene sets.<br>
The outcome was the binary prostate cancer status and the covariate includes age. Since the functional relationship between the cell growth pathway and the prostate cancer risk is unknown, the kernel machine method provides a convenient and flexible framework for the evaluation of the pathway effect on the prostate cancer risk. Specifically, we consider the following semiparametric logistic model<br>
(1)logit(P(y = 1)) = ?0 + ?1age + h(gene1, ..., gene5),<br>
where h(?) is a nonparametric function of 5 genes within the cell growth pathway. The detail of the estimation procedure is provided in the Methods section. In summary, we fit this model using the kernel machine method via the logistic mixed model representation and using the Gaussian kernel function in estimating h(?). Under the mixed model representation, we estimated (?0, ?1) and h(?) using penalized quasi-likelihood (PQL), and estimated the smoothing parameter ? and the Gaussian kernel scale parameter ? simultaneously by treating them as variance components. The results are presented in Table 1.<br>
The test for the cell growth pathway effect on the prostate cancer status H0: h(z) = 0 vs H1:h(z) ? 0 was conducted using the proposed score test as described in the Methods section. For the purpose of comparison, we also conducted the global test proposed by Goeman et al. [13] that assumed a linear pathway effect. Note that our test allows a nonlinear pathway effect and gene-gene interactions. Table 1 gives the p-values for both tests. The p-value of our test suggests that cell growth pathway has a highly significant effect on the disease status, while the test from Goeman et al. [13] indicates only marginal significance of the growth pathway effect.<br>
<br>
Simulation Study for the Parameter Estimates<br>
We conducted a simulation study to evaluate the performance of the parameter estimates of the proposed logistic kernel machine regression by using the logistic mixed model formulation. We considered the following model<br>
(2)logit(P(yi = 1)) = xi + h(zi1, ?, zip),<br>
where the true regression coefficient ? = 1. We consider p = 5 and set h(z1, ..., z5) = 2{sin(z1) ? z22 + z1 exp(-z3) -sin(z2) cos(z3) + z42 + sin(z4) cos(z1) + z52 + z3z5}. To allow xi and (zi1, ?, zip) to be correlated, xi was generated as xi = sin(zi1) + 2ui, where ui and zij (j = 1, ?, p) follow independent Uniform(-0.5, 0.5). The Gaussian kernel was used throughout the simulation. All simulations ran 300 times. Settings 1, 2, and 3 correspond to sample size n = 100, 200, and 300, respectively.<br>
The simulation results are shown in Table 2. Due to the multi-dimensional nature of the variables z, it is difficult to visualize the fitted curve h^(z). We hence summarized the goodness-of-fit of h^(?) in the following way. For each simulated data set, we regressed the true h on the fitted value h^, both evaluated at the design points. We then empirically summarized the goodness-of-fit of h^(?) by calculating the average intercepts, slopes and R2's obtained from these regressions over the 300 simulations. If the kernel machine method fits the nonparametric function well, then we would expect the intercept to be close to 0, the slope close to 1, and R2also close to 1.<br>
Our results show that even when the sample size is as low as 100, estimation of the regression coefficient and nonparametric function only has small bias. When the kernel parameter ? is estimated, these biases tend to be small compared with those when ? is held fixed. With the increase of sample size the estimates of ? and h become closer to the true values, especially when ? is estimated, while there are still some bias when ? is fixed at values farther away from the estimated one. Table 3 compares the estimated standard errors of ?^ with the empirical standard errors. Our results show that they agree to each other well when ? is estimated.<br>
<br>
Simulation Study of the Score Test for the Pathway Effect<br>
We next conducted a simulation study to evaluate the performance of the proposed variance component score test for the pathway effect H0: h(?) = 0 vs H1: h(?) ? 0. In order to compare the performance of our test with the linearity-based global test proposed by Goeman et al. [13], both tests were applied to each simulated data set. Nonlinear and linear functions of h(z) were both considered. For the nonlinear pathway effect, the true model is logit(y) = x +ah(z), where h(z) = 2(z1 - z2)2 + z2z3 + 3 sin(2z3)z4 + z52 + 2 cos(z4)z5. For the linear pathway effect, the true model is logit(y) = x + ah(z), where h(z) = 2z1 + 3z2+z3 + 2z4+z5. All z's were generated from the standard normal distribution, and a = 0, 0.2, 0.4, 0.6, 0.8. To allow x and (zi1, ?, zip) to be correlated, x was generated as x = z1+e/2 with e being independent of z1 and following N (0, 1). We studied the size of the test by generating data under a = 0, and studied the power by increasing a. The sample size was 100. For the size calculations, the number of simulations was 2000; whereas for the power calculations, the number of runs was 1000. Based on the discussions in Section "Test for the genetic Pathway Effect", the bound of ? is set up by interval [min?i?j?l=15(zil?zjl)2/5,10max?i?j?l=15(zil?zjl)2], and the interval is divided by 500 equally spaced grid points. All simulations were conducted using <software>R</software> 2.5.0, and the package "<package>globaltest</package>" v4.6.0 was used for the test proposed by Goeman et al. [13] as a comparison.<br>
Table 4 reports the empirical size (a = 0) and power (a &gt; 0) of the variance component score test for the pathway effect. When the true function h(z) is non-linear in z, the results show that the size of our test was very close to the nominal value 0.05, while the size of the global test of Goeman et al. [13] is inflated. The results also show that our test had a much higher power. This was not surprising since the test of Goeman et al. [13] was based on a linearity assumption of the pathway effect. When the true underlying model is far from linear, the linearity assumption breaks down and the test quickly loses power. The results also show that the proposed test works well for moderate sample sizes. When the pathway effect is linear, the results show that the size of both tests were very close to the nominal value 0.05 and their power were also very close. This demonstrates that our test is as powerful as the global test when the true underlying h(z) is linear. Therefore our test could be used as a universal test for testing the overall effect of a set of variables without the need to specify the true functional forms of each variable. This feature is especially desirable for genetic pathway data, because the relationship between genes and clinical outcome is often unknown.<br>
<br>
<br>
Conclusions and Discussion<br>
In this paper, we developed a logistic kernel machine regression model for binary outcomes, where the covariate effects are modeled parametrically and the genetic pathway effect is modeled nonparametrically using the kernel machine method. This method provides an attractive way to model the pathway effect, without the need to make strong parametric assumptions on individual gene effects or their interactions. Our model also allows for parametric pathway effects if a parametric kernel, such as the first-degree polynomial kernel, is used.<br>
A key result of this paper is that we have established a close connection between the generalized kernel machine regression and generalized linear mixed models, and show that the kernel machine estimators of regression coefficients and the nonparametric multi-dimensional pathway effect can be easily obtained from the corresponding generalized linear mixed models using PQL. The mixed model connection provides a unified framework for estimation and inference and can be easily implemented in existing software, such as <software>SAS PROC GLIMMIX</software> or <software>R GLMMPQL</software>. The mixed model connection also makes it possible to test for the overall pathway effect through the proposed variance component test. A key advantage of the proposed score test for the pathway effect is that it does not require an explicit functional specification of individual gene effects and gene-gene interactions. This feature is of practical significance as the pathway effect is often complex. Our simulation study shows the proposed test performs well for moderate sample size. It has similar power to the linearity-based pathway test of Goeman et al. [13] when the true effect is linear, but much higher power when the true effect is nonlinear.<br>
We have considered in this paper a single pathway. One could generalize the proposed semiparametric model to incorporate multiple pathways by fitting an additive model:<br>
logit(P(y = 1)) = xT? + h1(z1) + ? + hm(zm),<br>
where zj (j = 1, ?, m) denotes a pj ? 1 vector of genes in the jth pathway and hj(?) denotes the nonparametric function associated with the jth genetic pathway.<br>
Machine learning is a powerful tool in advancing bioinformatics research. Our effort helps to build a bridge between kernel machine methods and traditional statistical models. This connection will undoubtedly provide a new and convenient tool for the bioinformatics community and opens a door for future research.<br>
<br>
Methods<br>
The Logistic Kernel Machine Model<br>
Throughout the paper we assume that gene expression data have been properly normalized. Suppose the data consist of n samples. For subject i (i = 1, ?, n), yi is a binary disease outcome taking values either 0 (non-disease) or 1 (disease), xi is a q ? 1 vector of covariates, zi is a p ? 1 vector of gene expression measurements in a pathway/gene set. We assume that an intercept is included in xi. The binary outcome yi depends on xi and zi through the following semiparametric logistic regression model:<br>
(3)logit(?i)=xiT?+h(zi),<br>
where ?i = P (yi = 1| xi, zi), ? is a q ? 1 vector of regression coefficients, and h(zi) is an unknown centered smooth function.<br>
In model (3), covariate effects are modeled parametrically, while the multi-dimensional genetic pathway effect is modeled parametrically or nonparametrically. A nonparametric specification for h(?) reflects our limited knowledge of genetic functional forms. Note that h(?) = 0 means that genes in the pathway have no association with the disease risk. If h(z) = ?1z1 + ? + ?pzp, model (3) becomes the linear model considered by Goeman et al. [13].<br>
In nonparametric modeling, such as smoothing splines, the unknown function is usually assumed to lie in a certain function space. For the kernel machine method, this function space, denoted by HK, is generated by a given positive definite kernel function K(?, ?). The mathematical properties of HK imply that any unknown function h(z) in HK can be written as a linear combination of the given kernel function K(?, ?) evaluated at each sample point. Two popular kernel functions are the dth polynomial kernel K(z1,z2)=(z1Tz2+?)d and the Gaussian Kernel K(z1, z2) = exp{-|| z1 ? z2||2/?2}, where ||z1?z2||2=?k=1p(z1k?z2k)2 and ? is an unknown parameter. The first and second degree polynomial kernels (d = 1, 2) correspond to assuming h(?) to be linear and quadratic in z's, respectively. The choice of a kernel function determines which function space one would like to use to approximate h(z). The unknown parameter of a kernel function plays a critical role in function approximation. It is a challenging problem to optimally estimate it from data. In the machine learning literature, this parameter is usually pre-fixed at some values based on some ad-hoc methods. In this paper, we show that we can optimally estimate it from data based on a mixed model framework.<br>
<br>
The Estimation Procedure<br>
Assuming h(?) ? HK, the function space generated by a kernel function K(?, ?), we can estimate ? and h(?) by maximizing the penalized log-likelihood function<br>
(4)J(h,?)=?i=1n{yilog?(?i1??i)+log?(1??i)}?12?||h||HK2=?i=1n(yi{xiT?+h(zi)}?log?[1+exp?{xiT?+h(zi)}])??2||h||HK2,<br>
where ? is a regularization parameter that controls the tradeoff between goodness of fit and complexity of the model. When ? = 0, it fits a saturated model, and when ? = ?, the model reduces to a simple logistic model logit (?i) = xiT?. Note that there are two tuning parameters in the above likelihood function, the regularization parameter ? and kernel parameter ? Intuitively, ? controls the magnitude of the unknown function while ? mainly governs the smoothness property of the function.<br>
By the representer theorem [16], the general solution for the nonparametric function h(?) in (4) can be expressed as<br>
(5)h(zi)=?i?=1n?i?K(zi,zi?)=kiT?,<br>
where ki = {K(zi, z1), ..., K(zi, zn)}T and ? = (?1, ?, ?n)T, an n ? 1 vector of unknown parameters.<br>
Substituting (5) into (4) we have<br>
(6)J(?,?)=?i=1n[yi(xiT?+kiT?)?log?{1+exp?(xiT?+kiT?)}]?12??TK?,<br>
where K = K(?) is an n ? n matrix whose (i, i')th element is K(zi, zi') and often depends on an unknown parameter ?.<br>
Since J(?, ?) in (6) is a nonlinear function of (?, ?), one can use the Fisher scoring or Newton-Raphson iterative algorithm to maximize (6) with respect to ? and ?. Let (k) denote the kth iteration step, then it can be shown (for details see Appendix A.3) that the (k + 1)th update for ? and ? solves the following normal equation:<br>
(7)[XTD(k)XXTD(k)KD(k)X??1I+D(k)K]?[?(k+1)?(k+1)]=[XTD(k)y?(k)D(k)y?(k)].<br>
where y?(k)=X?(k)+K?(k)+D(k)?1(y??(k)), ? = 1/?, h(k) = K? (k), and D(k)=Diag{?i(k)(1??i(k))}. The estimators ?^ and h^ at convergence are the kernel machine estimators that maximize (6).<br>
<br>
The Connection of Logistic Kernel Machine Regression to Logistic Mixed Models<br>
Generalized linear mixed models (GLMMs) have been used to analyze correlated categorical data and have gained much popularity in the statistical literature [10]. Logistic mixed models are a special case of GLMMs. We show in this section that the kernel machine estimator in the semiparametric logistic regression model (3) corresponds to the Penalized Quasi-Likelihood (PQL) [10] estimator from a logistic mixed model, and the regularization parameter ? = 1/? and kernel parameter ? can be treated as variance components and estimated simultaneously from the corresponding logistic mixed model. Specifically, consider the following logistic mixed model:<br>
(8)logit(?i)=xiT?+hi,<br>
where ? is a q ? 1 vector of fixed effects, and h = (h1, ..., hn) is a n ? 1 vector of subject-specific random effects following h ~N{0, ? K(?)}, and the covariance matrix K(?) is the n ? n kernel matrix as defined in previous section.<br>
As K is not diagonal or block-diagonal, the random effects hi's across all subjects are correlated. The ith mean response ?i depends on other random effects hi' (i' ? i) through the correlations of hi with other random effects. To estimate the unknown parameters in the logistic mixed model (8), we estimate ? and h by maximizing the PQL [10], which can be viewed as a joint log likelihood of (? , h),<br>
(9)?i=1n[yi(xiT?+hi)?log?{1+exp?(xiT?+hi)}]?12?hTK?1h.<br>
Setting ? = 1/? and h = K? , one can easily see that equations (6) and (9) are identical. It follows that the logistic kernel machine estimators ?^ and h^ can be obtained by fitting the logistic mixed model (8) using PQL. In fact, examination of the kernel machine normal equations (7) shows that they are identical to the normal equations obtained from the PQL (9) [10], where y? in (7) is in fact the PQL working vector and Dis the PQL working weight matrix.<br>
Note that the estimators of ? and h depend on the unknown regularization parameter ? and the kernel parameter ?. Within the PQL framework, we can estimate these parameters ? = (?, ?) by maximizing the approximate REML likelihood<br>
(10)?R(?^(?),?)??12log?|V|?12log?|XTV?1X|?12(y??X?^)TV?1(y??X?^),<br>
where V = D-1 + ? K, and y? is the working vector as defined above. The estimator ?^ of ? can be obtained by setting equal to zero the first derivative of (10) with respect to ?. The estimating procedure for ?, h, and ? = (?, ?) can be summarized as follows: we fit the logistic kernel machine model by iteratively fitting the following working linear mixed model to estimate (?, h) using BLUPs and to estimate ? using REML, until convergence<br>
y?=X?+h+?,<br>
where y? is the working vector defined below equation (7), h is a random effect vector following N{0, ? K(?)}, and ? ~ N(0, D). The covariance of ?^ is estimated by (XTV-1X)-1, and the covariance of h^ is estimated by ?^K??^KPK, where P = V-1 - V-1X(XTV-1X)-1XTV-1 and V = V(?^). The covariance of ?^ can be obtained as the inverse of the expected information matrix calculated using the second derivative of (10) with respect to ?. The square roots of the diagonal elements of the estimated covariance matrices give the standard errors of ?^, h^, and ?. The above discussion shows that we can easily fit the logistic kernel machine regression using the existing PQL-based mixed model software, such as <software>SAS GLIMMIX</software> and <software>R GLMMPQL</software>.<br>
<br>
Test for the Genetic Pathway Effect<br>
It is of significant practical interest to test the overall genetic pathway effect H0: h(z) = 0. Assuming h(z) ? HK, one can easily see from the logistic mixed model representation (8) that H0: h(z) = 0 vs H1: h(z) ? 0 is equivalent to testing the variance component ? as H0: ? = 0 vs H1: ? &gt; 0. Note that the null hypothesis places ? on the boundary of the parameter space. Since the kernel matrix K is not block diagonal, unlike the standard case considered by Self and Liang [17], the likelihood ratio for H0: ? = 0 does not follow a mixture of ?02 and ?12 distribution. We consider instead a score test in this paper.<br>
When conducting statistical tests for pathways, two types of tests could be formulated. The first is called the competitive test and the second the self-contained test [18]. The competitive test compares an interested gene set to all the other genes on a gene chip. An example of the competitive test is the gene set enrichment analysis (GSEA) [1], where an enrichment score of a gene set is defined and a permutation test is used to test for the significance of the gene set based on the enrichment score. The self-contained test compares the gene set to an internal standard which does not involve any genes outside the gene set considered. In other words, the self-contained test examines the null hypothesis that a pathway has no effect on the outcome versus the alternative hypothesis that the pathway has an effect. The variance component test of [13] for the linear pathway effect is a self-contained test. Goeman and B?hlmann [18] pointed out that the self-contained test has a higher power than a competitive test and that its statistical formulation is also consistent for both single gene tests and gene set tests, and the statistical sampling properties of the competitive test can be difficult to interpret.<br>
Our pathway effect hypothesis H0: h(z) = 0 vs H1: h(z) ? 0 is a self-contained hypothesis. We propose in this paper a self-contained test for the pathway effect by developing a kernel machine variance component score test for H0: ? = 0 vs H0:? &gt; 0. The proposed test allows for both linear and nonlinear pathway effects and includes the tests by Goeman et al. [13,14] as a special case. A key advantage of our kernel-based test is that we do not need to explicitly specify the basis functions for h(?), which is often difficult for modeling the joint effects of multiple genes, and we all let the data to estimate the best curvature of h(?).<br>
Zhang and Lin [19] proposed a score test for H0: ? = 0 to compare a polynomial model with a smoothing spline. Goeman et al. [14] also proposed a global test against a high dimensional alternative under the empirical Bayesian framework. The variance-covariance matrix used in these tests do not involve any unknown parameters. However, the kernel function K(?, ?) in a kernel machine model usually depends on some unknown parameter ?. One can easily see from the mixed model representation (8) that under H0: ? = 0, the kernel matrix K disappears. This makes the parameter ? inestimable under the null hypothesis and therefore renders the above tests inapplicable.<br>
Davies [20,21] studied the problem of a parameter disappearing under H0 and proposed a score test by treating the score statistic as a Gaussian process indexed by the nuisance parameter and then obtaining an upper bound to approximate the p-value of the score test. We adopt this line of approaches for our proposed score test.<br>
Using the derivative of (10) with respect to ?, we propose the following score test statistic for H0: ? = 0 as<br>
(11)S(?)=Q?(?0?,?)??Q?Q,<br>
where<br>
Q?(?^0,?)=(y??X?^0)TDK(?)D(y??X?^0)=(y??^0)TK(y??^0),<br>
where ?^0 is the MLE of ? under H0: ? = 0, ?^0=logit?1(X?^0), ?Q = tr{P0K(?)}, ?Q2 = 2tr{P0K(?)P0K(?)}, and P0 = D0 ? D0X(XTD0X)-1 XTD0, where D0=diag{?^i0(1??i0)}. Note that under H0: ? = 0, model (3) reduces to the simple logistic model logit (?i) = xiT?. Hence the ?^0 is the MLE of ? under this null logistic model.<br>
If the Gaussian kernel is used, then an arbitrary nonlinear pathway effect is implicitly assumed. Our proposed test, which is derived to test for any nonlinear effect, is therefore more powerful than tests based on a parametric assumption. We show in Appendix A.1 that when ? becomes large in the Gaussian kernel, our test statistic reduces asymptotically to the one based on linearity assumption of genetic effects. Hence our test includes linear model based test as a special case. From (11) it is also clear that our test is invariant to the relative scaling of the kernel function K(?, ?).<br>
Under appropriate regularity conditions similar to those specified in [22], S(?) under the null hypothesis can be considered as an approximate Gaussian process indexed by ?. Using this formulation, we can then apply Davies' results [20,21] to obtain the upper bound for the p-value of the test. Since a large value of Q? (?^, ?) would lead to the rejection of H0, the p-value of the test corresponds to the up-crossing probability. Following Davies [21], the p-value is upper-bounded by<br>
(12)?(?M)+Wexp?(?12M2)/8?,<br>
where ? (?) is the normal cumulative distribution function, M is the maximum of S(?) over the range of ?, W = | S(?1) ? S(L)| + | S(?2) ? S(?1) | + ... + | S(U) ? S(?m) |, L and U are the lower and upper bound of ? respectively and ?l, l = 1, ..., m are the m grid points between L and U. Davies [20] points out that this bound is sharp. For the Gaussian kernel, we suggest to set the bound of ? as L = 0.1 min?i?j?l=1p(zil?zjl)2 and U = 100 max?i?j?l=1p(zil?zjl)2. For justifications, see Appendix A.2.<br>
<br>
Extension to generalized kernel machine model<br>
For simplicity, we focus in this paper on logistic regression for binary outcomes. The proposed semiparametric model (3) can be easily extended to other types of continuous and discrete outcomes, such as normal, count, skewed data, whose distributions are in the exponential family [23]. In this section, we briefly discuss how to generalize our estimation and testing procedures for binary outcomes to other data types within the generalized kernel machine framework and discuss its fitting using generalized linear mixed models.<br>
Suppose the data consist of n independent subjects. For subject i (i = 1, ..., n), yi is a response variable, xi is a q ? 1 vector of covariates, zi is a p ? 1 vector of gene expressions within a pathway. Suppose yi follows a distribution in the exponential family with density [23]<br>
(13)p(yi;?i,?)=exp?{yi?i?a(?i)?/mi+c(yi,?)},<br>
where ?i is the canonical parameter, a(?) and c(?) are known functions, ? is a dispersion parameter, and mi is a known weight. The mean of yi satisfies ?i = E(yi) = a'(?i) and Var(yi) = ? mia"(?i). The generalized kernel machine model is an extension of the generalized linear model [23] by allowing the pathway effect to be modeled nonparametrically using kernel machine as<br>
(14)g(?i)=xiT?+h(zi),<br>
where g(?) is a known monotone link function, and h(?) is an unknown centered smooth function lying in the function space HK generated by a positive definite kernel function K(?, ?). For binary data, setting g(?) = logit(?) = log??1?? gives the logistic kernel machine model (3); for count data, g(?) = log(?) gives the Poisson kernel machine model; for Gaussian data, g(?) = ? gives linear kernel machine model [9]. The regression coefficients ? and the nonparametric function h(?) in (14) can be obtained by maximizing the penalized log-likelihood function<br>
(15)J(h,?)=?i=1n?{yi,xi,zi;?,h(?)}?12?||h||HK2<br>
where ?(?) = ln(p) is the log-likelihood, p is the density function given in (13), and ? is a tuning parameter. Using the kernel expression of h(?) in (5), the generalized kernel machine model (14) can be written as<br>
g(?i)=xiT?+kiT?,<br>
and the penalized likelihood can be written<br>
(16)J(?,?)=?i=1n?(yi,xi,zi;?,?)?12??TK?,<br>
where K is an n ? n matrix whose (i, j)th element is K(zi, zj).<br>
One can use the Fisher scoring iteration to solve for ? and a. The procedure is virtually the same as that described in Section "The Estimation Procedure". The normal equation takes the same form as (7), except that now ?i is specified under (14) and D = diag{var(yi)} under (13). Similar calculations to those in Section "The Connection of Logistic Kernel Machine Regression to Logistic Mixed Models" show that model (14) can be fit using the generalized linear mixed model [10] via PQL<br>
g(?ib)=xiT?+hi,<br>
where ? = 1/?, and h = (h1 ..., hn) is an n ? n random vector with distribution N {0, ? K(?)}. The same PQL statistical software, such as <software>SAS PROC GLIMMIX</software> and <software>R GLMMPQL</software>, can be used to fit this model and obtain the kernel machine estimators of ? and h(?).<br>
The score test (11) also has a straightforward extension. The only change is that the elements in matrix D in (11) be replaced by appropriate variance function var(yi) under the assumed parametric distribution of yi.<br>
<br>
<br>
Appendix<br>
A.1 Proof of the relationship of the proposed score test and that of Goeman, et al [13] under the linearity assumption<br>
We show in this section when the scale parameter is large, the proposed nonparametric variance component test for the pathway effect using the Gaussian kernel reduces to the linearity-based global test of Goeman et al. [13].<br>
Suppose K(?) is the Gaussian kernel. It can be shown that the score statistic for testing H0: ? = 0 satisfies<br>
(17)Q?(?^,?)=(y??X?^0)TDK(?)D(y??X?^0)=(y??^)TK(?)(y??^0),<br>
where ?^0 is the MLE of ? under H0. The test statistic of Goeman et al. [13] takes the form<br>
(18)(y??^)TR(h??^),<br>
where R = ZZT. We now show when ? is large relative to max?i?j?l=1p(zil?zjl)2<br>
(19)?2(y??^)TK(?)(y??^)?(y??^)TR(y??^).<br>
Simple Taylor expansions show that<br>
(y??^)TK(?)(y??^)=?i=1n?j=1n(yi??^i)(yj??^j)exp?{??l=1p(zil?zjl)2/?}=?i=1n(yi??^i)2+?i?j(yi??^i)(yj??^j)exp?{??l=1p(zil?zjl)2/?}.<br>
When max?i?j?l=1p(zil?zjl)2/? is small, i.e., when ? is large relative to max?i?j?l=1p(zil?zjl)2, we have that exp?{??l=1p(zil?zjl)2/?}?1??l=1p(zil?zjl)2/? for any i ? j. Hence<br>
(y??^)TK(?)(y??^)=?i=1n(yi??^i)2+?i?j(yi??^i)(yj??^j)?1??i?j(yi??^i)(yj??^j)?l=1p(zil?zjl)2.<br>
Since ?j=1(yj??^j)=0 under the PQL, we have ?j?1(yj??^j)=?(yi??^i). Hence<br>
(y??^)TK(?)(y??^)??i=1n(yi??^i)2??i=jn(yi??^i)2?1??i?j(yi??^i)(yj??^j)?l=1p(zil2?2zilzjl+zjl2)=2??i=1n(yi??^i)2?l=1pzil2+2??i?j(yi??^i)(yj??^j)?l=1pzilzjl=2?(y??^)TR(y??^).<br>
This proves the approximate relation (19).<br>
<br>
A.2 Calculations of the lower and upper bounds of ?<br>
Although in theory ? could take any positive values up to infinity, for computational purpose we would require ? to be bounded. For the proposed test statistic (11), its value in fact only depends on a finite range of ? values. We describe why this is the case and how to find this range. For a given data set, the proof in Appendix A.1 shows that when is sufficiently large, the quantity 0.5?Q? (?^0, ?) converges to S0=(y???^0)TR(y???^0), which is free of ?.<br>
These arguments suggest that for numerical evaluation, it is not necessary to consider all ? values up to infinity. Instead, a moderately large enough value would suffice. Now the question comes down to how to decide on appropriate upper and lower bounds for ?. The proof in Appendix A.1 requires max?i?j?l=1p(zil?zjl)2/? be close to 0. Let C1 be some large positive number such that 1/C1 ? 0. If we take the upper bound of ? to be C1 max?i?j?l=1p(zil?zjl)2, then max?i?j?l=1p(zil?zjl)2/? would be close to 0. In practice we suggest taking C1 = 100, which would give good approximation. Using a similar idea, we can find a lower bound for ?. It is clear that when min?i?j?l=1p(zil?zjl)2/? ? ? any non-diagonal element of K(?) will be 0 and the kernel matrix reduces to an identity matrix. Hence, if we pick a small enough number C2 such that 1/C2 ? ?, we can effectively set the lower bound of ? to be C2 min?i?j?l=1p(zil?zjl)2. In practice we suggest take C2 = 0.1, which yields a good approximation.<br>
<br>
A.3 derivation of normal equation (5)<br>
Taking partial derivative of (6) with respect to ? and writing in matrix notation, we have XT(y-?). Similarly for ?, we have K(y ? ?) ? ? K?. The gradient vector is thus<br>
(20)q=[XT(y??)K(y??)??K?].<br>
Taking derivative of q with respect to ? and ?, we can get the following hessian matrix<br>
(21)H=?[XTDXXTDKKDX?K+KDK],<br>
where D = Diag{?i(1 ? ?i)}. The Newton-Raphson iteration states that the parameter value at the (k + 1)th iteration can be updated by the following relationship<br>
(22)?(k+1) = ?(k) ? (H(k))-1 q(k),<br>
where ? = (?T, ?T)T. Substitute (20) and (21) into (22), we arrive at normal equation (7).<br>
<br>
<br>
Availability<br>
Our algorithm is available at .<br>
<br>
Authors' contributions<br>
DL performed statistical analysis. All authors participated in development of the methods and preparation of the manuscript. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2572623</b><br>
Developing and validating predictive decision tree models from mining chemical structural fingerprints and high?throughput screening data in <database>PubChem</database><br>
<br>
<br>
Background<br>
High-throughput screening (HTS) is an automated technique and has been effectively used for rapidly testing the activity of large numbers of compounds [1-3]. Advanced technologies and availability of large-scale chemical libraries allow for the examination of hundreds of thousands of compounds in a day via HTS. Although the extensive libraries containing several million compounds can be screened in a matter of days, only a small fraction of compounds can be selected for confirmatory screenings. Further examination of verified hits from the secondary dose-response assay can be eventually winnowed to a few to proceed to the medicinal chemistry phase for lead optimization [4,5]. The very low success rate from the hits-to-lead development presents a great challenge in the earlier screening phase to select promising hits from the HTS assay [4]. Thus, the study of HTS assay data and the development of a systematic knowledge-driven model is in demand and useful to facilitate the understanding of the relationship between a chemical structure and its biological activities.<br>
In the past, HTS data has been analyzed by various cheminformatics methods [6-17], such as cluster analysis[10], selection of structural homologs[11,12], data partitioning [13-16] etc. However, most of the available methods for HTS data analysis are designed for the study of a small, relatively diverse set of compounds in order to derive a Quantitative Structure Activity Relationship(QSAR) [18-21] model, which gives direction on how the original collection of compounds could be expanded for the subsequent screening. This "smart screening" works in an iterated way for hits selection, especially for selecting compounds with a specific structural scaffold [22]. With the advances in HTS screening, activity data for hundreds of thousands' compound can be obtained in a single assay. Altogether, the huge amount of information and significant erroneous data produced by HTS screening bring a great challenge to computational analysis of such biological activity information. The capability and efficiency of analysis of this large volume of information might hinder many approaches that were primarily designed for analysis of sequential screening. Thus, in dealing with large amounts of chemicals and their bioactivity information, it remains an open problem to interpret the drug-target interaction mechanism and to help the rapid and efficient discovery of drug leads, which is one of the central topics in computer-aided drug design [23-30].<br>
Although the (Quantitative) Structure Activity Relationship-(Q)SAR has been successfully applied in the regression analysis of leads and their activities [18-21], it is generally used in the analysis of HTS results for compounds with certain structural commonalities. However, when dealing with hundreds of thousands of compounds in a HTS screening, the constitution of SAR equations can be both complicated and impractical to describe explicitly.<br>
Molecular docking is another widely used approach to study the relationship between targets and their inhibitors by simulating the interactions and binding activities of receptor-ligand systems or developing a relationship among their structural profiles and activities[31,32]. However, as it takes the interactions between the compounds and the target into consideration, it has been widely used for virtual screening other than to extract knowledge from experimental activities.<br>
Decision Tree (DT) is a popular machine learning algorithm for data mining and pattern recognition. Compared with many other machine learning approaches, such as neural networks, support vector machines and instance centric methods etc., DT is simple and produces readable and interpretable rules that provide insight into problematic domains. DT has been demonstrated to be useful for common medical clinical problems where uncertainties are unlikely [33-37]. It has been applied to some bioinformatics and cheminformatics problems, such as characterizations of Leiomyomatous tumour[38], prediction of drug response[39], classification of antagonist of dopamine and serotonin receptors[40], virtual screening of natural products[41].<br>
In this study, we propose a DT based model to generalize feature commonalities from active compounds tested in HTS screening. We utilized DT as the basis to develop the model because it has been successfully applied in many biological problems, and it is able to generate a set of rules from the active compounds which can then be used for filtering the untested compounds that are likely to be active in the biological system of interest. Moreover, it has the capability to handle the arbitrary degree of non-linear structurally diversified compounds.<br>
Many elegant algorithms for building decision tree models have been introduced and applied in real life problems, and <software>C4.5</software>[42] is one of the best known programs for constructing decision trees. In this work, the DT based model was developed on the basis of the Decision Tree C4.5 algorithm[42]. The representation of the molecular structures is described by the <database>PubChem</database> fingerprint system. The DT based model was further examined by four assays deposited in the <database>PubChem</database> Bioassay Database, the HTS assay for 5-Hydroxytryptamine Receptor Subtype 1a (5HT1a) antagonists(<database>PubChem</database> AID:612), HTS assay for 5HT1a agonists(<database>PubChem</database> AID 567), and two other assays with <database>PubChem</database> AID 565 and 372 for screening the HIV-1 RT-RNase H inhibitors. The results of 10-fold Cross Validation (CV) over these HTS assays suggest the self-consistency of the DT models. Since a model simply provides the rules based on the profiles of active compounds in a specific HTS assay, the computationally generated models were further examined using two HTS assays which tested the same HIV RNase target, but used different compound libraries and were performed independently by two individual research laboratories. Our results suggest that these developed models could be used to validate HTS assay data for noise reduction and to identify hits through virtual screening of additional and larger compound libraries.<br>
<br>
Results and discussion<br>
Development of DT model<br>
In this study, four DT models were developed for activity data contained in <database>PubChem</database> bioassay AID 612, 567, 565 and 372 respectively, where compounds were screened for various activities against several protein targets (Table 1).<br>
Model fitting accuracies are used to examine whether the proposed data model can handle the complex data and whether the chemical fingerprint descriptors are sufficient for model development. As shown in Table 2, the model fitting accuracies of the four DT models were in the range of 98.6% to 99.8%. This suggested that the DT based models are able to fit the majority of the HTS data for the model generalization. Sensitivity and specificity of each developed model, which indicate the ratio of the active and inactive compounds that can be successfully learned by the DT model, are also reported in Table 2. The sensitivities of the models built for AID 612, AID 567, AID 565 and AID 372 are 86.5%, 98.9%, 90.2% and 83.1% respectively, while the specificities are all greater than 98%. The model fitting accuracies for both active and inactive compounds suggest strong feature-activity relationship among compounds tested in the HTS screenings. The small fraction of misrecognized compounds might result from the HTS data noise, the discrepancy of bioactivities observed from the compounds with same or similar chemical structures, or the competition with the overwhelming inactive compounds<br>
As shown by the comparison of the sensitivity and its corresponding specificity for each individual DT model, the sensitivity is usually lower than the specificity and contributes less to the overall accuracy. One possible explanation is the existence of the data imbalance issue. Among the HTS data analyzed in this study, the ratio between the number of active compounds and that of inactives are ranged from 1:51~1:176. Thus comparing to the active compounds, the rules could be easily generalized for inactive compounds when the chance of pattern reoccurrence is higher. Data Imbalance problem is common in the high throughput screening assay data. One HTS assay could have tens of thousands of compounds tested and only yield few dozens of hits. Due to this problem, the specificity becomes less objective in performance evaluation. Therefore, we also use the Matthews Correlation Coefficient (MCC) [43] as additional measure to evaluate the model's performance.<br>
MCC took both sensitivity and specificity into account and it is generally used as a balanced measure in dealing with data imbalance situation. As shown in Table 2, MCC values fall in the range of 0.67~0.84 for the four HTS assays, which again suggests the satisfactory performance of the model training and indicates that the self recognition of the model is not random.<br>
<br>
Model validation by self-consistency test<br>
The validation of the DT based models and self-consistency test were performed by 10-fold cross validation (CV) method, in which the compound dataset tested in one HTS assay was randomly split into 10 folds. These models were set up using 9 randomly selected folds, and prediction was done on the remaining fold.<br>
The 10-fold CV results are given in Table 3. The overall validation accuracies of all DT models ranged from 96.9% to 98.9%. While the sensitivities of the models built for AID 612, AID 567, AID 565 and AID 372 are 64.5%, 80.5%, 75.2% and 57.2% respectively, where the specificities were 99.1%, 99.0%, 97.3% and 98.9% respectively. The more than 96% overall accuracies of the four DT based models suggest overall good performance and the CV analysis validates the reliability of the DT based models.<br>
The sensitivity and specificity values given here represent the classification accuracies for the active and inactive compounds respectively. The sensitivity is lower than specificity to a certain extent. For example, the DT model for the 4HTa antagonist activity data demonstrates 64.5% sensitivity but 99.1% specificity. From the evidence given in the previous section, imbalanced data, data noise and data discrepancy could again account for the lower sensitivities. Moreover, as about 90% percent of the data used for training during the cross validations, the generalization ability of the active compound dataset became easily affected due to the limited sample size as compared to that of the inactive compound dataset.<br>
The learning capability of the DT model could also be affected by the way the model was trained, such as the minimum count of compound instances required for a decision node. However, it primarily depends on the datasets used for training. Although the imbalanced active and inactive compound datasets have an effect on the performance of the 10-fold CV, our results still show that the models are self-consistent. In addition, compounds and their activity data in HTS screens are able to converge toward a discrimination model with encouraging accuracies. In addition, the MCC values ranged from 0.4 to 0.5, again indicating the potential of the models to identify potential hits.<br>
<br>
Application of DT models to select potential active compounds<br>
In this study, independent evaluation of the DT based models was attempted by using two HTS assays, <database>PubChem</database> AID 565 and 372, which were aimed at identifying HIV-1 RT RNase H inhibitors.<br>
Comparison of the compound libraries of these two HTS assays were first performed, demonstrating the extent of similarity of the active compounds between the two assays. By using Tanimoto coefficient[11,12] as a measurement for the compound similarities, there are only six active compounds that were found to be similar with Tanimoto coefficient threshold of 95%. This suggested the overlap of the active compounds in these two assays was very limited. It maybe of interest to investigate whether the DT model built with one compound set can be used to filter out hits identified with another assay where a different compound library was screened. To this end, DT models of these two HTS assays were first developed independently and then each model was applied to classify the compounds screened by the other assay. An enrichment factor, which simply describes the proportion of active compounds from any given collection compared with randomly picked compounds [44], was calculated as assessment for the classification performance of each model.<br>
Assume N compounds are tested in a HTS assay sample where A compounds have been experimentally verified as bioactive. By virtual screening, which is the activity classification using DT model in this study, Ns compounds are predicted as active and among these Na belongs to the group of known bioactive compounds. A randomly picked sample will on average contain ANs/N active molecules. Therefore, the formula for calculating the enrichment factor is NaN/NsA.<br>
The enrichment factors for cross dataset prediction of HIV RNase H inhibitors of AID 372 and 565 are 4.4 and 9.7 respectively. From the virtual screening point of view, which is focusing on selecting the true hits while excluding the false positives as much as possible, the results suggest that the model derived from these two bioassays have certain generalization abilities to increase the odds of selecting true hits.<br>
On the other hand, the sensitivities of the DT models based on data sets AID 372 and 565 are 0.4% and 6.9%, and the specificities are 98.5% and 99.9% respectively, which yield corresponding MCC value of 0.03 and 0.04, apparently the sensitivities and MCC values in this experiment are "poor" comparing to the cross validation study. This is not surprising, and indeed is well expected as the dramatic chemical structural differences between the two data sets (AID 372 and AID 565), and the models derived from the individual datasets may carry overwhelming localization features that might not be largely applicable to each other. This also leaves the gap to be filled in for a robust statistical model, better representation of physical chemical properties, enlarged and diversified dataset, and enhanced quality of the experimental accuracy in the future.<br>
Nevertheless, this preliminary test using DT model as virtual screening technique yields encouraging enrichment for selection of active compounds when applied to another HTS activity dataset. It suggests that, despite of the very low similarities between the active compounds from the two HTS assays, certain common profiles of the active compounds can be extracted using the DT model, which can ultimately be very useful for virtual screening tasks.<br>
<br>
<br>
Conclusion<br>
In this study, we use derived DT models based on structural fingerprints of compounds to select biologically interesting compounds from HTS assay dataset. Four HTS assays were analyzed to determine to what extent the designed models can be applied to the compound libraries of an unknown domain. Our results suggest that the DT based models can be successfully used to derive common characteristics from HTS data, and the models can serve as filters to facilitate the selection of compounds against the same target. These DT models could also be used to eliminate HTS hits arising from data noise or those lacking statistical significance.<br>
The development of the model is a learning process. Thus, the potential of the developed model is limited to the known active compounds and the properties used for training, and limited to the distribution of the compound collection to which the model is applied. With the growth in the number of compounds to be screened and the improvement over data quality produced with HTS assay, a more robust model could be developed with increased ability for selecting biologically interesting small molecules from a diverse compound library.<br>
<br>
Methods<br>
Datasets<br>
There are over 500 assays deposited in the <database>PubChem</database> Bioassay database as of May 1st, 2007. About 200 of them have protein targets. In this study, four HTS assays were selected from the <database>PubChem</database> bioassay database. The criteria for the selection were that a substantial number of compounds have been tested in one assay and that the number of active compounds is in the hundreds to demonstrate statistical significance. These HTS assays are 5-Hydroxytryptamine Receptor Subtype 1a (5HT1a) antagonists with <database>PubChem</database> BioAssay AID 612, HTS assay for screening 5HT1a agonists with AID 567, and <database>PubChem</database> bioassays AID 565, AID 372 for screening HIV-1 RT-RNase H inhibitors. The number of structurally distinct compounds for these four bioassays and the number of active and inactive compounds are summarized in Table 1. Compound bioactivities outcomes have been described in a binary form, active and inactive, as specified by the assay depositor, and were retrieved from <database>PubChem</database> BioAssay database.<br>
<br>
<software>PubChem Fingerprint System</software><br>
The numerical understanding of chemical structures is described by a binary substructure fingerprint generated by the <software>PubChem Fingerprint System</software>.<br>
A substructure is a fragment of a chemical structure, such as a type of ring system, atom pairing, or atom environment (nearest neighbours). A fingerprint is an ordered list of binary (1/0) bits. Each bit represents a Boolean determination of the presence of a fragment of a chemical structure. The <database>PubChem</database> fingerprint has a total of 881 bits and is composed of 7 sections such as Hierarchic Element Counts, chemical rings, and simple atom pairs, simple atom nearest neighbours, detailed atom information and two sections of SMARTS patterns. A detailed description and the full list of fingerprint bits can be accessed at .<br>
The 2D structure of each compound was used to generate a binary substructure fingerprint.<br>
<br>
Decision Tree based Filtering Method<br>
DT is an acyclic graph in which its interior vertices specify testing of a single attribute of a feature vector and its leaves indicate the class of the decision [45-47]. It was constructed by recursively splitting the sample set, with each subset giving rise to one new vertex connected with an edge to its parent. This procedure continues until all samples at each leaf belong to the same class. The working flow of DT is similar to a logical tree structure that starts from the topmost node, and every decision of the node determines the direction of next node movement until the end of the tree branch node is reached. In this study, we performed DT analysis by utilizing the <software>C4.5</software> core library [42].<br>
As the filtering system is designed to choose the compounds of interest with certain features' commonalities to those compounds considered as active in the HTS assay, the binary representation of the activity outcomes were used to categorize these compounds. The structural fingerprint of every compound was processed by the <database>PubChem</database> fingerprints system subsequently for numerical description of the dataset for model training. To derive the models, the logical decision tree is then examined for error pruning, which is the removal of branches that are deemed to provide little or no gain in statistical accuracy of the model.<br>
<br>
Model Self-consistency evaluation<br>
The model self-consistency evaluation is performed using the 10 fold CV approach. As the HTS data are usually diversely distributed and not error free, the CV evaluation of the DT model is subjected to representatives from both the compounds used for training and for testing. Thus, for the balance between the computation cost and the evaluation of the model generalization ability, the 10 fold CV approach is chosen to assess the self-consistency of the model [48].<br>
Under the assumption that the distribution of different subsets from one HTS assay is approximately equal, the quality of the model can be proven if the model built on the top of a portion of the data can be generalized to others during the self-consistency evaluation.<br>
<br>
Measurement of accuracies<br>
Model accuracy is measured by sensitivity, specificity, and a combined parameter called "overall accuracy." The sensitivity and specificity are defined as the following:<br>
Sensitivity=TP(TP+FN),Specificity=TN(TN+FP),<br>
where the true positive (TP) is the number of compounds correctly predicted as active, false negative (FN) is the number of compounds incorrectly predicted as inactive, true negative (TN) is the number of compounds correctly predicted as inactive, and false positive (FP) is the number of compounds incorrectly predicted as active. Thus, the overall accuracy is defined as Overall_Accuracy=TP+TNTP+FN+TN+FP?100%.<br>
In addition to compute sensitivity and specificity, to further evaluate the classification performance on a dataset containing imbalanced active and inactive compounds, Matthews correlation coefficient (MCC)[49] is also calculated as given by the following equation<br>
MCC=TP?TN?FN?FP(TP+FN)(TP+FP)(TN+FN)(TN+FP)<br>
MCC ranges from -1 to 1, and suggests the randomness of the model.<br>
<br>
Use of DT models to select compounds of interest<br>
The best model is optimized by pruning the decision tree to minimize the classification errors. With the knowledge learned from the tested compounds and their bioactivities, one can apply these trained rules to filter new compounds that are likely to be active. In addition to the validation of the DT models with CV approach, application of models for the prediction of bioactivity classification have been attempted by using two HTS assays for identifying HIV-1 RT RNase inhibitors. Both of these two HTS assays are designed for the screening of the HIV-1 RT RNase H target. In spite of the differences in the design of HTS assays and in the selection of compound libraries, the underlying knowledge of those inhibition compounds was assumed to be similar or interpretable from one another. To this end, the optimized DT models of these two HTS assays were developed independently and were applied to examine the other compound collection as virtual experiments for identifying potential inhibitors.<br>
<br>
Computation Software<br>
The implementation of the Decision Tree-based models was based on <software>PubChem Fingerprint System</software>, OpenEye OEChem C++ library, NCBI C++ toolkit library, and the <software>C4.5</software> core library.<br>
<br>
<br>
Authors' contributions<br>
All authors participated in development of the methods, discussions and preparation of the manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2630964</b><br>
An efficient genetic algorithm for structural RNA pairwise alignment and its application to non-coding RNA discovery in yeast<br>
<br>
<br>
Background<br>
The RNA worlds in both experimental and computational fields have recently grown rapidly, and non-coding RNAs (ncRNAs) have increased their importance in life sciences. One of the most important breakthrough from the experimental side is the high-throughput experiments which have unveiled the existence of many non-protein coding transcripts in various species [1,2]. While function-known ncRNAs, which often harbor family-specific conserved secondary structure, such as tRNAs and miRNAs have been intensively studied in detail, no functional annotation has been assigned to a number of known non-protein coding transcripts yet. Since experimental assessment whether all known non-protein coding transcripts are functional or not is quite time-consuming, computational screening for finding the ncRNAs with conserved secondary structure is an important step for determining not only expressed but also functional transcripts. Computational comparative genomics is a powerful approach to identify ncRNA candidates with conserved secondary structure from genomic sequences. To date, sequence-alignment-based ncRNA finders such as <software>RNAz</software> [3], <software>QRNA</software> [4] and <software>EvoFold</software> [5] have been successfully applied to ncRNA discoveries from various complete genomes [6-10]. While these methods are so efficient that they can be applied to genome-scale analysis, sequence-alignment-based methods need a pre-computed alignment as an input data. In other words, they implicitly assume that an adequately accurate RNA sequence alignment can be obtained by using pure sequence alignment method (e.g. <software>ClustalW</software>) which does not explicitly consider conserved secondary structure. Although this assumption is acceptable for the RNA sequences with relatively high sequence identity, sequence-alignment-based methods can fail to indentify the ncRNAs with low sequence identity; this is because conserved secondary structure should be taken into account to accurately align structured RNA sequences which are poorly conserved at sequence level.<br>
Finding related structured RNA sequences with low sequence identity from genomic sequences is more challenging compared to the case of high sequence identity. This is mainly due to the high computational complexities of structural RNA sequence alignment algorithms which explicitly take secondary structure into account (in the present paper, the term "structural RNA sequence alignment" is used to indicate "simultaneously determining RNA sequence alignment and conserved secondary structure without pre-defined secondary structure annotation"). For example, the computational complexities of Sankoff's algorithm which is the most basic algorithm for structural RNA sequence alignment are O(N3M) in time and O(N2M) in space, where N and M are the length and the number of RNA sequences to be aligned, respectively [11]. Even when one performs pairwise alignment, Sankoff's algorithm needs O(N6) in time and O(N4) in space. To improve the computational speed and memory usage of structural RNA sequence alignment, various variations of Sankoff's algorithm have been intensively studied [12-21].<br>
So far, Dynalign [13] and Foldalign [14] which are variations of Sankoff's algorithm, have been applied to the pairwise comparative genomics for novel ncRNA discoveries[22,23]. Indeed these 'structure-based' ncRNA finders have successfully predicted a number of ncRNA candidates with low sequence identities, these calculations needed long computational times and large computational resources. Although these programs have been updated recently and the latest versions are faster compared to their older versions, it is still time consuming to apply these programs to genome-scale applications. Recently an efficient structural RNA sequence alignment algorithm, LocARNA, has been proposed[15]. To our knowledge, however, there is no report related to the ncRNA discovery by using LocARNA. Since genomic scans by previous structural RNA sequence alignment methods are time consuming and need large computational resources in general, further development of efficient and accurate structural RNA sequence alignment algorithm is important to accelerate the genome-scale prediction of the ncRNAs with low sequence identities. Recently, CMfinder, which is structural RNA sequence alignment algorithm not for pairwise but for multiple RNA sequence alignment, has successfully predicted a number of novel structured RNA motifs from the ENCODE regions with low sequence identities [24].<br>
In the present paper, we propose an improved genetic algorithm (GA), Cofolga2, for structural RNA pairwise alignment which uses the base pairing probabilities (BPPs) by RNAfold[25] to evaluate the structural term of the objective function instead of directly using the free energy parameters as its version 1 does [21]. Since the present algorithm is efficient in both time and memory usage, we applied the algorithm to the pairwise comparisons between eukaryotic complete genomes to search for novel ncRNA candidates from low sequence identity regions. The rest of the present paper is organized as follows. First we describe the present structural RNA sequence alignment algorithm and a strategy for our comparative genomics in the 'Methods' section. In the next section, we show the results of the benchmark and the comparison between the present algorithm and previous ones, discussing the performance of our alignment algorithm. Finally, we present the detail of the ncRNA candidates obtained by the pairwise genome comparisons between S. cerevisiae and other six fungi.<br>
<br>
Methods<br>
In Cofolga2 algorithm, we employ a GA to search for the optimal solution of structural RNA pairwise alignment. Cofolga2 is an updated version of the previously proposed GA [21] which performs structural RNA pairwise alignment based on minimization of free energy and the GA frameworks proposed in RAGA [26] (in the present paper, we call the previous version as Cofolga1). Cofolga2 runs much faster compared to Cofolga1; this is mainly due to the improvement in the formulation of objective function and introduction of a new technique for random alignment generation. In standard GA, various GA operators (crossovers and mutations) are iteratively applied to a population of individuals (solutions) to search for the optimal solution with the highest value of a given objective function (OF) [27]. In the Cofolga algorithms, an individual of GA is represented by a pairwise alignment. This is because structural RNA sequence alignment problem can be decomposed into sequence alignment and alignment folding, and the optimal alignment folding is uniquely defined for a given alignment. As a result, the conformational space to be explored in the present structural RNA pairwise alignment is reduced to that of non-structural pairwise sequence alignment.<br>
The OF of Cofolga2 is represented by the following formula:<br>
(1)f = s + wP,<br>
where s is a sequence alignment score, P is a term for consensus secondary structure; w is a parameter for controlling the weights of s and P.<br>
For a given pairwise alignment of RNA sequence A and B, the P in Equation 1 is evaluated as follows.<br>
First an averaged BPP matrix B is constructed:<br>
(2)bij={(pkiljA+pminjB)/2pkiljA?0andpminjB?00otherwise.<br>
In Equation 2, bij is the matrix element of B, where i and j indicate the column positions in the pairwise alignment; ki and lj (mi and nj) are the nucleotide positions in sequence A (sequence B) corresponding to column position i and j in the pairwise alignment, respectively. The BPPs of single sequence A and B, pklA and pmnB, are computed by RNAfold [25]. Secondly, the P is evaluated by taking a summation of the elements in matrix B:<br>
(3)P=?i&lt;jbij.<br>
It is noteworthy that Equation 3 can be applied to any type of pseudoknotted structure without modification. This means that once the BPP matrixes taking pseudoknots into account are given, Cofolga2 algorithm can perform structural RNA sequence alignment of pseudoknotted RNAs without an increase of computational costs compared to the case of non-pseudoknotted RNAs.<br>
The flowchart of Cofolga2 algorithm is shown in Figure 1. In accordance with the standard GA, first, initialization is done to randomly generate an initial population, and then evaluation and reproduction procedures are iteratively performed to update the population. This iteration stops when the number of iterations reaches a user-defined maximum number or when no improvement has been observed for a user-defined number of iterations.<br>
As mentioned above, Cofolga2 was developed based on Cofolga1. In the following subsections, we will focus on explaining the detail of algorithms newly introduced for Cofolga2. Algorithmically common parts between the two versions will be briefly explained.<br>
Initialization<br>
An initial population of solutions is generated by adding a randomly generated pairwise alignment to the population one by one until the number of individuals reaches a user-defined population size. The random pairwise alignments are computed by using weighted stochastic backtracking (for detail, see next subsection). In weighted stochastic backtracking, the randomness of the alignment can be controlled with a 'noise' parameter where larger noise gives more randomized alignment; based on our experience, we used noise = 0.1 ? 0.4 to obtain random alignments.<br>
In addition to the random alignments, a non-random alignment taking structure information into account (computed by weighted stochastic backtracking with a very small noise such as noise = 1.0 ? 10-4) and a non-structural Needleman-Wunsch alignment [28] can also be included in the initial population through a command line option (a '-nrd' option). When invoked with the '-nrd' option, Cofolga2 works as a refinement program which improves the two non-random alignments. Inclusion of the non-structural alignment improves the quality of the alignments with a relatively high sequence identity. In the default setting of Cofolga2, duplicated individuals in one population are not allowed throughout the run.<br>
<br>
Weighted stochastic backtracking<br>
In the initialization step of the alignment algorithms utilizing GA such as SAGA [29] and RAGA [26], it is necessary to generate a number of random alignments. For example, in RAGA algorithm, random pairwise alignments are computed by using a Dynamic Programming with Added Noise (DPAN) in which random alignments are obtained by adding small random noises to each DP matrix elements [30]. Since DPAN constructs a DP matrix in accordance with non-structural Needleman-Wunsch algorithm, structural information is completely lacked in such a calculation. To obtain a better initial guess for the structural RNA sequence alignment, structural information should be taken into account.<br>
To generate random pairwise alignments which reflect structural information, we developed weighted stochastic backtracking. In this algorithm, first, we construct the DP matrix for a pairwise alignment according to StrAl algorithm [31]. StrAl algorithm is an efficient structural alignment algorithm, and it was derived from an affine gap version of Needleman-Wunsch algorithm [28]. An essential difference between StrAl and the Needleman-Wunsch algorithm is their similarity scoring scheme. In StrAl algorithm, the following similarity score sij is used when constructing the DP matrix for pairwise alignment instead of the nucleotide substitution matrix d (Ai, Bj) alone:<br>
(4)sij=?(?iA?jB+?iA?jB)+d(Ai,Bj)?iA?jB<br>
(5)?iX=?k&lt;ipikX,?iX=?k&gt;ipikX,<br>
(6)?iX=1?(?iX+?iX)<br>
where sij indicates the similarity score between position i of sequence A and position j of sequence B, and ? is the ratio of structure over sequence similarity. Nucleotide substitution matrix element d(Ai, Bj) is the substitution score between the ith nucleotide of sequence A and the jth nucleotide of sequence B. In the present study, we used RIBOSUM85-60 [32] for d(Ai, Bj) and ? = 0.9 which was taken from the StrAl paper [31]. Base paring probability vectors ?iX, ?iX, and ?iX are the probabilities defined for the position i of sequence X (= A or B) which represent probabilities of being paired upstream, paired downstream, and unpaired, respectively. The affine gap penalties which we used for weighted stochastic backtracking are also taken from the StrAl paper [31].<br>
After the construction of the DP matrix, we backtrack the DP matrix in accordance with a roulette wheel selection. Roulette wheel selection is a selection method frequently used in GAs, in which one of all choices is randomly chosen in accordance with the probability proportional to the size of a virtual 'slot' assigned to the choice.<br>
The size of the slots is determined by the following scaling function:<br>
(7)sloti=noisehmax??hi+noise,<br>
where index i corresponds to a backtracking path at a node of the DP matrix (i = 1, 2, 3 for pairwise alignment), hi is the score difference between the current node and the neighboring node for path i, and hmax is the largest hi among h1, h2, and h3. Larger noise parameter noise generates a more randomized alignment. A backtracking path is chosen in accordance with backtracking probability ?i which is defined as follows:<br>
(8)?i=sloti/?i=1,2,3sloti<br>
While backtracking, a real random number ranging from 0 to 1 is generated at each node and used to select a next path to be backtracked.<br>
In Figure 2, the curves drawn by the scaling function are plotted. As can be seen from the figure, higher noise increases the probability to choose low scoring paths, while noise ? 0 means the optimal alignment. Thus, the randomness of weighted stochastic backtracking is controllable through the single parameter noise. It is noteworthy that the principles of weighted stochastic backtracking can easily be applied to any type of DP algorithm, e.g. those of Nussinov's algorithm [33] and Sankoff's algorithm [11].<br>
<br>
Evaluation<br>
The OF, f in Equation 1, of each individual is evaluated in this step, where the alignment score s is calculated by using the RIBOSUM85-60 [32]. Opening and elongation gap penalties are left as free parameters. After the evaluation of the OF, the fitness of each individual is computed from the OF as fitness = OF - (the lowest OF in the population), and then a selection probability proportional to the fitness is calculated for each individual. The selection probability is used in reproduction step as the size of virtual slots for the roulette-wheel selection of parent individuals.<br>
<br>
Reproduction<br>
In reproduction step, half of the population with the lowest OFs is replaced by new child individuals. The child individuals are generated by applying GA operators to the parent individuals randomly selected from the population. We use a modified set of the GA operators taken from Cofolga1[21], which is comprised of two crossovers (random and greedy two-point crossovers) and three mutations (random and greedy gap-block shuffling operators, local re-alignment with weighted stochastic backtracking). Each GA operator is invoked with an equal probability and applied to one or two randomly selected parent individual(s); the crossovers need two parents, while the other operators are applied to a single parent. Selection of parent(s) is performed by roulette-wheel selection where the selection probability of each individual is used as the size of the slots. The GA operators are schematically illustrated in Figure 3. Crossover operators construct a new alignment by concatenating 'alignment blocks' taken from two parent individuals. Gap-block shuffling operator 'shuffles' a gap block (a block of continuous gaps) by a random shift size in a random direction. The maximum size of the gap shift is defined by shift size parameter max_shift.<br>
In 'local re-alignment with weighted stochastic backtracking', a randomly selected small region of the alignment is re-aligned using weighted stochastic backtracking. The region to be re-aligned is selected by the following gap-sensitive procedure. First, we initialize ?[i] = 1 for all i, where i indicates the column position of the alignment. Secondly, we scan the alignment with a sliding window of W columns. While scanning the alignment, we count the number of gaps in each window and add the number to the ?[i] whose i is the center of the sliding window. Thirdly, a column position k is randomly selected in accordance with the probability proportional to ?[i]. Finally, we define the region to be re-aligned around the k. The width of the region is randomly determined between lmin and lmax. When we meet a trivial case (i.e. when one of the alignment rows included in the region has no nucleotides), this operator is rejected and a next GA operator is randomly invoked. In this procedure, the W, lmin and lmax are the parameters to be given by user.<br>
In general, GA has several free parameters such as population size and iteration number which have to be given before execution. To reduce the number of such free parameters, we introduced unification parameter L. This L defines several parameters for the GA operators simultaneously through the following relationships: max_shift = lmax = L and lmin = (W - 1)/2 = ?L/2?. Hence once L is given, max_shift, lmax, lmin, and W are determined and only L is left as a free parameter. Unification parameter L controls the degree of modification, i.e. larger L leads to a wider search in the conformational space by the mutation operators. It is noted that too large L can cause a slow convergence of the GA.<br>
In the nomenclature of the GA operators, a 'greedy' means that the operator increases the OF of the child individual compared to that of its parents. Cofolga2 uses 'greedy' operators while Cofolga1 uses 'semi-greedy' operators. The 'greedy' operators of Cofolga2 reject the child individuals which do not satisfy (the OF of the child individual) &gt; (the OF of the parent individual(s)) while the 'semi-greedy' operators of Cofolga1 does not. Cofolga2 does not utilize 'anchor point for mutation operators' which is used in Cofolga1 to avoid gap insertion into highly conserved regions of an alignment (subsection 2.3.2 in [21]). Local Cofolga operator (ibid., subsection 2.3.6) is also not used in Cofolga2.<br>
<br>
Consensus structure prediction by a postprocessing<br>
Cofolga2 predicts the consensus secondary structure for the final alignment by backtracking in the averaged BPP matrix as a postprocessing of the GA procedures. To more accurately predict a consensus secondary structure based on the alignment computed by Cofolga2, it is better to use an alignment folding program such as <software>RNAalifold</software> [34] or <software>Pfold</software> [35] as a postprocessing.<br>
<br>
Measures for assessing alignment quality<br>
The quality of pairwise alignments was assessed with structure conservation index (SCI) and sum-of-pairs score (SPS). SCI and SPS were evaluated by <software>RNAz</software> [3] and bali_score.c [36], respectively.<br>
<br>
Determination of the free parameters<br>
In addition to the population size and maximum iteration number of the GA, <software>Cofolga2</software> has six free parameters: Cmax, noise, w, gap opening and elongation penalties for s, and L . We optimized these six parameters with fourteen pairwise alignments taken from the k2 dataset of <database>BRAliBase 2.1</database> [37]. The training RNA sequences are tRNAs, 5S rRNAs, and SRP RNAs with high or low sequence identities and high or low SCIs. For tRNA, alignments with a moderate sequence identity were also used. The file names of the RNA sequences are listed in Additional File 1. The parameter space to be explored was represented by a coarse grid and the parameter set corresponding to the grid point which scored the highest (mean SPS) ? (mean SCI) was adopted as the optimal parameter set (Cmax = 50, noise = 0.3, w = 50, gap opening = 30, gap elongation = 4, and L = 50). Throughout this optimization, we used a population size and maximum iteration number fixed to relatively large values, 150 and 150, respectively. The results presented at the Results and discussion section were obtained with this optimal parameter set. The population size and maximum iteration number are left as free parameters.<br>
<br>
Benchmark and comparison of alignment quality<br>
We performed an alignment quality benchmark using <database>BRAliBase 2.1</database> [37] from which RNA sequence pairs and their reference alignments were taken. In addition, we performed performance comparison with other structural and non-structural sequence alignment programs using the benchmark. In the performance comparison, we compared Cofolga2 with five structural and three non-structural alignment methods. The programs and command line options are summarized in Table 1. To perform the comparisons on an equal footing, global alignment mode was used for local alignment programs, <software>Foldalign</software> and <software>LocARNA</software>.<br>
<br>
Benchmark for the sequence pairs with low identities<br>
In addition to the <database>BRaliBase 2.1</database> benchmark, we have performed a benchmark with the sequences which have identities ? 40% and lengths of 100 to 150 nt. The sequences were extracted from the <database>internal transcribed spacer 2</database> (<database>ITS2</database>) database [38], where the sequences and annotated structures of Stramenopiles and "the original 5,000 sequences and structures" (ITS2.html, [39]) were used. Sequence identities were measured after aligning two <database>ITS2</database> sequences using <software>MAFFT</software> (see Table 1). We have performed non-redundant processing with a cutoff of 90%id. As a result, we obtained twenty-five <database>ITS2</database> sequence pairs (the <database>ITS2</database> dataset can be browsed at the <software>Cofolga2</software> website [40]); the average sequence identity of the dataset is 33%. This benchmark was performed for <software>Cofolga2</software>, <software>Foldalign</software> 2.1.0, and <software>LocaRNA</software>. Since annotated secondary structures are given in the <database>ITS2</database> database and reference alignments are not provided, the prediction accuracy for this benchmark was measured based on how correctly annotated (reference) base pairs are predicted. The correctness of the predicted base pairs was assessed with the approximated Matthews correlation coefficient (Equation 5 in [41]), CC, proposed by Gorodkin et al..<br>
<br>
SVM classification between true ncRNAs and shuffled data<br>
To predict ncRNAs on the basis of the pairwise alignment computed by <software>Cofolga2</software>, we trained SVM by using a SVM package software, <software>LIVSVM</software> (version 2.84) [42]. The elements of the feature vector for the SVM are as follows: OF, alignment length, and A, C, and U frequencies of the two sequences. These quantities except for the OF were calculated after eliminating all gapped columns of the alignment. The alignments &lt; 50 nt were removed from the input before SVM processing. This format of the feature vector is taken from the paper describing the ncRNA finding by <software>Dynalign</software> [43]. We use a default kernel (radial basis function kernel), and the prediction result of the SVM is outputted as a classification probability. To construct positive training and test datasets, we extracted 5,010 pairwise alignments from the k2 dataset of <database>BRAliBase2.1</database> [37]. The sequence identity of this dataset ranges from 16% to 75% and the dataset comprises thirtytwo RNA families. This original dataset was divided into two sub-datasets in a ratio of 1:2 (1,670 alignments for training, 3,340 alignments for test). Negative data were generated by removing all gapped columns of the positive alignments and shuffling the gap-free alignments. Two negative alignments were generated for each positive alignment, consequently we obtained 3,340 negatives for training and 6,680 negatives for test. The shuffling was performed by shuffle-aln.pl [34] with a '-m complete' option. After the training, we obtained a test accuracy of 87.7%.<br>
<br>
Visualization of ncRNA prediction performance<br>
When the performance of prediction methods depends on their own cutoff value, comparison of the methods becomes not straight forward, since varying the cutoff value leads to a simultaneous change of sensitivity and specificity (i.e. there is a tradeoff between sensitivity and specificity).<br>
In the present study, we used receiver operating characteristic (ROC) curve for visualizing the tradeoff between sensitivity and specificity for a range of cutoff value. The ROC curve has been used by Uzilov et al. to compare the performance of ncRNA finders [43].<br>
ROC curve is defined as sensitivity vs false positive rate plot; sensitivity and false positive rate are defined as follows:<br>
(9)false?positive?rate=(1?specificity)=FPTN+FP,sensitivity=TPTP+FN,specificity=TNTN+FP,<br>
where, TP, FP, TN and FN are the number of true positives, false positives, true negatives and false negatives, respectively. In the case of comparative ncRNA prediction, sensitivity indicates how many positive alignments (i.e. alignments containing true ncRNAs) are correctly predicted as ncRNA; false positive rate represents how many negative alignments are misclassified as ncRNA. For example, false positive rate = 1% means that one false positive is found when we evaluate 100 negative alignments.<br>
<br>
Genome sequences<br>
The genome sequences (excluding mitcondorial chromosome) of S. cerevisiae and the contigs of other six fungi (S. bayanus, S. castellii, S. kluyveri, S. kudriavzevii, S. mikatae, and S. paradoxus) were downloaded at <database>Saccharomyces Genome Database</database> (<database>SGD</database>) [44]. Annotated fasta files for S. cerevisiae (orf_coding.fasta, rna _coding.fasta, NotFeature.fasta, and other_features_genomic.fasta) were also downloaded at <database>SGD</database>. We masked the genome sequences of S. cerevisiae according to the other_features_genomic.fasta file to remove repetitive sequences from the genome sequences.<br>
<br>
Pairwise comparison of genomic sequences<br>
To efficiently search for ncRNA candidates with low sequence identity, we focused on our scan to the relatively short (50 bp to 2,000 bp) low-identity regions located between two regions which are conserved at sequence level. By exploring the regions neighboring such conserved regions, we can expect to find the ncRNAs hidden in a conserved synteny. The conserved regions were detected by using <software>WU-BLAST</software> [45] comparison (cutoff E-value = 10-3) between S. cerevisiae and the other fugal genomes. Then we constructed 'target regions', which are the regions scanned by <software>Cofolga2</software>, as follows. First, the S. cerevisiae genome sequence was divided into intergenic (NotFeature), orf_coding, and rna_ coding sequences in accordance with the annotations in <database>SGD</database> [44]. Then target region was defined for each divided S. cerevisiae sequence as illustrated in Figure 4 if the divided sequence overlaps the low-identity region located between the conserved regions. As a result, the target regions which we obtained by the <software>WU-BLAST</software> comparison cover 2,196,982 bp of the S. cerevisiae genome (this corresponds to 18% of all auto chromosomes of S. cerevisiae).<br>
The present approach for generating the target regions is similar to that used in the genome comparison between human and mouse by Torarinsson et al. [46]. Compared to their approach, however, ours is more conservative since it requires the target regions to be sandwiched by two conserved regions, while Torarinsson et al. scanned the regions neighboring to singly conserved regions. In other words, our definition is a subset of that of Torarinsson et al..<br>
We scanned each target region using a dual sliding window according to the following procedure. Let us call the two genome sequences belonging to a target region genome A and genome B. Subsequences were generated by moving a sliding window of 150 nt with a shift size of 50 nt on each genome sequence, and then all-vs-all pairwise alignment between the subsequences of genome A and those of genome B was performed with <software>Cofolga2</software>. After the comparison, each pairwise alignment was processed by the trained SVM to assign a SVM classification probability to discriminate whether the pairwise alignment contains ncRNA candidates or not.<br>
<br>
<br>
Results and discussion<br>
Convergence test with respect to GA population size and iteration number<br>
To know how the GA population size and iteration number affect the alignment quality, we studied the population size and iteration number dependence of the <software>Cofolga2</software>'s performance, where we define population size = iteration number to reduce the number of free parameters. Figure 5 shows the (mean SPS) ? (mean SCI) for the fourteen sequence pairs in Additional File 1 as a function of population size. As can be seen from the figure, the (mean SPS) ? (mean SCI) is almost saturated between population size 50 and 100. Based on this observation, we used population size (= iteration number) = 50 for the benchmarks and ncRNA discovery in the present study.<br>
<br>
RNA alignment benchmark and performance comparison with other methods<br>
Figure 6 shows the benchmark results for <software>Cofogla2</software> and other programs for structural or non-structural sequence alignment. In this benchmark, 5,010 pairwise alignments (? 75%id) taken from the k2 dataset of <database>BRAliBase 2.1</database> are used. The programs used for the comparison are summarized in Table 1. In this performance test, as can be seen from Figure 6, <software>Cofolga2</software> outperformed the light-weighted programs (<software>StrAl</software>, <software>LaRA</software>, and <software>LocARNA</software>, and the non-structural alignment programs) at ? 50%id in both SPS and SCI. In addition, <software>Cofolga2</software> showed a performance comparable with the other structural RNA alignment programs in SCI and was the second-best method between 30%id and 50%id in SPS, where <software>Foldalign</software> revealed the best performance. When the fourteen training sequence pairs were excluded from the dataset, the identical conclusion was obtained.<br>
Since GA is a sampling method utilizing random number, it is important to know how an initial random number affects the alignment quality. To examine random number dependence of <software>Cofolga2</software>, we performed five independent runs for the k2 dataset with different initial random numbers. As a result, we confirmed that the differences between the benchmark results due to the difference in initial random number are very small for a wide range of sequence identity (Additional File 2).<br>
In the benchmark with the <database>ITS2</database> dataset, we found that <software>Cofolga2</software> showed the best performance (the averaged CCs for <software>Cofolga2</software>, <software>Foldalign</software> 2.1.0, and <software>LocaRNA</software> are 0.42, 0.30, and 0.38, respectively).<br>
<br>
Computational time and memory usage<br>
The computational times (including those for the BPP computation by <software>RNAfold</software>) measured for <software>Cofolga2</software> and other structural RNA sequence alignment methods are shown in Figure 7. The computational times were measured with a Xeon PC (2.4 GHz/3 GB RAM/Red Hat Linux 9.0).<br>
For the RNA families shorter than approximately 85 nt, <software>Foldalign</software> showed computational times comparable with <software>Cofolga2</software>. For longer RNA families, <software>Foldalign</software> was much slower than <software>Cofolga2</software> except for K_ chan_ RES (the data of K_ chan_ RES can be found at 113 nt in Figure 7). The computational times of <software>Dynalign</software> were in general much longer than the other methods in the present benchmark. In addition, the computational times of <software>Foldalign</software> and <software>Dynalign</software> were not scaled monotonically with respect to sequence length. This could be due to the pruning algorithm of <software>Foldalign</software> and the constraint used in <software>Dynalign</software>, i.e. when these accelerators do not work well the programs become slower.<br>
<software>LocARNA</software> was faster than <software>Cofolga2</software> up to approximately 150 nt. For longer RNA families, however, the computational time of <software>LocARNA</software> became comparable with or longer than that of <software>Cofolga2</software>, e.g. average computational times of <software>Cofolga2</software> and <software>LocARNA</software> for Cobalamin (202 nt) were 7.5 sec. and 13.2 sec., respectively.<br>
To examine the memory usage of <software>Cofolga2</software>, we performed a structural alignment of two SRP _euk _arch RNAs, AP003253.3 (317 nt) and AC005275.1 (305 nt) taken from the <database>BRAliBase 2.1</database> k2-dataset (the file name of the sequence pair is included in Additional File 1). Since this computation is one of the largest calculations in the present study, we can estimate the upper bound of the memory usage from the result. Consequently, we found that <software>Cofolga2</software> needs only 10.0 MB RAM to perform the calculation. This memory usage is smaller than or comparable with those of other latest structural RNA sequence alignment methods. According to literature [47], <software>Foldalign</software> 2.1.0, <software>Dynalign</software> 4.5, and <software>LocARNA</software> 0.99 need at least 17.3, 13.3, and 7.6 MB RAM, respectively, to align the 5S rRNAs with an average sequence length of 119.4 nucleotides.<br>
<br>
SVM training results and ncRNA prediction benchmark<br>
In Figure 8, ROC curves by <software>Cofolga2</software>, <software>RNAz</software>, and <software>Dynalign</software> are plotted. To make the plot, first we ran each alignment program for the SVM test data, and then extracted the pairwise alignments satisfying (alignment length after removing gapped columns) ? 50 nt and %id ? 50%, where the sequence identity based on the <database>BRAliBase 2.1</database> alignments was used. As a consequence, each ROC curve was drawn based on approximately 4,700 RNA alignments. In Figure 8, "<software>ClustalW</software>+<software>RNAz</software>" indicates that an alignment is constructed by <software>ClustalW</software> and then the alignment is evaluated by <software>RNAz</software> to predict whether the alignment contains ncRNA candidates or not. We used <software>ClustalW</software> and <software>MAFFT</software> to construct input pairwise alignments for <software>RNAz</software>, since <software>ClustalW</software> is the standard sequence alignment program and <software>MAFFT</software> is the best non-structural sequence alignment method in accordance with the previous benchmark performed with <database>BRAliBase 2.1</database> [37]. In our comparison, we ran <software>Dynalign</software> 4.3 (maximum separation parameter M = 8 was used) not with the original SVM model trained in [43], but with a SVM model which was re-trained with the training dataset for the SVM model of <software>Cofolga2</software>. This is because the original SVM model of <software>Dynalign</software> was trained with only tRNA and 5S rRNA sequences, and the <software>Dynalign</software> with the original SVM model showed a poor prediction performance (data not shown) in our benchmark where more RNA families are included. In addition, we did not include <software>Foldalign</software> in the ncRNA prediction benchmark using <database>BRAliBase 2.1</database>, since the ncRNA prediction by <software>Foldalign</software> needs flanking sequences of a ncRNA sequence to obtain a statistical value [48], and the ncRNA sequences of <database>BRAliBase 2.1</database> do not have flanking sequences. As can be seen from Figure 8, <software>Cofolga2</software> outperformed <software>RNAz</software> when sequence identity is lower than 50%. Although the re-trained <software>Dynalign</software> showed better prediction results compared to <software>Cofolga2</software>, <software>Dynalign</software> was much slower compared to <software>Cofolga2</software>.<br>
When one performs a genomic scan, it is important to use a cutoff value which gives a very low false positive rate, since genome-scale calculations usually process a number of sliding windows containing negative data. To reduce the false positive rate as small as possible, we chose a cutoff PSVM = 0.9 whose sensitivity and false positive rate are 25.3% and 0.06%, respectively. <software>Cofogla2</software> with this cutoff PSVM gives a better false positive rate compared to <software>RNAz</software> (e.g. the false positive rate of <software>MAFFT</software>+<software>RNAz</software> was 2.9 times larger than that of <software>Cofolga2</software> at sensitivity = 27.5%).<br>
<br>
Comparative prediction of yeast ncRNAs<br>
We obtained 6,349 target regions whose average sequence lengths for S. cerevisiae and the other fungi are 446 bp and 980 bp, respectively. These target regions cover 2,196,982 bp of S.cerevisiae and 2,885,670 bp of the other fungi. After processing the 2,383,802 sequence pairs (generated from the target regions using a dual sliding window) by <software>Cofolga2</software> and the SVM we trained, we obtained 2,807 pairwise alignments which have SVM probabilities ? 0.9. The S. cerevisiae sequences of the obtained pairwise alignments were clustered into 'ncRNA candidate regions' by a single linkage clustering, where overlapped or neighboring sequences are clustered. The ncRNA search in yeast (with a PC cluster consisting of thirteen Pentium4 PCs) took approximately twenty days. The obtained candidate regions are summarized in Table 2. As shown in the table, we found ncRNA candidates at 714 intergenic regions, 1,311 protein-coding regions, and twenty known ncRNA regions in the S. cerevisiae genome. Based on the total number of alignments processed by <software>Cofolga2</software> and the false positive rate (0.06%) obtained in the benchmark, we estimated the number of false positive alignments = 1430, leading to (the estimated number of false positive alignments)/(the number of alignments predicted as ncRNA) = 51%. This value is almost same with the corresponding value obtained in the human ncRNA finding by <software>CMfinder</software> [24].<br>
In the present predictions, we obtained 53 intergenic regions, 43 protein-coding regions, and 12 known ncRNA regions as ncRNA candidates (Table 3), which overlap at least one of the previous <software>RNAz</software> and <software>QRNA</software> predictions; where we classified a candidate as an "overlapped" region if ? 10% of the nucleotides of the candidate overlaps an <software>RNAz</software> or <software>QRNA</software> prediction. Relatively small overlaps between our ncRNA candidates and those by <software>RNAz</software> and <software>QRNA</software> are not surprising because our method does not require sequence conservation of ncRNA candidates while <software>RNAz</software> and <software>QRNA</software> directly utilize the sequence similarity between ncRNA candidates. For example, the lowest sequence identity in the alignments containing our ncRNA candidates was 15% (Figure 9).<br>
Figure 10 shows the histogram of the GC content for all ncRNA candidates we predicted. In the previous comparative ncRNA predictions [5] in which <software>RNAz</software> and <software>EvoFold</software> have been used to predict human ncRNAs, it was found that there are biases in the base composition distribution of the predicted ncRNA sequences, i.e. <software>RNAz</software> favors GC-rich sequences, while <software>Evofold</software> tends to predict AU-rich ones as ncRNA candidates [49]. As indicated in Figure 10, our prediction result is not biased in GC content (the average GC content of whole DNA sequences of S. cerevisiae auto chromosomes is approximately 37%).<br>
The target regions we obtained includes 82 known ncRNAs. Of these, eighteen loci were included in the alignments with PSVM ? 0.9. The detail of the predicted known ncRNAs is summarized in Additional File 3. An estimated sensitivity for the ncRNA prediction calculated based on this observation is approximately 22%. It is noteworthy that <software>Cofolga2</software> correctly predicted the strand of fifteen known ncRNAs, i.e. correct strand was assigned to approximately 83% of the eighteen known ncRNAs. In these strand predictions, we adopted the strand with the highest PSVM when PSVM ? 0.9 was assigned to the both strands.<br>
Table 4 shows how many ncRNA candidates overlap the loci of the experimentally determined transcripts. In the table, experimental data taken from tilling array [50-52] and cDNA [53] are included. Here we found that the genomic positions of the results by David et al. and those by Miura et al. significantly (? 50% of the nucleotides of a candidate) overlap 112 and 69 ncRNA candidates of the present study, respectively, while our results show fewer overlaps with the transcripts by Davis et al. and Samanta et al.. Consequently, it turned out that 176 intergenic ncRNA candidates (this corresponding to approximately 25% of our intergenic candidates of S. cerevisiae) have at least one experimental support for their expression. By comparing the genomic positions of our results and the annotation (orf_coding.fasta) of <database>SGD</database>, we found that 95 and 71 intergenic candidates are located within 120 bp from the 5'-end and 3'-end of a CDS, respectively. In addition, eight intergenic candidates (SC000040I, SC000063I, SC000083I, SC000157I, SC000233I, SC000331I, SC000485I, SC000531I) are found at within 120 bp from the 5'- and 3'-ends of two CDSs, i.e. these eight candidates are sandwiched by two protein-coding genes. It is noted that the 5' and 3' ends of our ncRNA candidates can have an ambiguity of a few tens of nucleotides due to the gapped 5' and/or 3' edges of the pairwise alignments.<br>
Recently, the ncRNAs found in protein-coding regions have been reported. In our prediction, we obtained more than one thousand ncRNA candidates in protein-coding regions. In these, 628 candidates are predicted at sense strand, and 684 candidates were predicted at antisense strand. One ncRNA candidate (SC000407F) simultaneously overlaps two protein-coding genes as sense and antisense since these two protein-coding genes overlap each other.<br>
The detail of our prediction results and annotations can be browsed at our web server [40] in which the prediction results are retrieved through <software>MySQL</software> queries.<br>
<br>
ncRNA candidates conserved among multiple sequences<br>
By manually inspecting our prediction results, we found four intriguing examples containing conserved secondary structures across multiple species/sequences which have characteristic secondary structures in spite of their low average sequence identities. Figure 11 shows the alignment and structure of an intergenic S. cerevisiae sequence and two paralogous sequences of S. mikatae taken from the ncRNA candidate SC000056I. Since genomic separation between these two S. mikatae sequences are small (302 bp), these two S. mikatae sequences are a possible ncRNA cluster. Figure 12 shows the S. cerevisiae sequence of an intergenic ncRNA candidate (SC000383I) which was found at 39,052 bp to 39,158 bp of chromosome 3 and aligned with the sequences of S. mikatae and S. paradoxus. As can be seen from figures 11 and 12, these ncRNA candidates reveal characteristic consensus secondary structures in spite of their low average pair sequence identities (31%id for SC000056I and 28%id for SC000383I).<br>
The ncRNA candidate SC000983F is one of the longest regions predicted in the present study. This candidate contains a consensus secondary structure motif shared by three species (Figure 13). In addition to the structure shared by three species, ncRNA candidate SC000983F contains a relatively long (300 bp) secondary structure (Additional File 4) conserved between two species (S. cerevisiae and S. kudriavzevii). It is noteworthy that almost all sequences contained in SC000983F are antisense sequences. The sequences of S. cerevisiae are antisense sequences of a gene CLN1 coding G1 cyclin which is involved in regulation of the cell cycle, and the sequences of S. kluyveri and S. kudriavzevii are antisense sequences of predicted ORFs according to the <database>SGD</database> annotation. These results imply that this ncRNA candidate is a functional antisense ncRNA with characteristic secondary structures.<br>
The S. cerevisiae sequence of ncRNA candidate SC01074F is located at the sense strand of PMS1, a verified ORF coding an ATP-binding protein. The sequences of S. paradoxus and S. bayanus, which overlap the predicted ORFs of each genome according to the <database>SGD</database> annotation, are structurally aligned to the S. cerevisiae sense sequence in SC01074F (Figure 14). These sequences are new candidates of functional RNA secondary structure within a coding region such as the localization elements of ASH1 which do not show sequence conservation but harbor conserved RNA secondary structure [54]. The multiple alignments for SC000056I, SC000383I, SC000983F, and SC001074F were constructed by manual operation (including partial re-alignment by a progressive alignment with <software>Cofolga2</software>) based on the pairwise alignments by <software>Cofolga2</software>. Since the progressive multiple alignment using <software>Cofolga2</software> has not fully tested yet, we didn't benchmark it in this paper. The figures of consensus secondary structure and alignment were drawn by processing the multiple alignments at <software>RNAalifold</software> web server [34].<br>
The examples described in this section (SC000056I, SC000383I, SC000983F, and SC01074F) have at least one experimental evidence for their expression according to the tilling array/cDNA data in literature [50-53].<br>
<br>
<br>
Conclusion<br>
As can be known from a number of recent papers describing various structural RNA sequence alignment programs, it is a difficult problem to find a good RNA alignment with low sequence conservation. In the present study, we developed a new efficient GA for constructing structural RNA pairwise alignment with a new objective function and random alignment generation algorithm. The new GA is accurate and efficient in both time and memory usage, hence we applied it to the comparative ncRNA discovery between S. cerevisiae and related species using a SVM trained with the sequences and alignments taken from <database>BRAliBase 2.1</database>. As a result, we successfully obtained ncRNA candidates located at 714 intergenic regions and 1,311 protein-coding regions including antisense sequences, &gt; 92% of which is novel candidates since they show no overlaps with the genomic positions of the previous predictions and known ncRNAs. Indeed, our approach is not suitable for identifying all ncRNA sequences embedded in a genome, it gives a valuable tool complementary to the sequence-alignment-based ncRNA finders such as <software>RNAz</software> and <software>QRNA</software>, since the present method often found the ncRNA candidates which cannot be found by such sequence-alignment-based ncRNA finders. The results of the present study indicate that still a number of structured RNA transcripts with significant structural and evolutional signals is hidden in genomic sequences, and further exploration for novel ncRNAs using computational methods is inevitable to unveil the RNomics of genomes.<br>
<br>
Availability and requirements<br>
Non-profit, academic users can download and use the executable files at the <software>Cofolga2</software> website [40].<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2639440</b><br>
<software>SimHap GUI</software>: An intuitive graphical user interface for genetic association analysis<br>
<br>
<br>
Background<br>
While the growth in the volume of genetic data available has led to many new discoveries, it is becoming increasingly important to find ways in which to easily analyse large of volumes of data. This is certainly the case with genetic association studies, where high-throughput genotyping technologies have brought about the potential for hundreds of thousands of data points per individual subject [1].<br>
A graphical user interface (GUI) is still a rare feature amongst currently available genetic analysis packages, particularly those used to analyse single nucleotide polymorphisms (SNPs) or haplotypes. A well designed user interface would allow users without a comprehensive knowledge of statistical modelling or command line operation to perform complex analyses.<br>
Commercially available statistics software packages, such as <software>SPSS</software> (SPSS Inc., 2008) and <software>Stata</software> (StataCorp. 2008), may be useful, but are not specifically designed to analyse genetic data, requiring sophisticated prior knowledge for the end-user. Another major annoyance is the lack of integration between statistical and analytical packages [2], often with one program required for epidemiological analysis, a separate program for SNP analysis, and a third used for haplotype analysis.<br>
<package>SimHap</package> [3] is a statistical analysis package for genetic association testing, available in <software>R</software> [4], which amongst other features, infers haplotypes for unrelated individuals with unknown phase. Although various programs currently exist for haplotype analysis, <package>SimHap</package> is unique in a number of ways. It uses a multiple-imputation (MI) based approach to test for association, which incorporates information about uncertainty around inferred haplotypes. This approach uses current expectation maximisation (EM) methods for the estimation of haplotype frequencies from unphased genotype data [5]. To utilize the posterior distribution of diplotype (a haplotype pair) probabilities, the MI approach of Rubin [6] was implemented, where a series of "complete" data sets are generated containing all data from the original set as well as additional dummy variables for each haplotype, the values of which indicate the number of copies of that haplotype observed in an individual's diplotype (0, 1 or 2). For individuals with known phase (only one diplotype), the values for these haplotype variables remain constant for each of the generated data sets. For individuals with ambiguous phase, their haplotype values will be sampled from their predictive distribution, containing only those diplotypes consistent with their genotypes. This is a novel approach that provides an empirical distribution of the haplotypic effects and their significance levels.<br>
We have developed <software>SimHap GUI</software> as an intuitive graphical tool for conducting genetic association analysis. At its core, <software>SimHap GUI</software> utilises the <package>SimHap</package> <software>R</software> package and the R statistical language. <software>SimHap GUI</software> is a novel custom-designed integrated tool for conducting epidemiological, single SNP and haplotype-based association analyses within a single application, and provides a free alternative to commercially available statistics packages.<br>
<br>
Results and discussion<br>
Implementation<br>
<software>SimHap GUI</software> is written in Java (requires Java 1.5+) and will operate on platforms where Java is available. This tool has been successfully tested on Windows, Linux and MacOS operating systems. <software>SimHap GUI</software> requires an installation of the R statistics lanuguage (2.4.0+) and an installation of the <package>SimHap</package> <software>R</software> package. This tool runs optimally on a computer with a monitor resolution of 1024 ? 768, at least 128 Mb of RAM and a Pentium 4+ CPU. <software>SimHap GUI</software> has been successfully operated on datasets with thousands of individuals, hundreds of phenotype variables, and thousands of SNPs. <software>SimHap GUI</software> is generally only limited by the amount of system memory available to Java.<br>
The <software>SimHap GUI</software> interface is written in Java Swing, and uses the Synthetica look-and-feel suite [7] to enhance the useability and functionality of the interface (compared with standard Swing interfaces). We have also utilised the Swing Worker [8] library, which provides a mechanism for providing updates to the user interface while running long analytical tasks, such as performing thousands of haplotype simulations. Both Synthetica and Swing Worker are provided with the <software>SimHap GUI</software> installation. <software>SimHap GUI</software> is provided as a single cross-platform installer, using the <software>IzPack</software> [9] packaging system, which provides a simple standardised graphical installer tool that both technical and non-technical users will be comfortable with.<br>
<br>
Graphical User Interface (GUI)<br>
<software>SimHap GUI</software> allows the user to conduct association analysis of binary, quantitative, longitudinal and survival (right-censored) outcomes using phenotypic data, and genetic SNP data and haplotype data, in unrelated individuals.<br>
One key feature of <software>SimHap GUI</software> is the workflow interface, which guides the user through each logical step of the analytical process. This workflow concept is central to providing an intuitive user interface accessible to both novice and advanced users.<br>
The user initially selects a standard comma separated value (CSV) file containing phenotypic information for a set of individuals (one row of data per person), as can be obtained from most spreadsheet and statistics software. The user also selects a CSV file containing genotypes for a series of SNP markers for the same individuals (not required for non-genetic modelling), and selects the character(s) signifying missing data in the input files. <software>SimHap GUI</software> examines the input files to ensure correct formatting, completeness, and the correct corresponding individual identifier between phenotype and genotype files. Genotype files are examined to ensure biallelic SNPs are input, where the user is given the option to remove multi-allelic markers. Once data checking is complete, the user can choose to perform epidemiological modelling (without genetic markers), single SNP association analysis, or haplotype association analysis. Users are guided through each of these analytical tasks in a straight-forward series of steps, with a standardised model building screen central to each of the analysis types.<br>
Figure 1 is an example of the model building screen for a single SNP analysis with a quantitative outcome using <software>SimHap GUI</software>. At the top of the screen, hdl (cholesterol) has been selected as the outcome of interest, with the outcome normally distributed (Untransformed). Log base 10 and natural log of the outcome are available to transform non-Normally distributed outcomes. In the MAIN EFFECTS section are the available and selected covariates for this model, namely sex, age, bmi and smoke. Covariates can also be added as squared or cubic terms, logged (base 10 or natural log), and as factors (for categorical terms). In the GENOTYPES section are the available and selected SNPs to be analysed in the model. SNP covariates are denoted with the S_ prefix, while the _add, _dom and _rec terms refer to analysing the SNP under an additive, dominant or recessive genetic model. SNPs can also be analysed under a codominant model by adding the SNP as a factor. In the INTERACTIONS section are available and selected covariate terms to be analysed for statistical interactions; in this case, an interaction between sex and SNP_1 under a codominant model. Additional files 1, 2, 3, 4, 5, 6, 7, 8 provide a graphical representation of each of the phases of analysis for an example single SNP analysis. The <software>SimHap GUI</software> software manual also provides a detailed description of the analysis process.<br>
<br>
Case Studies<br>
<software>SimHap GUI</software>, and its earlier Beta 1 and Beta 2.1 releases, have been extensively utilised in a range of genetics projects recently published.<br>
In the area of cancer research, <software>SimHap GUI</software> has been used in studies such as Sak et al [10], to examine the association between polymorphisms in the XPC gene and bladder cancer susceptibility. Choudhury et al [11] also examined haplotypes of DNA repair proteins to find genetic variants that may modulate predisposition to bladder cancer.<br>
<software>SimHap GUI</software> has been used extensively in the field of cardiovascular disease genetics. Several studies has used this tool to examine SNP and haplotype effects of genes related to abdominal aortic aneurysm [12-14]. Studies by both Horne et al [15] and McCaskie et al [16] have used <software>SimHap GUI</software> to investigate the association between genetic variation in the cholesteryl ester transfer protein gene and cardiovascular disease. <software>SimHap GUI</software> has also been used to investigate SNP and haplotype associations with metabolic syndrome [17-20] and atherosclerosis [21-24] related outcomes.<br>
In the area of genetic epidemiology related to the Mendelian Randomization (MR) technique, a number of groups have utilised <software>SimHap GUI</software>. Brunner and colleagues [25] used <software>SimHap GUI</software> to generate haplotypes for three tagging polymorphisms from the C-reactive protein (CRP) gene in a study of 5,274 men and women. Studies by Lawlor et al [26] and Kivimaki et al [27] similarly this software for analysis of CRP mutations using MR.<br>
Other diverse studies include the use of <software>SimHap GUI</software> to investigate genetic influences of the melanocortin 1 receptor with sensitivity to photochemotherapy [28], polymorphisms within the macrophage migration inhibitory factor with relation to acute lung injury in patients with sepsis [29], associations between cytokine polymorphisms and outcomes after renal transplantation [30], and genetic predictors for the development of microalbuminuria in children with type 1 diabetes [31].<br>
The wide range of example publications described here highlights the significance of the <software>SimHap GUI</software> software providing an easy-to-use powerful interface for both novice and advanced genetic association analyses.<br>
<br>
GUI versus <software>R</software> package<br>
One of the critical distinctions to make with the <software>SimHap GUI</software> software is the difference between the <package>SimHap</package> <software>R</software> package, and the Java based interface described in this manuscript. The backend <package>SimHap</package> <software>R</software> package simply provides the statistical operations to conduct particular analytical tasks, with the onus on the user to have technical knowledge of the statistical methods being employed and expertise with the command line interface of the R language. Users who are not professional statisticians may be discouraged by the difficulty of operating under a command-line interface.<br>
The <software>SimHap GUI</software> interface provides the functionality, accessibility and the guided analytical approach that cannot be found in the command line package. The user interface is designed around the premise of a workflow analysis model, which mimics the logical analytical processes required to conduct a particular statistical test. This user-friendly, intuitive interface has been designed to satisfy the needs of both the technical and non-technical statistical user, and does not require sophisticated informatics knowledge to operate. Using the novel model building interface, users can perform tasks ranging from simple univariate linear modelling, through to more sophisticated tasks such as multivariate modelling of longitudinal outcomes with gene:gene and gene:environment interactions. A standardised interface is provide for users to conduct epidemiological (no genetics factors), single SNP and haplotype association analyses.<br>
Features of <software>SimHap GUI</software> that are not provided in the <package>SimHap</package> <software>R</software> package include: an intuitive GUI for model building and guiding the overall analysis process; sophisticated data checking of phenotype and genotype data; automatic conversion of data for single SNP and haplotype association analysis; automatic calculation of allele frequencies and genotype distribution; quantile-quantile plotting for Normality of quantitative traits; and real-time estimation of the haplotype imputation simulation progress. <software>SimHap GUI</software> implements all of the functions from the <package>SimHap</package> <software>R</software> package.<br>
<br>
<br>
Conclusion<br>
In summary, <software>SimHap GUI</software> provides a cross-platform, intuitive and integrated interface for conducting a range of genetic and non-genetic association analyses.<br>
<br>
Availability and requirements<br>
- Project name: SimHap GUI<br>
- Project home page: <br>
- Operating system(s): Platform independent (tested on Windows, Linux and MacOS)<br>
- Programming language: Java<br>
- Other requirements: Java 1.5+; <software>R</software> 2.4.0+ (available from ); <package>SimHap</package> <software>R</software> package from <database>CRAN</database> (available from )<br>
- Licence: Free for non-commercial use<br>
<br>
Authors' contributions<br>
KWC designed and developed the Java GUI interface. PAM assisted with integration of statistical methods and aided with design of the GUI. LJP supervised the design and coordinated the development of the software.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2691739</b><br>
Most parsimonious haplotype allele sharing determination<br>
<br>
<br>
Background<br>
With the completion of the Human Genome Project, coordinated effort has made available millions of single nucleotide polymorphisms (SNPs). These SNPs represent many of the genetic variants in the human genome and they will greatly facilitate the identification of genetic variants underlying human diseases, the goal of association studies. Indeed, under the "common disease ? common variant (CD-CV)" hypothesis, genome-wide association studies (GWASs) have achieved numerous successes in the last three years, particularly in genetic mapping in human diseases [1]. For example, reproducible associations have been described for many human common conditions and diseases, such as obesity [2], diabetes [3], and rheumatoid arthritis [4]. The genetic markers revealed by these studies may provide insights into the underlying molecular pathways, and may lead to novel strategies for disease diagnosis, treatment, and prevention.<br>
In general, there are three different kinds of association studies: case-control, categorical disease outcomes, and quantitative (continuous). Each association study may deal with only a single SNP and multiple SNPs. It appears that the first two kinds of association studies are much easier than the quantitative ones, and in fact the past successes are all on the first two kinds. For the last kind, most association study methods are regression based, and they become either ineffective (undesirable results) or inefficient (exceptionally long computational time) with increasing numbers of SNPs [5]. It is recognized that, despite the many achieved successes, the power of the association study methods are nevertheless still low, and there remain many more important diseases to be studied, particularly quantitative ones.<br>
The major issue in the current technical themes of GWASs is the data dimensionality, where the number of samples is far less than the number of genotyped SNPs. This issue becomes particularly severe when dealing with rare diseases. Impacted by linkage disequilibrium (LD), and that the human genome can be partitioned into large blocks with high LD and relatively low recombination (or crossover, or breakpoint), separated by short regions of low LD [6-8], SNP tagging has been proposed to reduce the number of SNPs to the minimum while retaining as much the genetic variation of the full SNP set as possible. However, in practice, tagging is only effective in capturing common variants. A popular and prosperous strategy, suggested by this block-like structure of the human genome, is to use haplotypes to try to capture the correlation structure of SNPs in regions of little recombination [9-11]. This approach can lead to analyses with significantly reduced degrees of freedom and, more importantly, haplotypes are able to capture the combined effects of tightly linked causal variants.<br>
Haplotypes are very expensive to assay. For the vast majority of applications that involve large numbers of samples, only genotype data are available through high-throughput genotyping technologies. One of the potential problems underlying the haplotype-based association study methods is that haplotypes are not observed but rather inferred, and it can be difficult to account for the uncertainty that arises in phase inference when assessing the overall significance of the association. One solution to this problem is to determine the haplotype allele sharing status among all members in the study [10,12]. In particularly, it is expected that the availability of high density SNP genotype data can be used to unambiguously determine the haplotype allele sharing.<br>
In this work, we demonstrate that, for pedigree genotype datasets, such haplotype allele sharing can indeed be deterministically, efficiently, and accurately determined, even for very small pedigrees. This confirms that haplotype-based association studies are promising for flexible and interpretable analyses that exploit evolutionary insights.<br>
We determine the haplotype allele sharing status among pedigree members using two distinct parsimonious haplotyping algorithms with different optimization objective functions: one minimizes the total number of crossovers to explain the pedigree genotype data by Mendelian inheritance rules; the other minimizes the total number of crossover sites, or equivalently, minimizes the number of maximal zero-recombination chromosomal regions. Both haplotyping results give unambiguous haplotype allele sharing status among the members, as well as the shared haplotype alleles, though the phases for each individual might not be completely determined. These shared haplotype alleles can provide additional support for mapping phenotype genes [13-19], and they may also lead to insights on the factors influencing the dependencies amongst genetic markers, i.e. linkage disequilibrium. Such insights may prove essential to understanding genome evolution [20,21] (see the <database>International HapMap Project</database> [22]).<br>
There is a rich and growing literature on haplotype inference, or haplotyping. Some works focus on unrelated individuals [23-30]; Research on related individuals include those based on either exact-likelihood computations [31-34], approximate-likelihood computations [31,32,35], or rule-based strategies [12,36-41]. Conceivably, all of these haplotyping algorithms, methods and programs have elements in common and have their own strengths and weaknesses. For example, the likelihood-based methods are in general limited to a small number of markers and small pedigrees, owing to the extensive computations required. Additional information and assumptions, such as marker recombination rates and Hardy-Weinberg equilibrium, are generally required to calculate the likelihood. The rule-based methods are ad hoc but they rely on fewer assumptions and generally run faster than likelihood-based methods. Most recently, Lin et al. developed a greedy haplotyping algorithm that takes advantage of the high density SNP markers [12]. This algorithm is incorporated into the <software>i Linker</software> program, which determines the haplotype allele sharing among pedigree members. This method will be used in this study. Essentially, <software>i Linker</software> uses a minimum number of breakpoints to explain the genotype data, in the presence of missing genotypes and genotype errors. Then, during the data interpretation process, parental haplotype phases are revised when more members are added, as long as the revision reduces the total number of breakpoints and still explains the genotype data. The substantial simulation study in [12] has previously demonstrated its efficiency, effectiveness, and reliability.<br>
The computational problem of finding an optimal haplotype configuration (i.e., containing a minimum number of breakpoints) for a pedigree genotype dataset is in general NP-hard [42]. That is, there is unlikely a polynomial time algorithm that guarantees reconstruction of an optimal haplotype configuration for any pedigree genotype dataset. When no recombination events are allowed, the haplotype inference becomes easy. In the absence of missing genotype data, Li and Jiang [42] proposed a polynomial time exact algorithm for this zero-recombination haplotype configuration (ZRHC) problem, to reconstruct all compatible haplotype configurations without recombination. This algorithm, implemented within <software>PedPhase</software> [42], runs in O(m3n3) time, where m is the number of SNPs under consideration and n is the number of non-founder members in the pedigree. Following several major advances [43,44], Liu and Jiang [45] recently designed an O(mn) time algorithm which generates a particular solution to the ZRHC problem and, in O(mn2) time, it generates a general solution (or all solutions). Other works on the ZRHC problem include Cox et al. [46], Haplore by Zhang et al. [47], and ZRHI by Wang et al. [48]. It should be mentioned that one immediate application of ZRHC is the association studies that involve many tightly linked markers in a small chromosomal region. Within such a small region, recombination is an unlikely event and thus it is reasonable to assume that there is no recombination among these markers across the pedigrees studied [8].<br>
In this work, we extend the <software>PedPhase</software> zero-recombination haplotyping algorithm to <software>xPedPhase</software> to determine all maximal zero-recombination chromosomal regions, as well as their respective haplotype configurations, in one whole genome scan. Subsequently, the haplotype allele sharing status among the pedigree members at each maximal zero-recombination chromosomal region can be determined. Note that it is infeasible to assume no crossover events for a whole chromosome. One may run the original <software>PedPhase</software> multiple times on all possible chromosomal intervals to identify the maximal zero-recombination chromosomal regions. However, this would take a prohibitive amount of time as <software>PedPhase</software> runs in cubic time. Therefore, our extension <software>xPedPhase</software> to determine all maximal zero-recombination chromosomal regions in one whole genome scan is non-trivial. We note that this way of haplotype allele sharing determination differs from that by <software>i Linker</software> in that, when a subset of members share a haplotype allele, the haplotype allele is maximally extended to the point where the sharing on the particular chromosomal region changes. In other words, the number of chromosomal regions of the same sharing status is minimized in <software>xPedPhase</software>, and so the determined haplotype allele sharing is the most parsimonious. We see this as an advantage over <software>i Linker</software>, which only deduces haplotype allele sharing from the greedy haplotyping results. Afterwards, the shared haplotype alleles and their sharing information can readily be used in various association studies [9-11].<br>
<br>
Results<br>
Breakpoint recovery<br>
In the genotype data simulation process (see "Methods" for details), a simulated parental breakpoint could arise between two consecutive homozygous SNP sites and thus it is not possible to be precisely recovered by any computational haplotyping algorithms. In addition, <software>i Linker</software> is a greedy program that introduces a breakpoint only if necessary; and <software>xPedPhase</software> minimizes the number of zero-recombination chromosomal regions. That is, both programs work in certain most parsimonious ways.<br>
We define the following measures of effectiveness for our haplotype allele sharing status determination process. A simulated breakpoint is classified as recovered if there is an inferred breakpoint (by <software>i Linker</software> or <software>xPedPhase</software>, respectively) that (1) is identical to the simulated breakpoint, or (2) can be "moved" to the simulated breakpoint site [12], i.e., the parental SNPs in between these two sites are all homozygous. Subsequently, the breakpoint recovery precision is defined as the number of simulated breakpoints being recovered (true positives) divided by the number of breakpoints generated by <software>i Linker</software> or <software>xPedPhase</software> (true and false positives). The breakpoint recovery recall is defined as the number of simulated breakpoints being recovered (true positives) divided by the number of simulated breakpoints (true positives and false negatives).<br>
For each of the 10 distinct pedigrees tested, we used 5 sets of real, unrelated, chromosome 1 genotype data obtained from GeneChip Human Mapping 10 K Xba [49], as well as 5 sets from 50 K Xba [12] arrays to assign haplotypes for founders, by randomly specifying the paternal and the maternal SNP alleles at the heterozygous sites. There were, in total, 877 and 4, 658 SNP sites in the 10 K and 50 K data, respectively. For each set (and each pedigree), we simulated 10 instances. The breakpoint recovery precision and recall associated with each pedigree are computed as the averages over the 50 corresponding instances. These values on the 10 K and 50 K data are collected in Tables 1 and 2, respectively, where pedigree n1-n2 [-n3] has n1(n2, [n3,] respectively) members in the first (second, [third,] respectively) generation. The overall breakpoint recovery precision and recall by <software>i Linker</software>, averaged over 500 10 K simulated instances, were98.4% and 96.4%, respectively; The corresponding ones by <software>xPedPhase</software> were 91.2% and 97.8%, respectively. On 500 50 K instances, <software>xPedPhase</software> were not able to return results for pedigrees 2-2 and 2?3 (see "Discussion" for some explanations); Its overall breakpoint recovery precision and recall, averaged over 400 50 K simulated instances, were 95.7% and 98.3%, respectively. <software>i Linker</software> finished all of the 50 K instances, achieving overall breakpoint recovery precision and recall of 99.6% and 97.2%, respectively.<br>
We also collected the breakpoint recovery results by the Block-Extension algorithm inside the <software>PedPhase</software> package [50]. Due to the fact that it has a limit on the number of SNPs, Block-Extension was ran only on the 500 simulated 10 K genotype datasets. Overall its running time was very similar to <software>i Linker</software>, that both programs ran in seconds. Its precision and recall are collected in Table 1, with the average breakpoint recovery precision 21.3% and recall 99.8%. An interesting observation is that the Block-Extension algorithm always generated about 5 times more breakpoints than the simulated ones, which is one of the main causes for low precisions.<br>
<br>
Haplotype allele sharing recovery<br>
For each simulated pedigree genotype dataset, the simulated mutation region was recorded (for the case-control association study; such a region is shared by all diseased members but no healthy members). We also recorded all the chromosomal regions that are exclusively shared by all the diseased members in the simulated haplotype dataset, before transforming it into the genotype dataset. We call them the simulated shared regions. Note that the simulated mutation region is always contained in one of the simulated shared regions. After running <software>i Linker</software> and <software>xPedPhase</software>, based on the reported haplotype allele sharing, we identify all the chromosomal regions that are shared by all and only diseased members too, which we call the discovered shared regions. If the discovered shared regions contain the simulated mutation region, then the simulated mutation region is recovered.<br>
Among the 500 simulated 10 K genotype datasets, <software>xPedPhase</software> missed 14 simulated mutation regions, 10 of which are among the 23 ones missed by <software>i Linker</software>. That is, the simulated mutation region recovery accuracies by <software>xPedPhase</software> and <software>i Linker</software> were 97.2% and 95.4%, respectively. The Block-Extension algorithm missed 102 simulated mutation regions, achieving a significantly lower accuracy of 79.60%. For the 400 simulated 50 K genotype datasets, <software>xPedPhase</software> missed only 4 simulated mutation regions; <software>i Linker</software> missed 2 additional ones but it recovered 100 more instances; These gave <software>xPedPhase</software> and <software>i Linker</software> the simulated mutation region recovery accuracies of 99.0% and 98.8%, respectively.<br>
For each simulated dataset, we compared the simulated shared regions and the discovered shared regions, by <software>i Linker</software> and <software>xPedPhase</software> separately, to determine whether or not each simulated shared region was recovered by checking if they overlap or not. If a simulated shared region was not recovered, then the corresponding discovered region was set to [-1, -1]. For the 500 simulated 10 K genotype datasets, there were 725 simulated shared regions in total. 7 of them were not recovered by either <software>xPedPhase</software> or <software>i Linker</software>; 2 additional were not recovered by <software>xPedPhase</software> and 5 additional were not recovered by <software>i Linker</software>. We collected the starting SNP site and the ending SNP site for each of these simulated shared regions (x-axis) and those for the corresponding discovered shared region (y-axis) by <software>xPedPhase</software> and <software>i Linker</software>, respectively, and plotted them in Figures 1, 2, 3 and 4. Essentially, these plots show the extent to which the discovered shared regions are off the simulated shared regions. The correlation coefficients between the two sets of starting and ending sites of recovered shared regions were 0.99981 and 0.99989 by <software>xPedPhase</software>, and 0.99980 and 0.99981 by <software>i Linker</software>. On 400 50 K datasets that both <software>i Linker</software> and <software>xPedPhase</software> finished, every shared region was recovered by <software>xPedPhase</software> and <software>i Linker</software> missed only two. The correlation coefficients were 0.999993 and 0.999928 by <software>xPedPhase</software>, and 0.999988 and 0.999983 by <software>i Linker</software>, respectively (the scatter plots are not included since they are basically straight lines).<br>
<br>
<br>
Discussion<br>
<software>i Linker</software> vs. <software>xPedPhase</software>: breakpoint recovery<br>
Both <software>i Linker</software> and <software>xPedPhase</software> determine the haplotype allele sharing status among pedigree members through partial haplotyping. That is, <software>i Linker</software> repeatedly runs a greedy haplotyping algorithm on the smallest nuclear families in the pedigree, with an objective function designed to minimize the total number of breakpoints. <software>xPedPhase</software>, on the other hand, repeatedly searches for the maximal zero-recombination chromosomal regions along the chromosome, and thus minimizes the total number of breakpoint sites. Both programs therefore determine the haplotype allele sharing in certain most parsimonious ways.<br>
From our extensive simulation study, we found that <software>xPedPhase</software> generated slightly more breakpoints per meiosis than <software>i Linker</software>. For example, on average, the number of simulated breakpoints per meiosis (disregarding male and female difference) was 2.38, and the number of breakpoints per meiosis generated by <software>i Linker</software> was 2.30, which is slightly less, likely due to its greedy nature. But the number of breakpoints per meiosis generated by <software>xPedPhase</software> was 2.76, greater than 10% more than simulated. Nevertheless, among these 2.76 breakpoints per meiosis by <software>xPedPhase</software>, 2.35 were actually true positives; while among the 2.30 breakpoints per meiosis by <software>i Linker</software>, 2.27 were true positives. This explains the slightly higher recalls by <software>xPedPhase</software> than <software>i Linker</software> ? <software>xPedPhase</software> introduced a few more breakpoints, of which some were true positives though the others were false positives. Also interestingly, for almost all instances, the numbers of breakpoint sites by <software>xPedPhase</software> were equal to the numbers of breakpoints by <software>i Linker</software>. This fact explains why the correlation coefficients of starting (and ending, respectively) SNP sites between the simulated shared regions and the shared regions discovered by <software>xPedPhase</software> and <software>i Linker</software> are nearly identical.<br>
<br>
Mutation region recovery<br>
Among the 500 simulated 10 K genotype datasets, <software>xPedPhase</software> missed 14 simulated mutation regions and <software>i Linker</software> missed 23. They both missed the simulated mutation region on 10 datasets. (On the other hand, the Block-Extension algorithm missed 102, a much larger number of, simulated shared regions.) We carefully examined these 10 datasets and found out a common pattern of the simulated mutation regions. These simulated mutation regions were short, containing only 2 to 4 SNPs, and the specified diseased haplotype allele was not unique, i.e., when this allele was paternal, there was a maternal allele exactly the same. Such a phenomenon is caused by our simulation process, which does not do the checking for uniqueness. The consequence is that, the diseased allele was not exclusively found in diseased members but rather, it was shared among some healthy members. Therefore, none of <software>i Linker</software> and <software>xPedPhase</software> were able to recover it.<br>
<br>
SNP density<br>
From the simulation study results, one can see that, using higher density SNP makers, 50 K over 10 K in our case, both <software>i Linker</software> and <software>xPedPhase</software> performed better, in terms of the breakpoint recovery and the haplotype allele sharing recovery. In particular, on 50 K genotype datasets, the discovered shared chromosomal regions exclusive to all the diseased members were almost identical to the simulated shared regions, achieving all higher than 0.999 correlation coefficients. This fact is certainly desirable in the case-control association studies. On the other hand, both <software>i Linker</software> and <software>xPedPhase</software> also performed very well on 10 K genotype datasets. This hints that they can be useful in haplotype-based association studies on species, such as cattle [51] and soybean [52], for which no high but only medium density SNP arrays are available.<br>
<br>
<software>i Linker</software> vs. <software>xPedPhase</software>: running time<br>
The zero-recombination haplotyping algorithm inside <software>PedPhase</software> runs in O(m3n3) time, where m is the number of SNP makers and n is the size of the pedigree. <software>xPedPhase</software> thus needs cubic time as well on each maximal zero-recombination chromosomal region. For the pedigrees used in the simulation study, preliminary testing using a zero-recombination segment of more than 600 SNPs caused the program to either crash or run for hours. In the final batch run of the programs to collect results, several restarts were required on simple pedigrees such as 2-2 and 2?3, even on 10 K instances. Program <software>i Linker</software> did not have the running time issue, where it always returned a solution within seconds. <software>xPedPhase</software>, on the other hand, could not return results on most of the 50 K instances for pedigrees 2-2 and 2?3. Therefore, the collected results for <software>xPedPhase</software> on 50 K data are only for 8 pedigrees.<br>
Given that <software>xPedPhase</software>, which non-trivially employs the cubic time zero-recombination haplotyping algorithm, could run for hours on small pedigrees, part of our future attention is to implement the linear time algorithm by Liu and Jiang [45], which does the same zero-recombination haplotyping but was described as difficult to implement. Eventually, one might want to design a novel linear time zero-recombination haplotyping algorithm that, similarly in one whole genome scan, determines all maximal zero-recombination chromosomal regions, together with their zero-recombination haplotyping solutions.<br>
<br>
Choice of <software>i Linker</software> or <software>xPedPhase</software><br>
<software>i Linker</software> performed better in terms of breakpoint recovery precision, but it had slightly lower recalls than <software>xPedPhase</software>. <software>xPedPhase</software> seemingly generated more breakpoints, some of which picked up the simulated ones. Both <software>i Linker</software> and <software>xPedPhase</software> have the algorithmic nature to push breakpoints to the end of the chromosome, which is validated from the scatter plots of the starting and ending SNP sites of the discovered shared regions versus the simulated ones (Figures 1, 2, 3, 4). Also, in terms of shared region recovery, <software>xPedPhase</software> did better than <software>i Linker</software>, possibly due to the more breakpoints it generated.<br>
<software>i Linker</software> accepts datasets containing genotype errors, and it has a step to correct not only those sites violating Mendelian inheritance rules but also pairs of unlikely close breakpoints, in terms of their physical distance. In fact, there were instances in which two adjacent breakpoints on a member are less than one million basepairs apart, and <software>i Linker</software> smoothed the region by revising the genotype data. This is another reason that <software>i Linker</software> missed several very short simulated shared regions. Note that <software>PedPhase</software>, and consequently <software>xPedPhase</software>, does not tolerate any genotype errors, neither missing data.<br>
On each of the 1, 000 instances, disregarding the SNP density, <software>i Linker</software> ran in only seconds. The running time of <software>xPedPhase</software> varied a lot, from seconds to minutes to hours. Given a pedigree, the running time of <software>xPedPhase</software> is determined by the length of the chromosomal region under consideration. When the pedigree is not too small, the lengths of zero-recombination chromosomal regions were only tens or hundreds and <software>xPedPhase</software> was able to deal with them in seconds to minutes too. For pedigrees 2-2 and 2?3, such lengths could be thousands. We had waited for days without the complete results and thus terminated <software>xPedPhase</software>. In summary, <software>xPedPhase</software> performed slightly better, in particular when we do not want to miss many breakpoints, at the cost of longer running time.<br>
<br>
Dealing with missing genotype values in <software>i Linker</software><br>
Note that <software>PedPhase</software> does not tolerate any missing genotype data, neither errors. Consequently, <software>xPedPhase</software> does not work on datasets with missing data or errors. But <software>i Linker</software> deals with missing genotype data, by ignoring them during the haplotyping process and then imputing them using the haplotype inheritance. Conceivably, such a way of treating missing genotype data would reduce the haplotyping accuracy. On each of the 1000 simulated genotype datasets, we manually erased a portion of data points, at 0.5%, 1%, 1.5%, 2%, 2.5% and 3% respectively, and then ran <software>i Linker</software> to collect its breakpoint recovery and the mutation region recovery. The breakpoint recovery precision and recall are collected in Tables 3 and 4, respectively, for 10 K and 50 K datasets. Clearly seen, while precision remained largely the same, the recall dropped a little with increasing missing rates. Nevertheless, overall these small percentages of missing genotype data did not affect the breakpoint recovery accuracies much. However, the mutation region recovery by <software>i Linker</software> could drop a lot when the SNP density is low. For example, 23, 28, 29, 42, 52, 56, 56 simulated mutation regions were missed among the 500 10 K datasets with 0%, 0.5%, 1%, 1.5%, 2%, 2.5%, and 3% missing data points, respectively; while only 6, 11, 9, 12, 9, 11, 10 simulated mutation regions were missed among the 500 50 K datasets, respectively.<br>
<br>
Other possible applications<br>
Studies have shown that the human genome can be partitioned into large blocks with high LD and relatively low recombination, separated by short regions of low LD [6-8]. The same things are also expected for other species such as cattle and soybean. In the cattle breeding industry, normally small to medium size pedigrees can be easily collected. By running our haplotype allele sharing status determination programs on these pedigrees, we will be able to locate those crossover sites for each pedigree. These results can thus be used to compose the map of crossover sites and thus identify the crossover hotspots along the genome. We expect that our programs will be useful in many genomics selection projects, for example, to provide deterministic haplotype allele sharing and shared haplotype alleles for various quantitative trait locus (QTL) identification and quantitative association studies.<br>
<br>
<br>
Methods<br>
Haplotype allele sharing by <software>xPedPhase</software><br>
<software>PedPhase</software> is a haplotyping program consisting of four algorithms [42]. The constraint-finding algorithm first determines whether a pedigree genotype dataset has zero-recombination haplotype configurations and identifies all such configurations if it does. More precisely, the algorithm first identifies all necessary (and sufficient) constraints on the haplotype configurations derived from the Mendelian inheritance rules and the zero-recombination assumption, represented as a system of linear equations on binary variables over the cyclic group ?2 (i.e., integer addition module 2). It then solves the equations to obtain all consistent haplotype configurations satisfying the constraints, using a simple method based on Gaussian elimination. These consistent haplotype configurations are shown to be feasible zero-recombination solutions. The running time for representing and solving the equations is O(m3n3), where m is the number of SNPs under consideration and n is the number of non-founder members in the pedigree, and the time for enumerating all configurations is proportional to the number of feasible zero-recombination solutions.<br>
This cubic running time is due to the Gaussian elimination procedure that is employed to solve a system of (at most) 2(m - 1)n linear equations. When considering one more SNP site, the increase in the number of equations is at most 2n. This observation leads to our extension of the <software>PedPhase</software>, denoted as <software>xPedPhase</software>, for one whole genome scan to determine all maximal zero-recombination chromosomal regions. Let M denote the total number of SNPs on the chromosome under consideration. We first run <software>PedPhase</software> on the foremost two SNPs. If the Gaussian elimination procedure reaches no solution, then there is a breakpoint in between these two sites, and we proceed to run <software>PedPhase</software> on the second and the third SNPs. Otherwise, generate (at most) 2n linear equations by considering the last SNP and the next SNP which has not been examined. Append these new linear equations to the existing reduced system, and continue applying the Gaussian elimination procedure. Again, if the procedure reaches no solution, then there is a breakpoint in between the last two SNP sites, and we proceed to run <software>PedPhase</software> on the last SNP and the next SNP which has not been examined. Otherwise, generate (at most) 2n linear equations by considering the last SNP and the next SNP which has not been examined, and so on.<br>
Note that in order to output the haplotype configurations for each maximal zero-recombination chromosomal region, we need to save the last set of solutions before we consider a new SNP site. <software>xPedPhase</software> thus runs in O(M3n3) time, where M denotes the total number of SNPs. In the haplotype configuration for each maximal zero-recombination chromosomal region, the haplotype alleles of the founders are carefully swapped so that the total number of breakpoints is minimized. For this purpose, we set the rule as that, for each pair of founders, their haplotype alleles on two consecutive maximal zero-recombination chromosomal regions are such that the total number of breakpoints in their children is no more than half the number of their children. On almost all maximal zero-recombination chromosomal regions identified in our extensive simulation experiments, <software>xPedPhase</software> returned a unique solution. In the case of multiple solutions, we chose to use the first solution returned from <software>xPedPhase</software>, though ideally we would check for the one that results in the minimum number of breakpoints. After the haplotype alleles for each member have been determined, we report the sharing information among all the pedigree members.<br>
<br>
Haplotype allele sharing by <software>i Linker</software><br>
<software>i Linker</software> determines the haplotype allele sharing status for individuals in a pedigree [12]. The key component of this program is a rule-based and greedy haplotype inference algorithm that assigns haplotypes to the smallest nuclear families extracted from the pedigree. A smallest nuclear family is either a trio, or one parent and a child. <software>i Linker</software> traverses the pedigree in a top-down fashion, and determines haplotypes of family members in sequence. Overall, the program tries to use a minimum number of breakpoints to explain the pedigree genotype data. During the genotype data interpretation process, parental haplotype phases can be revised when more members are added, as long as the revision reduces the total number of breakpoints and still explains the genotype data. Additionally, <software>i Linker</software> has an error correction step that detects unlikely crossover events.<br>
It is worth noting that <software>i Linker</software> is the first to emphasize correctly inferring allele sharing status (rather than haplotype phases) among pedigree members. This greatly reduces computational complexity, even in the face of large high-density SNP genotype datasets. <software>i Linker</software> has been extremely successful in both breakpoint recovery and identifying linked regions for case-control association studies [12].<br>
<br>
Genotype data simulation<br>
In this study, we have implemented a simulation process which generates the haplotypes for a child using the parents' haplotypes according to the ?2(m)-model for crossover events with m = 4 [53,54]. Using this trio generation process as a basis, genotype datasets for large pedigrees can be simulated for testing our programs. We note that this genotype data generation process slightly differs from what has been done in Lin et al. [12], and differs completely from several existing genotype/haplotype data generation programs such as <software>Simlink</software> [55], <software>Simulate</software> [56], <software>Ilink</software> in the <software>Fastlink/Linkage</software> package [57], <software>Slink</software> [32], <software>Allegro</software> [58], <software>Merlin</software> [59], <software>Simla</software> [60], and <software>SimPed</software> [61]. In many of these programs the crossover events are simulated according the probabilities specified for every two adjacent marker sites. In the case of high density SNP markers, all these probabilities are tiny (e.g., from the <database>International HapMap Project</database> [22]).<br>
In our genotype data simulation process, the input to the trio generation process consists of the haplotypes (for one chromosome) for both parents, the physical loci for all the SNP markers, the genetic map corresponding to the chromosome (from the <database>International HapMap Project</database> [22]), and the average numbers of crossovers on the chromosome among the female and male population, respectively.<br>
The children haplotypes are generated through random inheritance of parental alleles after simulating crossover events. The ?2(m)-model assumes that crossover intermediates (C events) are distributed along the four-strand sister chromatid bundle with a rate of 2(m + 1) C events per chromosomal interval, and every C event resolves in either a crossover (Cx) or not (Co). Furthermore, when a C event resolves in a Cx, the next m C events must resolve as Co's, followed by another Cx event, i.e. the C events resolve in a sequence of ... Cx(Co)mCx(Co)m... The leftmost C event has an equal chance to be one of Cx(Co)m [53]. The simulation process determines the chromosomal intervals by reading, from the genetic map, the physical loci for all the SNP markers, and the average numbers of crossovers. It then divides the whole chromosome from head to tail such that the length of each interval (except the last one) is equal to the genetic distance (in Morgans) required for one crossover. For human chromosome 1, this genetic distance is about 1.7 Morgans for males and 0.9 Morgans for females, respectively. The simulation process assumes no chromatid interference, and the child is simulated to randomly inherit one strand of the four-strand chromatid bundle from each parent.<br>
To generate a pedigree genotype dataset, the simulation process locates the individual(s) both of whose parents' haplotypes are either known or have been simulated, and then generates its haplotypes. It then locates the siblings who have only one parent in the pedigree, where its haplotypes are either known or have been simulated, randomly generates the haplotypes for the other parent, and then generates all the children haplotypes one by one. Finally, when all individuals' haplotypes have been simulated, the allele parental information is erased at each SNP site, to give the genotype data.<br>
To validate that the haplotype allele sharing status can be correctly recovered by our programs, we simulate the pedigree genotype data for case-control association study. Before the simulation process, a mutation region of length in between 0 and 10 Mbps, and containing at least one heterozygous site, is randomly assigned to be close to a SNP site in one haplotype of one of the founders. Then, during the simulation process, the affected offspring are forced to inherit one of the mutation strands (there are exactly two among the four strands) and the unaffected are forced not to inherit any of the mutation strands. Note that when a Cx event cuts into the mutation region, then it has to be pushed to the last Co event overlapping with the mutation region. That is, the affected offspring should inherit the complete mutation region.<br>
Since pedigrees of two or three generations are the most common in our previous experience with human disease association studies and more recent experience in cattle breeding industry, we used 10 pedigrees of two or three generations and of size 4 to 13 in the simulation study for validation purpose, in which the disease status for the members are semi-randomly assigned. For each pedigree, we used 5 sets of real unrelated genotype data for the founders; And for each set, we simulated 10 genotype datasets. Those real unrelated genotype data were obtained from two different SNP microarrays, GeneChip Human Mapping 10 K Xba and 50 K Xba arrays. In total, we have simulated 500 10 K and 50 K genotype datasets, respectively.<br>
<br>
<br>
Conclusion<br>
We have showed that for pedigree genotype datasets, the haplotype allele sharing status among the members can be deterministically, efficiently, and accurately determined, even for very small pedigrees, by two most parsimonious partial haplotyping methods. Given its excellent performance in both the breakpoint recovery and the shared region recovery, the program can be useful in many applications including haplotype based association studies.<br>
<br>
Authors' contributions<br>
GL conceived the overall project. ZC and GL detailed the methods. ZC, HS, JX, and YW performed all the experiments, and all authors were involved in the result interpretation and discussion. GL drafted, and finalized the manuscript with RG and PS. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2753849</b><br>
Efficient oligonucleotide probe selection for pan-genomic tiling arrays<br>
<br>
<br>
Background<br>
Microarrays are well known for their success in studying gene expression [1]. As one of their many other roles, DNA microarrays can also be used to characterize both large-scale and small-scale genetic variations. For instance, array comparative genomic hybridization (aCGH) is commonly used in human cancer studies to genotype cell lines by detecting gene loss and copy number variations [2]. At a finer resolution, microarrays are also used to detect single nucleotide polymorphisms at targeted loci [3]. In addition to human screens, microarrays have been widely used for the detection and genotyping of microbial species. Notably, a viral genotyping microarray [4] was one of the methods used to etiologically link severe acute respiratory syndrome (SARS) to a novel coronavirus [5]. Arrays for the detection and comparative analysis of bacterial genomes have also been developed, including arrays for Listeria monocytogenes [6-10], and many other bacterial species. However, these earlier, low-density arrays did not contain enough probes to target the entire genome of the bacterium, and were forced to probe only a small subset of the known genes.<br>
As the density of DNA microarrays increased in recent years, it has become possible to probe the entire genome of an organism in addition to only specific genes. An array providing unbiased coverage of probes across a genome is commonly referred to as a whole-genome tiling array. Such arrays have been very successful for genome-scale analysis, including the discovery of novel transcripts, splicing variants, protein binding sites, and polymorphisms [11]. Depending on the offset between adjacent probe locations, whole-genome tilings can be either gapped, end-to-end, or overlapping (Figure 1a).<br>
In the human genome, tiling arrays are designed to probe the genome at evenly spaced intervals. To maximize the expected specificity of the array, repetitive probes must be avoided and experimental conditions, such as melting temperature, equalized. This creates an optimization problem in choosing which sequences should be included on the array [12,13]. In smaller microbial genomes, it is possible to target every position of the genome with overlapping probes, simplifying the design process. For example, extreme high-density arrays can now accommodate 2.1 million variable length probes on a single chip (Roche NimbleGen, Inc). For an average 2 Mb sized bacterial genome and 50 nt probe length, probes can be offset by only a single base-pair and still span the entire genome, generating a coverage of 50?. By tiling the entire genome, some suboptimal probes will be included on the array, but can be identified and corrected for in the analysis. These overlapping arrays are capable of identifying polymorphism at a much finer resolution than gapped arrays.<br>
Tiling arrays have traditionally been constructed based on the genome of a single reference strain and used to locate genomic differences contained in the experimental strains. However, single-genome arrays can only detect and analyze sequences similar to those included on the array, and cannot discover or analyze sequences absent in the reference strain. After the introduction of the pan-genome concept [14,15], it has become increasingly clear that some microbial species contain significant genetic diversity, and it is not suitable to compare against only a single reference strain. The pan-genome hypothesis states that any given species has two sets of genes. First, a set of core genes present in all strains that define the species; and second, a set of dispensable genes present in only one or a few of the strains that presumably mediate adaptation. A single genome describes the genomic material for a particular strain, but the pan-genome describes the genomic makeup for an entire species. Single reference tiling arrays cannot survey this full diversity. Ideally, an array for analyzing new strains should cover the genomic diversity of the entire pan-genome.<br>
With the explosion in microarray densities, it is now possible to design pan-genome tiling arrays that contain all genomic sequence from the known pan-genome. The simplest strategy is to fully tile the genomes of each strain independently. However, due to similarities between the strains, some sequences would be tiled with excessive redundancy, and this approach would be cost ineffective. Instead, a pan-genome array should aim to minimize costs by using the minimal probe set necessary to target every element of the pan-genome with adequate coverage. The typical approach for targeting multiple strains is to group individual genes into gene families and then probe only the conserved sequences of those families [16-18]. For example, Willenbrock et al. designed an innovative 32 strain Escherichia coli pan-genome array by clustering homologous genes based on pairwise alignment similarity [16]. Homology was defined as gene alignments with an E-value &lt; 10-5, a bitscore &gt; 55, and alignment coverage of at least 50% of the gene length. For each resulting gene group, a consensus sequence was generated via multiple alignment, and probes were designed to target the most conserved regions of the consensus. The resulting array comprised 224,805 probes, targeting 9,252 gene groups, with a median coverage of 27 probes per gene group.<br>
Targeting only the conserved sequence of gene families is an effective and efficient method for detecting--at a low resolution--the presence and absence of gene families; however, for studies that require a finer resolution, this method omits many potentially significant sequences from the array. Firstly, a slight variation in a gene (e.g. a partial deletion) can be responsible for a significantly different phenotype. By only targeting the conserved portion of gene families, the variable regions responsible for these differences will not be included on the array. Secondly, a gene-centric design includes only coding sequences. Therefore, these designs cannot be used to detect differences in intergenic regions which may include regulatory elements, or used for studies that require a whole-genome tiling, such as transcriptome mapping or chromatin-immunoprecipitation-chip (ChIP-chip) studies. Finally, gene-centric design models depend on an accurate annotation of the genome. If genes have been mis-annotated or omitted from the annotation, such genes cannot be properly represented on the array. This is particularly troublesome for many draft-quality genomes that have highly fragmented sequence assemblies and lack accurate annotations. For these reasons, a whole-genome tiling is preferable for applications that require more flexibility or an unbiased tiling of the genome. However, no methods have been described for efficiently tiling multiple whole-genome sequences.<br>
This paper describes a method for pan-genome tiling array design that both minimizes the number of probes required and guarantees that all sequences in the pan-genome are fully tiled by the array. The prior gene-centric approaches are abandoned in favor of a more concrete, probe-centric approach that relies only on the genomic sequences and not the annotation. To summarize the new approach, let the pan-genome G be the set of all genomes from a species, and let P be the non-redundant set of all length k substrings from G. Due to sequence conservation between genomes, a single probe may match to multiple locations (genomes) of the pan-genome. Call these matches the probe targets. The Pan-Tiling problem is to find a minimum cardinality subset H ? P such that all sequences of G are targeted by probes in H and no target is offset more than maxoff from an adjacent target (or sequence end).<br>
Constructing a full tiling of the pan-genome seems like it would require a large number of probes, but by leveraging the similarities between strains, a reasonably sized probe set can be constructed that fully covers a large pan-genome with adequate redundancy. The key to the strategy is choosing probes that will hybridize to as many of the strains as possible, while using only a necessary amount of probes to cover polymorphisms (insertions, deletions, variants). For example, Figure 1b shows a pan-genome tiling for two miniature genomes, with a maxoff of one-third the probe length. Genomes A and B are identical except for a small insertion in the middle of B. Fully tiling both genomes requires a total of 19 probe targets (9 for A and 10 for B), but probe set H illustrates that these 19 targets can be tiled with just 12 probes. Conserved probes are used to tile the left and right of both genomes, and distinct probes are used to tile the two polymorphism variants. This is obviously a simplified example. The problem becomes more difficult as the number of genomes and complexity of polymorphisms increases.<br>
The methods presented in this paper were developed to aid the design of a pan-genome CGH tiling array for Listeria monocytogenes--the causative agent of listeriosis and a NIAID category B biodefense agent that is of significant food safety and public health concern [19]. The species of L. monocytogenes is composed of three primary genetic lineages (named I, II, and III) that display different capabilities of environmental survival and pathogenic potential to cause human infectious disease [20]. In order to both characterize new strains based on genetic content, and detect polymorphism at a higher resolution in small RNAs (sRNAs) and intergenic sequences, the array was required to cover all pan-genomic sequences with a high density of probes. This bacterial species is particularly well suited for pan-genome array design because there are a remarkable number of strains that have been sequenced. At the time of writing, a total of 20 L. monocytogenes complete or draft genome sequences were available, totaling 57.9 Mbp (Table 1). Genomic sequences and annotations were obtained from The <database>National Microbial Pathogen Database Resource</database> (<database>NMPDR</database>) [21]. The sequence conservation for the sequenced strains was computed with <software>Nucmer</software> [22], and ranges between 94% and 99% in nucleotide identity versus the completed EGD-e reference strain. Even with such substantial diversity within the species, the PanArray algorithm is able to design a pan-genome tiling covering each genome at more than twofold coverage using only 385,000 50-mer probes. A similar density tiling for a single L. monocytogenes strain would require 125,000 probes, meaning the PanArray design covers 20? more genomes using only 3? more probes. A description of this design, along with array designs for six other bacterial pan-genomes is given in the Results section.<br>
<br>
Methods<br>
The general strategy of the PanArray design algorithm is best summarized by analogy to the well-known Minimum Hitting Set problem in computer science [23,24]. Let P be a set of n points and F = {P1, P2,..., Pm} be a family of m subsets of P. Minimum Hitting Set is the problem of selecting the minimum cardinality subset H ? P such that H contains at least one element from each subset in F. Although finding a minimum hitting set is known to be NP hard, it is a well studied problem and efficient approximation algorithms are known.<br>
To see the similarities between the Pan-Tiling and Minimum Hitting Set problems, let the sequence G be a concatenation of all the genomes from a species, and let W = {w1, w2,..., wm} be the set of m intervals that results from segmenting G into non-overlapping, end-to-end, length l windows. Let P be the non-redundant set of length k substrings from G. A probe candidate p ? P is said to hit a window w ? W if a match between p and a substring of G begins in the interval w. Let Pi ? P be the subset of probes that hit the window wi, and F = {P1, P2,..., Pm} for the m windows of W. A minimum hitting set H of F is a minimum cardinality subset of probes H ? P such that every window of the pan-genome is hit by at least one probe in H. Therefore, finding H effectively tiles the entire pan-genome using a small number of probes.<br>
Window and probe indexing<br>
Windowing the genome simplifies the Pan-Tiling problem by casting it is a Minimum Hitting Set problem, and at the same time enforces the maxoff constraint. Because each window is forced to contain at least one target, any two adjacent targets cannot be separated by more than twice the window length. Therefore, the window length is equal to one half maxoff. For example, given a maximum offset of 2l, windows are marked off every l bases of the pan-genome--with the first window w1 covering the interval [1, l], and the second window w2 covering [l+1, 2l], and so on. Assuming one target is chosen per window, and the target locations are evenly distributed within windows, the average distance between adjacent targets is expected to be equal to the window length. For a window length l, equal to the probe length k, the resulting depth of coverage averages one, because the probes are spaced k bases apart on average. For any other window length l, the resulting depth of coverage c is expected to be c ? k/l. The extreme case being l = 1, which results in exactly k-fold coverage because a probe must hit every position in G.<br>
To solve the Minimum Hitting Set problem, once the pan-genome is discretized into a set of windows, each window must be mapped to the set of probe hits it contains. As before, a probe p hits a window if a match between p and G begins within the window's interval. Thus far exact matches have been assumed, but a match can be defined by any criteria necessary for efficient hybridization. To help reduce probe redundancy, the PanArray implementation can optionally use inexact matches containing a single mismatch. Any suitable k-mer indexing algorithm can be utilized for this phase, but allowing for mismatches can be computationally expensive. The implementation uses a fast, but memory intensive, compressed keyword tree for indexing all probe hits. Alternatively, a slower, but memory efficient, hashing scheme would also work. To index the 1-mismatch hits, each probe's 3k possible 1-mimsatch permutations are added to the index as well. The result of the indexing is a list of positions and windows for all k-mers of the pan-genome (the probe candidates). At this stage, the final list of probe candidates may be manually filtered based on typical criterion such as melting temperature, GC content, secondary structure, etc. For ungapped tilings, it is impossible to avoid suboptimal probes. However, highly repetitive probes can be identified by the number of genomic positions they map to, and should be discarded if they threaten to confound the array analysis (e.g. by affecting normalization). Alternatively, the input sequences may be masked prior to k-mer indexing to avoid repetitive or unwanted sequence altogether.<br>
For CGH arrays, each probe is considered equivalent to its reverse complement, but for expression or transcriptome arrays, forward and reverse strand probes must be considered independently. Probe matches are listed on the strand on which they appear, so for single-stranded samples, the sequence to be synthesized for the array will need to be reversed complemented. For DNA tiling arrays it is helpful to assume the sample will be double-stranded so that genomic inversions in one or more of the strains do not have to be tiled separately.<br>
<br>
Probe selection<br>
As detailed above, selecting a minimum probe set for tiling S is equivalent to finding the minimum hitting set of P. As before, W is the windowed pan-genome. Let Wp be the subset of windows hit by probe p, and U be the set of currently uncovered windows. Let a window hit by at least one probe be termed as covered, and the coverage of a probe be the number of windows it hits |Wp|. A naive algorithm for finding a small hitting set H is to choose, for each uncovered window, a probe hitting the window that also hits the most other windows. The idea being that choosing probes with the highest coverage will minimize the total number of probes necessary to cover all windows. However, this approach does not properly account for the probe coverages. Only a single probe is needed to cover a window, so after selecting a probe p, all other probes that hit a window in Wp will see their effective coverage reduced. Take for instance two probes p and q that hit the exact same set of windows. Choosing p reduces the effective coverage of q to zero, because all of q's windows have already been covered by p. Let the residual coverage rp of a probe be the effective coverage after some other set of probes have already been chosen (rp = |Wp ? U|).<br>
A greedy algorithm first suggested by Johnson [25] improves on the naive approach by allowing to reconsider the residual coverage of probes after each iteration. This algorithm has since been shown to be essentially a best-possible approximation for the Minimum Hitting Set problem [26]. When adapted for the current problem, the algorithm chooses, while uncovered windows remain, the probe that hits the most currently uncovered windows. The Greedy PanArray Algorithm is:<br>
???Greedy PanArray Algorithm<br>
??????H = ?<br>
??????U = W<br>
??????while U ? ?<br>
?????????select <br>
????????????U ? U - Wp<br>
????????????H ? H ? {p}<br>
??????return H<br>
The algorithm itself is straightforward, but it must be carefully implemented to run efficiently. It is infeasible to recompute the residual coverage |Wp 	? U| for all Wp during each iteration, because both P and W can be on the order of millions for a large pan-genome. To avoid this complexity, the PanArray implementation exploits a property of the residual coverages that allows it to recompute only a few values at each iteration. Note that for any p, its residual coverage rp can never increase. A probe's coverage either remains the same, or decreases because one of its windows was hit by the prior iteration. Therefore, instead of recomputing all residuals after each iteration, it is sufficient to maintain a priority queue of residual coverages and only update stale values at the front of the queue.<br>
At the start of the algorithm, all initial coverages are inserted into the queue. To maintain the priority queue after a new probe is chosen, all residual coverages are considered invalid. During the next iteration, a new rp value is computed for the front of the queue, marked as valid, and reinserted into the queue. This process is repeated until a valid residual returns to the front of the queue. Often, newly computed residuals will return quickly to the head of the queue before the others have been updated. At this point it is unnecessary to update any other residuals because their new values cannot be greater than their current value. Therefore, the head of the queue must be the updated maximum. This lazy evaluation of the residuals avoids many unnecessary computations and drastically improves the performance of the algorithm. The greedy algorithm without this speedup takes days to complete, but with the speedup runs in a matter of seconds.<br>
<br>
Probe annotation<br>
The flexibility of the PanArray design algorithm is a result of its probe-centric approach. Because it does not require any identification or clustering of genes, the design is independent of any genome annotation. Therefore, instead of building the annotation into the design of the array, the annotation can be mapped onto the array after the design. Most importantly, this strategy allows for intergenic sequence and unannotated genomes to be included on the array, and annotation updates to be incorporated as they become available. For example, after the L. monocytogenes array had been designed (see Results), over 40 new sRNAs were discovered in Listeria [27]. Neatly, the sequences of each had already been tiled by the array design, and the updated annotation was easily remapped onto the array. As another example, the gene counts provided by NMPDR in Table 1 are inconsistent and vary between 3,000 and 5,000 genes per genome, suggesting considerable annotation error. Uncoupling the array design from the annotations removes any possibility that annotation errors will affect the design.<br>
Included with the final probe set H is the list of locations on the pan-genome that each probe matches. If the genome sequence is updated, the location information can be easily recovered by remapping the probes to the genome using a matching tool such as <software>MUMmer</software> [22] or <software>Vmatch</software> [28]. To annotate the array, probes are mapped to all annotation features with a coinciding location. The result is a many-to-many mapping with each feature being targeted by multiple probes, and a single probe possibly targeting multiple features (e.g. conserved genes between strains). With this mapping, all probes targeting a specific gene in the pan-genome can be quickly recovered.<br>
<br>
<br>
Results<br>
Listeria monocytogenes pan-genome array<br>
As suggested in the Introduction, L. monocytogenes is a good candidate for constructing a pan-genome tiling array because the species has been widely sequenced, with 20 complete or draft genome sequences available. To confirm that the sequenced genomes contain the majority of L. monocytogenes genetic diversity, the pan-genome size was estimated using the methods of Tettelin et al. [15] as implemented in the <software>Ergatis</software> package [29]. Seventeen of the eighteen L. monocytogenes genomes listed as annotated by <database>NMPDR</database> in Table 1 were used in the analysis (strain 1/2a F6854 was unavailable at the time). According to the cited method, the addition of an Nth genome was simulated by searching the annotated genes of each genome against all possible permutations of N-1 other genomes. Genes without a match over 50% protein similarity for at least 50% of their length were recorded as "new". The number of new genes n expected to be discovered in the Nth sequenced genome was modeled by the power law n = ?N-?, and the parameters ? and ? were estimated from the data via non-linear least squares regression using the <software>R</software> function nls [30]. The regression was performed on the full set of over 1 million data points. A power law model was found to fit the L. monocytogenes data better than the originally proposed exponential model. This agrees with a recent suggestion that a power law is a more appropriate model of the pan-genome phenomenon [31]. The estimated number of undiscovered genes is shown in Figure 2. The power law exponent ? was found to be 1.38 ? 0.002, suggesting that the L. monocytogenes pan-genome is closed (i.e. has a finite number of genes), and the sequencing of more genomes would eventually sample the entire set of dispensable genes. Therefore, it appears the vast majority of L. monocytogenes genes have been sequenced and are included on the array. This model predicts that the addition of a 21st genome to Table 1 would yield only ~ 6 new genes. However, only a single lineage III genome was included in this analysis, so this prediction might be artificially low for a new lineage III strain. The sole lineage III strain analyzed (FSL J2-071) contains 31 genes absent in any of the lineage I and II strains.<br>
To capture the full diversity of L. monocytogenes, all 20 genomes listed in Table 1 were included in the design, with a combined sequence length of 57,946,621 bp and a total of 65,431 annotated genes. To avoid tiling low quality or contaminant sequence, contigs less than 2 Kbp in length were discarded--reducing the tiled sequence length to 54,810,759 bp. The design was constrained to a 385,000 feature NimbleGen array with a probe length of 50 nt. Because hybridization of a 50-mer probe will tolerate a few mismatches, probes differing by a single mismatch were considered equivalent during the design phase. The window length was set to 24 bp, enforcing a maximum target offset of 48, an expected depth of coverage of about 50/24 = 2.08?, and resulting in approximately 2.3 million windows. These parameters guarantee that every base-pair of the pan-genome will be covered by at least one probe, since the maximum offset is less than the probe length.<br>
To cover each window, the PanArray algorithm selected 373,389 distinct probes mapping to 2,893,387 positions in the pan-genome. On average, each probe in the design targets about 8 different positions in the pan-genome. Rather than being repeated sequences within the same genome, these different locations most often refer to a conserved locus in multiple strains (Figure 3). Interestingly, the degree of probe reuse corresponds well with the known evolutionary relationship of the strains. Included on the chip are 8 genomes from lineage I, 10 from lineage II, and 2 from lineage III. This would suggest that the peak at Genomes = 1 in Figure 3 is for strain-specific probes; the peaks around 2 and 9 are for lineage-specific probes; and the peak around 20 is for species-specific probes that are conserved in all 20 L. monocytogenes genomes.<br>
Because this is a dense tiling of the entire genome, it was unnecessary to optimize probes for uniqueness, as is done in standard expression arrays with only a few probes per gene. Probes were screened for repetitive sequences, but the L. monocytogenes strains were found to contain few repeats. The most repetitive 15-mer occurs only 28 times per genome, and the most repetitive 50-mer probe used in the design targets a "cell wall surface anchor protein" family and occurs a maximum of 16 times per genome. Altogether, 99.2% of the probes target at most one location per genome.<br>
To augment the original PanArray design, an additional 228 negative control probes were added to the array, chosen from Bacillus spp., which is a known cohabitant of Listeria. The negative control probes were chosen to be specific to Bacillus spp. using the <software>Insignia</software> genomic signature design pipeline [32]. The remaining 11,838 features on the array were filled by selecting individual probes to supplement the lowest coverage regions of the design. All probes were checked to conform to NimbleGen design specifications, and a few probes were trimmed to meet synthesis cycle limits. The resulting L. monocytogenes pan-genome array has an average depth-of-coverage of 2.65?, with a median probe offset of 21 bp, and a modal offset equal to the window length of 24 bp. The full distribution of probe offsets is given in Figure 4. As expected, the average offset is equal to the window length (24 bp). The uneven distribution and pronounced mode is the caused by non-random tie breaking. In the case of a conserved sequence, where every probe hits the same number of genomes, the first probe of the window is always chosen. Also, the heavy left tail indicates that many windows are covered by more than one probe and the solution that is slightly denser than expected (2.65? actual vs. 2.08? expected). This may be a consequence of the sequence composition, or may indicate a non-optimal solution. Finally, the majority of targeted sequences exactly match their probe (75%) and the remainder match with a single mismatch (25%).<br>
The performance gain of PanArray over more naive methods is significant. For instance, selecting a single probe from each window requires roughly 2.3 million probes. The slightly more principled naive algorithm, that does not recompute residual coverages, chooses 1,739,242 probes, but is still well over the 385,000 probe limit. The Greedy PanArray algorithm meets this limit and vastly outperforms the other methods--requiring only 373,389 probes to cover the entire pan-genome. With the lazy evaluation speedup, the PanArray algorithm is also comparable in runtime to the naive algorithms. On a single 2.4 GHz processor, the naive algorithm took 29 seconds; the greedy algorithm without lazy evaluation was terminated without completing after a few days; and the Greedy PanArray algorithm with lazy evaluation took only 130 seconds. The runtime for the final design process was dominated by building the k-mer index, which required 84 minutes using a compressed keyword tree.<br>
<br>
Design analysis for additional pan-genomes<br>
Using PanArray, additional arrays were designed for a total of seven bacterial pan-genomes, for which a large number of genomes have been sequenced. The additional species include: Francisella tularensis, Staphylococcus aureus, Bacillus anthracis, Vibrio cholerae, Burkholderia pseudomallei, Escherichia coli, and Shigella spp. Due to their high similarity, E. coli and Shigella spp. were considered as a single pan-genome. To facilitate easy comparison, all designs were created with a window length of 25 bp, a probe length of 50 nt, and allowing for probes to contain a single mismatch to their target. As with the L. monocytogenes design above, draft genomes were included, but contigs less than 2 Kbp were discarded. The results are given in Table 2. Probe "reuse" is measured in the average number of targets per probe. It is rare for a 50-mer probe to match to more than one location per genome, so the number of targets per probe is roughly equivalent to the average number of genomes that a probe matches.<br>
The highly conserved species of B. anthracis exhibits near perfect probe reuse. Almost every B. anthracis probe matches all of the included strains; therefore, the number of probes required to tile the nine sequenced strains is nearly the same as is required to tile one strain. This is because the pan-genome of B. anthracis is closed and the strains are highly conserved at the nucleotide level (usually containing only a few SNPs per strain). Adding successive B. anthracis strains to the array would increase the required number of probes very gradually.<br>
In contrast, L. monocytogenes has the lowest degree of probe reuse, with each probe targeting on average only 39% of the included strains. This is a reflection of the diversity of strains that have been sequenced and the low level of nucleotide conservation between strains, with some strains differing by as much as 8% (see Table 1). Any SNP rate of higher than 2% (1 per 50 bp) exceeds the 1 mismatch threshold per probe and requires additional probes to target the divergent sequence. However, as more variants are added to the array, the addition of each successive genome requires fewer new probes than the last, on average. Figure 5 shows this relationship for the L. monocytogenes strains. Successive strains are added by order of lineage, from the bottom of Table 1 to the top, and the design is recomputed at each step. There are pronounced jumps in the number of probes required when the first of a new lineage is added, but the number of probes needed to tile the rest of the lineage quickly levels off.<br>
Escherichia coli and Shigella spp. form the largest pan-genome currently sequenced, totaling over 144 Mbp of genomic sequence. Even for a pan-genome of this size and diversity, PanArray effectively tiles all sequences at an average of 2? coverage using only 674,697 probes--well below the maximum number of probes available on current arrays. The B. pseudomallei pan-genome is roughly equivalent in total number of pan-genome bases, but requires considerably fewer probes because of higher probe reuse. Due to the large number of sequenced genomes and relatively high similarity between strains, the B. pseudomallei design exhibits the highest probe reuse factor of all the designs (13.8?). Creating a 2? coverage tiling by choosing one probe every 25 bp would require roughly 5.4 million probes for the B. pseudomallei pan-genome, but PanArray was able to create a 2.5? tiling of the same pan-genome with only 491,231 probes.<br>
<br>
Implementation and availability<br>
The PanArray algorithm was implemented in C++, and the source code is freely available at . The Listeria monocytogenes array design described above is available from the <database>Gene Expression Omnibus</database> [33] under <database>GEO</database> accession number GPL8942.<br>
<br>
<br>
Discussion<br>
The PanArray algorithm described above is ideal for high-density tilings of overlapping or closely spaced probes. The Results section has shown that this algorithm is applicable for all currently available bacterial pan-genomes. However, if the maximum number of probes is limited, or the genome size is extremely large, it may be necessary to design a tiling with gaps between the probe targets (i.e. a maximum offset greater than the probe length). In this case, it is necessary to choose unique probes that avoid unwanted cross hybridization between repetitive sequences within the genome. To achieve this, repetitive probes can be filtered, or the coverage scores used in the PanArray algorithm can be weighted to penalize repetitive probes. For example, probe coverage can be redefined as the number of genomes a probe targets, rather than the number of windows, and probes targeting multiple windows in the same genome can be appropriately down-weighted. In many cases, probes within the same window will share the same coverage score, and rules can be applied for breaking the tie and choosing the most reliable probe. Similar schemes could be devised to favor probes with any other desirable criteria.<br>
Array analysis of aCGH experiments is typically conducted on signal ratios between a reference and experimental hybridization. Duplications or deletions in the experimental samples are evident as non-zero values of the log ratio of the two normalized signals. So-called segmentation algorithms examine this log ratio across multiple positions in reference sequence to determine the boundaries of the variations [34,35]. The most accurate methods consider not just individual probes, but a context of probes around a genomic location, and can identify even small polymorphisms between the strains. These analyses require both a reference signal and a reference coordinate system on which the probes are tiled. Usually a whole-genome tiling is constructed for a single reference strain, but because PanArray provides a whole-genome tiling for every reference strain included in the array, the same array design can be used to perform segmentation analysis against any reference strain on the array.<br>
In addition to segmentation analysis versus a reference genome, a pan-genome array makes it possible to analyze uncharacterized strains in the context of the entire pan-genome. In some cases, it is preferable to use a multi-strain control [36], but depending on the number of genomes, it can be impractical to co-hybridize all reference strains included on the chip. In these cases, traditional segmentation or log-ratio analysis must be replaced by a method that does not require a reference hybridization signal. For gene-level analysis, direct analysis of the individual probe intensities provides comparable sensitivity and specificity versus segmentation analysis [16], and various methods have been developed that operate independently of a signal ratio [16,37,38]. A probe-based approach provides the most flexibility for pan-genome array analysis, because each probe can be individually scored based on its own intensity, and the genes can be classified based on the aggregated scores of the individual probe scores without the need for a control hybridization.<br>
Pan-genome tiling arrays have all the applications of single-strain tiling arrays, but with enhanced flexibility and the ability to analyze previously uncharacterized strains. Pan-genome aCGH offers an economical alterative to sequencing for determining the genomic makeup of uncharacterized strains in a species and explaining the causative factors of phenotypic differences between strains. Probe based methods, like microarray, are especially well suited for situations where sequencing is inefficient because there is a low abundance of target DNA and a high abundance of background DNA intermixed. For example, applications such as real-time pathogen detection, surveillance, and diagnostics require a known sequence of DNA to be targeted from a vast environment [32,39,40]. A pan-genome array could be used for the detection and genotyping of pathogens from a large environment, without needing to isolate the individual cells. Pan-genome arrays could also be used to capture all species- or locus-specific genomic material from an environment, which could then be directly processed or sequenced separately from the metagenome. Microarray based genomic capture has already been applied to targeted human resequencing as an efficient means of enriching for desired sequencing templates [41-43].<br>
<br>
Conclusion<br>
Without the need for sequencing additional genomes of the same species, pan-genomic aCGH has become an increasingly popular and cost-effective approach to compare and characterize genomic contents of unknown bacterial isolates. Prior multi-strain arrays have targeted the conserved sequences of gene families, or a selected group of polymorphisms; therefore, providing only partial coverage of the pan-genome. PanArray is a probe selection algorithm capable designing a tiling array that fully covers all genomes of a species using a minimal number of probes. The viability of this method is demonstrated by array designs for seven different bacterial pan-genomes, each of which can fit on a single microarray slide. By constructing an unbiased tiling of all known sequences, these unique pan-genome tiling arrays provide maximum flexibility for the analysis, detection, or capture of genomic material for entire species.<br>
<br>
Authors' contributions<br>
AMP conceived the problem, designed and implemented the algorithm, performed the analyses, and wrote the manuscript. XD contributed to the design and analysis, and edited the manuscript. WZ and SLS helped conceive the problem, edited the manuscript, and coordinated the project. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2848234</b><br>
<software>Mayday</software> - integrative analytics for expression data<br>
<br>
<br>
Background<br>
Since their inception in the early 1990s, DNA microarrays have revolutionized many areas of biological research. They are a fast and relatively inexpensive tool used for genome-wide studies of gene expression, epigenetic modifications, binding sites of DNA-binding proteins, copy-number variation as well as for resequencing projects. Their success is largely due to the ever growing number of features that can be represented on a single array, allowing for the simultaneous investigation of a large number of genomic loci.<br>
Yet the large number of features, and a concomitant increase in the number of experiments conducted (such as fine-grained time-series experiments), also poses the problem of finding the data of interest. Essential to any microarray experiment is thus the filtering of the large data matrix, the aim is to find (full-width) submatrices ("clusters") with common characteristics. Furthermore, assigning statistical significance values to the features (row-vectors of the matrix) is a very common task. A large number of different methods have been developed for automated as well as exploration-driven analysis of complex data, some of them specific to the field of microarray analyses, others are more general in application.<br>
However, most of these methods are available only as stand-alone programs or proof-of-concept implementations. During a normal microarray experiment, several of these methods have to be used in combination. Which methods are used and in what order depends on the nature of the data, the experimental conditions and on observations made during the analysis itself. Thus, bioinformaticians need an integrative framework combining many of these methods to be able to efficiently analyze their data. Such a framework must also allow the quick addition of new methods and support their development via rapid prototyping.<br>
<software>BRB-ArrayTools</software> is such an integrated software system developed by biostatisticians [1]. It is an add-in to <software>Microsoft Excel</software> under the Microsoft Windows family of operating systems. Among the tools are algorithms for normalization, the computation of differentially expressed genes, cluster analysis, and class prediction. <software>BRB-ArrayTools</software> focuses mainly on the development of new statistical methods for expression data analysis.<br>
<software>EMMA 2</software> provides a wide collection of algorithms and a database to store, retrieve, and analyze genome-wide datasets in a MIAME and MAGE-ML compliant format [2]. For the user it features a web interface, however no offline version is available. <software>EMMA</software>'s main emphasis is the analysis of MAGE-compliant data. It is fully open-source offering a large number of various analysis algorithms encompassing preprocessing and normalization, statistical methods for the detection of differentially regulated genes, various cluster algorithms and visualization features. The user can setup pipelines that allow automatic analysis.<br>
The <software>Gene Expression Profile Analysis Suite</software> (<software>GEPAS</software>) offers a similar approach to the analysis of microarray data as <software>EMMA</software> [3]. It also provides a web-based interface. Its main strength is the multitude of tools offered ranging from preprocessing to functional profiling.<br>
The <software>TM4</software> suite of tools consist of four major applications, <software>Microarray Data Manager</software> (<software>MADAM</software>), <software>Spotfinder</software>, <software>Microarray Data Analysis System</software> (<software>MI-DAS</software>), and <software>Multiexperiment Viewer</software> (<software>MeV</software>), as well as a Minimal Information About a Microarray Experiment (MIAME)-compliant <software>MySQL</software> database [4]. <software>MeV</software> is a microarray data analysis tool written in Java. It is free, open-source software incorporating algorithms for clustering, visualization, classification, statistical analysis and biological theme discovery. <software>MeV</software> offers a number of visualizations. However, it does not allow users to interactively explore data through the combined use of several different linked plots and does not offer many possibilities for using meta information to enhance visualizations.<br>
The importance of appropriate visualization methods for microarray data has long been recognized. A framework for the visual integration of additional meta-information of gene expression data was introduced in [5] and demonstrated in an application of the heat colormap. The enhanced heatmap showed the clear advantages of the integration of supplemental data from different sources for the visual exploration of microarray data.<br>
As the raw experimental data is the biologists' most valuable resource, researchers want to be able to perform their analyses in-house, preferably on their personal computer. The size of modern datasets also makes repeated transfers over the network infeasible.<br>
<software>Mayday</software><br>
<software>Mayday</software> [6] is a platform-independent framework for data analysis and visualization. Written in Java, it can be installed locally or run without any installation as WebStart application. <software>Mayday</software> provides efficient core data structures as well as a powerful plugin management system which allows for fast extension via custom plugins. A large number of plugins is already available, covering such areas as clustering, classification, and visualization. All methods presented here are implemented in Java except for the import from Affymetrix CEL files (see below).<br>
Clustering is one of the most common tasks in microarray analyses. <software>Mayday</software> offers several clustering methods with different optimization criteria. Besides the well-established partitioning methods such as k-means and SOM [7,8], hierarchical clustering methods such as UPGMA, WPGMA and Neighbor-Joining are available. All clustering methods can be performed with a wide range of distance measures (among them Euclidean, Minkowski, Pearson correlation distance, many more).<br>
Offered visualization tools should be of great assistance in interpreting the results of microarray experiments. Among the most commonly used ones are heatmaps, boxplots, MA scatter plots and histograms. Thus, <software>Mayday</software>'s main strength lies in visualization and visualization-driven data exploration. Data can be visualized in many different ways, including profile (parallel coordinate) plots, box plots, scatter plots and heatmaps. All <software>Mayday</software> plots can be exported as publication quality files, using different bitmap formats (JPG, PNG, TIFF) as well as the scalable vector graphics format (SVG). The different views on the data are linked so that interaction with a profile plot is reflected in a simultaneously opened heatmap, for instance. Meta-information can be used to enhance the plots, i.e. add additional data to the visualizations. These can come from clusterings (cluster ids) or external sources (e.g. <database>Gene Ontology</database> identifiers), or can be the result of statistical tests applied within <software>Mayday</software>, such as p-values. These can, for instance, be used to add additional columns to <software>Mayday</software>'s heatmap, to sort the heatmap's rows, to add transparency or a second color dimension or to change the height of rows according to their significance. Furthermore, users can inspect all meta information associated with the probes in a tabular view, sort the table by any meta information column, or use meta information to filter probes.<br>
Finding significantly differentially expressed genes is one of the core functions offered by <software>Mayday</software>. A host of different methods are already available (e.g. Student's t test, SAM [9], etc.) and can be combined with correction methods for multiple testing. ANOVA analyses are supported as well.<br>
<br>
<br>
Implementation<br>
The current version of <software>Mayday</software> offers many enhancements and new features. The core structures were optimized and rewritten to improve performance and simplify the addition of new functionality. Among the new features are the ability to create a hierarchical structure within datasets, a much-improved user-interface with customizable profile previews, matrix operations such as merge and split, new statistical methods for the identification of differentially expressed genes (WAD [10], Rank Product [11]), online data transformations (e.g. z-scoring, smoothing, centering) and many more. See figure 1 for an overview of <software>Mayday</software>'s user interface. Some of the highlights will be presented in the following sections.<br>
Automated Processing<br>
Since many analysis steps are common to the first-level analysis of virtually all microarray data, <software>Mayday</software> offers a powerful processing pipeline construction framework allowing for the automation of such tasks and their rapid application to new data sets. Pipelines can be stored persistently and shared with other users.<br>
<br>
Dynamic Filtering<br>
A dynamic filtering framework has been integrated into <software>Mayday</software>, to create so-called Dynamic Pro-beLists. By chaining together any number of filter-ing modules and logical operators, arbitrarily complex filters can be created in an easy to use graphical editor. A large number of modules are available for filtering on expression values, meta data, feature names, the content of other (dynamic) ProbeLists or similarity measures (query-by-example). Dynamic ProbeLists react to changes in the underlying data and are updated accordingly.<br>
<br>
New clustering methods and visualizations<br>
While k-means is one of the most used clustering algorithms in microarray analyses, new methods have been developed that overcome some of k-means deficits and have been shown to give good results. One such method is quality-threshold (QT) clustering [12], now available in <software>Mayday</software>. Instead of a predefined number of clusters, the input parameter is the desired quality (the radius) of clusters to be found. We have implemented a graphical interface that aids users in determining the correct parameter values for their dataset, depending on the distance measure of choice. Furthermore, a density-based clustering [13] method has been added. Clustering result quality can now be assessed using silhouette plots and different clustering methods can be compared with each other or with a partitioning defined by a priori knowledge.<br>
To speed up hierarchical clustering of large datasets, we included an efficient implementation the rapid neighbor-joining algorithm [14]. The trees produced by all hierarchical clustering methods are now stored and can be attached to heatmap plots in addition to being displayed in separate viewers using different layout algorithms.<br>
We extended the idea of Sequence Logos [15] to visualize the general direction of expression within experiments: The ProfileLogo plot shows stacked probe expression bins, scaled to their frequency within each experiment. Expression bins are defined by thresholds, e.g. for up and down-regulated genes. Histogram plots have been implemented to gain insight into the distributions of statistical and experimental values, as well as meta data values attached to the data.<br>
Selected probes resp. genes in each plot can be used as the basis for database queries in a large number of public databases, among them NCBI, <database>Ensembl</database>, <database>Gene Ontology</database>, <database>KEGG</database>, and <database>PubMed</database>.<br>
<br>
Machine Learning<br>
Training, evaluation and application of classification models of numerous different types are further applications of <software>Mayday</software>. For dimensionality reduction and identification of marker genes several feature selection methods are available. The machine learning techniques are provided using the <package>WEKA</package> [16] library which has been integrated into <software>Mayday</software>. In addition, the <software>Gene Mining</software> plugin provides a number of methods to select genes separating classes among the experiments.<br>
<br>
Project management<br>
<software>Mayday</software>'s <database>ProjectDB</database> implements central and organized storage of datasets and can be used for data mining purposes. As back-end it can either use <software>Apache Derby</software> [17] (included in the Java WebStart version) or dedicated database management systems (<software>PostgreSQL</software>, <software>MySQL</software>). Datasets can be organized in Projects and Project States, allowing to take snapshots of different stages of their analysis. The graphical <database>ProjectDB</database> browser provides previews of each object, including profile plots and boxplots of the experimental data. The data can also be queried directly using an interactive shell.<br>
Alternatively, <software>Mayday</software> implements a snapshot file format that can be used to save the current state of a data set including meta-information, de-fined clusters, hierarchical clustering trees etc. The snapshot format is specifically designed for fast data storage and retrieval while still being a very space-efficient compressed representation of the data.<br>
<br>
Programmers' access<br>
Bioinformaticians will especially like our programmers' access to the data. We have a tightly integrated efficient <software>R</software> shell that integrates the full functionality of <software>R</software> [18] and its wealth of available packages and thus allows the application of third-party methods directly on <software>Mayday</software>'s data. <software>R</software> processes can also be connected to <software>Mayday</software> over the network allowing complex calculations to run on a powerful workstation or cluster and communicating with a <software>Mayday</software> instance running on the researcher's laptop, for instance. Furthermore, all gene expression data and meta information currently opened in <software>Mayday</software> can be queried using standard SQL, including the possibility to create new views and custom tables. These shells both feature syntax-highlighting editors with persistent history, greatly increasing programmers' productivity (see figure 2).<br>
<br>
Cross-dataset analyses<br>
Time series analyses as well as replicate studies often require researchers to compare different datasets, e.g. to find systematic shifts in expression over time.<br>
<software>Mayday</software> now offers a specialized view for this purpose in addition to the cross-dataset analyses possible with our <software>R</software> and <software>SQL</software> command-line interfaces.<br>
<br>
Integrated analyses - Systems biology<br>
For integrative pathway analyses, biochemical pathways from several sources, including <database>KEGG</database> [19] and <database>MetaCyc</database> [20] can be visualized as networks. The expression data of enzymes and concentration data of metabolites can be summarized and visualized on the network in different forms, including profile plots and heatmaps.<br>
Gene annotations can be imported from external databases. We currently offer direct support for the <database>Gene Ontology</database> [21] and <database>KEGG</database> databases. Gene identifier mapping can be done automatically using the <software>PICR</software> [22] service.<br>
<br>
<br>
Results<br>
Application study: Dynamic architecture of the metabolic switch in S. coelicolor<br>
To demonstrate the new functionalities of <software>Mayday</software>, we present here an analysis of a large time series in Streptomyces coelicolor. For streptomycetes it has proved very difficult to identify the key regulators that control expression of the pathway specific regulators. <software>Mayday</software> was used to monitor the expression dynamics of the bacterium in a time series dataset with unprecedented resolution.<br>
A custom-designed Affymetrix array containing 22,779 probe sets interrogating genes, intergenic regions, and predicted noncoding RNAs was used to study the gene expression in mostly hourly intervals starting at 20 h after inoculation, up to 60 h [23]. Altogether, 32 time points were studied. Phosphate was depleted in the medium at 36 h.<br>
All oligos of the probe sets were mapped to their genomic locus on the chromosome or on one of the two plasmids of Streptomyces coelicolor. For each probe set the start and end genomic coordinate together with the strand orientation were written to a tab-separated file.<br>
Within <software>Mayday</software> we imported data from 32 CEL files using <software>Mayday</software>'s <software>R</software> interpreter. For normalization we used the robust multi-array average method (RMA) [24] as provided in the <package>affy</package>[25] package of <database>BioConductor</database> [26]. We imported genomic locus information from the tab-separated file described above for later steps in the analysis.<br>
Using a custom processing pipeline, we automatically compute regularized variance for each probe and then apply a filtering step to create a probe list of most variant probesets. Of 22,779 probesets, 64 remain after filtering with a regularized variance threshold of 0.3.<br>
Based on this probelist of variant probesets, we create a new dynamic probelist to select only those probes that, apart from being the most variant, interrogate protein coding genes (SCOxxxx), and query the plus strand of the Sco genome (see figure 3). 32 probesets remain. Changing any of the filter parameters automatically updates all plots based on the dynamic probelist.<br>
The time series sampling reflects the development of Streptomyces coelicolor from early growth phase to stationary phase. Accordingly, the expression differences between the samples taken at two consecutive time points should, in general, be smaller than those between samples from time points that lie further apart. Furthermore, the differences between time points should reflect the rate of change in the metabolic state of the culture. To assess this hypothesis, we performed a hierarchical clustering of the transposed matrix, i.e. clustering of the experiments, using the most variant genes. We used the Euclidean distance and <software>MAYDAY</software>'s implementation of the rapid neighbour-joining algorithm [14]. The resulting cluster tree is visualized along with a heatmap in figure 4. As expected, the early (20 h) and late time (60 h) points are at the outermost leaves of the tree and consecutive time points are clustered very closely together. The tree nicely depicts the consecutive points of time along the growth curve of the organism. It also shows the major expression change occurring between 35 and 36 hours after inoculation. This largest expression change coincides exactly with the time of complete phosphate depletion in the fermenter.<br>
Since the heatmap suggests the existence of distinct groups of genes within the probelist, we use QT clustering with a diameter of 0.4 and use the resulting clusters to color a profile plot showing the z-scored profiles of the genes (figure 5). The dynamic architecture of the metabolic switch is clearly visible with different groups of genes being up-resp. down-regulated in a successive order of time points (35, 39 and 43 hours in this subset).<br>
The heatmap also shows that there are some genes that clearly separate the time points 46-60 from the earlier ones. Using the <software>GeneMining</software> plugin, we search for those genes that optimally separate these two groups of experiments (using the quartet mining algorithm, for details see <software>MAYDAY</software>'s website). Of the 32 genes in the dynamic probelist described above, 15 belong to the list selected by the quartet mining algorithm. These genes all exclusively belong to the actinorhodin pathway, a genomic cluster of genes (SCO5071-SCO5092).<br>
The experimental data also contains optical measurements of the amount of actinorhodin produced. Combining <database>ScoCyc</database> [27] pathway information, expression values and external measurements of actinorhodin levels, we produce an interactive visualization of the actinorhodin pathway (figure 6). On first glance, it is obvious that spectrometrically measured actinorhodin concentration rises in response to the upregulation of several enzymes in this pathway. Interesting target compounds for analysis can be selected from the pathway image for further wet-lab investigation.<br>
Since the dataset used here is part of a larger experiment where biological replicates were produced in separate fermentation runs, we decided to investigate whether we could detect systematic differences between these replicates. Figure 7 shows <software>Mayday</software>'s time series alignment tool with one of the QT clusters as an example. The genes in that cluster are up-regulated one hour later in the second fermentation (F202) than in the reference fermentation (F199). This time shift could be traced to a one-hour delay in phosphate depletion in the second fermentation.<br>
<br>
<br>
Discussion<br>
<software>Mayday</software> is a comprehensive platform for the analysis and the visual exploration of microarray data. According to Allison et al. [28] the most important statistical components of a microarray experiment analysis involve the following steps: design, preprocessing, inference or classification and validation. During the last years analysis of microarray data has become highly sophisticated, new methods are published almost daily. These range from preprocessing and normalization to novel statistical and machine learning methods. A software that wants to keep pace with these developments has to provide possibilities to enable the rapid integration of new methods as well as making them as usable as possible.<br>
An important focus of exploration of high-dimensional data, such as microarray data, lies on visualization. The advantage of our design is the tight integration of both analysis and visualization as well as the various visualization techniques themselves.<br>
This combination of automatic and visual analysis leads to a visual analytics approach that provides more insights in the structure of the data. We think that with <software>Mayday</software> such a visual analytics approach for the analysis of high-dimensional microarray data has been realized.<br>
<br>
Conclusions<br>
We present a very versatile open-source framework for efficient microarray data analysis, designed for biologists and bioinformaticians. All common tasks of microarray analyses are already covered and the wide range of functionality from the already existing plugins can swiftly be extended with new plugins written in Java, ad-hoc scripting interfaces facilitate rapid prototyping of new algorithms as well as interactive specialized data exploration. <software>Mayday</software>'s interactive visualization methods in conjunction with the meta-data concept provide significant insight into complex data and have successfully been applied in many microarray analyses.<br>
New methods and tools are continuously added to <software>Mayday</software>'s platform to keep up with new developments. Our coming release includes two new visualizations based on genomic locus information: A track based visualization and a view showing expression (or meta information) values as colored boxes aligned to a linear chromosome laid out continuously in stacked rows. Both are fully interactive and integrated with all other visualizations.<br>
Most recently, novel ultra-high throughput DNA sequencing technologies have been developed that enable researchers to obtain the complete genomes of organisms faster and at a lower cost than classical methods [29]. Moreover, these technologies can be applied to measure gene expression (RNA-Seq) [30] and protein-DNA interactions (ChIP-Seq) [31], and many current studies use RNA-Seq and microarray data comparatively. Our new genomic plots will be especially useful in the context of such new types of data. We're currently working on an integration of these new data types into <software>Mayday</software>, separately or in multi-platform settings.<br>
<br>
Availability and requirements<br>
? Project name: Mayday<br>
? Project home page: http://microarray-analysis.org<br>
? Operating systems: Platform independent<br>
? Programming languages: Java<br>
? Other requirements: Java 6 or higher<br>
? License: GNU GPL version 2<br>
<br>
Authors' contributions<br>
SY and FB are coordinating development of <software>Mayday</software>. FB performed the data analysis and wrote the manuscript. KN coordinated and designed the study. All authors read and approved the final manuscript. None of the authors have any competing financial or other interests in relation to this work.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2909217</b><br>
A semi-supervised learning approach to predict synthetic genetic interactions by combining functional and topological properties of functional gene network<br>
<br>
<br>
Background<br>
Genetic interaction analysis, in which two mutations have a combined effect not exhibited by either mutation alone, can reveal functional relationship between genes and pathways, and thus have been used extensively to shed light on pathway organization in model organisms [1,2]. For example, proteins in the same pathway tend to share similar synthetic lethal partners [3]. Given a pair of genes, the number of common genetic interaction partners of these two genes can be used to calculate the probability that they have physical interaction or share a biological function. Therefore, identifying gene pairs which participate in synthetic genetic interaction (SGI) is very important for understanding cellular interaction and determining functional relationships between genes. Usually, SGI includes synthetic lethal (SL, where simultaneous mutation, usually deletion, on both genes causes lethality while mutation on either gene alone does not) and synthetic sick (SS, where simultaneous mutation of two genes causes growth retardation) interactions. However, so far little is known about how genes interact to produce more complicated phenotypes like the morphological variations.<br>
Recently, modifier screening such as synthetic genetic arrays (SGA) has been applied to experimentally test the phenotype of all double concurrent perturbation to identify whether gene pairs have SGI [3]. Although high-throughput SGA technology has enabled systematic construction of double concurrent perturbation in many organisms, it remains difficult and expensive to experimentally map out pairwise genetic interactions for genome-wide analysis in any single organism. For example, the genome of S. cerevisiae includes about 6,275 genes. About 18 million double mutants need to be tested if the analysis is carried out based on their combinatorial nature. This number will expand to about 200 million for the simple metazoan C. elegans (with ~20,000 genes), posing insurmountable technical and financial obstacles.<br>
Therefore, many computational methods for predicting SGI have been proposed in previous works in order to alleviate the experimental bottleneck [4,5]. A promising solution is to predict the SGI by integrating various types of available proteomics and genomics data. Candidate gene pairs with SGI are computationally predicted and validated experimentally. In [4], SS or SL gene pairs in S.cerevisiae are successfully predicted, with 80% of the interactions being discovered by testing 20% of all possible combinations of gene pairs. Various supervised algorithms, such as artificial neural network, SVM and decision tree, have been developed to tackle the synthetic genetic interaction prediction problem [4,6]. In spite of being able to handle large input spaces and deal with noisy samples in an efficient and robust way, a main difficulty facing all supervised methods is that they predict the SGI only from labelled samples and the learning process heavily relies on the quality of the training dataset [7]. For example, in [4] about 519,647 experimentally tested gene pairs of S. cerevisiae are adopted as training dataset, which is impossible in most cases.<br>
Usually, obtaining labelled samples is much more difficult than getting unlabelled samples. When the size of available training set is small, traditional approaches based on supervised learning may fail. Worse still, experiment-supported genetic interactions gene pairs are far more less in metazoans than in S. cerevisiae, thus it is more difficult in metazoans to generate genome-wide predictions by using supervised algorithms. Therefore, it is desirable to develop a predictive learning algorithm using both labelled and unlabelled samples. In this context, it becomes natural that semi-supervised classifiers are employed. SSL classifier uses available label information as well as the wealth of unlabelled data as the input vector. We propose a graph-based SSL method, previously presented in [8], in the context of SGI prediction. One advantage of SSL is the compatibility to small training sets, thus it could have great potentials in organisms, especially metazoans with less experiment-supported genetic interaction gene pairs. We concentrate on graph-based method due to their solid mathematical background, as well as the close relationship with kernel methods and model visualization.<br>
In recent years, it has been a growing and hot topic to combine information from diverse genomic or proteomic evidence in order to arrive at accurate and holistic network [9-13]. The heterogeneous data sources, in one way or the other, carry interaction information reflecting different aspects of gene associations and their function relationships. Therefore, one of the major challenges is to integrate these data sources and obtain a system level view on functional relationships between genes [14]. The successful applications have proved that an integration of heterogeneous types of high-throughput biological data can improve the accuracy of the groupings compared with any single dataset alone [10,15-19]. However, despite the success of integrated networks in other area [10], most previous works on synthetic genetic interaction prediction mainly consider PPI or gene expression data alone [20-22].<br>
In this study, we integrate PPI, protein complex and gene expression data simultaneously to utilize more information for more accuracy of genetic interaction prediction taking the following observations into consideration. PPI data is believed to contain valuable insight for the inner working of cells. Therefore, it may provide useful clues for the function of individual protein or signalling pathways [23,24]. Although it is unclear which proteins are in physical contact, protein complexes include groups of proteins perform a certain cellular task together and contain rich information about functional relationships among the involved proteins. The high-throughput gene expression profiles are becoming essential resources for systems-level understanding of genetic interaction [25-28]. Gene expression profiles measure the expression levels of certain genes in genome scale. Relative to randomly paired genes, functionally interacting genes are more likely to have similar expression patterns and phenotypes [5,29,30]. It is assumed that genes with similar expression profiles are involved in the control of the same transcriptional factors and thus they are functionally associated [25,31].<br>
Network analysis is a quantitative method originating from the social science; it studies the nodes' topology properties related to its connectivity and position in the network. It has become increasingly popular in diverse areas, especially in molecular biology and computational biology [9,32]. Network analysis is a powerful tool for studying the relationships between two nodes in a network. It has been proved in recent work that genetic interactions are more likely to be found among proteins that are highly connected and highly central in protein interaction network [33]. This finding demonstrates the correlations between topological properties of PPI network and SGI between proteins. In this study, we study the extent to which pairwise SGI can be predicted from the topological properties of the corresponding proteins in a FGN.<br>
In previous works, they only consider the topological properties of the binary protein interaction network while ignore the underlying functional relationships which can be reflected by the gene expression profile [4,20]. A major limitation of these methods stems from the fact that the weight of ties is not taken into account. For FGN, the weights often reflect the function similarity performed by the ties. Exploring the information that weights hold allows us to further our understanding of networks [34,35]. In this paper, we also present a straightforward generalization of a number of weighed network properties which originally defined on the unweighted networks. Concretely, the weighted network properties are defined by combining weighted and topological observables that enable us to characterize the complex statistical properties and heterogeneity of the actual weight of edges and nodes. This information allows us to investigate the correlations among weighted quantities and the underlying topological structure of the network. The topological properties of the FGN are examined with the aim of discovering the relationship between the network properties of gene pairs and the existence of a SGI relationship.<br>
<br>
Results<br>
General approach<br>
The aim of the proposed approach is to predict genetic interactions in Saccharomyces cerevisiae using topological properties of two proteins in a weighted functional gene network. The first input feature vector for the algorithm is a set of network properties corresponding to pairwise genes. The second input is a set of synthetic genetic interaction and non-interaction pairs found from previous large scale mutant screens. The output of this approach is scores corresponding to the propensity of a particular gene pair to be synthetic genetic interaction. The overall workflow is illustrated in Figure 1.<br>
We can see from Fig. 1 that PPI data, protein complex data, and gene expression profiles are integrated to build a high coverage and high precision weighted FGN. More specifically, PPI and protein complex data are used to determine the topology of the network. Then a clustering analysis method is utilized to identify functionally related groups from the gene expression profile and the weights of the interaction are calculated based on the gene expression profile and clustering centroids, i.e. the weight of a PPI network derives from a metric considering the distance of expression of individual gene and the centroids of its cluster, as well as the distance between the two cluster centroids themselves. The weights are assigned as the confidence scores which represents their functional coupling. Considering weights of interactions instead of binary linkage information allows more accurate modelling and will have better classification performance [15,17].<br>
And then, a set of topological properties are extracted from the FGN. These network properties and the experimentally obtained gene pairs which have been confirmed to have or do not have the synthetic genetic interaction are considered as an input vector of a SSL classifier to predict other unknown interacting gene pairs. Concretely, we use a SSL classifier to model correlations between network properties and the existence of a SGI. The output labels of the SSL classifier are soft labels yi ? [0, 1], which measure if the two corresponding genes participate in a SGI. The details of above procedure are described in the method section.<br>
<br>
Cross validation<br>
Performance comparisons are based on the following Cross Validation (CV) procedures. CV is a way of choosing proper benchmarking samples to assess the accuracy and validity of a statistical model. Specifically, we randomly select 1,500 known SGI pairs and 1,500 non-SGI pairs from the dataset provided by Tong et al [3]. Thus, the sampled dataset contain an equal number of SGI and non-SGI gene pairs. In n - fold CV, we randomly divide the known SGI pairs into n subsets of approximately equal size. Equal number of non-SGI pairs corresponding to above n divided subsets are randomly selected and assigned to the n subsets. Then n - 1 such subsets are combined for training the classifier, which is subsequently tested on all other SGI and non-SGI pairs from the withheld subset. This procedure is repeated n times with each subset playing the role of the test subset once.<br>
We use the standard Receiver Operating Curve (ROC) to assess performance overall. We compute the sensitivity (or true-positive rate, defined here as the fraction of SGI gene pairs correctly predicted) and false-positive (defined here as the fraction of non-SGI gene pairs incorrectly predicted to be SGI) by decreasing stringency levels of the classifier (outputs soft labels). By using alternative score thresholds, this approach can be tuned to predict a subset of SGI with higher confidence at a small cost of sensitivity.<br>
<br>
Experiment results<br>
SVM has emerged as one of the most popular supervised approaches with a wide range of applications. In particular, the previous studies have demonstrated that SVM has better learning performance and accuracy than other supervised algorithms, such as Artificial Neural Network and Decision Trees [36]. Therefore, in this study we implemented our graph-based SSL algorithm and compared it with the SVM in distinguishing SGI versus non-SGI gene pairs on the same benchmark dataset. We test the capability of our method using different levels of sparsity of training set. In the experiment, 80% (5-fold CV), 50% (2-fold CV), and 20% of the known SGI and non-SGI gene pairs are randomly chosen for training the classifier respectively, which was subsequently tested on all other SGI and non-SGI gene pairs from the withheld group (This is repeated several times with each group playing the role of the test group at least one time). Since the gene pairs to be classified for cross-validation are randomly chosen, we repeated each experiment five times and computed the average of all the results.<br>
Figure 2 shows a comparison result between SSL algorithm and SVM method when 20% of gene pairs are assumed to be unlabelled. Figures 3, 4 demonstrate the performance of the two tested algorithms when 50% and 80% of gene pairs are assumed to be unlabelled respectively. The proposed SSL algorithm outperforms SVM in almost all the range of threshold. In particular, we can see from Figure 2 that when 20% of nodes are unlabelled, SVM has a slightly better performance in the first part of the ROC curve while SSL achieves better results in other part. Conversely, when 80% of nodes are unlabelled, SSL shows much higher accuracy than SVM (see Figure 4). In summary, the accuracy of SSL appears higher than that of SVM classifier. Further, when labelled nodes in training dataset are very small, the performance of SSL is significantly better than that of SVM. SSL method can reach a true positive rate of 92% against a false positive rate of 9% accuracy at a maximum in our experiment. However, Wong et al [4] reported that they predicted SSL gene pairs in S. cerevisiae with a success rate such that 80% of the interactions are discovered by testing &lt;20% of the pairs. Our algorithm has higher accuracy than their method. Moreover, our approach only depends on protein interaction data and gene expression data, and does not require other data source like genomic sequence data. Our results clearly demonstrate that the FGN integrating of proteome and genomic data can be used to predict the SGI. We exhibit that the topological properties of FGN for pairwise genes serve as compelling and relatively robust determinants for the existence of synthetic genetic interaction between genes.<br>
As a supplementary result we also compare the performance of proposed method on the same training dataset between the weighted network and binary network. The binary network is constructed by combining PPI and protein complex data. From Figure 5, we can see that the weighted network has higher performance than that of binary network in almost all the range of threshold. We believe this is because for binary network the weights of interactions are not taken into account and the information that weights hold is not employed.<br>
<br>
<br>
Discussion<br>
In order to assess the suitability of using certain network properties to classify SGI gene pairs and non-SGI pairs, we draw the distributions of probability density for these properties across SGI pairs and non-SGI pairs, respectively. To make it simple, we just give detailed description of four network properties, such as centrality degree, betweenness centrality, closeness centrality and clustering coefficient. For each property, we plot the distribution of the average value over pairwise genes and the absolute difference across the two genes. For most properties here, the difference in distribution of probability density across SGI and non-SGI pairs is statistically significant (see Additional file 1 Figure S1-S16). The distributions of the average and difference value of each property across two proteins in case of SGI pairs (blue lines) and non-SGI pairs (red lines) are displayed in figure 6.<br>
We used the Kolmogorov-Smirnov (K-S) test to compare the two distributions. The null hypothesis is that the two distributions are from the same continuous distribution. The alternative hypothesis is that they are from different continuous distributions. The major contribution of K-S test is that no distribution assumption is needed for the data. As shown in Table 1 that all the P-values of the KS-test are less than 0.05. From Additional file 1 Figure S17-S32, we can see that the empirical distributions of cumulative function across SGI and non-SGI pairs are also different. According to the result, the difference between SGI and non-SGI samples is significant enough. Also when viewed as part of a FGN, comparing with non-SGI pairs, SGI pairs tends to have higher average degree, higher average closeness centrality. We also compared the KS-test performance in weighted FGN and binary network. We can see from Table 1 that the P-values of all network properties in weighted network are much less than those in binary network.<br>
<br>
Conclusions<br>
In conclusion, a SSL prediction approach was proposed in this paper to predict SGI by combining functional and topological properties of FGN. Using a clustering-based data integration method, large-scale protein interaction data, protein complex data and multiple time-course gene expression datasets were combined in order to build FGN in yeast. Greater coverage and higher accuracy were achieved in comparison with previous high-throughput studies of PPI networks in yeast. Then, we show that topological properties of protein pairs in a FGN can be served as compelling and relatively robust determinants for the existence of synthetic genetic interaction between them. Finally, a graph-based SSL is utilized as a classifier to model correlations between FGN properties and the existence of a synthetic genetic interaction.<br>
Our results clearly demonstrate that the proposed algorithm can achieve better performance comparing with previous methods. Our framework of feature representation is a general form, and it is straightforward to add other topological properties that are relevant to this problem. It is also possible to add other types of biological evidences. For example, information about the function of proteins can be encoded in our framework as well. We hope to extend this work and improve feature representation in future so that we can detect other types of interaction groups.<br>
<br>
Methods<br>
Biological datasets<br>
There are four different types of data sets used in the study. 1) Golden standard dataset of known genetic interactions (True positives, TPs) and non-interacting protein pairs (True negatives, TNs). 2) Experimental protein-protein interaction data. 3) Experimental protein complex data. 4) Time-lapse gene expression profiles.<br>
Golden standard genetic interaction dataset<br>
Using the Synthetic Genetic Array (SGA) technology, Tong et al. screened 132 query strains (carrying mutations in genes with diverse functions in cell polarity, cell wall biosynthesis, chromosome segregation and DNA synthesis and repair) against the complete library of ~4700 viable haploid deletion strains, and ~650,000 gene pairs were experimentally tested and identified a total of ~4,000 synthetic lethal synthetic sick interactions, at 0.65% frequency [3]. We used this dataset as golden standard dataset to investigate synthetic genetic interaction in S. cerevisiae.<br>
<br>
Protein-protein interaction dataset<br>
To computer network properties associated with protein-protein interaction in S. Cerevisiae, we download protein interaction data from the <database>BioGrid</database> database [37]. This network contains 12,990 unique interactions among 4,478 proteins.<br>
<br>
Protein complexes dataset<br>
For protein complex, we assigned binary interactions between any two proteins participating in a complex. Thus in general, if there are n proteins in a protein complex, we add n(n - 1)/2 binary interactions. We get the protein complex data from [38,39]. Altogether about 49,000 interactions are added to the protein interaction network.<br>
<br>
Microarray gene expression data<br>
Four sets of time course data from the DNA microarray of S. cerevisiae are used in this study. These datasets have also been used to study the genetic interactions in previous work [40]. The first set contains 17 time points during the mitotic cell cycle [41]. The second set contains 6 time points during heat shock and the third set contains 9 time points during sporulation [31], and the fourth set contains 32 time points during cell cycle [42]. Altogether 64 experimental conditions for all the genes in S. cerevisiae related to cell cycle are used. For the missing values in each experiment, we substituted its gene expression ratio to the reference state with the average ratio of all the genes under that specific experimental condition.<br>
<br>
<br>
Construction of functional gene networks<br>
Linkages of the FGN carry confidence scores to represent the functional coupling between two biological entities they represent. In this section, we calculated the confidence score of each linkage following the previous works [25,26].<br>
For the gene expression data, the clustering analysis is carried out to identify functionally related groups of genes. We denote a gene expression data set as X = {x1, x2,...,xM}, where xi = {xi1, xi2,...,xiN} is a N dimensional vector representing gene i with N conditions. We use the clustering algorithm to group the M genes into S,(S ? M -1) different clusters C1, C2,...,Cs.<br>
As proposed in [43], the Pearson Correlation Coefficient (PCC) is employed as a measure of similarity to cluster genes with similar or different expression patterns, which means genes with co-expressed pattern are assigned to same cluster and vice versa. A positive PCC value means that two genes are co-expressed, while negative value denotes that they are the opposite expressed gene pairs. Let us consider genes xi and xj and the PCC can be calculated as(1)<br>
where xikand xjkare the expression values of the kth condition of the ith and jth genes respectively.  and  are the mean values of the ith and jth genes respectively. PCC is always in the range of (-1, 1).<br>
At first, all genes of the gene expression profiles are considered as a single cluster and the cluster is partitioned into two disjoint clusters. Partitioning is done in such way that xi and xj which have the most negative value of PCC will be assigned into two different clusters. Genes having larger PCC value with xi compared with xj are assigned in the cluster that contains xi. Otherwise, they are placed in the cluster that contains xj. In the next iteration, a cluster having a gene pair (xi, xj) with the most negative PCC value will be selected and the above partitioning procession is repeated until there is no negative PCC value present between any pair of genes inside any cluster. This kind of cluster method ensures that all pairs of genes in any cluster are only positively correlated. It has been proven that this method is able to obtain clusters with higher biological significance than that obtained by some other algorithms such as Fuzzy K-means, GK and PAM clustering methods [43].<br>
Based on the above obtained gene expression profile which has been partitioned into a couple of clusters, we calculate the weighted confidence scores of the interactions between two proteins as below:(2)<br>
where xi and xj represent genes i and j with N conditions respectively.  and  denote the centroids of the clusters in which genes xi and xj located respectively. ||?||2 denotes the Euclidean distance. In equation (2), the constant L1 is a tradeoff parameter used to tune the ratio of the first and second term in the weight function. According to [44], we choose L1 = 0.3 because we assume that the distance between centroids of two cluster more significant comparing with the distance of each gene from its centroid. The outcome of the integration method is a weighted undirected graph, i.e. functional gene network.<br>
<br>
The properties of functional gene network for predicting SGI<br>
For using as input feature vector of the SSL classifier, we compute the following topological properties of FGN for each protein or protein pair. Here we report a total of 18 features representing 10 network properties. These network properties reflect the local connectivity and global position of the nodes in the network and are assumed to be correlated to its functional properties. Table 2 lists the 10 types of topological properties used in this paper. The details can be seen as below.<br>
(1) Centrality degree<br>
A network can be expressed by its adjacency matrix aij, whose elements take the value 1 if an edge connects the node vi to the node vj and 0 otherwise. In an unweighted graph, the degree of node vx is equivalent to the number of neighbors of node vx , which can be denoted as(3)<br>
However, the weighted degree of node vi is the sum of the weights of the edges between vi and its neighbors [45].(4)<br>
where ? is the weight between two nodes, in which ?ij is greater than 0 if node vi is tied to node vj , and the value is the weight of the tie, which represents the strength of the relation between the two nodes.<br>
<br>
(2) Clustering coefficient<br>
The clustering coefficient of a node in a network quantifies how close the node and its neighbors are to being a clique. Let Ccl(i) denote the clustering coefficient of node vi , and it is given by the proportion of links between the nodes within its neighbourhoods divided by the number of links that could possibly exist between them. For an unweighted graph, the clustering coefficient can be defined as:(5)<br>
where ei is the number of the links between the neighbourhoods of node i and ki is the number of the neighbourhoods of node vi . For a weighted graph, the definition of the clustering coefficient is defined as [45](6)<br>
<br>
(3) Weighted Shortest Path<br>
Both the closeness centrality and betweenness centrality rely on the calculation of shortest path in a network. Therefore, a first step towards extending these measures to weighted networks is to generalize how shortest path is defined in weighted networks.<br>
In weighted networks, the shortest path is a path between two nodes with the minimal sum of the weights of its constituent edges. Since all edges have the same weight in unweighted networks, the shortest path between two nodes is through the smallest number of intermediary nodes. However, a complication arises when the ties in a network do not have the same weight attached to them. There have been several attempts to calculate shortest distances in weighted networks in previous work [46,47]. In our work, we applied Dijkstra's algorithm to the weighted biological network by inverting the positive weights in the network [47]. Thus, high values represent weak ties, whereas low values represent strong ties.<br>
<br>
(4) Betweenness centrality<br>
Betweenness is a centrality measure of a node within a networks. Nodes that occur on many shortest paths between other nodes have higher betweenness than those that do not. For an unweighted network, to calculate the betweenness  of node vi, we firstly count the number of shortest paths between two nodes passing the node vi . Let bi be the ratio of this number to the total number of shortest paths existing between these two nodes. Then the betweenness of node vi is the sum of bi over all pairs of nodes in the network. We normalize it to lie between 0 and 1 by dividing above value by the total number of pairs in the network. The betweenness for node vi is as follow(7)<br>
where gjk is the number of shortest geodesic paths from node vj to vk. gjk(i) is the number of shortest geodesic paths from vj to vk which pass through the node vi.<br>
In the case of weighted network, we assume that the flow in the network occurs over the paths that Dijkstra's algorithm identifies and use this algorithm to find the nodes that funnel the flow in the network. Then the weighted betweenness centrality is extended by counting the number of paths found by Dijkstra's algorithm on a weighted network instead of the number found on a binary network [48].<br>
<br>
(5) Closeness centrality<br>
Closeness is a centrality measure of a node within a network. Nodes which tend to have short geodesic distances to all other nodes within the network have higher closeness. In unweighted network, closeness centrality is defined as the inverse of the average distance from one node to all other nodes. For a weighted network, this definition changes slightly. Within the adjacency matrix, for any two nodes, vi and vj , if dij is the shortest distance from vi to vj, then the closeness centrality of node vj is defined as [49](8)<br>
where n is the total number of nodes in the network.<br>
<br>
(6) Eigenvector centrality<br>
Eigenvector centrality is a measure of the importance of a node in a network. It assigns relative scores to all nodes in the network based on the principle that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes.<br>
Let xi denotes the score of the ith node. Let Aij be the adjacency matrix of the network. In weighted network, the entries of A are real numbers representing connection strengths. For the ith node, let the eigenvector centrality score be proportional to the sum of the scores of all nodes which are connected to it. It can be formulated as [49]:(9)<br>
where ? is a constant. Defining the vector of centralities x ={x1,x2,...,xn}, we can rewrite this equation in matrix form as(10)<br>
Hence we see that x is an eigenvector of the adjacency matrix with eigenvalue ?. In our work, we used the free software package named <package>igraph</package> to calculate the eigenvector centrality of weighted network [50].<br>
In addition to above six weighted network properties, we also calculated several other binary network properties, such as stress centrality [51], information centrality [52], flow betweenness centrality [53], the number of mutual neighbors between proteins vi and vj. All of the above ten network properties can reflect the local network structure around the node or the global network topology.<br>
<br>
<br>
Graph-based semi-supervised classifier<br>
The SSL is halfway between supervised and unsupervised learning, which is very active and has recently attracted a considerable amount of research [7,54]. In essence, there are three different kinds of SSL algorithms being applied, i.e., Generative models, Low density separation algorithms, and Graph-based methods. In our study, we use graph-based SSL method because of its solid mathematical background, their relationship with kernel methods, visualization, and good results in many areas, such as computational biology [32], web page classification [54], or hyperspectral image classification [7]. We here present the whole formulation of the graph-based SSL algorithm.<br>
Consider the whole dataset being represented by ? = (?l, ?n) of labelled inputs ?l = {x1, x2,...,xl} and unlabelled inputs ?n = {xl+1, xl+2,...,xn} along with a small portion of corresponding labels {y1, y2,...,yl}. Consider a connected weighted graph G = (V, E) with vertex V corresponding to above n data points, with nodes L = {1, 2,...,l} corresponding to the labelled points with labels y1, y2,...,yl and U = {l + 1, l +2,...,n} corresponding to unlabelled points. For SSL, the objective is to infer the labels {yl+1, yl+2,...,yn} of the unlabelled data {xl+1, xl+2,...,xn}, typically l ? n.<br>
Firstly, the n ? n symmetric weight matrix W on the edges of the graph can be(11)<br>
where xi and xj denote the different points in the graph G. The constant ? is a length scale hyperparameter. Therefore nearby points in Euclidean spaces are assigned large edge weight, and vice versa.<br>
Then let F denotes a series of n ? l matrices with non-negative elements. A matrix  corresponds to one certain classification on ? = (?l, ?n) by assigning each point xi a label yi = argmax xj?l.Fij. We define an n ? l matrix Y ?F with Yij = 1 if xi is labelled as yi = j and Yij = 0 otherwise.<br>
Secondly, we build the matrix S = D-1 2WD-1 2 where D is a diagonal matrix with the (i, i) -elements equal to the sum of the ith row of W. Then take the iteration F(t + 1) = ?SF(t) + (1 - ?)Y until the similarity matrix F converges, where ? is a predefined constant which ranges from 0 to 1.<br>
Thirdly, let F* represent the limit of the sequence {F(t)}. Label each point xi as a label yi = argmax xj?C.F*ij. Because 0 &lt;? &lt; 1 and the eigenvalues of S ranges from -1 to 1.(12)<br>
Then the classification matrix can be calculated as: F* = (1 - ?S)-1Y. As in [8], F* can be obtained without iteration. After the above steps, the labels of unlabelled data {xl+1, xl+2,...,xn} will be assigned.<br>
<br>
Support vector machines classifier<br>
SVM algorithm has been proposed by Vapnik as an effective and increasingly popular learning approach for solving two-class pattern recognition problems [55]. SVM as a typical supervised machine learning method is attractive because it is not only well founded theoretically, but also superior in practical applications. Intuitively, SVM classifier is based on the structure risk minimization principle for which error bound analysis has been theoretically motivated. The method is defined over a vector space where the problem is to find a decision surface that "best" separates the data points in two classes by finding a maximal margin. SVM has been widely applied to a number of pattern recognition areas like text categorization [56], object recognition [57], etc. In most of these cases, the performance of SVM is significantly better than that of other supervised machine learning methods, including Neural Network and Decision Tree classifier [17]. The SVM has a number of advanced properties, including the ability to handle large feature space, effective avoidance of overfitting, and information condensing for the given data set, etc. A brief introduction about SVM is given in the Additional file 1.<br>
Here, we describe the use of the <software>LibSVM</software> provided by Chih-Chung Chang. <software>LibSVM</software> is an integrated software for support vector classification [58]. It is much easy to construct a SVM classifier. We only need to choose a kernel function and regularization parameter to train the SVM. In this study, we adopt the radial basis function (RBF) as the kernel function whose parameters were optimized by taking a n-fold cross-validation on the training set [55]. Specifically, the grid search was used to find optimal kernel parameters such as C, Gamma, which tries values of each parameter across the specified search range using geometric steps. Although grid search method is computationally expensive, it is computationally feasible in our cases.<br>
<br>
<br>
Authors' contributions<br>
ZHY &amp; XBZ &amp; DSH conceived the original idea, wrote the main body of the manuscript and implemented the experiments. ZY &amp; KH attended the discussion of the work and revised the manuscript. All authors have read and approved the final version of this manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2921410</b><br>
Estimating the individualized HIV-1 genetic barrier to resistance using a nelfinavir fitness landscape<br>
<br>
<br>
Background<br>
Management of antiviral resistance is an important consideration in the treatment of HIV-1 patients with antiviral drugs [1]. Facing high viral loads and fast replication rates, a combination of multiple drugs is needed to suppress viral replication so that the viral load in the plasma becomes undetectable. HIV-1 has a high mutation rate, and in conjunction with the large intra-host population and fast generation time [2], the virus is able to develop resistance mutations quickly. Therefore, a strict adherence to the treatment is regarded as crucial in the prevention of suboptimal drug concentrations and subsequent viral replication.<br>
As part of the management of antiviral treatment, genotypic resistance testing is recommended when starting or switching treatment [3]. When virological failure is detected timely and a genotypic resistance test performed immediately, in many cases the test shows that the virus has developed resistance but not to all drugs in the regimen [4]. Some drugs, such as the currently used non-nucleoside reverse transcriptase inhibitors, have a low genetic barrier to resistance since only a single nucleotide mutation is required to completely loose drug activity. By contrast, other drugs (including most protease inhibitors) require an ordered accumulation of multiple mutations to confer resistance, and thus have a higher genetic barrier to resistance. At treatment failure, the virus is more likely to have developed resistance against the drug with the lower genetic barrier [4-6]. However, the actual genetic barrier is not merely the number of mutations needed to confer resistance, since the likelihood of a mutation is not uniform due to evolutionary restrictions. A mutation must also be considered in the context of in vivo fitness, reflecting the combination of phenotypic resistance and intrinsic replication capacity. Epistatic fitness interactions between mutations may alter the prevalence of a mutation depending on the presence of another mutation.<br>
Genotypic resistance testing aims at uncovering mutational patterns in the virus and interpreting their impact on drug resistance. This interpretation is difficult because of the complexity of resistance patterns, the existence of cross-resistance and resensitization, and the high natural variation of HIV-1. Ideally, a genotypic resistance test not only helps in selecting a treatment regimen that will immediately inhibit viral replication, but also in selecting a treatment with a high genetic barrier to resistance, and thus a durable response. Therefore, not only the contribution to resistance of detected mutations, but also their impact on lowering the genetic barrier towards resistance should be considered. Because there is no readily available measure for the genetic barrier (unlike for the resistance phenotype, which may be measured using an in vitro assay), the impact of many mutations and mutational patterns on genetic barrier is not well understood. With a few exceptions, such as the so-called revertants at reverse transcriptase position 215 [7], the clinical relevance of a supposedly decreased genetic barrier has not been shown. A lower genetic barrier not only poses a higher long-term risk for failure in case of nonoptimal adherence, but may also impact treatment options available at failure, under the assumption that development of resistance at treatment failure is more likely for drugs with a lower genetic barrier, and because of the extensive cross-resistance within drug classes.<br>
The extensive natural variation within the HIV-1 main group (reflected partly in subtype diversification) is not believed to impact drug susceptibility substantially [8,9]. Still, this variation may affect the genetic barrier to resistance for some drugs, even in treatment-naive patients, and this could in principle be predicted from the genotype [10]. Several studies have indeed suggested that the presence of polymorphisms, known as minor mutations, impact virological outcome [11-14]. However, these studies usually lacked statistical power to assign the effect on virological outcome to the presence of particular polymorphisms because of the small prevalence of many polymorphisms, and the confounding effect of adherence.<br>
In previous work, we presented a method to estimate a fitness landscape experienced by the virus during treatment, and applied this in the context of the protease inhibitor nelfinavir (NFV) [15]. Simulated evolution from a baseline sequence, over such a fitness landscape, together with a criterion for resistance, allows the estimation of the individualized genetic barrier to resistance. In the present study, we investigate the association of the individualized genetic barrier with development of resistance at failure, as predicted by an expert rule-based genotypic interpretation system, in patients fully susceptible to NFV at baseline. We also explore genotypic factors that impact this estimated genetic barrier for viruses predicted to be fully susceptible to NFV.<br>
<br>
Results<br>
Predicting development of NFV resistance at treatment failure<br>
The final longitudinal data set included 201 protease sequence pairs with a subtype distribution largely dominated by subtype B (78%). A Neighbor-Joining phylogenetic tree constructed from the baseline sequences revealed no intra-subtype clustering according to data source (data not shown). At treatment failure, the Rega algorithm predicted full NFV resistance (R), i.e. with GSSNFV = 0, in 73 cases (36%) and intermediate NFV resistance (I), i.e. with GSSNFV = 0.5, in 6 cases (3%).<br>
In these pairs, genotypic susceptibility to NFV treatment as estimated in vivo fitness value and estimates of the simulated genetic barrier to resistance were computed from the baseline sequence (Table 1). Despite the fact that each patient was predicted at baseline to be fully susceptible to NFV by a genotypic interpretation system (Rega V8.0.1), we observed variation in estimated fitness under NFV treatment at baseline as well as substantial variation in estimated genetic barrier to NFV resistance (Table 1). The genotypic susceptibility of the virus to the remaining drugs in the combination, predicted by Rega, was high. For most patients (67%), the activity score for the combination excluding NFV (GSSOther) summed up to ? 2, which suggests that the majority of the NFV-based regimens was potent enough at the time of therapy initiation. The median time to treatment failure was 12 months.<br>
The results of the univariable analysis are shown in Table 2. A lower estimated genetic barrier, both in terms of mutations (OR = 0.65 (0.45 - 0.94) per additional mutation, p = .02) or in terms of generations (OR = 0.98 (0.97 - 0.99) per 10 more generations, p = .01) and lower activity of the other drugs in the combination (OR = 0.53 (0.39 - 0.71), p &lt; 0.001) were associated with a higher risk of developing NFV resistance at treatment failure. Estimated fitness under NFV selective pressure, duration on therapy or subtype B virus were not associated with NFV resistance development.<br>
These significant associations remained in the multivariable analysis (Table 3). A lower genetic barrier in terms of mutations (OR = 0.54 (0.32 - 0.91) per additional mutation, p = .02) or in terms of generations (OR = 0.98 (0.97 - 0.99) per 10 more generations, p = .0075) associated significantly with an increased risk for developing NFV resistance at treatment failure. Also a lower backbone activity (OR = 0.49 (0.35 - 0.67), respectively p &lt; .001 and p = .001) was independently indicative for acquiring NFV resistance.<br>
The two measures for genetic barrier were highly correlated (R2 = 0.97 (0.96 - 0.98), p &lt; 10-16), since each additional mutation in general requires extra evolutionary time to evolve. This also explains why the results were similar when using  versus .<br>
<br>
Genotypic correlates of estimated genetic barrier<br>
To investigate contributions of protease mutations and polymorphisms on the predicted genetic barrier, evolution was simulated for a virtual cohort of 2764 patients on NFV treatment, and for each simulation, the number of generations GR to develop NFV resistance was recorded. Because the estimated fitness landscape only models intra-subtype variation for each subtype, this analysis was done only using HIV-1 subtype B, the most prevalent subtype in our data set. Phylogenetic reconstruction indicated interspersion of multiple lineages of sequences sampled in Portugal. Therefore, separation of sequences in the tree conditioned on the center of data collection could not be established (data not shown).<br>
A step-wise model search was performed to identify a best linear model for log GR, which thus included the independent, multiplicative contributions of single mutations. In the final model, 22 mutations (10F/I/V, 12K, 13V, 20R/T, 33F, 35 D, 36I/V, 45R, 62V, 64M/V, 70R, 71T/V, 72V, 75I, 77I and 88D) independently decreased the genetic barrier (p &lt; .05), while 7 mutations (12P, 17 D, 37A, 41K, 69Y and 89I/M) increased the genetic barrier (Figure 1). Although Figure 1 indicates contributions of pro-tease mutations to the genetic barrier with a fixed extent, these values resulted from averaging over the entire population (of 2764 sequences) and, since only independent and individual mutational contributions were considered, as well over mutations epistatically interacting with the respective mutation listed (see additional file 1 for the full model). As such, these findings do not contradict with the observation that the genetic context contributes to fitness in the landscape, and consequently to the genetic barrier to resistance. For example, mutation 71V was present in 85 isolates (3.1%), of which 45 (53%) selected 30N as first mutation and 9 (11%) 90 M (which are considered major resistance mutations by Rega). Baseline sequences lacking this mutation only selected in 487 (18%) and in 106 cases (4%) 30N and 90 M respectively. On the other hand, 9 isolates harboured mutation 17 D and 30N was only selected in 1 (11%) and 90 M (0%) zero cases, compared to 531 (19%) and 115 (4%) for isolates lacking 17 D.<br>
For several of the mutations that contributed to a decreased genetic barrier (10V, 13V, 20R, 33F, 35 D, 36I/V, 45R, 62V, 64M/V, 70R, 71T/V, 77I, 88D) and one mutation that increased genetic barrier (89I), predicted selection by the fitness landscape model was shown previously to correlate with observed evolution in longitudinal data from patients on NFV treatment [15]. Thus, for these mutations, the fitness function modeled interactions with polymorphisms or other resistance mutations that affects their selection. Rega considers ten mutations (10I/V, 20R/T, 33F, 62V, 64V, 71T/V, and 88D) to contribute to resistance as minor mutations. Eleven mutations that were predicted to decrease genetic barrier (12K, 13V, 35 D, 36I/V, 45R, 64 M, 70R, 72V, 75I, and 77I), and five mutations that were predicted to increase the genetic barrier (12P, 17 D, 37A, 41K, 69Y, and 89M) are not included in the rules for NFV resistance in Rega. Some of these mutations have been described previously in relation to resistance to NFV or other protease inhibitors: mutations 36I and 77I are polymorphisms that are involved in NFV resistance [16]; mutation 45R has recently been associated with NFV treatment [17]; mutations 13V, 36I/V, 45R, 72V, 75I and 77I are associated with protease inhibitor treatment [18] and mutation 13V has been associated with reduced response to tipranavir [19]. Mutation 89I has been linked to treatment failure in several non-B subtypes, where the wild-type is 89 M [20]. 89I/M are rare mutations in subtype B, and the model indicates that in subtype B, they increase the genetic barrier to resistance because they are reverted to the wild type (89L), although the same model correctly predicts selection of 89I during NFV treatment in other subtypes [15].<br>
<br>
<br>
Discussion<br>
In this study, we evaluated retrospectively the association of genotypic information contained in the baseline genotype with the risk of developing NFV resistance at treatment failure, when treated with a NFV containing regimen, in longitudinal sequence pairs. The baseline sequences were interpreted using an estimated fitness function for HIV-1 under NFV selective pressure, which was used to compute the estimated fitness (log ) and two measures of genetic barrier: the expected number of mutations  or generations  to evolve a mutational pattern that is considered by Rega as causing resistance to NFV. As expert resistance interpretation system, Rega was chosen because it has been clinically evaluated for prediction of treatment outcome [21]. This will allow us to investigate if the fitness landscape could be used to predict treatment options available at treatment failure as predicted by Rega.<br>
Both in univariable and multivariable analyses, a lower genetic barrier was found to increase the risk for developing NFV resistance at treatment failure. Such estimated genetic barrier may provide unique and useful information to a clinician contemplated a change of treatment, allowing to take into account available therapy options in case of subsequent treatment failure. This is, to our knowledge, the first proof of direct clinical impact of (individualized) genetic barrier on risk of development of resistance at treatment failure.<br>
With the goal of life-long treatment, options at treatment failure are taken into consideration at start of treatment, and therefore current HIV-1 treatment guidelines take into account proper drug sequencing and the sparing of inhibitor classes [22]. An individualized prediction of (cross-)resistance development at treatment failure may therefore contribute to a more informed treatment choice. Noteworthy, a lower activity of the regimen accompanying NFV, predicted by Rega, was associated with an increased risk of NFV resistance at therapy failure. This association can be expected since a suboptimal, less potent regimen may favor evolution and development of NFV resistance more easily. The accuracy of the predictions may be further improved by using the genetic barrier to resistance for the other drugs in the combination, instead of a susceptibility score.<br>
The association of a lower genetic barrier with an increased risk for resistance development at failure implies indirectly that the estimated genetic barrier could also be predictive for long-term treatment response. Indeed, these results show that a lower genetic barrier facilitates resistance development, and may therefore be expected to increase as well the risk for treatment failure because of resistance development under non-optimal adherence. Although the Rega system for genotypic resistance interpretation also scores the presence of several minor resistance mutations as intermediate resistance (motivated by the principle that they may reduce the genetic barrier to resistance), in this analysis only patients were included for which Rega predicted full susceptibility to NFV at baseline. Assuming that viral fitness during treatment depends on the susceptibility of the virus to the drug, log  can be considered an in vivo resistance phenotype. The restriction of the study to patients predicted susceptible may explain why viral fitness, visualized by the virus position in the landscape, did not relate to the emergence of NFV resistance. Overall, these results provide additional indication that the estimated fitness landscape may outperform an expert system for prediction of treatment outcome, in particular for patients who are considered fully susceptible by the expert system [15].<br>
Although clinical response in terms of viral load measurements was not available for these patients, the availability of a follow-up genotype is indicative of treatment failure. By requesting a genotypic test, the clinician presumed failure of the current regimen, and successful genotyping implied high enough viremia. We previously evaluated the performance of this landscape to predict virological outcome in a clinical cohort of patients, starting with a combination of zidovudine (AZT), lamivudine (3TC) and NFV. Differently from the current study, patients were not required to be fully susceptible to NFV. A higher genetic barrier was significantly associated with higher viral load reduction on short term and with lower odds of virological failure on long term [23].<br>
For this analysis, sequence data were combined that originated from different geographic locations. Genotypic variation was accounted for by adding HIV-1 subtype to the model. Additionally, phylogenetic analysis did not unveil geographical withinsubtype sequence differences. By pooling data from multiple sources, (unknown) variables, besides epidemiology, could differ between patient groups and influence resistance development. The objective of the fitness estimation procedure was not to predict resistance development as such, but to quantify the influence of mutational patterns on viral fitness under drug selective pressure and eventually to predict virus evolution under this pressure. Resistant virus was defined by an independent interpretation algorithm. As measures of "time", we considered the number of mutations or simulated generations. The actual time to therapy failure is besides the evolutionary distance under drug selective pressure (quantified by the genetic barrier), a function of the rate at which HIV-1 will bridge this distance (quantified by the strength of drug selective pressure). Next to drug activity, the potency of treatment is the outcome of different parameters. Though information on patient-specific parameters (such as therapy adherence) or on management of HIV-1 infection is missing, these parameters most likely do not influence the actual evolutionary distance to resistance, but do affect drug potency and subsequently the time to therapy failure. Hence, the time between therapy initiation and failure was included as a variable to correct for (hidden) variables that influence the amount of virus evolution tolerated.<br>
To obtain an insight into the contributions of mutations and polymorphisms towards estimated genetic barrier to NFV resistance in isolates susceptible to NFV, we simulated resistance evolution during NFV treatment in subtype B sequences from a large virtual clinical patient cohort. A total of 29 mutations and polymorphisms were identified that independently contribute to the genetic barrier to NFV resistance (Figure 1).<br>
Because of the combined use of the fitness landscape with an expert system, a mutation may influence the estimated genetic barrier either because it contributes to resistance (as predicted by Rega), or because it influences, in the fitness function, the selection of mutations that contribute to resistance, or both. A number of mutations (12K/P, 13V, 17 D, 35 D, 36I/V, 37A, 41K, 45R, 64 M, 69Y, 70R, 72V, 75I, 77I and 89M) are not included in the rules for NFV resistance. Therefore, each of these mutations contributes to a lower (respectively higher) estimated genetic barrier through their inclusion in the fitness landscape model, where they cause a faster (respectively slower) selection of resistance mutations that are considered by Rega. The mechanism for the contribution to a lower genetic barrier of mutations (10I/V, 20R/T, 33F, 62V, 64V, 71T/V, and 88D) which are considered by Rega to contribute to resistance (as minor resistance mutation), may be because of their inclusion in the rule for predicting resistance in Rega, or because of an influence on selection of (major) resistance mutations in the fitness landscape model, or both.<br>
The contribution of a mutation to viral fitness is highly dependent on the genetic background, and a mutation with an impact on the genetic barrier was identified by the model conditioned on the presence of polymorphic variation. Despite the recruitment of only subtype B sequences, and phylogenetic analysis that indicated distributed sequences among the tree, intra-subtype variation, as a consequence of founder effects, is inevitable and has also been reported [24]. A total of 10 mutations listed in Figure 1 differed significantly (p &lt; 0.05) in prevalence between the two patients groups (see additional file 2). However, these mutations still contributed significantly to the genetic barrier when the analysis was restricted to data source, highlighting the role of sequence variability. Application of the same methodology to another subtype B dataset may conceivably not identify exactly the same set of mutations, given that genotypic (geographical) variation exists within a subtype. These findings argue the usefulness of the genetic barrier to predict resistance development, and the influence of the genetic background on this parameter. Knowledge extracted from this analysis could be used to enhance prediction of therapy outcome.<br>
As evolutionary simulator of the HIV-1 intrahost population, an ideal Wright-Fisher model of molecular evolution was assumed, which is a well accepted model for evolution in a finite population. A number of assumptions were implemented to reduce the (computational) complexity of the model (see additional file 3). The model did not include recombination. These simplifications may be avoided with availability of a more accurate, but also more computationally demanding simulator. Although recombination can speed up resistance accumulation, the fitness landscape attempts to capture the selective advantage of mutational patterns under drug selective pressure, what is not expected to be influenced by recombination.<br>
<br>
Conclusions<br>
In conclusion, we have demonstrated for the first time the existence of intra-patient variation in genetic barrier to resistance (in this study, to nelfinavir) in patients considered fully susceptible by an expert system. The estimated genetic barrier not only reflects the amount of genetic change needed for resistance, but also takes into account the influence of virus genetic background, evolutionary constraints as well as the relative impact of a mutation on the in vivo fitness. We found that a lower individualized genetic barrier was associated with a higher risk for development of resistance at treatment failure. The genetic barrier to resistance, estimated at baseline, may uncover more information predictive for developing resistance than currently used genotypic algorithms.<br>
<br>
Methods<br>
Clinical data<br>
Clinical data was pooled from the <database>Stanford HIV Drug Resistance Database</database> [25] and from a clinical database maintained at the Molecular Biology Laboratory of Centro Hospitalar de Lisboa Occidental, on behalf of the Portuguese HIV Resistance Study Group.<br>
To investigate a correlation between estimated genetic barrier to NFV resistance and development of NFV resistance at failure, patients were selected that failed on a NFV as first containing treatment, and for whom a protease sequence was available both at start of treatment and at treatment failure. Patients did not have previous PI history. The activity of NFV at baseline and failure was predicted using the Rega v8.0.1 algorithm for genotypic resistance interpretation [21]. Only patients with full genotypic susceptibility to NFV at baseline (Genotypic Susceptibility Score (GSSNFV = 1) were included, and at most one sequence pair per patient. None of these sequences were included in the data used to estimate the fitness function under NFV selective pressure. Genotypic susceptibility (GSSOther) for the therapy combination (excluding NFV) was computed by summing up the individual GSS of the other drugs in the combination. HIV-1 subtype distribution of the population was determined from the protease and partial reverse transcriptase sequences using the <software>REGA</software> HIV-1 Subtype tool v2.0 [26]. Isolates were classified as either subtype B or nonB (Sub), as B was the majority subtype in the longitudinal data set.<br>
To investigate genotypic correlates in protease with the estimated genetic barrier to NFV resistance, evolution under NFV treatment was simulated for a large cohort of susceptible protease sequences (with GSSNFV = 1). At most one sequence per patient was used. The technique of fitness landscape was developed to particularly take into account the large natural diversity of HIV-1, in order to estimate a fitness landscape that could be used across subtypes. However, since the method relies on within patients evolution towards higher fitness under drug selective pressure, evolution from one subtype to another, even if more fit under treatment, will never be observed. Therefore, resistance evolution can never capture fitness differences between subtypes, and thus, in vivo fitness and derived genetic barrier are directly comparable only within each subtype individually, but not across subtypes [15]. To avoid a subtype bias, only subtype B sequences were included.<br>
<br>
NFV fitness function<br>
A fitness landscape of HIV-1 under NFV selective pressure was previously estimated from cross-sectional data, as described in detail in [15] (an overview of the methodology and a list of mutations included in the fitness function can be found in additional file 3). Briefly, we estimated a fitness function compatible with differences in prevalence of mutational patterns observed in sequences from untreated and treatment experienced patients that are the result of convergent evolution under selective pressure modeled by the fitness function. More specifically, we contrasted 7774 sequences obtained from protease inhibitor naive patients with 1026 sequences from patients treated with NFV as single protease inhibitor. These sequences were of diverse subtypes (B: 66%, G: 15%, C: 7% and other: 13%).<br>
Estimated fitness is based on the assumption that when a mutation, or pattern of mutations, is independently fixed in a population under selective pressure of the same treatment in multiple patients, this convergent evolution may indicate that the mutation or pattern increases the fitness of the virus in that environment. Since an interaction between two mutations is expected to lead to a different observed prevalence of one mutation depending on the presence of the other, conditional dependencies in mutations prevalence (identified by Bayesian Network Learning) may indicate epistatic fitness interactions between these mutations [27]. These interactions are incorporated in a multiplicative fitness function, which describes fitness as a product of independent contributions of presence of 114 amino mutations at 48 protease positions, augmented with independent contributions for combinations of interacting mutations. So the fitness contribution of a mutation is dependent on the presence of mutations with which it interacts.<br>
To estimate fitness function parameters, the fitness function was combined with a simulator of HIV-1 intra-host evolution, making the connection between naive protease sequences, treatment selective pressure, and sequences from patients failing treatment. The fitness landscape was scaled so that fitness of the HIV-1 subtype B reference strain HXB2 was 1, so that for any given sequence put in the landscape, a fitness number is computed that represents the relative fitness compared to HXB2.<br>
<br>
Correlation of estimated genetic barrier to NFV resistance with resistance development at treatment failure<br>
The NFV fitness landscape was used to estimate, for each baseline sequence of 201 longitudinal pairs, viral fitness under NFV treatment and the simulated genetic barrier to NFV resistance. The position of the viral sequence in the landscape can be considered as quantification of genotypic susceptibility. For a baseline sequence, this predicted viral fitness under NFV treatment () (fitness number as explained above and expressed in log scale) was computed as the average fitness of 100 baseline sequences, in which nucleotide mixtures were removed from each sequence by random sampling one of the pure nucleotides from the mixture.<br>
The genetic barrier to NFV resistance for a sequence was calculated by simulating HIV-1 evolution using the estimated fitness landscape and the simulator of HIV-1 intra-host evolution [15]. For each sequence, the genetic barrier was quantified as the average number of mutations () or number of simulated generations () until full resistance was predicted by Rega (GSSNFV = 0) in 100 evolution simulation runs. At the start of each simulation, nucleotide mixtures were removed as described before.<br>
The associations of log ,  and  with odds for development of NFV resistance R at failure, were investigated using univariable and multivariable logistic regression models. Variables included in the multivariable models were log ,  or  (per 10 generations), duration between baseline and follow-up sample, GSSOther and subtype distribution (Sub).<br>
<br>
Identifying genotypic correlates of estimated genetic barrier<br>
To investigate genotypic correlates of the estimated genetic barrier to NFV resistance, evolution under NFV treatment was simulated for a virtual clinical cohort of 2764 patients fully susceptible to NFV at baseline (GSSNFV = 1). For each sequence, one simulation run was performed using the fitness landscape and the number of simulated generations GR was recorded until full resistance was predicted by Rega.<br>
Subsequently, a step-wise linear model selection was performed for log GR, in order to investigate the independent multiplicative contributions of the presence of individual mutations to the estimated genetic barrier. The model selection started from an empty model, and considered all 114 mutations that were included in the fitness function model. At each step, the addition to or removal from the model of each mutation was considered, and the change that resulted in a model with lowest Akaike Information Criterion (AIC) score was selected, until no more improvement was observed. All statistical analyses were performed using <software>R</software> 2.2.1 [28].<br>
<br>
<br>
Authors' contributions<br>
KT, KD and GB performed the analyses. KD designed and implemented the analysis software. RJC, EvW, SYR and RWS contributed clinical and virological data. PL, KvL and A-MV contributed to discussions and A-MV supervised the work. All coauthors contributed to the design of the study and interpretation of the results. All authors have read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2964685</b><br>
Quadratic variance models for adaptively preprocessing SELDI-TOF mass spectrometry data<br>
<br>
<br>
Background<br>
Mass spectrometry is a promising technology for biomarker discovery [1]. There are a wide variety of mass spectrometers from which one could choose from during the design of a biomarker discovery experiment, reviewed in [2]. Matrix assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS, or just MALDI) can ionize whole proteins intact over a wide range of protein mass values, making it suitable for biomarker discovery in complex media such as blood serum, where both protein concentrations and masses vary greatly [3]. Surface-enhanced laser desorption/ionization time-of-flight mass spectrometry (SELDI-TOF MS, or just SELDI) [4] is a variant of MALDI that adds an on-chip chromatographic separation step at the front end of the analysis pipeline. This, combined with robot-automated sample preparation, enables SELDI to be high-throughput, an attractive feature for many laboratories. For a recent review of the application of SELDI in the context of biomarker discovery, see [5].<br>
The typical SELDI work flow involves the collection of samples (e.g.- blood serum) from patients, application of the samples to SELDI ProteinChips? selected for desired physicochemical properties, and analysis in the SELDI mass spectrometer. The raw data must be preprocessed to detect relevant peaks which correspond to proteins in the sample. Typical signal preprocessing steps performed are spectral alignment, denoising/smoothing, peak detection, peak matching, normalization, and quantification (see Figure 1 of [6]). The preprocessing of the raw SELDI spectra is typically accomplished using one of several available software packages (reviewed in [6-8]). Artifacts due to insufficient preprocessing of the data have, in the worst case, led to erroneous biological conclusions in early SELDI studies [9-11]. This fact inspired several important comparison studies of SELDI preprocessing algorithms [6-8,12]. We now briefly summarize a few of the major contributions. For a more detailed overview, see the introduction of [6].<br>
Coombes et al introduced the use of wavelets for denoising SELDI spectra [13], providing a more adaptive approach to denoise compared to moving average filters (e.g., as in [14]). Meanwhile, Morris et al introduced the notion of a mean spectrum, which represents average protein activity of a group of spectra. Under non-restrictive assumptions, the mean spectrum has less noise and allows one to circumvent complicated peak matching algorithms that consolidate peak predictions among individual spectra into a consensus prediction. Malyarenko et al introduced a novel baseline removal algorithm based on a proposed charge accumulation model of the saturation phenomenon of the detector [15]. This was one of the first algorithms that was designed from the "bottom-up", starting with physical considerations of SELDI. Later, deconvolution filters were shown to be a possible approach for improving mass resolution of SELDI [16-18].<br>
Sk?ld et al analyzed single-shot spectra [19], the basic components of a final SELDI spectrum obtained by summing the results of many laser shots. They suggested that the observed counts in the single shot spectra may be proportional to a Poisson random variable, proposing a heteroscedastic model for the data. Meuleman et al also make use of single-shot spectra (sub-spectra) to derive a preprocessing algorithm based on analyzing these components separately [20].<br>
In an attempt to improve on the bottom-up approach to preprocessing, we analyze the statistics of the SELDI signal over a wide range of intensity values. Based on data presented herein, we propose a natural exponential family model with quadratic variance function for the statistics of the detector response for SELDI experiments. We believe this model is a plausible explanation for acquisition of single-shot spectra, summing of single-shot spectra into a final spectrum, and extracting protein estimates from a mean spectrum under a unified framework. Under this framework, we introduce a new preprocessing approach, adaptive to changing noise characteristics per spectrum and per experiment, and show favorable peak prediction performance.<br>
<br>
Results<br>
Buffer-only intensity measurements<br>
Electronic measurements exhibit natural random fluctuations [21]. In many cases, these fluctuations are independent of the signal and are modeled as additive white Gaussian noise. In order to understand the nature of the noise fluctuations inherent to SELDI, we study the response of the detector under controlled experiments applying different buffers instead of protein samples under varying laser intensities (as in [22]). This eliminates the complexity introduced by adding serum to the chips while facilitating measurements of ion counts over a wide range of intensity values. In principle, this gives us a set of n repeated experiments from which we can study the statistics of the detector response compounded with noise and interference inherent to SELDI. In this fashion, we have generated two separate buffer + matrix datasets, denoted BUFFER1 and BUFFER2, which represent data generated on the same SELDI PBS IIc machine by different scientists and different machine parameters. BUFFER1/BUFFER2 contain 183/114 spectra, respectively.<br>
We visualize all of the spectra in BUFFER1 and BUFFER2 in Figure 1. In particular, we are interested in analyzing the region between 3 and 30 kDa, since this is the mass focusing region in our experiments. In this region, the observations across spectra for a fixed time (mass) point represent approximately independent, identically-distributed measurements in BUFFER1 or BUFFER2, respectively. Figure 1 shows the median, 75% quantile, and 25% quantile of BUFFER1 and BUFFER2. The median spectrum shows the form of an ordinary measurement, with any measurement between the 75% and 25% spectrum lines considered typical as well.<br>
Figure 1 shows us the behavior of the typical buffer + baseline signal component seen in all SELDI raw spectra. Indeed, we see that changing different machine settings leads to different response properties. For BUFFER2, the median spectral response is large in the range shown, and the distribution of responses is symmetric about the median, whereas the distribution of detector response values for BUFFER1 are heavily skewed, and thus certainly not normally distributed.<br>
We study the detector response (intensity output) for SELDI under varying input conditions, creating a detector response curve as follows. For each fixed time (mass) point across spectra from BUFFER1 in the mass focused region [3kDa; 30kDa], we estimate the mean intensity observed and the corresponding variance, with the same repeated for BUFFER2. These are displayed as a scatter plot in Figure 2 along with the best fit quadratic curve. Observing Figure 2 we see<br>
1. Intensity fluctuation/variance increases monotonically with the mean.<br>
2. The variance of the detector response is a quadratic function of the mean, to a very good approximation<br>
3. The detector response curves for BUFFER1 and BUFFER2 are quite different, and thus are dependent on the machine settings.<br>
The detector response statistics thus exhibit a quadratic variance function. Briefly, a random variable X is said to have a quadratic variance function (QVF) if<br>
(1)V(?)=?0+?1?+?2?2,<br>
with ? being the mean of X, V(?) the variance, and ?0, ?1, ?2 constants, some of which may be zero.<br>
From these observations, summarized in Figures 1 and 2, it seems unlikely that an algorithm optimized for BUFFER1 would work well on BUFFER2 and vice versa. Further, neither a homoscedastic approach (e.g. - standard wavelet shrinkage [23]) or a simple heteroscedastic approach (e.g. - Poisson regression formulation [24]) to preprocessing the data is likely to be sufficient.<br>
<br>
Data for evaluating preprocessing algorithms<br>
We have generated two new datasets for evaluating preprocessing algorithms in order to improve upon purely simulation-based datasets used in previous comparison studies [6,7]. A good comparison dataset should have the following properties (discussed previously in [6]):<br>
1. Exact protein content is known (and thus expectation of where "true" peaks will appear)<br>
2. Analyzed sample is complex containing many proteins/peaks<br>
3. Noise and baseline characteristics should be as close to those of real SELDI data as possible.<br>
If one uses simulated data [6,7,25], complete control can be attained over requirements 1) and 2) at the expense of having noise/baseline characteristics that are overly ideal. If one uses purely real data, the noise, baseline, and artifacts that arise in actual experiments are present. However, this usually accompanies the trade-off of either not knowing the exact protein content (e.g.- complex serum data) or an overly simplified scenario (e.g. - spike-in data).<br>
We combine the advantages of purely simulated and real data by introducing the notion of a hybrid spectrum. To generate a hybrid spectrum, we use an implementation of the <software>SimSpec</software> 2.1 SELDI simulator [25,26]http://bioinformatics.mdanderson.org/Software/Cromwell/simspec.zip to generate a "clean" SELDI spectrum, shown at the top of Figure 3. This gives an accurate peak shape characteristic as would be seen in low resolution SELDI/MALDI for given mass and ion abundance values, without any electronic noise or baseline present. We then select one of our buffer + matrix spectra (from either BUFFER1 or BUFFER2) and add the two together to produce the hybrid spectrum shown at the bottom of Figure 3. Thus, in a hybrid spectrum we know the exact virtual protein content specified to the simulator a priori while maintaining exactly the same noise, baseline, and other artifacts one encounters with real SELDI data.<br>
Further details on the hybrid spectra can be found in the Methods section and in Additional file 1. The collection of hybrid spectra under different operating conditions results in test sets, denoted HYBRID1 and HYBRID2, with each test set containing thirty datasets of fifty hybrid spectra each. The mean performance of a preprocessing algorithm on HYBRID1 and HYBRID2 can be interpreted as the expected performance of the preprocessing approach in each separate operating condition in a repeated experiment or sampling from a homogeneous population (e.g. - cancer group or control group).<br>
<br>
New preprocessing algorithms for SELDI<br>
We have developed a set of <software>MATLAB</software>? scripts for preprocessing SELDI spectra named <software>LibSELDI</software>. For information on how to obtain <software>LibSELDI</software> and the associated scripts used to produce the figures in this paper, contact the authors. We compare our preprocessing package to the <package>MassSpecWavelet</package> package from the <database>Bioconductor</database> project [27]. <package>MassSpecWavelet</package> has been established as one of the best approaches in terms of peak finding in recent comparison studies [6,7], and has been downloaded &gt; 6000 times in the past two years as of March 2010 http://bioconductor.org/packages/stats/bioc/MassSpecWavelet.html. Both packages have the advantage of having only one main user-adjusted parameter.<br>
In order to compare the performance of each preprocessing program, we generate operating characteristic curves (OC curves) [6,20], one for each of the 30 datasets of HYBRID1 and HYBRID2, by varying the Peak Area threshold (<software>LibSELDI</software>) and signal-to-noise ratio threshold (Snr.Th in <package>MassSpecWavelet</package>) parameters in the programs. Code snippets showing how <package>MassSpecWavelet</package> was tested can be found in Additional file 1. This allows us to understand the trade-offs between false discovery rate (FDR) and sensitivity (TPR) achieved by each algorithm. The results for both the HYBRID1 and HYBRID2 collections are shown in Figure 4, where we have plotted the FDR-axis in log scale to emphasize the low FDR region which is usually of most interest in biomarker discovery applications. Note that, since both HYBRID1 and HYBRID2 are collections of datasets representing repeated trials (or equivalently a homogeneous population), the OC curves we show in Figure 4 are the mean OC curves across the 30 datasets for each.<br>
The results show that <software>LibSELDI</software> tends to have a considerable advantage in the low FDR region, while <package>MassSpecWavelet</package> tends to have higher sensitivity for FDR &gt; 25%. One way to summarize the performance of the algorithms is using the area under the OC curve for the FDR region of interest. We compute two area under the curve values, PAUC [6] (calculated for FDR ? [0, 50%]), and PAUC25 (calculated for FDR ? [0, 25%]). The results are shown in Table 1, where we have normalized each score separately so that a perfect PAUC25 (likewise, PAUC50) score is 100.<br>
In Figure 5, we show the specific operating characteristics for <software>LibSELDI</software> and <package>MassSpecWavelet</package> for Dataset 2 of HYBRID1. While both algorithms perform well, <software>LibSELDI</software> resolves more than 90 proteins correctly before making a mistake. Since operating characteristics show false discovery rate along the x-axis rather than false positive rate (as in the traditional ROC curves), they tend to penalize more when false predictions are made with very few true proteins found. Indeed, in this case <package>MassSpecWavelet</package> got its first protein prediction correct but its second prediction wrong, leading to the point at FDR = 50%, TPR = 7%. Thus, operating characteristics with false discovery rate along the x-axis enforce the principle of conservative decision making, rewarding approaches that are successful with their initial large threshold (conservative) predictions and penalizing those that make mistakes early.<br>
At FDR values greater than 30%, <package>MassSpecWavelet</package> outperforms <software>LibSELDI</software>. However, this is at the expense of generally more promiscuous predictions, since <package>MassSpecWavelet</package> generates 586 potential protein predictions compared to 250 for <software>LibSELDI</software>.<br>
<br>
<br>
Discussion<br>
We posit that the detector response is a member of the Natural Exponential Family with Quadratic Variance Function (NEF-QVF), which is a proper subset of the exponential family of distributions [28]. Figures 1 and 2 show that assuming the detector response takes the form of a specific distribution is impractical, but that the detector response V(?) has a QVF. The NEF-QVF family of distributions occur often in practice and have the following useful properties, characterized by Morris [28]:<br>
1. If a random variable X ? NEF-QVF, it is completely specified by its variance function V(?)<br>
2. If X ? NEF-QVF, a, b constants then aX + b is also NEF-QVF<br>
3. Additivity: If X1; X2 ? NEF-QVF, then X1 + X2 is NEF-QVF<br>
4. Affine combinations of normal, Poisson, gamma, binomial, negative binomial, and generalized hyperbolic secant distributed random variables generate all possible distributions in the NEF-QVF family.<br>
There are some physical reasons as to why the NEF-QVF assumption could be reasonable as well. Some plausible justifications for the first two terms in Eq. (1) are:<br>
1. Constant Term: This is possibly due to thermal noise (additive Gaussian noise) which is common to all electronic measurement devices [21]<br>
2. Linear Term: The ability to detect an ion in a multiple stage electron multiplier, a common type of detector in MALDI-like instruments, is described by compound Poisson statistics [29].<br>
The existence of a plausible physical explanation for the quadratic variance term remains an open question. However its effect is measured in both BUFFER1 and BUFFER2 and cannot be neglected. While the QVF model explains the data well in the mass focused region between 3 and 30 kDa, it is likely to break down at lower masses around 2-2.5 kDA where the baseline reaches a maximum. In this region the detector often saturates, introducing a non-linearity into the data that we have not accounted for.<br>
The success of our univariate model for SELDI may indicate that we have selected the most important feature to consider in the preprocessing of the data: namely, the fluctuations in the response of the ion detector subject to different inputs. The analysis of expression values of preprocessed data, on the other hand, requires multivariate methods as there are significant statistical dependencies between the peak heights corresponding to proteins that may be interacting. While these correlations are important in the analysis performed after the data is preprocessed, our results indicated it may be safe to ignore them during the preprocessing. While we have shown <software>LibSELDI</software> to be accurate for estimating peak m/z values, we have not assessed the usefulness approach for estimating peak intensities in this work. The utility of <software>LibSELDI</software> for accurately estimating peak intensities remains an open question and subject of future work.<br>
It is entirely possible that the quadratic variance model could be applicable to other similar technologies such as MALDI and newer SELDI mass spectrometers. This, however, has not been confirmed.<br>
Having buffer only spectra allows one to estimate the parameters of the detector response curve. Knowledge of the detector response curve enables us to apply the modified Antoniadis-Sapatinas denoising scheme described in the methods. Using this approach in our <software>LibSELDI</software> package yields excellent peak detection performance. We have proved this concept on HYBRID1 and HYBRID2 by estimating the QVF parameters of (1) using the buffer-only spectra that were randomly selected from BUFFER1 and BUFFER2 respectively. This implies that spots on SELDI chips should be reserved for buffer-only spectra. Thus, the trade-off for using our approach is increased cost in terms of the number of chips one must use. The modified Antoniadis-Sapatinas denoising is computationally intensive as well, taking approximately seven minutes per spectrum on a high-end workstation.<br>
We argue that some of the cost is recovered by the potential for adaptive and accurate preprocessing, but not all. It may be possible to use QC and/or calibration samples to estimate the QVF as well rather than buffer-only spots. However, this would add in some additional variation due to the nature of the medium (serum, plasma, etc).<br>
While <software>LibSELDI</software> outperforms <package>MassSpecWavelet</package> on the HYBRID1 and HYBRID2 test sets, the applicability of this comparison and of these results to purely real data remains an open question. There is some basic biological variability modeled in our test sets (see description in supplement of [6]). However, data from complex biological samples such as serum or plasma likely contains more biological variation and artifacts than we have modeled in HYBRID1 and HYBRID2. The investigation of how biological variation affects the model in QC samples is a work in progress.<br>
In addition to achieving a better mean OC curve at lower FDR values, <software>LibSELDI</software> consistently predicts fewer peaks than <package>MassSpecWavelet</package>, leading to protein predictions closer to the true number of proteins in the data, as shown in Figure 6. This is further evidence that the adaptive modified Antoniadis-Sapatinas denoising approach using the NEF-QVF model for the detector response is smoothing the spectra by close to the right amount.<br>
<br>
Conclusions<br>
We have shown that the variance of the intensity of a SELDI spectrum is quadratic in the mean signal strength. We further make the flexible assumption that the underlying distribution of the intensities is from a natural exponential family. From this point of view, we use a modified Antoniadis-Sapatinas wavelet shrinkage approach for denoising SELDI spectra. With this method at the core of our <software>LibSELDI</software> program for preprocessing SELDI data, we demonstrate excellent sensitivity at low false discovery rates. For applications that can tolerate higher false discovery rates, the <package>MassSpecWavelet</package> algorithm performs better in this region.<br>
Our work has implications in the design of SELDI experiments. Namely, the modified Antoniadis-Sapatinas denoising technique performs well but requires an estimate of the quadratic variance function (QVF) describing the SELDI detector. This, in turn, is affected by machine settings. We have used buffer-only spectra to estimate the QVF. Thus, buffer-only spots could be interlaced on chips. We are investigating less expensive ways to estimate the QVF in future work.<br>
<br>
Methods<br>
Protocol for generating buffer-only spectra<br>
Buffer-only spectra were generated by interspersing buffer only samples with protein samples from subjects (e.g. serum samples) and with pooled subject samples (for quality control) on the same chip. The buffer-only samples were spotted with wash buffer that was either PBS (phosphate buffered saline with various concentrations of phosphate and NaCl) based or acetonitrile + TFA (triflouroacetic acid) based, as manufacturer recommended per chip type. These buffer only samples were processed with the same washing steps as the subject samples, as described in [22], and then SPA matrix was applied to all spots.<br>
The samples were analyzed with the Protein Biological System IIc? SELDI mass spectrometer (Ciphergen Biosystems, Freemont, CA). The machine settings (e.g. laser intensity, detector sensitivity) and precise washing steps varied from buffer only spot to buffer only spot, and were generally different between BUFFER1 and BUFFER2. Note especially that laser intensities were generally higher for BUFFER2 than for BUFFER1. A detailed list of machine settings is given in the Additional file 1.<br>
<br>
Hybrid data<br>
Calculating performance statistics for comparison of <package>MassSpecWavelet</package> and <software>LibSELDI</software> requires a large number of spectra emulating an experiment that was repeated many times. To generate the HYBRID1 dataset, we combine each clean spectrum with one buffer+matrix spectrum from BUFFER1, and similarly we form HYBRID2 from BUFFER2 by combining those spectra with the same clean spectra.<br>
A basic model of repetitive experiments for SELDI is available with <software>SimSpec</software> 2.1 that takes into account fluctuations in protein concentrations, m/z values, and prevalence in the data. Using the <software>SimSpec</software> 2.1 model developed at the MD Anderson Cancer Center [25,26], we generate 30 datasets containing 50 clean (noise and matrix-free) spectra each. Each dataset consists of 150 virtual proteins and each spectrum within the given dataset contains a proper subset of these proteins with fluctuating parameters according to the model described in [25] and its supplement. The goal for the preprocessing programs in our performance evaluation is to reconstruct the master list of 150 virtual proteins characterizing the dataset. Repeated across all 30 datasets, we can calculate useful performance statistics. The properties of the 150 virtual proteins themselves are drawn from a prior distribution that was estimated from real data. See [25], or alternatively, the description in the supplement of [6].<br>
We use sampling to overcome the limitation of having much fewer spectra in BUFFER1 and BUFFER2 than we have clean spectra in preparation for testing the algorithms. In principle the best way to construct the hybrid test sets would be to have one unique spectrum in BUFFER1 (likewise BUFFER2) for each spectrum in our clean protein-only set. However, this would require 1500 buffer+matrix runs to be performed for both BUFFER1 and BUFFER2, an impractical amount of blank chips to run. Sampling from BUFFER1 (BUFFER2) provides a cost effective way to introduce variation in the noise/matrix characteristics between the datasets in HYBRID1 (HYBRID2).<br>
<br>
Preprocessing the spectra<br>
First we consider a model for a single SELDI spectrum, X(t). We observe X(t), a random process, on a discrete time grid t1,..., tm, where X(t) represents the intensity of the raw SELDI spectrum observed at time (equivalently mass) point t. For all t, we assume that X(t) is distributed according to a natural exponential family (NEF) with quadratic variance function (QVF) equal to V(?(t)) as in Eq. (1). The variance function V(?) completely characterizes the NEF-QVF family. The goal of preprocessing in SELDI is to estimate ?(t), the expectation of X(t), which is the signal corresponding to ions that hit the detector. With a good estimate of ?(t), extracting peaks and estimating protein m/z values in a dataset is relatively straightforward.<br>
As a side note we point out that a SELDI spectrum is actually a sum of single shot spectra. However, the additivity property of the NEF-QVF family guarantees the sum is NEF-QVF provided that the single-shot spectra are NEF-QVF, agreeing with our detector response model and experimental observations.<br>
Multiple spectra considerations<br>
Rather than observe a single spectrum, the typical biomarker discovery approach is to generate at least one spectrum for each of n samples from an approximately homogeneous population. For example, one homogeneous population may be a group of early stage prostate cancer patients matched for age, race, etc. Assuming the samples are run on the same SELDI machine with the same operating conditions, we have<br>
(2)X1(t),...,Xn(t)??NEF-QVF(V(?(t))).<br>
Our assumption that all n patients have the same underlying ?(t) is equivalent to assuming that the underlying biological condition being observed in each patient is approximately the same. Thus, we wish to estimate the underlying commonality ?(t) related to the biology of their condition expressed through the SELDI signal. We can mitigate some of the effects of the QVF by forming the mean spectrum (first introduced by [25]).<br>
(3)X?(t)=1n?k=1nXk(t).<br>
It is straightforward to show that<br>
(4)E{X?(t)}=?(t)<br>
(5)Var(X?(t))=1nV(?(t)).<br>
Thus, the mean spectrum concept is valuable under the assumptions of the NEF-QVF model as well.<br>
<br>
Modified Antoniadis-Sapatinas denoising<br>
We now discuss estimation of ?(t) from the mean spectrum (3). Since the Xk(t) are sampled on a discrete time grid (and thus X?), we introduce vector notation<br>
x?=[X?(t1),...,X?(tm)]??=[?(t1),...,?(tm)]?.<br>
For any estimate ?^(x?) of, ? we measure its fitness using the mean-squared-error (MSE)<br>
(6)M?S?E(?^(x?),?)=E{??^(x?)???2}.<br>
Antoniadis and Sapatinas proposed a wavelet shrinkage scheme to solve for ?^ in (6) in the context of NEF-QVF regression [30]. We summarize their main results. For our denoising, we use the orthogonal discrete wavelet transform with respect to the Symmlet 8 basis [31]. The transform can be represented by an m ? m orthogonal matrix W,<br>
(7)w=Wx?.<br>
Let h be a length m vector with entries taking values between 0 and 1. Let H = diag(h) be the m ? m matrix defined by placing the entries of h along the main diagonal, all other entries 0. The class of estimators for ?^(x?) considered by [30] take the form<br>
(8)?^(x?)=W?Hw=W?HWx?.<br>
This is the typical wavelet denoising scenario where each wavelet coefficient is left alone or shrunk towards zero according to some criterion, and is completely defined by the vector h. Antoniadis and Sapatinas showed that a good estimator for data from the NEF-QVF family is given by choosing<br>
h?(i)=[w(i)2??^2(i)]+w(i)2,i=1,...,m[z]+={z,z?00,z&lt;0.<br>
The term ?^2 is estimated as<br>
(9)?^2=11+?2(W?W)V(x?).<br>
Where V(x?) is the vector constructed by applying the QVF from (1) to each term of x?. (W ? W ) is the matrix whose i, j element is the square of the i, jelement of W. The parameters ?0, ?1, ?2 in (1) are measured from the buffer-only spectra, as described in the Results and Discussion section.<br>
We make an intuitive modification to (9)<br>
??2=11+?2(W?W)V?(x?).V?(x?(i))=max{V(x?(i)),?0}.<br>
Thus our modified Antoniadis and Sapatinas estimator h? uses ?^2 in (8) rather than ?^2. The modification was introduced to account for cases when (9) may underestimate the noise when low amounts of observed signal are detected. Define<br>
h?=[w(i)2???2(i)]+w(i)2H^=diag(h?).<br>
Then, our modified Antoniadis-Sapatinas estimate of ? is defined as<br>
(10)??=W?H?Wx?.<br>
<br>
Peak detection/baseline removal<br>
We consolidate the two preprocessing steps of baseline removal and peak detection typically performed separately into a single step as follows. We assume that the underlying ?(t) shown in (4) is the superposition of protein ions, s(t), and energy-absorbing matrix ions, b(t) striking the detector. It is well known that the distribution of the isotopes in our analyte of interest gives rise to a roughly Gaussian peak shape. Thus, we propose<br>
(11)?(t)=s(t)+b(t)<br>
(12)s(t)=?jajG3?j(tj,?j)<br>
where, G?(tj,?j) denotes a Gaussian kernel function centered at tj with standard deviation ?j and zero outside the interval [tj - ?, tj + ?].<br>
Typically, s(t) is very sparse in the sense that it is mostly zero over the domain of the observed signal. Therefore, the local minima of our estimated baseline + noise signal ?? are points we may assume touch the baseline. From this point of view, once we have detected all the local minima in ??, the baseline curve estimation problem reduces to an interpolation problem amongst these points. We have found through experimentation that piecewise cubic Hermite interpolating polynomials [32] are excellent interpolation functions.<br>
The minima and maxima in ?? are found in one pass using the extrema function downloadable from <software>MATLAB</software>? central file exchange. The maxima are the peaks in the mean spectrum potentially indicating proteins represented in our sample population while the minima correspond to samples from the baseline signal.<br>
Each detected peak is quantified using peak area and a threshold is chosen based on the peak area measurement to generate the final prediction set.<br>
<br>
<br>
Operating characteristics<br>
The peaks we detect in ?? represent the initial set from which we choose our final estimates of proteins that are active in the population of interest. The choice of final estimate is accomplished using a peak area threshold (<software>LibSELDI</software>) or signal-to-noise ratio measurement (Snr.Th in <package>MassSpecWavelet</package>). From each prediction, we calculate the observed false discovery rate (FDR) and true positive rate (TPR, also called sensitivity)<br>
(13)FDR=FPFP+TP<br>
(14)TPR=TPTP+FN.<br>
Where TP (the number of true positives) is the number of the 150 virtual protein m/z values having at least one predicted m/z value within 0.3% relative error. The FP is defined as the number of predicted m/z values not within 0.3% of any of the 150 virtual protein m/z values for this dataset. Similarly, FN is the number of the 150 virtual protein values without any predicted m/z value within 0.3% relative error.<br>
For each dataset, a curve is fit to the operating points. Each operating curve is averaged to produce a mean operating characteristic, as shown in Figure 4. From this curve, the calculation of the area-under-the curve is straightforward. For more details, see sections 2.2 and 2.2.1 of [6].<br>
<br>
<br>
Authors' contributions<br>
VE and BG conceived and developed the theoretical aspects of the study, analyzed the results, and wrote the manuscript. VE developed <software>LibSELDI</software> and ran the simulations. Both authors have read and approve of the manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2973963</b><br>
A genetic ensemble approach for gene-gene interaction identification<br>
<br>
<br>
Background<br>
It is widely acknowledged that complex diseases are most likely caused by a combination of environmental factors and interactions among multiple genes [1]. With the fast development of the genotyping technologies, single nucleotide polymorphisms (SNPs) have become one of the most commonly used biomarkers for disease associated gene identification in case-control designed genome wide association (GWA) studies [2-5]. However, there are several practical problems in analyzing the SNP genotype data. First, to identify true disease associated SNPs from a massive set of candidate SNPs, an accurate SNP selection strategy is of critical importance. However, the accurate identification of disease associated SNPs for phenotype classification is hindered by the curse-of-dimensionality and the curse-of-sparsity [6]. Furthermore, the datasets generally contain high level of data noise, high redundancy, and many missing values, and most seriously, it is evident that epistasis is a ubiquitous phenomenon in complex diseases [7,8], or in other words, gene-gene interactions and gene-environment interactions are likely to be important contributors to the development of many complex diseases (note the phrases gene-gene interaction and SNP-SNP interaction are used interchangeably in this paper). The explanations from the biological perspective are as follows: (1) a SNP in a coding region may cause amino acid substitution, leading to the functional alteration of the protein; (2) a SNP in a promoter region can affect transcriptional regulation, causing the change of the protein expression abundance; and (3) a SNP in an intron region can affect splicing and expression of the gene [9]. All these effects contribute quantitatively and qualitatively to the ubiquity of bio-molecular interactions in biological systems. Although it is a common characteristic in complex disease development, the identification of those genetic interactions have been proven to be very challenging [10].<br>
Most of the earliest studies focused on identifying a set of SNPs in which individual SNP has a strong association with the phenotype by applying statistical measures such as ?2-statistic and logistic regression [11,12]. However, several problems arise when applying these methods. First, it is unclear how to best adjust the resulting p-values after testing for a very large number of possibly non-independent hypotheses. Second, complex diseases are usually caused by the action of multiple genes in a nonadditive fashion. The standard analytical approaches in GWA studies often proceed by identifying only a very small number of SNPs (usually only one or two) that exhibit strong statistical association with the target phenotype. In other words, only SNPs that independently have a strong discriminating ability are selected, while other SNPs that individually have weaker association are not discovered [13]. However, it is common that a combination of two or more SNPs, each having weak association with the phenotype, can classify the phenotypes of samples with a higher accuracy. This is natural since complex diseases are most likely caused by multiple genes and their interaction with environmental factors.<br>
To cope with these problems, it is desirable to develop new methods which can consider multiple loci jointly. A number of such methods have been developed recently. Among these methods, nonparametric methods like Polymorphism Interaction Analysis (PIA) [14], Multifactor Dimensionality Reduction (MDR) [15], and Combinatorial Partitioning Method (CPM) [16] are the most popular ones probably due to their good generalization property on different interaction models. Specifically, PIA tries to apply multiple evaluation metrics for ranking and scoring SNP combinations while MDR and CPM attempt to modify the feature dimension to discriminate SNP-SNP interactions. However, there is no one-size-fits-all method for the detection and characterization of gene-gene interaction relationships in GWA studies. Several comparison and evaluation studies suggested that applying a combination of multiple complementary algorithms, each having its own strength, could be the most effective strategy to increase the chance of a successful analysis [10,17,18].<br>
A recent study by McKinney et al. [19] proposed to use a combination of a machine learning filter and an information theoretic approach to identify SNP-SNP interactions. McKinney et al. found that combining the set of SNP-SNP interactions into a graph can yield interesting insights about the underlying biological processes. It is anticipated that similar network-based analysis approaches can be used as a down-stream analysis for any gene-gene interaction identification algorithms.<br>
In this study, we attempt to address the problem from an alternative perspective by converting the issue into a combinatorial feature selection problem. From the data mining perspective, a sample from a SNP dataset of an association study is described as a SNP feature set of the form si={g1, g2, ..., gn}, (i = 1, ..., m) where each SNP, gi , is a categorical variable which can take the value of 0, 1, and 2 for genotypes of aa, Aa, or AA at this locus, and m is the number of samples in the dataset. The dataset can, therefore, be described as an m ? n matrix Dmn={(s1, y1), (s2, y2), ..., (sm, ym)}, where yi is the class label of the ith sample. One can evaluate the discrimination ability of a set of SNPs jointly by applying the following two steps:<br>
? Generating a reduced SNP feature set s?i={g1,g2,?,gd}, (s?i?si) in a combinatorial manner which restrains the dataset matrix into Dmd={(s?1,y1),(s?2,y2),?,(s?m,ym)}. A key observation is that feature selection algorithms which evaluate SNPs individually are not appropriate since they cannot capture the associations among multiple SNPs.<br>
? Creating classification hypothesis h using an inductive algorithm, and evaluating the quality of the trained classification model using criteria such as accuracy, sensitivity, and/or specificity with an independent test set.<br>
Without lose of generality, we use notation s to denote applying a SNP subset to restrain the SNP dataset Dmn. If a SNP combination s yields a lower misclassification rate than others, we shall consider that it possibly containing SNPs with main effects or SNP-SNP interactions with major implications. We now have two challenging problems for the SNP interaction identification. The first challenge is to generate SNP combinations efficiently since the number of SNP combinations grows exponentially with the number of SNPs and it is infeasible to evaluate all possible combinations exhaustively. The second challenge is to determine which inductive algorithm should be applied for the goodness test of SNP combinations. To tackle the first problem, we shall apply genetic algorithm (GA) since it has been demonstrated to be one of the most successful wrapper algorithms in feature selection from high-dimensional data [20,21].<br>
Furthermore, its intrinsic ability in capturing nonlinear relationships [22] is valuable for modeling various nonadditive interactions. With regard to the second problem, there is no guiding principle on which inductive algorithms are preferable for identification of multiple loci interaction relationships. However, a promising solution is to employ an ensemble of classifiers and then to integrate/balance the evaluation results from these classifiers [23]. The key issue in applying this method is that the base classifiers used for ensemble integration should be able to capture multiple SNP interactions which commonly have nonlinear relationships. This may be achieved by using appropriate nonlinear classifiers.<br>
The rationale of using an ensemble of classifiers can be described as follows: suppose that a given classifier i generates a hypothesis space ?i for sample classification. If the number of training samples m is large enough to characterize the real hypothesis f (in this context, f is the set of disease associated SNPs and SNP combinations) and the data are noise-free, the hypothesis space generated by i should be able to converge to f through training. However, since the amount of the training samples is often far too small compared to the size of the hypothesis space which increases exponentially with the size of the features (SNPs), the number of hypotheses a classifier can fit the available data is often very large. One effective way to constrain the hypothesis space is to apply multiple classifiers each with a different hypothesis generating mechanism. If each classifier fulfills the criteria of being accurate and diverse [24], it can be shown that one is able to reduce the hypothesis space to better capture the real hypothesis f by combining them with an appropriate integration strategy [25].<br>
By combining GA and the ensemble of classifiers, we obtain a genetic ensemble (GE) algorithm for gene-gene interaction identification. The proposed algorithm has the following advantages:<br>
? It is a nonparametric and model-free approach. Unlike traditional parametric methods (e.g. linear regression etc.), there is no need to specify and assume the number of parameters and the interaction models. As a consequence, the proposed method generalizes well and can capture a range of interaction relationship such as additive and dominant effects.<br>
? It accommodates the detection of both linear and nonlinear relationship of gene-gene interactions. As aforementioned, the ensemble could be formed by classifiers with nonlinear separation abilities. Both linear and nonlinear gene-gene interaction relationships are common in complex disease [26], and could be captured by a nonlinear classifier [27].<br>
? Unlike many other methods which often study different sizes of multi-loci interactions separately and repeatedly, our algorithm identifies different sizes of interactions in parallel. This feature makes the proposed algorithm particularly attractive in identifying higher-order gene-gene and gene-environment interactions.<br>
? The system is flexible. Different inductive algorithms as well as integration methods can be readily added in for further improvements.<br>
One other motivation for developing alternative methods for SNP-SNP interaction identification is in the hope that different algorithms may complement each other to increase the overall chance in identifying true interaction relationships. Therefore, to evaluate the degree of complementarity of multiple algorithms for SNP-SNP interaction identification is also an important part of this study. Specifically, based on the notion of double fault [28], we designed a formula for calculating the co-occurrence of mis-identification which gives an indication of the degree of complementarity between two different algorithms. Accordingly, the joint identification of using multiple algorithms is derived. Therefore, the contribution of this work is two-fold: (1) designing a genetic ensemble algorithm for SNP-SNP and SNP-environment interaction identification; and (2) proposing a method for evaluating the degree of complementarity among multiple algorithms.<br>
<br>
Methods<br>
Overview of genetic ensemble<br>
In our previous study, a multi-objective GA system is implemented for high dimensional data analysis [29]. Here, we implement the GE algorithm in a similar way. The algorithm executes in an iterative manner and results collected through multiple iterations are used to assess the relative importance of SNPs and SNP-SNP combinations.<br>
As illustrated in Figure 1, the GE algorithm is applied to SNP selection repeatedly. In each run, randomly generated SNP subsets are fed into an ensemble committee for goodness evaluation. Two classifier integration strategies namely blocking and voting, and a diversity promoting method called double fault statistic are employed to guide the optimization process. When the evaluation of a SNP subset is completed, the feedbacks of this SNP subset are combined through a given set of weights and sent back to GA as the overall fitness of this SNP subset. After the entire population of GA is evaluated, selection, crossover and mutation are applied and the next generation begins. At the last generation of GA, the chromosome with the highest fitness is selected, and the SNP subset it represents is said to be the best SNP subset genrated by GA. The entire GA procedure is repeated (with different seeds for random initialization) n times (n = 30 in our experiments) to generate n best SNP subsets. These SNP subsets are then analyzed to identify frequently occurring SNP-pairs, SNP-triplets, and higher-order SNP combinations.<br>
For SNP interaction identification, a combinatorial ranking is applied to the n selected SNP subsets. Each possible SNP combination is then given an identification frequency score (the number of times it appears divided by the total number of iteration n). For example, if the SNP combination {snp1, snp2} appears in 25 out of 30 iterations, then its identification frequency score is 25/30 = 0.833. Two alterative criteria can be used to decide whether a SNP combination should be called or not. The first criterion is to set a frequency score cut-off, say 0.8, and call all SNP combinations with frequency score higher than this cut-off as functional SNP combinations. The second criterion is to set a cut-off rank, and call all SNP combinations with equal or higher to that rank as functional SNP combinations. As will be demonstrated in subsequent sections, the choice between this two criteria is likely a balance between detection power and false discovery rate.<br>
<br>
Genetic algorithm<br>
The number of SNPs considered by the genetic ensemble algorithm for potential interaction detection ranges from the lower bound of 2 to the upper bound of d, where d is the "chromosome" size of GA. The size of the GA chromosome has two implications. Firstly, it controls the number of factors we can identify. For example, if the size of d = 15 is used, we can identify from 2-factor up to 15-factor interactions in parallel. Secondly, d also influences on the size of the combinatorial space to be explored. It is a trade-off between the computational time and the combinatorial space to be searched. Therefore, for different SNP sizes (that is, the number of SNPs in the dataset), we shall use different sizes of d accordingly. Similar to the size of GA chromosome, the population size p and the generation of GA g are also specified according to the SNP size in the dataset. In our implementation of the GE algorithm, the parameters d, p, and g can be specified by users. The default values of these parameters are chosen empirically such they work well in a range of datasets.<br>
For GA selection operation, we employ the tournament selection method as it gives the control of convergence speed. Specifically, the tournament selection size, denoted as t, is dependent on the size of the population, varying from 3 to 7. The measure for determining the winner is as follows:<br>
(1)Winner=argmaxs?pfitness(Ri(p))?(i=1,2,...,t)<br>
where Ri(.) is the random selection function which randomly selects gene subset s from the GA population p, t is the tournament size, and fitness(.) determines the overall fitness of the randomly selected gene subset. Single point crossover is adopted with the probability of 0.7. In order to allow pair mutations, we implemented a multi-mutation strategy; that is, when a single mutation occurs (configured with the probability of 0.1) on a chromosome, another single point mutation may occur on the same chromosome with a probability of 0.25 and so on. The chromosome coding scheme is to assign an id to each SNP in the dataset, and to represent the chromosome as a string of SNP ids which specify a selected SNP subset. For each position on a chromosome, it could be a SNP id or a "0" which specifying an empty position.<br>
Therefore, different sizes of SNP combinations are explored in a single GA population in parallel. Table 1 summarizes the parameter settings.<br>
The fitness of GA is defined as follows:<br>
(2)fitness(s)=w1?fitnessB(s)+w2?fitnessV(s)+w3?fitnessD(s)<br>
where s denotes a SNP combination under evaluation. The functions fitnessB(s), fitnessV (s) and fitnessD(s) denote the fitness of a SNP combination s as evaluated by the blocking, voting and double fault diversity measures, respectively. A complexity regularization procedure is implemented in the GE algorithm to favor shorter SNP combination if two SNP combinations have the same fitness value. The computation details of each component of the fitness function are described in the next section.<br>
<br>
Integration strategies<br>
Blocking<br>
Blocking is a statistical strategy that creates similar conditions to compare random configurations in order to discriminate the real differences from differences caused by fluctuation and noise [30]. Suppose a total of M classification algorithms, each having a different hypothesis denoted as his, (i = 1, ..., M), are used to classify the data using a SNP subset s. The fitness function determined by blocking integration strategy is as follows:<br>
(3)fitnessB(s)=?i=1MBC(p(t|his,D),y)<br>
where y is the class label vector of the test dataset D, function p(.) predicts/classifies samples in D as t using his, and BC(.) is the balanced classification accuracy devised to deal with the dataset with an imbalanced class distribution. In the binary classification, it is the area under ROC curve (AUC) [31], which can be approximated as follows:<br>
(4)BC(p(t|his,D),y)=Se+Sp2<br>
(5)Se=NTPNcase?100,?Sp=NTNNcontrol?100<br>
where Se is the sensitivity value calculated as the percentage of the number of true positive classification (NTP ) divided by the number of cases (Ncase). Sp is the specificity value calculated as the percentage of the number of true negative classification (NTN) divided by the number of controls (Ncontrol). Such a balanced classification accuracy measure can accommodate the situation in which the dataset contains an imbalanced class distribution of cases and controls [32].<br>
The idea of applying this strategy for classifier integration in SNP selection is that by using more classifiers to validate a SNP subset, we are able to constrain the hypothesis space to the overlap region ?o, increasing the chance of correctly identifying functional SNPs and SNP-SNP interactions.<br>
<br>
Voting<br>
The second classifier integration strategy applied in our GE system is majority voting [33]. Majority voting is one of the simplest strategies in combining classification results from an ensemble of classifiers. Despite its simplicity, the power of this strategy is comparable to many other more complex methods [34]. With a majority voting of M classifiers, consensus is made by k classifiers where:<br>
(6)k?{M/2+1:if?M?is?even(M+1)/2:if?M?is?odd<br>
Again, suppose a total of M classifiers, each having a different hypothesis denoted as his, (i = 1, ..., M), are used to classify the data using SNP subset s, the fitness function derived from majority voting is as follows:<br>
(7)fitnessV(s)=BC(Vk(t?|?i=1Mp(t|his,D)),y)<br>
where y is the class label vector of the test dataset D, Vk(.) is the decision function of majority voting, and t' is the voting prediction. Here the balanced classification accuracy BC(.) is calculated with voting results.<br>
The reason for using the majority voting integration is to improve sample classification accuracy while also promoting the diversity among individual classifiers implicitly [35].<br>
<br>
Double fault diversity<br>
The third objective function is an explicit diversity promoting strategy called double fault statistic. This statistic is commonly used to measure the diversity of ensemble classifiers [28].<br>
Let ca, cb ? {F, S} in which F denotes the sample being misclassified by a classifier while S denotes the sample being correctly classified. We define Ncacb as the number of samples that are (in)correctly classified by two classifiers in which the correctness of the two classifiers is denoted by ca and cb respectively. Using this notation, we can obtain the term:<br>
(8)D(p(t|hcas,D),p(t|hcbs,D))=NFFN<br>
which is the estimation statistic of coincident errors of a pair of classification models hcas and hcbs (hence the name "double fault") in classification of a total of N samples in test dataset D, using SNP subset s.<br>
The fitness with regard to the diversity measurement of M classifiers over subset s (denoted as fitnessD(s)) derived from the double fault statistic are defined as follows:<br>
(9)fitnessD(s)=1?2M(M?1)?a=1M?b=a+1MD(p(t|hcas,D),p(t|hcbs,D))<br>
The value of this fitness function varies from 0 to 1. The value equals 0 when all classifiers misclassified every sample. It equals 1 when no sample is misclassified or there is a systematic diversity, leading to no sample been misclassified by any pair of classifiers.<br>
<br>
<br>
Classifier selection<br>
The motivation of applying nonlinear classifiers is based on the assumption that nonlinear and nonadditive relationships are commonly presented in gene-gene interaction [26]. This is particularly relevant in detecting complex epistatic interaction that involves both additive and dominant effects. Therefore, in ensemble construction, we focus on evaluating nonlinear classifiers. Moreover, we prefer classifiers that are relatively computationally efficient since the identification of gene-gene interaction is carried out in a wrapper manner. Thus, our attention has been focused on decision tree based classifiers and instance based classifiers, as well as their hybrids because they are fast among many alternatives, while also being able to perform nonlinear classification. However, we note that any combination of linear and nonlinear classifiers can be used in our framework.<br>
With above considerations, an initial set of experiments is conducted to select candidate classifiers for ensemble construction. The classifier selection details are presented in Results Section.<br>
<br>
Datasets<br>
Simulation datasets<br>
In this study, we use the simulated datasets generated by gene-gene interaction models described by Moore et al. [32]. In each dataset, a pair of functional SNPs which simulate gene-gene interaction are embedded along with nonfunctional SNPs, and the task is to identify this functional SNP pair from the nonfunctional ones.<br>
For the datasets with balanced class distribution, the class ratio is 1:1 with 100 case samples and 100 control samples. The datasets are simulated under three different genetic heritability models (heritability of 0.2, 0.1, and 0.05), and two SNP sizes (SNP size of 20 and 100). This gives six sets of datasets and each set contains 100 replicates each generated with a different random seed [36]. The property of the imbalanced datasets used for evaluation is the same as the balanced datasets, except that the class ratio is approximately 1:2 with 67 case samples and 133 control samples. For imbalanced data, we restrict the evaluation to SNP size of 20, and therefore, we obtain three sets of datasets with each set containing 100 replicates. Table 2 summarizes the characteristics of the simulated datasets used for evaluation.<br>
<br>
Age-related macular degeneration dataset<br>
Age-related macular degeneration (AMD) is the major cause of uncorrectable blindness of the elderly in many countries. As a typical complex disease, AMD is influenced by complex interactions among multiple genes and environmental factors, making it ideal for testing gene-gene interaction identification methods. In our experiment, the proposed GE algorithm, PIA, and MDR are applied to the dataset generated from a GWA study of AMD [2]. This dataset contains a genome-wide screening of 96 AMD cases and 50 controls and more than 100,000 SNPs have been genotyped for each individual.<br>
<br>
<br>
Evaluation statistics<br>
Evaluation statistics for single algorithm<br>
We compare the detection power of the proposed GE algorithm with PIA (version: PIA-2.0) and MDR (version: mdr-2.0_beta_6). In the previous studies of MDR [37] and PIA [14], the power of an algorithm for identifying gene-gene interactions is estimated as the percent of times the algorithm "successfully identifies" the true functional SNP pair from 100 replicates of simulated datasets. This is repeated for every heritability model to quantify how well each algorithm performs when dealing with datasets of different difficulties (lower heritability being more difficult). An algorithm is said to have successfully identified a functional SNP pair in a dataset if the true SNP-pair is reported as the top rank. For comparison with MDR and PIA, we follow this approach and estimate the power of GE, MDR, and PIA using following statistics:<br>
(10)Power=NSN<br>
where N is the number of datasets tested (N = 100 in our case), and NS is the number of successful identification.<br>
For GE in particular, we are also interested in estimating the distribution of false discovery rate (FDR) and true positive rate (TPR) since, in the worst case, if there is no SNP-SNP interaction in the dataset, a top-ranked interaction list only contains false positive identifications. Formally, we estimate FDR as:<br>
(11)FDR(c)=NFP(c)N(c)<br>
where FDR(c) is the FDR at the cut-off of c, NFP (c) is the number of accepted false positive identifications at the cut-off of c, and N(c) is the number of accepted identifications at the cut-off of c.<br>
Similarly, TPR can is calculated as:<br>
(12)TPR(c)=NTP(c)NTP(c)+NFN(c)<br>
where TPR(c) is the TPR at the cut-off of c. NTP (c) and NFN (c) are the number of accepted true positives and the number of false negatives at the cut-off of c.<br>
Both the rank and the identification frequency score of each SNP combination can be used as the cut-off to calculate FDR and TPR at different confidence levels. We consider both approaches and using the 100 replicate datasets of each heritability model, we obtain the average FDR and TPR at different cut-offs for each heritability model.<br>
<br>
Evaluation statistics for combining algorithms<br>
One major motivation for developing a genetic ensemble algorithm for gene-gene interaction identification is to harness the complementary strength of different classifiers such that a more robust and predictive SNP subset can be obtained. To extend this idea further, we propose to combine the inferred SNP-SNP interaction from different algorithms (such as MDR and PIA), in the hope that more robust results can be obtained. However, such benefits may come only when the results yielded by different SNP-SNP interaction identification algorithms are complementary to each other, which is analogous to the idea of the ensemble diversity.<br>
By modifying the equation of double fault, we design the following terms to quantify the degree of complementarity (CD) of a pair of algorithms in SNP-SNP interaction identification:<br>
(13)SF(X,Y)=NFS+NSF,?DF(X,Y)=NFF<br>
(14)CD(X,Y)=SF(X,Y)DF(X,Y)+SF(X,Y)<br>
where NXY is the number of datasets with certain identification status using algorithms X and Y , and X, Y ? {F, S} in which F denotes an algorithm fails to identify the functional SNP pair while S denotes it succeeds to identify the functional SNP pair. SF (X, Y ) (single fault) is the number of times algorithms X and Y give inconsistent identification result, which is the situation that one algorithm succeeds while the other one fails. DF (X, Y ) (double fault) is the number of times both X and Y fail. The pairwise degree of complementarity of the algorithms X and Y is determined by CD(X, Y ).<br>
Excluding the case in which both X and Y achieve 100% successful identification (which gives 00), the value of CD(X, Y ) varies between 0 and 1. When the results produced by X and Y are completely complementary to each other, the value of DF (X, Y ) decreases to 0, and the value of CD(X, Y ) reaches 1.<br>
On the contrary, the value of CD(X, Y ) decreases with the decrease of the degree of complementarity between algorithms X and Y , and reaches 0 when no degree of complementarity is found.<br>
Our premise is that combining algorithms with higher degree of complementarity will yield higher identification power. In this study, we estimate the joint power of two or three algorithms as:<br>
(15)PowerJ(X,Y)=N?DF(X,Y)<br>
(16)powerJ(X,Y)=N?TF(X,Y,Z);?TF(X,Y,Z)=NFFF<br>
where TF (X, Y, Z) is the "triple fault" which gives the coincident errors of three identification algorithms, and PowerJ (X, Y ) and PowerJ (X, Y, Z) are the joint power of combining two and three identification algorithms respectively.<br>
<br>
<br>
<br>
Results<br>
Classifier selection and ensemble construction<br>
One of the most important steps in forming an ensemble of classifiers is base classifier selection. As described above, characteristics such as nonlinear separation capability, computational efficiency, high accuracy and diversity should be taken into account. With those considerations, a classifier selection and ensemble construction experiment was carried out. Specifically, we tested the merits of each candidate classifier using datasets with model number of 10, 11, 12, 13 and 14 from Moore et al. [36], all of which have minor allele frequency of 0.2, heritability of 0.1, and sample size of 200 (100 case and 100 control). These are considered as "difficult" datasets since they are simulated to have low minor allele frequency, low heritability, and small sample size [14]. Twenty replicates from each model were used for evaluation and the power of each classifier in identifying the functional SNP pair was calculated. Figure 2(a) shows the 12 candidate classifiers we evaluated in this study. They are REPTree (REPT), Random Tree (RT), Alternating Decision Tree (ADT) [38], Random Forest (RT) [39], 1-Nearest Neighbor (1NN), 3-Nearest Neighbor (3NN), 5-Nearest Neighbor (5NN), Decision Tree (J48), 1-Nearest Neighbor with Cover Tree (CT1NN), 3-Nearest Neighbor with Cover Tree (CT3NN) [40], entropy based nearest neighbor (KStar) [41] and 5-Nearest Neighbor with Cover Tree (CT5NN).<br>
The identification power of each classifier was estimated using the simulated datasets. Among the twelve classifiers, six of them successfully identified the functional SNP pair more than 50% of the time. Five of them were selected to form the ensemble (colored in red in Figure 2(a)). They are J48, KStar, and three Decision Tree and k-Nearest Neighbor hybrid - CT1NN, CT3NN, and CT5NN.<br>
The configuration of parameters such as GA chromosome mutation rate and integration weights of diversity measure, blocking, and voting were tested using the same sets of data as above. Specifically, the mutation rates tested were 0.05, 0.1 and 0.15. The integration weights of diversity tested were also 0.05, 0.1 and 0.15 while the integration weights for blocking and voting were kept equal, and the three weights add up to 1. This gives 9 possible configurations for the ensemble of classifiers. The identification powers of the ensemble of classifiers using these 9 configurations are shown in Figure 2(b). It is observed that all the ensembles achieved better results than the best single classifier which has an identification power of 53.8%. Among them, the best parameter setting is (0.1, 0.15) which specifies the use of a mutation rate of 0.1 and an integration weights of 0.15, 0.425, and 0.425 for diversity, blocking, and voting, respectively. This configuration gives an identification power of 60.8% which is a significant improvement from 53.8%. This setting was then fixed in our GE in the follow up experiments.<br>
<br>
Simulation results<br>
Gene-gene interaction identification<br>
In the simulation experiment, we applied GE, PIA, and MDR for detecting the functional SNP pairs from 20 candidate SNPs and 100 candidate SNPs, respectively. Table 3 shows the evaluation results. By fixing the candidate SNP size to 20 and testing datasets generated with three heritability values (0.2, 0.1, and 0.05), we observed a decrease of the average identification power of the three algorithms (taking the average of the three identification algorithms) from 98.33 ? 0.94 to 78.67 ? 2.62 and to 43.67 ? 0.94. By fixing the candidate SNP size to 100 and testing datasets generated with three heritability value (0.2, 0.1 and 0.05), the average identification power drops to 93.67 ? 0.94, 48.33 ? 2.49, and 19.00 ? 1.63, respectively. It is clear that both heritability and SNP size are important factors to SNP-SNP interaction identification. By comparing the power of each algorithm, we found no significant differences. The standard deviation is generally small ranging from 0.94 to 2.62, indicating that the three algorithms have similar performance.<br>
To investigate whether an imbalanced class distribution affects identification power, we applied GE, PIA, and MDR to imbalanced datasets with a case-control ratio of 1:2 and a candidate SNP size of 20. From Table 4, we found that the power of the three identification algorithms decreased in comparison to those of the balanced datasets (Table 3). Such a decline of power is especially significant when the heritability of the dataset is small. This finding is essentially consistent with [32] in that datasets of larger heritability values are more robust to imbalanced class distribution. Since the sample size and other dataset characteristics between the balanced and the imbalanced datasets are the same, the observed decline of power could be attributed to the imbalanced class distribution. It is also noticed that the identification power of PIA is relatively lower compared to GE and MDR. This indicates that PIA may be more sensitive to the presence of the imbalanced class distribution than GE and MDR.<br>
For the GE algorithm, two approaches were used to study the distribution of the TPR and FDR. For the first approach, we calculated the TPR and FDR by varying the rank cut-off of the reported SNP pairs. Figure 3 shows the distribution by using a rank cut-off of 1 to 10 (the lower the number, the higher the rank). Note that the rank cut-off of 1 gives the results equal to the power defined in Equation (10). For the second approach, we calculated the TPR and FDR by varying the identification frequency cut-off of the reported SNP pairs. Figure 4 shows the distribution by decreasing the frequency cut-off from 1 to 0. By comparing the results, we found that the decrease of the heritability (from 0.2, to 0.1 and to 0.05) has the greatest impact on TPR of GE. Sample size appears to be the second factor (from 20 SNPs to 100 SNPs), and the imbalanced class distribution is the third factor (from a balanced ratio of 1:1 to an imbalanced ratio of 1:2).<br>
Generally, by decreasing the cut-off stringency (either rank cut-off or identification frequency cutoff), the TPR increases, and therefore, more functional SNP pairs can be successfully identified. However, this is achieved by accepting increasingly more false identifications (higher FDR). The simulation results indicate that FDR calculated by using the identification frequency cut-off is very steady regardless the change of heritability, SNP size, or class ratio. In most cases, an FDR close to 0 is achieved with a cut-off greater than 0.78.<br>
<br>
The degree of complementarity among GE, MDR, and PIA<br>
As illustrated in Table 3 and Table 4, large candidate SNP size, low heritability value, and the presence of imbalanced class distribution together give the worst scenario for detecting SNP-SNP interaction. Is there a way to increase the chance of success identification in such a situation? One solution is to combine different identification results produced by different algorithms, which extends the idea of ensemble method further. However, similar to the notion of diversity in ensemble classifier, the improvement can only come if the combined results are complementary to each other. Hence, the evaluation of the degree of complementarity among each pair of algorithms becomes indispensable.<br>
We carried out a pairwise evaluation using Equations (13) and (14). Tables 5 and 6 give the results for balanced and imbalanced situations, respectively. We observed that higher degree of complementarity is generally associated with higher identification power. For the balanced datasets, the degree of complementarity of PIA and MDR is relatively low compared to those generated by GE and PIA, or GE and MDR. The results indicate that the GE algorithm, which tackles the problem from a different perspective, is useful in complementing methods like PIA and MDR in gene-gene interaction identification. As for the imbalanced datasets, the difference of the complementarity degree between each pair of algorithms is reduced. This suggests that more methods need to be combined for imbalanced datasets in order to improve identification power.<br>
The last columns of Tables 5 and 6 show the joint identification power of the three algorithms in analyzing balanced and imbalanced data. These results indicate a significant recovery of detection ability in functional SNP pair identification by applying three algorithms collaboratively. This is especially true when analyzing imbalanced datasets and the heritability of the underlying genetic model is low. For example, the average identification power of three algorithms for imbalanced datasets with heritability of 0.1 and 0.05 are 55.33% and 27.67%, respectively (Table 4). By combining the results of the three algorithms, we are able to increase the power to 76% and 47%, respectively, improving by around 20% (Figure 5).<br>
<br>
<br>
Real-world data application<br>
As an example of real-world data application, we applied the GE algorithm, PIA and MDR, to analyze the complex disease of AMD. To reduce the combinatorial search space, we followed the two-step analysis approach [42] and used a SNP filtering procedure that is similar to the method described in [23] which can be summarized as follows:<br>
? Excluding SNPs which have more than 20% missing genotype values of total samples;<br>
? Calculating allelic ?2-statistics of each remaining SNP and keeping SNPs which have a p-value smaller than 0.05 while discarding others. A number of 3583 SNPs passed filtering.<br>
? Utilizing <software>RTREE</software> program [43] to select top splitting SNPs in AMD classification. Two SNPs with id of rs380390 and rs10272438 are selected.<br>
? Utilizing <software>Haploview</software> program [44] to construct the Linkage Disequilibrium (LD) blocks around above two SNPs.<br>
After the above processing steps, we obtained 17 SNPs from the two LD blocks. They are rs2019727, rs10489456, rs3753396, rs380390, rs2284664, and rs1329428 from the first block, and rs4723261, rs764127, rs10486519, rs964707, rs10254116, rs10486521, rs10272438, rs10486523, 10486524, rs10486525, and rs1420150 from the second block. Based on the previous investigation of AMD [45-47], we added another six SNPs to avoid analysis bias. They are rs800292, rs1061170, rs1065489, rs1049024, rs2736911, and rs10490924. Moreover, environment factors of Smoking status and Sex are also included for potential environment interaction detection. Together, we formed a dataset with 25 factors for AMD association screening and gene-gene interaction identification.<br>
Tables 7 and 8 illustrate the top 5 most frequently identified 2-factor interactions and 3-factor interactions, respectively. At the first glance, we see that the identification results given by different methods are quite different from one another. Considering the results of 2-factor and 3-factor interaction together, however, we find that two gene-gene interactions and a gene-environment interaction are identified by all three methods. Specifically, the first gene-gene interaction is characterized by the SNP-SNP interaction pair of rs10272438?rs380390. The first SNP is a A/G variant located in intron 5 of BBS9 located in 7p14, while the second SNP is a C/G variant located in intron 15 of CFH located in 1q32. The second frequently identified gene-gene interaction is characterized by the SNP-SNP interaction pair of rs10490924?rs10272438. The first SNP in this interaction pair is a nonsynonymous coding SNP of Ser69Ala alteration located in exon 1 of ARMS2 located in 10q26. And the second SNP is again the A/G variant located in intron 5 of BBS9 located in 7p14. As to the gene-environment interaction pair, it is characterized by rs10272438?Sex. This pair indicates that SNP factor of the A/G variant located in intron 5 of BBS9 in location of 7p14 is likely to associate with the disease differently between male and female.<br>
We also test the association of Age factor with AMD by using Gaussian discretization to partition the age value of each sample into three categories as follows:<br>
(17)age(x)={?young?x????/2?medium??elderly????/2&lt;x &lt;?+?/2x??+?/2<br>
where ? is the average age value and ? is the standard deviation of age values.<br>
After including the Age factor to the dataset, all three algorithms identified the gene-environment interaction of rs1420150?Age as the interaction with major implication, indicating Age factor is, expectedly, strongly associated with the development of AMD. The SNP that interacted with the Age factor is a C/G variant located in intron 9 of BBS9 located in 7p14.<br>
Table 9 summarizes the factors involved in potential interactions identified by all of the three different algorithms. Overall, the experimental results suggest that genes of BBS9 (Bardet-Biedl syndrome 9), CFH (complement factor H), and ARMS2 (age-related maculopathy susceptibility 2) with the external factors of Age and Sex, and the interactions among them are strongly associated with the development of AMD. This is essentially consistent with current knowledge of AMD development in the literature [2,45-47].<br>
<br>
<br>
Discussion and conclusion<br>
How multiple genes contribute to the development of complex diseases is an essential question for complex disease study. This is because a single gene often does not have the power to discriminate the status of the complex disease, and it is likely that multiple genes each with a weak or moderate effect together contribute to the development of complex disease. Although great effort has been devoted to characterizing such gene-gene interactions in complex disease analysis, the results remain unsatisfactory.<br>
The advance of high-throughput genotyping technologies provides the opportunity to elucidate the mechanism of gene-gene and gene-environment interaction via SNP markers. However, current algorithms have limited power in terms of identifying true SNP-SNP interactions. Moreover, the simulation results indicate that the factors such as heritability, candidate SNP size, and the presence of imbalanced class distribution all have profound impact on a given algorithm's power in identifying functional SNP interactions. One practical way to improve the chance of identifying SNP-SNP interactions is to combine different methods where each addresses the same problem from a different perspective. The rationale is that the consensus may increase the confidence of identifications and complementary results may improve the power of identification.<br>
Due to these considerations, we proposed a hybrid algorithm using genetic ensemble approach. Using this approach, the problem of SNP-SNP interaction is converted to a combinatorial feature selection problem. Our simulation study indicates that the proposed GE algorithm is comparable to PIA and MDR in terms of identifying gene-gene interaction for complex disease analysis. Furthermore, the experimental results demonstrate that the proposed algorithm has a high degree of complementarity to PIA and MDR, suggesting the combination of GE with PIA and MDR will likely lead to higher identification power.<br>
For the practical application of the GE algorithm, the experimental results from the simulation datasets suggest that taking the top-ranked result generally gives a higher sensitivity of identifying SNP-SNP interactions than using a frequency score cut-off. However, if the delectability of the SNP-SNP interaction is low or no such interaction is present in the dataset, the top-ranked result is likely to be a false positive identification. A more conservative approach is to use an identification frequency cut-off of 0.75-0.8 which in our simulation study gives identification results with an FDR close to 0. For any identified SNP pair with an identification frequency higher than 0.8, the confidence is very high.<br>
As a down-stream analysis, we can fit the identified SNP pairs using logistic model with interaction terms and calculate the p-value of its coefficient in order to quantify the strength of the interaction. In particular, to test additive and dominant effects, we can fit the reported SNP combinations using the model described by Cordell [12] and analyze the coefficients associated with additive and dominant effects of each SNP.<br>
Current GWA studies commonly produce several hundreds of thousands of SNPs, yet the gene-gene interaction identification algorithms like MDR, PIA and the proposed GE algorithm can only cope with a relatively small number of SNPs in a combinatorial manner. Therefore, a filtering procedure is required to reduce the number of SNPs to a "workable" amount before those combinatorial methods can be applied to datasets generated by GWA studies [48,49]. More efforts are required to seamlessly connect these two components to maximize the chance of detecting complex interactions among multiple genes and environmental factors [42].<br>
In conclusion, we proposed a GE algorithm for gene-gene and gene-environment interaction identification. It is comparable to two other state-of-the-art algorithms (PIA and MDR) in terms of SNP-SNP interaction identification. The experimental results also demonstrated the effectiveness and the necessity of applying multiple methods each with different strengths to the gene-gene and gene-environment interaction identification for complex disease analysis.<br>
<br>
Authors' contributions<br>
PY conceived the study, designed, and implemented the algorithms, and performed the experiments. PY and JWKH interpreted the results and drafted the manuscript. BZ and AYZ revised the manuscript critically and supervised research. All authors read and approved the final manuscript.<br>
<br>
Availability<br>
The GE algorithm (<software>GEsnpx</software>) is implemented in Java. It is freely available from the supplementary website at http://www.cs.usyd.edu.au/~yangpy/software/GEsnpx.html<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1761652</b><br>
Connectivity in the Yeast Cell Cycle Transcription Network: Inferences from Neural Networks<br>
A current challenge is to develop computational approaches to infer gene network regulatory relationships based on multiple types of large-scale functional genomic data. We find that single-layer feed-forward artificial neural network (ANN) models can effectively discover gene network structure by integrating global in vivo protein:DNA interaction data (ChIP/Array) with genome-wide microarray RNA data. We test this on the yeast cell cycle transcription network, which is composed of several hundred genes with phase-specific RNA outputs. These ANNs were robust to noise in data and to a variety of perturbations. They reliably identified and ranked 10 of 12 known major cell cycle factors at the top of a set of 204, based on a sum-of-squared weights metric. Comparative analysis of motif occurrences among multiple yeast species independently confirmed relationships inferred from ANN weights analysis. ANN models can capitalize on properties of biological gene networks that other kinds of models do not. ANNs naturally take advantage of patterns of absence, as well as presence, of factor binding associated with specific expression output; they are easily subjected to in silico ?mutation? to uncover biological redundancies; and they can use the full range of factor binding values. A prominent feature of cell cycle ANNs suggested an analogous property might exist in the biological network. This postulated that ?network-local discrimination? occurs when regulatory connections (here between MBF and target genes) are explicitly disfavored in one network module (G2), relative to others and to the class of genes outside the mitotic network. If correct, this predicts that MBF motifs will be significantly depleted from the discriminated class and that the discrimination will persist through evolution. Analysis of distantly related Schizosaccharomyces pombe confirmed this, suggesting that network-local discrimination is real and complements well-known enrichment of MBF sites in G1 class genes.<br>
<br>
Introduction<br>
Hundreds of yeast RNAs are expressed in a cell cycle?dependent, oscillating manner. In both budding yeast and fission yeast, these RNAs cluster into four or five groups, each corresponding roughly to a phase of the cycle [1?9]. Large sets of phase-specific RNAs are also seen in animal and plant cells [10?12], arguing that an extensive cycling transcription network is a fundamental property of Eukaryotes. The complete composition and connectivity of the cell cycle transcription network is not yet known for any eukaryote, and many components may vary over long evolutionary distances [3?5,13], but some specific regulators (e.g., MBF of yeast and the related E2Fs of plants and animals) are paneukaryotic, as are some of their direct target genes (DNA polymerase, ribonucleotide reductase). Coupled with experimental accessibility, this conservation of core components and connections make the yeast mitotic cycle an especially good test case for studies of network structure, function, and evolution.<br>
To expose the underlying logic of this transcription network, a starting point is to decompose the cell cycle into its component phases (i.e., G1, S, G2, M) and link the pertinent regulatory factors with their immediate regulatory output patterns, here in the form of phasic RNA expression. One way to do this is to integrate multiple genome-wide data types that impinge on connection inference, including factor:DNA interaction data from chromatin IP (ChIP) studies, RNA expression patterns, and comparative genomic analysis. This is appealing partly because these assays are genome-comprehensive and hypothesis-independent, so they can, in principle, reveal regulatory relationships not detected by classical genetics. However, the scale and complexity of these datasets require new methods to discover and rank candidate connections, while also accommodating considerable experimental and biological noise (e.g., [14?19]). Microarray RNA expression studies in budding yeast have identified 230 to 1,100 cycling genes, the upper number encompassing nearly a fifth of all yeast genes [1,2,8,20]. Specifics of experimental design and methods of analysis contribute to the wide range in the number of genes designated as cycling, but there is agreement on a core set of nearly 200. Yeast molecular genetic studies have established that transcriptional regulation is critical for controlling phase-specific RNA expression for some of these genes, though this does not exclude modulation and additional contributions from post-transcriptional mechanisms. About a dozen Saccharomyces transcription factors have been causally associated with direct control of cell cycle expression patterns, including repressors, activators, co-regulators, and regulators that assume both repressing and activating roles, depending on context: Ace2, Fkh1, Fkh2, Mbp1, Mcm1, Ndd1, Stb1, Swi4, Swi5, Swi6, Yhp1, and Yox1. These can serve as internal control true-positive connections. Conversely, a majority of yeast genes have no cell cycle oscillatory expression, and true negatives can be drawn from this group. A practical consideration is how well the behavior of a network is represented in critical datasets. In this case, cells in all cell cycle phases are present in the mixed phase, exponentially growing yeast cultures used for the largest and most complete set of global protein:DNA interaction (ChIP/array) data so far assembled in functional genomics [21]. These data are further supported by three smaller studies of the same basic design [22?24]. This sets the cell cycle apart from many other transcription networks whose multiple states are either partly or entirely absent from the global ChIP data. Equally important are RNA expression data that finely parse the kinetic trajectory for every gene across the cycle of budding yeast [1,2] and also in the distantly related fission yeast, S. pombe [3?5]. This combination of highly time-resolved RNA expression data and phase-mixed (but nevertheless inclusive) ChIP/array data can be used to assign protein:DNA interactions to explicit cell cycle phases, while evolutionary comparison with S. pombe highlight exceptionally conserved and presumably fundamental network properties.<br>
Many prior efforts to infer yeast transcription network connections from genome-wide data ([15?17,25,26] were designed to address the global problem of finding connection patterns across the entire yeast transcriptome by using very large and diverse collections of yeast RNA, DNA, and/or chromatin immunoprecipitation data. The present work focuses instead on a single cellular process and its underlying gene network, which represents a natural level of organization positioned between the single gene at one extreme and the entire interlocking community of networks that govern the entire cell. To model regulatory factor:target gene behavior, we adapted neural networks to integrate global expression and protein:DNA interaction data.<br>
Artificial neural networks (ANNs) are structural computational models with a long history in pattern recognition [27]. A general reason for thinking ANNs could be effective for this task is that they have some natural similarities with transcription networks, including the ability to create nonlinear sparse interactions between transcriptional regulators and target genes. They have previously been applied to model relatively small gene circuits [28?30], though they have not, to our knowledge, been used for the problem of inferring network structure by integrating large-scale data. We reasoned that a simple single-layer ANN would be well-suited to capture and leverage two additional known characteristics of eukaryotic gene networks. First, factor binding in vivo varies over a continuum of values, as reflected in ChIP data, in vivo footprinting, binding site numbers and affinity ranges, and site mutation analyses. These quantitative differences can have biological significance to transcription output by affecting cooperativity, background ?leaky expression? or the lack of it, and the temporal sequencing of gene induction as factors become available or disappear. This is quite different from a world in which binding is reduced to a simple two-state, present/absent call. Neural networks are able to use the full range of binding probabilities in the dataset. Second, ANNs can give weight and attention to structural features such as the persistent absence of specific factors from particular target groups of genes. This ?negative image? information is potentially important and not used by other methods applied to date [15,21,31,32]. The inherent ability of ANNs to use these properties is a potential strength compared with algorithms that rest solely on positive evidence of factor:target binding or require discretization of binding measurements into a simplified bound/unbound call.<br>
ANNs have been most famously used in machine learning as ?black boxes? to perform classification tasks, in which the goal is to build a network based on a training dataset that will subsequently be used to perform similar classifications on new data of similar structure. In these classical ANN applications, the weights within the network are of no particular interest, as long as the trained network performs the desired classification task successfully when extrapolating to new data. ANNs are used here in a substantially different way, serving as structural models [33]. Specifically, we use simple feed-forward networks in which the results of interest are mainly in the weights and what they suggest about the importance of individual transcription factors or groups of factors for specifying particular expression outputs.<br>
Here ANNs were trained to predict the RNA expression behavior of genes during a cdc28 synchronized cell cycle, based solely on transcription factor binding pattern, as measured by ChIP/array for 204 yeast factors determined in an exponentially growing culture [21]. The resulting ANN model is then interrogated to identify the most important regulator-to-target gene associations, as reflected by ANN weights. Ten of the twelve major known transcriptional regulators of cell cycle phase-specific expression ranked at the very top of the 204-regulator list in the model. The cell cycle ANNs were remarkably robust to a series of in silico ?mutations,? in which binding data for a specific factor was eliminated and a new family of ANN models were generated. Additional doubly and triply ?mutated? networks correctly identified epistasis relationships and redundancies in the biological network. This approach was also applied to two additional, independent cell cycle expression studies to illustrate generality across data platforms, and to probe how the networks might change under distinct modes of cell synchronization.<br>
Analysis of the weights matrices from the resulting models shows that the neural nets take advantage of information about specifically disfavored or disallowed connections between factors and expression patterns, together with the expected positive connections (and weights) for other factors, to assign genes to their correct expression outputs. This led us to ask if there is a corresponding bias in the biological network against binding sites for specific factors in some expression families as suggested by the ANN. We found that this is the case, in multiple sensu stricto yeast genomes relatively closely related to Saccharomyces cerevisiae, and also in the distantly related fission yeast S. pombe. This appears to be a deeply conserved network architecture property, even though very few specific orthologous genes are involved.<br>
<br>
Results<br>
Classifier ANNs were trained to predict membership in cell cycle phase-specific RNA clusters, based on global transcription factor binding data (Figure 1). As expression input data, these ANNs used time course microarray data [2] for 384 cycling genes that had been grouped into five clusters by an expectation maximization (EM) algorithm [9]. As measured by receiver operator characteristic (ROC) analysis, these clusters are quantitatively well-separated from each other, with less than 10% overlap at their margins with any other clusters, except that the S-phase cluster (EM3) was somewhat less well-separated from its kinetic neighbors, EM2 and EM4 [9]. The primary goal of the ANN modeling is to infer the set of regulatory connections that underlies each of the cell cycle?phased expression groups. Note that a given cluster might be composed of more than one regulatory subgroup; it need not be the case that all associated regulators interact with all?or even most?of the genes in a cluster. ANNs were trained to assign expression cluster membership for each gene based on 204 measured binding probabilities from ChIP/array experiments ([21]). To accommodate the scarcity of data, while minimizing effects of overtraining, we generated an average-of-bests artificial neural network (aobANN) (Methods). As anticipated, the aobANN classified input genes best, correctly assigning the expression class of 86% of included cell cycle genes (Figure 2). Individual best-of-ten networks, each trained on 80% of the data and tested on the remaining 20% correctly assigned expression class membership for ?50% of the genes, with an accuracy range between 40% and 65%, whereas only 27% of genes would be expected to be classified correctly if genes were classified by a random process (Figure S1). As shown in Figure S2, a substantial fraction of genes (32%) are always classified correctly by every ANN, another subset (28%) are never classified ?correctly,? and the remaining fraction (40%) are intermediate. An examination of possible correlates of high or low predictability, including absolute level of RNA expression and bidirectional versus unidirectional orientation of the gene relative to its upstream neighbor found no correlation except that the EM2 (late G1) class is enriched in highly predictable genes, while the EM5 (M phase expression peak) is most impoverished (Figure S2). The major conclusion from global statistics is that individual ANNs and the aobANN have developed weighting schemes that are effective in connecting factor binding information from ChIP/array to RNA expression patterns, even in the presence of considerable experimental noise that is a widely acknowledged property of the input datasets.<br>
Parsing the ANN Weight Matrix to Infer Regulatory Relationships<br>
We next interrogated the aobANN weight matrix to find out which regulators are most important for assigning genes to specific gene expression behavior. Regulators were sorted by a sum-of-squares (SOS) rank calculation (see Methods) over the expression classes. The factor ranking, based exclusively on the ANN weights, assigned nearly all transcription factors previously definitively associated with phase-specific regulation to the very top of the ordered list. Figures 3 and 4 summarize data from the weight matrix of the aob network. A plot of the sum of squared weights for each factor shows that the top 10% of all regulators carry much higher weights than all the rest, and the dropoff in weight is quite dramatic (Figure 3A). Focusing on the top 20%, the relative contribution to each sum derived from positive (blue) versus negative (red) weights is shown (Figure 3B). Both negative and positive weights contribute substantially, and the way in which weights associate with each individual expression class is shown in Figure 3B. The top regulators in this ranking are Swi6, Ndd1, Stb1, Fkh2, and Mbp1, all of which are known direct regulators of the cell cycle. In most instances high positive weight for a factor (blue) is associated with the expression class or pair of classes expected from more detailed molecular genetics studies. For instance, Swi6, Stb1, and Mbp1 are the first, second, and sixth ranked regulators, and they are known to function together at genes expressed in EM2 (G1). Mbp1 binds DNA directly, and Swi6 and Stb1 bind to Mbp1 [34,35]. Ndd1 and Fkh2, the second and fourth ranked regulators, also function together in a molecular complex [36]. In the aobANN model, they are associated with EM3/4 (S/G2), again recapitulating expected domain of action.<br>
<br>
ANN Stability<br>
Regulator-to-target relationships suggested by the ANNs were very stable with respect to permutation of the input DNA binding data and to a range of biologically reasonable differences among input expression clusterings (classifications). We find the relative ranking of the top regulators to be stable across all networks generated during the training paradigm (Figure 5). The ranking of regulators was also stable across networks that were trained to predict expression classes derived from clusterings with either more or fewer clusters (the experiment was performed over K = 4, 5, 6, 7, or 8, and results are summarized in Figure S4). Lower K values than 4 fit the data poorly and are therefore irrelevant; and still higher K values above 8 force an entirely unjustified oversplitting of clusters that is clearly inappropriate.<br>
<br>
In Silico Network Mutations<br>
We next performed a series of in silico network mutations in which binding data for one, two, or three top-ranked regulators were removed before training a new set of ANNs. The resulting deletion ANNs were used to produce a new aob network, as before, and the corresponding sum of squared weights ranking was constructed (Figure 6). These perturbations further test network stability and also identify specific instances of factor redundancy. Overall the ANNs proved remarkably stable to elimination of high-ranking factors. When each of the top 20 were eliminated singly, the identity of the remaining top regulators proved very stable (Figure 6A). The color code for each cell reflects its rank order from the parental, unperturbed network (shown in the bottom row). Each subsequent row reports the outcome for the mutant network with the indicated factor or factors removed. Although the cells are placed according to their rank order in the mutant AOB network, the color is based on the ranking from the unperturbed, ?wild-type? network. In general, factors from lower rankings were not promoted into the high ranking (dark blue) domain, nor were previously highly ranked factors (blue) demoted significantly into yellow and red domains. Thus, the first major conclusion from the mutation experiments is that neither the connections the ANNs infer nor the absolute performance of the ANNs depends heavily on a single factor or even a factor pair. The ability of the models to highlight other important connections is not compromised by elimination of any high-scoring factor.<br>
Figure 6B shows the same mutant networks at higher resolution, so that all factors whose original rank was &gt;50 appear in the summary as white cells. Original rank order is again indicated by the color of each cell, although the color scale has been shifted to make it more sensitive to changes in rank among the top 50 regulators. A few specific exceptions to overall stability were observed, in which a relatively low-ranked regulator has been elevated by mutation into higher ranks. The most striking example is Swi4, which is demarcated with a star. Swi4 is a very well-studied cell cycle transcription factor that did not fall in the top 10% in the wild-type network (it ranked 80th). As shown in Figure 6C, ?mutant? networks for all factors associated with the G1 (EM2) caused Swi4 to advance in rank, with double or triple mutations moving it progressively higher. We discuss later the causes and consequences of Swi4?s initial low ranking in the wild-type ANN and the implications for detecting biological redundancy. However, the general conclusion for ANN analysis is that systematic single and multiple perturbations of high-ranking regulators provide a way to detect redundancy, even when a connection?here Swi4 with G1?was not evident in the unperturbed wild-type ANN. Additional double and triple mutations for the major cycle classes were performed and no other change as remarkable as Swi4 was found.<br>
<br>
Out-of-Sample Accuracy<br>
We next tested out-of-sample accuracy, which is the ability of the training paradigm to generalize to another set of independently collected binding measurements, in which both experimental error and biological error will differ from the first series of models. We constructed a new aobANN trained again from data collected from Harbison, but included only binding measurements from the 111 regulators available in both the Harbison et al. (2004) study and the independent Lee et al. (2002) study. Despite biological and experimental difference between the two datasets, this aobANN delivered a highly significant out-of-sample accuracy of 56%, which is 17 standard deviations from the average linear assignment score (.27 ? 0.017) of a random partitioning of the genes, where class sizes are determined by drawing from a multinomial distribution based on the cluster sizes.<br>
<br>
Regulator Rank Stability and Power<br>
The stability of weight ranks across the 40 individual ?best? networks that contribute to the aobANN was examined. We postulated that factors whose rankings are less stable across many individual networks would also be less likely to be functionally significant than factors showing high stability across the individual networks, even if the median SOS weight is quite high in all cases. The well-known regulators of cell cycle transcription, ranking in the top dozen, showed greatest stability, and a substantial discontinuity was found to separate the top 20 from the remaining factors (Figure 5). We then asked how well the top regulators can perform if they are used to build a new aobANN over a sweep that ranges from three to 28 regulators. This experiment showed that a network built from the top 20 regulators performed almost as well as the full 204-regulator network and ranked its regulators very similarly (Figure S3). The top five regulators on their own (Swi6/Mbp1/Stb1 plus Fkh2 and Ndd1) were surprisingly powerful in parsing G1 versus G2/M. Conversely, an aobANN composed from the bottom 184 regulators was much less successful in predicting expression.<br>
<br>
ANN Models from Independent Cell Cycle Experiments<br>
We next independently clustered Cdc15 TS and alpha factor synchronized cell cycle RNA expression data [1], and used these new clusters to build two new ANN cell cycle models. These datasets are from two different cell cycle experiments, each measured using deposition microarrays and a ratiometric design, in contrast to the cdc28 arrest described above, which used Affymetrix data. By focusing on each synchronization method individually, rather than using a merged dataset, we aimed to capture possible differences in the biology that might arise from different methods of synchronization, while also revealing the relationships that are robust across the three experiments and two assay platforms. The ChIP/chip dataset is unique and was therefore used to build ANNs across cdc28, cdc15, and alpha factor experiments.<br>
As demonstrated with the cdc28 data above, we found these additional ANN models return the same core cell cycle regulators highlighted by the cdc28 ANNs. Six of these; Ndd1, Mbp1, Swi5, Stb1, Swi6, and Fkh2 are among the top seven regulators found, regardless of which cell cycle data and clusterings were used as input to the ANNs. This robustness in the central regulatory relationships is quite remarkable considering that, of 780 genes belonging to at least one of the cycling datasets, only 147 genes are common to all three experiments. Quantitation of pairwise clustering overlap, using the linear assignment metric, makes it very clear that the gene number and clustering patterns differ substantially (Figure 7). Thus, ANNs highlight major shared cell cycle relationships, even though the gene sets used and the clusterings are quite different (Table 1).<br>
Cdc15ts-synchronized cells are arrested at the end of M phase [1]. Correspondingly, we find the expression cluster that peaks first?at 10 min in the Cdc15 data?associates strongly with the early G1 factors Swi5 and Ace2 (EM1 in Figure 7). Note that in the previous cdc28 ANN, the same association was made, even though?under that release condition?genes of this regulatory group are not upregulated until the second cycle after release [9] and above). Alpha factor arrest is similar in this way to cdc28, reflecting their similar blockade points. Thus, the ANNs easily related the cdc15 early G1 cluster to the alpha factor and cdc28 early G1 clusters, even though the cluster trajectory is strikingly different and the clusters themselves contain no individual genes in common with the cdc28 or alpha factor datasets (Figures 4, 8, and 9). Other high-ranking regulators appear in one or two, but not all three ANN cell cycle models. Yox1 and Yhp1, for example, differ among the models, because the gene classes derived from the RNA clusterings differ in content. Finally, Pho2 emerges as a potentially significant regulator associated with an M-phase kinetic pattern in the two Spellman datasets, consistent with the previously reported Pho2/Pho4 mediated, cell cycle expression for some phosphate-regulated genes [37]. This is thought to be due to intracellular polyphosphate pools, which vary through the cycle in some culture conditions, but can also be influenced by growth media and history.<br>
<br>
<br>
 Discussion<br>
We found that single-layer ANN classifier models can effectively integrate global RNA expression and protein:DNA interaction data (ChIP/chip). The resulting models prominently highlight factors known to drive the transcriptional regulatory network underlying cell cycle phase-specific expression. The weight matrices from these ANN models generally associated previously known cell cycle transcription factors with the cell cycle phase they are thought to regulate, and they did so as well as or better than other methods, based on flexible iterative thresholding [15], network dynamics [16], or, most recently, Bayesian methods [31]. In general, we feel that more conventional statistical approaches and ANNs complement each other. Both generate hypothesized relationships and rank them. The strength of the single layer neural network architecture used here is that it mirrors several basic properties of natural gene networks: 1) both presence and absence of factor binding determine when and where a gene is expressed; 2) factor occupancy in vivo is a continuum, not an all-or-nothing phenomenon, and the graded differences can have biological significance. For example, graded binding of the transcription factor Pha4 creates spatiotemporal gradients of target gene expression during pharyngeal development in C. elegans [38]. These features of the neural network distinguish it from algorithms that depend solely on positive evidence of binding and require discretization of the binding signal to bound or unbound. A further distinction is that the neural network models can be easily and informatively ?mutated? to ask how the overall network connection patterns and outputs are affected by specific changes, such as eliminating data for individual factors, combinations of factors, or making even larger structural changes. The obvious complementary strength of statistical methods is in quantitative thresholding based on significance measures.<br>
A general conclusion that can be drawn from this work comes from the overall success of ANNs in classifying expression output according to transcription factor binding patterns. This might not have been true, but this overall observation argues strongly that transcriptional regulation, rather than differential post-transcriptional regulation, is the dominant mechanism in shaping phase-specific RNA prevalence clusters. This observation does not preclude a role for other mechanisms operating on a minority of genes (perhaps explaining some difficult-to-predict genes) or a post-transcriptional role that is uniform over an entire class. For example, confusion matrix analysis of expression classes versus the predicted expression pattern from the ANNs identified a group of genes with EM3 (S phase) kinetics that comprise 10% of that cluster, but are associated with the EM2 G1 group by the ANN model (Figure 2), and these are reasonable candidates to be differentially regulated by post-transcriptional processes such as slower turnover.<br>
Relating the Inferred Connections to Known Biology<br>
The sum-of-squared weights metric proved to be simple and useful for objectively ranking regulators according to their importance in the network model, regardless of the input expression dataset. Even though ANN weights are not direct physical measures of binding, the resulting rankings correspond remarkably well with what is known from decades of work on transcription in the yeast cell cycle. The ANN models even highlighted subtle regulatory differences between different cell cycle synchronization methods. The top dozen of the 204 total regulators in the cdc28 ANN model contained ten of 12 transcription factors present in the Harbison ChIP dataset and are known to operate on cycling genes. Swi6 ranked at the top of the cell cycle regulators list in the cdc28, cdc15, and alpha factor ANN models, and is always associated with G1 expression. Swi6 also shows a relative absence of binding to genes highly expressed during G2. The pattern of weights evaluated across the RNA expression clusters provides additional information. For instance, the cdc28 ANN weight vector for Mbp1 across the cell cycle clusters tracks very closely with Swi6 (correlation coefficient r = .92). This mirrors underlying molecular biology in which Mbp1 and Swi6 combine to form the heteromeric active G1 transcription factor MBF. Stb1 is similarly grouped with Swi6 and Mbp1 as a co-regulator of G1 (cdc28 EM2) genes (r = .95 and .89 for Stb1 with Mbp1 or with Swi6, respectively). Ace2 and Swi5 are paralogous factors with similar DNA binding target sites [39,40], and both are positively associated with the early G1 (cdc28 EM 1) expression profile with similar in-weights profiles (r = .71).<br>
Also confirming expectations from studies of target genes and epistatsis predictions, Fkh1 and Fkh2 were associated with cdc28 S/G2 expression clusters by the ANN. This implies that joint association is consistent with double knockout experiments, which indicate that the two complement each other [41], and with studies showing the two factors bind the same sites in vitro [42]. Examined in detail, the cdc28 ANN weights suggest a more nuanced view, in which both Fkh1 and Fkh2 are important for some genes in early S/G2 (EM3), whereas S/G2 class genes (cluster EM4) rely more heavily on Ndd1 and Fkh2 and less on Fkh1. RNA expression data for Fkh1 and Fkh2 is consistent with this, since Fkh1 increases in expression nearly 20 min before Fkh2, in expression data collected by Cho et al. in 1998 [2]. This is also consistent with a detailed study of in vivo binding at a few specific target genes [42], which showed that the two Fkh factors do not bind identically in vivo, and that there is a distinction between genes of the so-called Clb2 cluster (a subset of Cluster EM4 here), that are dominated by Fkh2 in conjunction with Mcm1/Ndd1, versus Fkh1, which is thought to bind independently. The alpha factor and cdc15 ANNs place diminished emphasis on Fkh1, compared with cdc28 ANNs, which is consistent with the idea that the two factors have different molecular activities and targets.<br>
Time and sign of action.<br>
Cdc28 ANN weight vectors for Mcm1 and Yox1 were also correlated (r = .69), defining an association with EM5 target genes where they displayed the two highest positive weights. They are known to act on some of the same genes, including EM5 group members [43]. In this example, the ANN is picking up molecular effects that are of opposing molecular activity, with Yox1 repressing Mcm1 activity. This illustrates an issue of interpretation. Because the original binding data are from a mixed phase cell population, it reveals nothing about when during the cycle detected binding occurs. For positive acting factors whose binding and function are contemporaneous, we see a peak of binding simply correlated with a peak of RNA expression. But for a repressor acting on genes expressed in M phase, binding occurs at other times (late G1, S, G2 alone, or in combinations [43]). Thus, the ANN correctly connected the factor with its targets; but only by independently determining the mode of Yox1 action, or by adding temporally resolved binding data, can the sign and timing of action be discerned. For factors whose action?repressing or activating?is unknown or is conditional depending on context, temporally resolved ChIP data will be needed to infer the mode and time of action.<br>
<br>
Swi4, a ?missing? regulator.<br>
The ANN models did not assign high weight to Swi4, which one would expect to rank highly. Although Swi4 is a well-known direct transcriptional regulator of Early G1 genes, providing the DNA binding moiety of SBF factor [44], it was not even close to the top 20 in the cdc28 aobANN, ranking 80 of 204. Its preferential association with G1 target genes only came to light when we performed in silico mutation analyses, eliminating one or more G1 factors. There are two possible explanations for its weak values in the wild-type ANNs, and they are not mutually exclusive. One simple possibility is that redundancy with other G1 regulatory factors is widespread, and this masks Swi4 when training the ANNs. Especially if coupled with generally less robust signals in the ChIP assay, the ANNs might have simply ignored Swi4. A second explanation is that Swi4 has greater breadth of binding across multiple clusters than its paralog, Mbp1. In this scenario, Swi4 spills over, binding to members of multiple cell cycle expression clusters when compared with other G1-specific regulators such as Mbp1, Swi6, or Stb1. This would give Swi4 less discrimination power in classifying genes, despite active G1 binding and could arise from purely technical issues, or from an unappreciated biological role outside its function in SBF.<br>
An independent analysis of the Harbison ChIP data in the context of a much larger library of expression data across many conditions other than cell cycle phases, using a different computational approach, supports the idea of broad Swi4 distribution among cell cycle regulatory classes [15]. Specifically, the GRAM algorithm uses coexpression patterns to incorporate into the connection map ChIP interactions that are below statistical significance when evaluated on their own [15,21,24]. They reported regulatory modules consisting of pairs of factors in which Swi4 is partnered by binding and expression data with one or more factors from each and every expression cluster: Ace2, Fhk2, Ndd1, and Mcm1, as well as the ?classic? associated G1 factors, Mbp1, Stb1, and Swi6. In addition, an entirely independent set of ChIP/chip measurements and analysis from Snyder and colleagues [22] showed substantial Swi4 binding activity upstream of non-G1 genes. Taken together, these data suggest Swi4 might have one or more previously unappreciated functions within exponentially growing cells that are distinct from its classic role as part of SBF.<br>
Finally, a picture of partly, but not entirely, redundant functions for the Swi4/Mbp1 paralogs was also emphasized in a recent genetic study [45]. We therefore think it likely that the way the unperturbed ANNs treat Swi4 reflects partial biological redundancy combined with its more widely distributed binding across non-G1 clusters.<br>
<br>
<br>
Potential Newly Identified Regulatory Connections<br>
Do the ANNs suggest new factors associated with phase-specific expression? Focusing on the cdc28 example, and using stability across ANNs as an added filtering criterion, factors ranking above Leu3 stood out. In particular, both Usv1 and Dal81 are interdigitated among the otherwise well-documented ten major cell cycle regulators, although not previously associated with this function to our knowledge. A different explanation is that factors such as Usv1, Dal81, and a handful of others ranking in the top 20, may be in the ANN model for reasons having nothing to do with the cell cycle explicitly, but having much to do with the partially overlapping architecture of transcriptional networks in eukaryotes. Thus, we expect that some genes?perhaps most?within cycling clusters will also belong to one or more other functional modules. In the context of those other functions, they will presumably be regulated by factors that have nothing to do with directing cell cycle phase patterns. This kind of network intersection and partial overlap is strikingly evident in global module maps [25]. Some factors appearing in the ANN top 20 may be there for this reason. There are others (Pho2, for example) that seem to be drawn into regulating phase-specific expression because of metabolic links (in this case through polyphosphate pools and membrane biogenesis [37]). We expect that the overall approach we have taken for the cell cycle network, using global ChIP/chip data, could easily be extended to any network whose states of interest are well-represented in available ChIP/chip data, and whose RNA datasets are of sufficient quality and resolution to cluster the expression behaviors of interest. However, a decisive improvement in sophistication of the ANN model, and the hypotheses it generates, will come with time-resolved ChIP data.<br>
<br>
Neural Network Weights Predict Evolutionarily Conserved Binding Motif Frequencies<br>
If binding data are predictive of expression class, and if meaningful transcription factor binding is motif-specific, then it should be possible to independently verify relationships from the weights matrix by measuring the frequency of binding motifs. We can also ask if any observed site enrichment and depletion are evolutionarily conserved, as would be expected if they mediate functionally relevant factor binding. Motif frequency across cell cycle clusters in multiple yeast species correlated remarkably well with binding probabilities from the ChIP data and also with the ANN weights trajectories across the same clusters (Figure 10). The conserved motif data for Mbp1 and Swi5/Ace2, and Fkh1/Fkh2, all factors with well-defined binding motifs, provided independent support for conclusions from the ANN, since the ANN was constructed without any input information about DNA sites.<br>
<br>
Conservation of Site Enrichment and Depletion over Great Evolutionary Distance<br>
The distribution of MCB sites across the cycle phases was striking and prompted us to ask if both enrichment and depletion holds over very great evolutionary distance. If specific depletion is a functionally important network characteristic, then we would predict that it would be retained over very great evolutionary distance. We performed the same site enrichment analysis across cell cycle gene classes in S. pombe, which is said to be as distant from budding yeast as are humans (?500 my). We used the EM algorithm to cluster the S. pombe cycling data of [3] in the same way that the various Saccharomyces experiments had been clustered [9]. At this evolutionary distance there are no large blocks of conserved noncoding DNA sequence. S. pombe does, however, have an identified MBF ortholog, and the short binding motif for MBF shows significant site enrichment in our expression cluster 3, together with significant depletion from cluster 5, mirroring the pattern in budding yeast (Figure 11). The positive regulator-to-target group conservation was noted previously [3?5], but in this study we were able to detect it without strongly prefiltering gene sets for their explicit experimental responsiveness to MBF. The new observation here is that depletion of MBF sites, operating specifically in the group of genes normally expressed later in the cell cycle, is a very highly conserved property. This cis-motif depletion suggests there is selective restriction against MBF sites and that it is phase-specific: it does not apply broadly to most genes in the genome, but does apply preferentially to genes in late cell cycle cluster (in this case cluster 5 for S. pombe, cluster 4 for S. cerevisae). In both organisms, this cluster contains genes whose products are involved in mitosis, and it seems possible that their heterochronic expression during G1/S phases, as MCB sites might cause, could disrupt proper control or execution of S phase. However, the observed conservation is apparently a network property, even though the specific genes in each phase group are?mainly?not orthologous. Thus, the surprising observation that most genes in these oscillating clusters are not the same ones in pombe and Saccharomyces (reviewed in [13]), if correct, suggests that conserved enrichment and depletion of regulatory motifs are network architecture properties that are shared across hundreds of millions of years, even though most specific genes involved are different.<br>
<br>
Materials and Methods<br>
Data pre-processing.<br>
The primary expression dataset for modeling is Affymetrix microarray data measuring RNA levels of nearly every gene in yeast through two cell cycles, following release from conditional CDC28TS arrest [2]. That time course sampled RNA levels at 10-min intervals over 170 min, which covers two cycles. These data were obtained from the original authors and preprocessed in three steps. 1) Any gene that did not show sustained absolute expression greater than the 2.5% quantile of the data (an absolute signal of 8) for three consecutive timepoints was eliminated. 2) For the remaining 6,174 expression vectors, each time point measurement was divided by the median expression value across all time points for the gene. 3) The log2 of each ratio was then taken, and these values comprised the expression matrix for all further analysis. For key model building in this work, we focused on the subset of expression vectors (384) that had been identified by Cho et al. as displaying a cell cycle dependent pattern and also passed the above filter for absolute expression; operationally we refer to this set as the ?cycling? set.<br>
The primary in vivo protein:DNA interaction dataset (ChIP/array) used here is from [21]. These data were obtained at http://web.wi.mit.edu/young/regulatory_code/ and the reported p-values were used directly. Briefly, for each of 204 transcriptional regulators, Harbison and colleagues constructed a yeast strain containing a myc-epitope-tagged version of the factor that was inserted into the corresponding transcription factor locus. Each strain was used to perform three independent ChIP/array measurements taken from freely cycling exponential phase cultures. The cells were subjected to standard formaldehyde crosslinking to attach transcription factors to their in vivo binding sites, the chromatin was sheared, factor-bound DNA was enriched by IP, amplified by LMPCR, and fluorescently labeled. ChIP-enriched DNA was then co-hybridized with control DNA to microarrays containing essentially all intergenic sequences in yeast. A binding ratio was then calculated for each array feature based on the relative hybridization signal for targets synthesized from ChIP enriched DNA versus whole cell extract control DNA. Three biological replicate experiments were performed, each beginning from an independent yeast culture. Based on an error model first described in [46] and the three replicate binding ratios for each intergenic sequence, a p-value was reported for each upstream intergenic sequence. This p-value roughly estimates the probability that a given transcription factor is bound to a particular intergenic sequence.<br>
<br>
Neural network implementation and training.<br>
Figure 1 illustrates the overall structure of the ANN trained in this study. Backpropagation was implemented by the <package>UWBP</package> package [47] to train a single layer network with no hidden units. Each ANN was trained using 300 epochs using a learning rate of .002. RNA expression array data for the subset of 384 cycling genes as described above were clustered using an expectation maximization algorithm fitting the data to a mixture of Gaussian probability distributions with diagonal covariances (EM MoDG [9,48]). Networks to predict cluster membership for each gene based on an input vector composed of ChIP derived in vivo factor binding probabilities for the 204 measured regulators in the Harbison dataset. Individual networks were trained using 80% of the data and tested on 20% of the data. For each 80/20 dataset split, ten neural networks were trained using different random seeds for each network. The network with the best prediction accuracy on the testing dataset was then selected and denoted as ?best.? This process was then repeated 40 times, splitting the dataset into different testing and training datasets. The network weights from the resulting 40 selected ?best? networks were then averaged together to create the aobANN. We focus on this network for subsequent biological interpretation, with the primary goal of identifying regulatory connections between transcription factors and their direct target genes. Because the purpose of this network is not to repeatedly classify similar data, the implications of overtraining are different than they would be for classical uses of ANNs. In this unconventional usage, we show by measuring the behavior of ten internal ?gold standard? known cell cycle regulators, that any ?overtraining? is not deleterious for the intended goal, which is extracting a series of ranked hypotheses about regulator-to-output relationships. Regulators within aobANNs are ranked based on the median SOS rank across all the individual ANNs trained to generate the aobANN. The SOS ranking for a regulator within an individual network is simply the sum of squared weights across the classes in the weight matrix (<br>
						).<br>
					<br>
<br>
Consensus site enrichment and depletion calculations.<br>
To determine whether an expression cluster showed an enrichment in genes that contain a particular consensus site, we calculated the likelihood of the observed enrichment, or depletion, being a chance occurrence according to a binomial model of occurrence probabilities. We count the observed number of genes that have at least one instance of a consensus sequence within the 1 KB directly upstream of the coding sequence for all genes in an expression cluster versus the number of genes that would be expected by chance. As no known background sequence model is completely provably correct, for each consensus sequence we calculate the expected background frequency (<br>
						f?) using a bootstrapping method. We randomly selected 1,000 different sets of genes the same size as the cluster being compared (n). These randomly selected background sets are drawn from either the entire genome or from only the ?cycling? genes, which were used in training the ANNs. The number of genes that contain at least a single instance of the consensus is counted for each randomly selected set. The average count across the 1,000 samples is normalized and used as our estimate of the expected number of genes within a cluster that have a single occurrence within 1 KB upstream (EC). Since the chances of any given gene within a cluster having a given consensus sequence within the 1 KB upstream can be assumed to be independent, we can estimate the probability of finding the observed number of counts (OC) using a standard binomial distribution (Equation 1). If the site is enriched, we estimate the p-value for the likelihood of finding at least the observed count, but if the site is depleted we calculate likelihood of finding at most the observed count (Equation 2).<br>
						<br>
						<br>
					<br>
<br>
<br>
<br>
Supporting Information<br>
Accession Numbers<br>
Table of top regulators. Gene descriptions for the top ten positively and negatively associated regulators for each cluster as determined by the ANN weights matrix in Figure 4 with annotations from http://www.yeastgenome.org.<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2494868</b><br>
The Effect of a ?K280 Mutation on the Unfolded State of a Microtubule-Binding Repeat in Tau<br>
Tau is a natively unfolded protein that forms intracellular aggregates in the brains of patients with Alzheimer's disease. To decipher the mechanism underlying the formation of tau aggregates, we developed a novel approach for constructing models of natively unfolded proteins. The method, energy-minima mapping and weighting (EMW), samples local energy minima of subsequences within a natively unfolded protein and then constructs ensembles from these energetically favorable conformations that are consistent with a given set of experimental data. A unique feature of the method is that it does not strive to generate a single ensemble that represents the unfolded state. Instead we construct a number of candidate ensembles, each of which agrees with a given set of experimental constraints, and focus our analysis on local structural features that are present in all of the independently generated ensembles. Using EMW we generated ensembles that are consistent with chemical shift measurements obtained on tau constructs. Thirty models were constructed for the second microtubule binding repeat (MTBR2) in wild-type (WT) tau and a ?K280 mutant, which is found in some forms of frontotemporal dementia. By focusing on structural features that are preserved across all ensembles, we find that the aggregation-initiating sequence, PHF6*, prefers an extended conformation in both the WT and ?K280 sequences. In addition, we find that residue K280 can adopt a loop/turn conformation in WT MTBR2 and that deletion of this residue, which can adopt nonextended states, leads to an increase in locally extended conformations near the C-terminus of PHF6*. As an increased preference for extended states near the C-terminus of PHF6* may facilitate the propagation of ?-structure downstream from PHF6*, these results explain how a deletion at position 280 can promote the formation of tau aggregates.<br>
<br>
Introduction<br>
Alzheimer's disease (AD) pathology is characterized by extracellular aggregates of A?-amyloid (A?) and intraneuronal tau aggregates, known as senile plaques and neurofibrillary tangles (NFTs), respectively [1]. Despite much focus on A? amyloid in AD research, tau seems to play an important role as well. For example, the number of NFTs and not the number of senile plaques in the neocortex correlates with the severity of dementia in AD patients, and there are data that imply that abnormalities in tau alone may cause neurodegeneration [2]. In light of these observations, a detailed characterization of the structure of tau protein may provide insights into the pathogenesis of AD and other neurodegenerative disorders associated with tau pathology. However, probing the structure of tau is difficult because tau protein is natively unfolded (or intrinsically disordered) in solution. Several studies suggest that tau retains its function after heat or acid-induced denaturation and both CD and X-ray scattering experiments imply that tau does not adopt a well-defined folded structure in solution 3?5. Consequently, obtaining structural and hence functional information on tau is problematic because the direct observation of unfolded states is typically difficult to achieve experimentally.<br>
Initially, unfolded proteins were described as random coils whose properties are derived from Flory's statistical description of chain molecules [6]. For such polymers, the radius of gyration, RG, follows the scaling law RG?=?R0N?, where R0 is the radius of gyration of a monomeric subunit (a function of the persistence length), N is the number of subunits in the polymer, and ? is a scaling factor that depends on the solvent characteristics. The most common measure of whether a protein behaves as a random coil is to test whether its radius of gyration follows this scaling law. However, while a structurally disordered molecule can exhibit random coil statistics, the converse is not necessarily true; i.e., random coil statistics do not imply that the structure is completely disordered [7]. Slight structural preferences may exist for some natively unfolded proteins and small changes in the distribution of conformers within an unfolded ensemble may play a role in the normal and pathological functioning of intrinsically disordered systems. A recent study, for example, suggests that inducer-mediated tau polymerization involves an allosterically regulated conformational change [8]. This is consistent with the notion that the formation of tau fibrils is associated with a shift in the conformational distribution of tau such that the unfolded state has a preference for proaggregatory conformations in the presence of an inducer. In light of this, constructing detailed ensembles that model the unfolded ensemble of tau may facilitate the identification of structural properties that promote aggregation.<br>
As full-length tau contains more than 400 amino acids (441 residues for the htau40 isoform [9]) constructing detailed ensembles that model the unfolded state of this protein is a daunting task. Fortunately, tau contains three or four imperfect microtubule-binding repeats (MTBRs) near the C-terminus of the protein, and almost all known mutations of tau that are associated with inherited forms of neurodegenerative diseases are located in MTBR domains or their nearby flanking regions [10]. As these data suggest that MTBRs play an important role in the progression of inherited tauopathies, we first focus on building ensembles that model the structure of individual MTBRs. It is important to note, however, that we do not strive to model the structure of a given MTBR fragment alone in solution. Rather, our goal is to generate ensembles that model the range of conformations that a MTBR can adopt when it is part of full length tau. In the present study we focus on building ensembles for the second MTBR, henceforth referred to as MTBR2. This repeat is of particular interest because it contains both a six amino-acid repeat, PHF6*, which is a minimum interaction motif that can initiate tau aggregation in vitro [11],[12], and the site of the proaggregatory mutation, ?K280, which is associated with some forms of frontotemporal dementia [13]?[16].<br>
We have developed a method, called energy-minima mapping and weighting (EMW), to construct ensembles that model the unfolded state of proteins. The underlying assumption that forms the basis of this approach is that the unfolded state can be modeled as a set of energetically favorable conformers, where each conformer corresponds to a local energy minimum. The method involves constructing a library of energetically favorable conformations and selecting conformations from this library to form ensembles that are consistent with a given set of experimental data. We use EMW to build ensembles for wild-type (WT) MTBR2 and the corresponding ?K280 mutant. By comparing data from the two sets of ensembles, we deduce structural preferences in the ?K280 ensemble that explain its increased propensity to form tau aggregates.<br>
<br>
Results<br>
The EMW method begins by constructing sets of energetically favorable conformations for a sequence of amino-acids within a natively unfolded protein (Figure 1). In the case of tau we focus on MTBR2 since this region contains the aggregation-initiating sequence PHF6* as well as the site of a mutation that is associated with increased tau aggregation in vitro [17]. A set of local energy minima is then constructed for this subsequence, hence forming the candidate ensemble (Figure 1). Associated with each structure in this ensemble is a weight, ?i, which corresponds to the probability that the given subsequence adopts the ith conformation in the candidate ensemble. We say that an ensemble is fully specified when the local energy minima and weights are known.<br>
Initial weights for structures in the candidate ensemble are calculated from the relative energies of each structure, as shown in Figure 1. However, as sampling is performed on a relatively small subsequence these weights may not reflect the relative probabilities of different conformations when the subsequence is part of the larger protein. For example, compact states may be preferred over extended states when the subsequence is in isolation but not when part of tau. Therefore, the composition of the ensemble is optimized and the members of the candidate ensemble are reweighted in light of experimental data that is obtained on a larger segment of tau protein. Sampling small subsequences increases the chance that we will observe a relatively large number of accessible states for this system. Using experimental data obtained on a larger region of tau (and not just the subsequence of interest) helps to ensure that the calculated ensemble represents the local structure of the sequence as it appears within full length tau.<br>
A central component of EMW is that we do not strive to construct a single model for the unfolded state. We recognize that the construction of unfolded ensembles that agree with any given set of experimental data is largely an underdetermined problem; hence it is likely that there are a number of different ensembles that are consistent with a given set of experimental data. Consequently, we constructed several ensembles that are all consistent with the experimental measurements and focused our analysis on local structural motifs that are present in all ensembles. For this study, we focused on NMR data that are available for both WT MTBR2 and a ?K280 mutant. These data were kindly provided by Marco Mukrasch, Daniela Fischer, and Markus Zweckstetter [17],[18].<br>
Using the EMW method, 100 ensembles were constructed for both wild-type (WT) and ?K280 sequences of MTBR2 (a total of 200 ensembles). Each ensemble was constructed to minimize the difference between calculated 13C? chemical shifts and the corresponding experimentally determined 13C? chemical shifts. The number of structures in each ensemble corresponds to the minimal number of structures needed to fit the available chemical shifts. Preliminary calculations found that 15 conformers were needed; i.e., fewer structures resulted in worse fits to the 13C? chemical shifts and more structures did not significantly improve the quality of fits. We note that other models examining residual structure in the unfolded state have utilized a similar number of representative conformers [19].<br>
Application of EMW yielded ensembles that were in excellent agreement with experimentally determined absolute 13C? chemical shifts (Figure 2A and 2B). The average RMS error between the calculated 13C? chemical shifts and the corresponding experimental values was 0.1 ppm?well below the error associated with <software>SHIFTX</software> chemical shift predictions and similar to the error associated with experimental chemical shift measurements on K18 constructs [17],[20]. However, given that measured absolute chemical shifts for the 20 amino acids vary significantly according to the amino-acid type, reasonable correlations to absolute chemical shifts may be achieved by simply predicting amino-acid specific random coil values. Given this, we analyzed the relationship between the chemical shifts, after subtracting out residue-specific random coil chemical shift values; i.e., the secondary chemical shifts. Overall, there is excellent agreement between calculated secondary chemical shifts and the corresponding experimental values for each residue in the sequence (Figure 2C and 2D). These data demonstrate that the calculated models yield agreement with experiment on a per residue basis.<br>
In the next step of our protocol, carbonyl carbon (13CO) chemical shifts were used to test whether the resulting ensembles can predict experimental observations that were not used to construct the model. This helps to ensure that our models are not ?overly fit? to the 13C? chemical shifts. In general, a model that is over-fit to a given set of experimental data can reproduce that data remarkably well but cannot reproduce data that was not used to generate the model. Therefore we consider an ensemble to be validated if new experimental results can be accurately predicted from the ensemble. For both the WT and ?K280 sequences, each of the 100 ensembles was ranked based on its ability to predict 13CO chemical shifts. Based on these data the thirty best ensembles were chosen for further analysis. The RMS difference between the calculated 13CO chemical shifts and the corresponding experimental values are below 0.9 ppm; i.e., below the error associated with available chemical shift prediction algorithms (Table 1) [20]. To further demonstrate that these thirty ensembles can reproduce additional data not used in the model constructed, we computed the error between calculated amide hydrogen (1HN) chemical shifts and the corresponding experimental values. The resulting values agreed with the experimentally measured ones to within 0.3 ppm (Table 1).<br>
As expected, structures that comprise the WT (Figure 3A) and ?K280 (Figure 3B) ensembles are heterogeneous in that they sample a wide range of conformations. Since each of the 30 ensembles represents an independent representation of the unfolded state, we searched for local structural motifs that are found in all of the ensembles. More precisely, the existence of a local conformation that is consistently adopted by a given subsequence in MTBR2 suggests that this conformation is needed to reproduce the experimental results. We therefore consider conserved motifs to represent local conformational preferences.<br>
We begin with an assessment of the local conformation of PHF6* in both the WT and ?K280 ensembles. Since PHF6* in the WT sequence spans residues 275?280, the ?K280 mutant sequence has a deletion in the six-residue stretch corresponding to PHF6*. However, since residue 281 is also a lysine, the ?K280 mutant contains an equivalent PHF6* subsequence at its N-terminus (Figure 4). This allows us to directly compare the conformation of PHF6* in both sequences. To identify preserved conformations of PHF6*, we first determined the different types of structures that this subsequence can adopt by clustering structures using only the backbone atoms of PHF6* (Figure 5). The probability that a given cluster occurs in an ensemble is equal to the sum of the weights of structures in that ensemble that contains a motif in the cluster. Preserved structural motifs are defined as clusters that have a nonzero weight in every ensemble (Figure 5); i.e., a preserved motif is found in all ensembles. For comparison, we repeated this procedure for all contiguous six-residue subsequences within MTBR2, yielding a collection of approximately 300 clusters that represent all possible structural motifs in our ensembles that any six-residue sequence in MTBR2 can adopt. Using the criterion outlined above, roughly 5% of these clusters were preserved across all ensembles.<br>
In WT MTBR2, clustering based on the conformation of PHF6* yielded 12 distinct conformations. However, only one of these states was present in all 30 ensembles (Figure 6A and 6B). Similarly, while PHF6* clusters into 11 distinct conformations in the mutant ?K280 ensembles, only one conformation was preserved (Figure 6C and 6D). In both cases, the preserved conformation of PHF6* is extended and has ?, ? angles that fall within the broad region of the Ramachandran plot corresponding to ?-structure. This observation is consistent with the notion that PHF6* a priori adopts extended conformations that can readily form cross ?-structure with other tau monomers [21]. Since the formation of cross ?-structure is believed to play an essential role in the formation of protein aggregates, these data are consistent with the notion that PHF6* promotes aggregation by forming ?-structure between tau monomers [11],[12].<br>
To explore the effect of the ?K280 mutation on the local structure of MTBR2, we analyzed the structure of the subsequences 278INKKLD283 and 278IN-KLDL284 in the WT and ?K280 sequences, respectively. For WT MTBR2, two conformations for 278INKKLD283 were found in all ensembles. The first is a loop/turn that is associated with a change in the direction of the mainchain (Figure 7A and 7B). In this structure residue K280 has ?, ? angles of approximately ?102? and ?30?, respectively; i.e., mainchain dihedral angles consistent with an ?-helical/turn conformation. The second conformation is more extended, having ?, ? angles that place its residues within the broad region corresponding to extended ?-structure (Figure 7C and 7D). In the mutant sequence, residue K280 is absent and the corresponding sequence, 278IN-KLDL283, has one preserved conformation. The deletion of residue 280, which can adopt an ?-helical/turn conformation in the native sequence, leads to a relative increase in results in extended states in this region (Figure 7E and 7F). The deletion, however, also introduces a slight kink in the mainchain of the sequence (Figure 7F).<br>
In a prior work, N?H residual dipolar coupling (RDC) values were measured for residues in the WT K18 construct in polyacrylamide gel [22]. While most residues in MTBR2 have relatively large negative RDC values, S285 has a large positive value [22]. This difference can be explained by either a change in the local alignment tensor at S285, or the presence of ?-helical/turn structure at this site [23]?[26]. Accelerated molecular dynamics simulations of WT K18, however, confirm that the sequence 283DLSN286 samples turn conformations with relatively high frequency [22]. In light of these observations, we explored the structure of the six residue segment, 282LDLSNV287, which includes residue S285. This region adopts two conformations that are preserved across all WT ensembles. One of the conformations contains a loop/turn (Figure 8A and 8B) where residue S285 has ?, ? angles of ?63? and ?39?, respectively; i.e., near the optimal ?-helical values (Figure 8B). The alternate conformation is extended and does not result in a change in the direction of the mainchain (Figure 8C and 8D). However, in the ?K280 mutant, 282LDLSNV287 has one structure that is preserved across all ensembles (Figure 8E and 8F). In this structure S285 again adopts ?, ? angles (?95? and ?63?, respectively) that are consistent with an ?-helical/turn conformation (Figure 8F). These data agree with the RDC data mentioned above and suggest that this region in both the WT and mutant sequences is able to adopt turn-like conformations in solution as well as in a polyacrylamide gel.<br>
<br>
Discussion<br>
Dynamical simulations provide a valuable tool for the analysis of unfolded proteins, providing insights that would be difficult to obtain from experiments alone [27]. A number of simulation methods have been developed to model the unfolded states of proteins and useful insights have been obtained with these techniques. Many of these approaches generate ensembles by directly incorporating experimental constraints into molecular dynamics simulations in order to facilitate conformational sampling. These methods bias molecular trajectories to sample conformers that are consistent with a given set of experimental data. One problematic issue with biased sampling, however, is that it can suffer from over-fitting?a process that may yield a distribution of conformers that does not accurately model the range of structures that comprise the unfolded state [27]. Given this concern, a number of unbiased methods have been developed to generate ensembles for unfolded proteins. These approaches utilize fast algorithms, which do not employ a physical potential energy function, to obtain representative structures of the unfolded state, and in some cases experimental data can then be used to improve the resulting ensembles [28]?[31]. The algorithm ENSEMBLE, for example, adjusts population weights for pregenerated conformers to improve agreement with experimental data in a manner similar to that described here [30].<br>
A unique feature of the present method is that it does not strive to generate a single ensemble that represents the unfolded state. Given that accurate modeling of an unfolded protein is an undetermined problem, it is likely that there are a number of different ensembles that agree with any given set of experimental data. Moreover, given the immense number of potential conformations that an unfolded protein can adopt, this may be true even when a relatively large number of experimental constraints are used to construct the ensemble. Hence our goal was to construct several candidate ensembles, each of which agrees with a given set of experimental constraints, and focus our analysis on local structural features that are preserved across all ensembles. Local structural features that are found in all independent ensembles likely represent motifs that are required to reproduce the experimental data. In other words, given the underdetermined nature of the problem, it is not clear how to determine when one has the ?correct? ensemble. However, local structural motifs that consistently appear in all independent ensembles are likely to also be present in the ?correct? ensemble. Consequently, we consider locally preserved structural motifs to represent local conformational preferences.<br>
An important consideration in our method is the choice of experimental data that is used to build and validate the constructed ensembles. In principle, EMW can use any set of experimental measurements to optimize and validate model ensembles. Indeed, as more structural information is made available, additional data can and should be used to further refine the set of model ensembles. In this regard, we note that although a number of NMR measurements have been made on native tau constructs, the data available for constructs containing a ?K280 mutation is relatively limited. In a prior study, nuclear chemical shifts and HSQC spectra were measured for the K18?K280 construct, which contains all four MTBRs and the ?K280 mutation [17]. Data were obtained for both free K18?K280 and for K18?K280 in the presence of the polyanion heparin and microtubules [17]. However, as we are interested in building structural models for MTBR2 in solutions free of compounds that promote tau self-association (e.g., heparin) and free of proteins known to bind tau, we focused on measurements obtained with the free K18?K280 construct. Additionally, as there are a number of existing methods that relate chemical shift measurements to three dimensional protein structures [20], [32]?[34] we considered 13C?, 13CO, 1HN, and 15N chemical shift measurements; i.e., the only available chemical shifts for K18?K280 [17]. Furthermore, established methods for estimating NMR chemical shifts can predict carbon and amide proton chemical shifts with an error of approximately 1 ppm or less, while the error associated with predicting nitrogen chemical shifts is substantially larger (?2?2.5 ppm) [20], [33]?[35]. Therefore we focused on the 13C?, 13CO, and 1H chemical shifts for this study because these data represent measurements that can be calculated with the greatest accuracy and that are available for both native tau constructs and the ?K280 mutant.<br>
It has long been recognized that chemical shifts of a given residue are, in general, largely a function of the local environment of the residue in question [36],[37]. Since we generate ensembles that agree with chemical shifts, a limitation of the results reported here is that we do not explicitly include experimental data that more directly reveal information about non-local interactions. While long range contacts have been identified in some natively unfolded proteins (e.g., [19]), the dimensional scaling characteristics of intrinsically disordered proteins suggests that stable long-range contacts are sparse in these systems [38]. Nevertheless, we suggest that the combination of a physical potential energy function, which can in principle model long range interactions, and experimentally determined chemical shifts can provide insight into the structure of proteins in general. In this regard we note that data are emerging that suggest that backbone chemical shifts, when used in conjunction with a physical energy function, may be sufficient to adequately predict tertiary folds, and consequently stable non-local contacts, for some proteins [39],[40].<br>
Although our work focuses on the structure of the MTBR2 without explicitly including other MTBRs, our findings may also have implications for full length tau. Once a representative set of conformers for MTBR2 is generated, we strive to ensure that the calculated chemical shifts agree with chemical shifts obtained using a construct that contains all MTBRs. This helps to guarantee that the ensemble models the structure of MTBR2 as it appears in full length tau. In short, we are not interested in the structure of MTR2 as it appears alone in solution; instead we hope to deduce structural features of MTBR2 as it appears in full length tau. In addition, as MTBR2 contains an aggregation-initiating sequence that is known promote tau aggregation in vitro as well as the site of a mutation that leads to in increased tau aggregation in vitro and in vivo, studies of both its WT and mutant forms may lead to insights into the mechanism of tau aggregation [12],[15],[41].<br>
The ability to form intermolecular ?-sheet conformations appears to be a relatively general property of polypeptide chains that are associated with disorders of protein misfolding and aggregation [42]?[45]. Therefore it is likely that an inherent propensity to form extended conformations, that are consistent with ?-structure, will promote aggregation in natively unfolded systems. When EMW is applied to MTBR2, we find that the aggregation-initiating sequence, PHF6*, adopts an extended conformation in both the WT and ?K280 ensembles, a finding consistent with the observation that these peptides can initiate tau aggregation [11],[12]. Interestingly, in a prior work we demonstrated that a related hexapeptide, PHF6, preferentially adopts an extended state that can facilitate the formation of cross-?-structure between tau monomers [21]. The present study suggests that this property is preserved when aggregation-initiating sequences are part of their corresponding MTBRs. That is, PHF6* a priori adopts extended conformations that can readily form hydrogen-bonded ?-structure. Additionally, a recent survey of amyloidogenic proteins suggests that fibrillogenesis for natively unfolded proteins involve the formation of partially folded intermediates that can subsequently go on to form amyloid fibrils [45]. Our findings are consistent with these observations. That is, our results imply that formation of a locally stable, and extended, conformation plays a role in the formation of tau aggregates.<br>
Recently, several studies have attempted to characterize residual structure of MTBRs in tau [17], [18], [22], [46]?[48]. These studies can be roughly divided into two categories: descriptions of ensemble average characteristics based on NMR measurements [17],[18],[22],[46], and NMR solution structures of local regions obtained by adding organic solvents to stabilize a unique fold [47],[48]. Since the presence of organic solvents leads to significant changes in the conformational distribution of states, as evidenced by dramatic changes in the CD spectra [5],[47],[48], the physiologic relevance of these latter results remains unclear. However, early characterizations of MTBRs in nonorganic solvents, found that the PHF6 region likely has a higher propensity for extended, ?-strand-like conformations?a finding in accord with our data [18],[46].<br>
Given that both WT and ?K280 tau contain aggregation-initiating sequences (Figure 4), it is not clear how ?-strand propensity in this region explains the difference in aggregation potential between the two sequences. Therefore to deduce structural features of the ?K280 mutant that explain its proclivity to form aggregates, we analyzed the structure of MTBR2 in the vicinity of the mutation site. Unfolded ensembles of WT MTBR2 contain two conformations at the mutation site that were present in all ensembles?a loop/turn conformation and an extended state. In contrast to the WT MTBR2 ensembles, models of ?K280 in the same region had one conformation that was present in all ensembles. This state is relatively extended and contains a kink at the site of the deletion. While the slight disruption in the extended state of the mutant may also influence the ability to form hydrogen-bonded cross-?-structure, a loop/turn at the C-terminus of PHF6* constitutes a much greater impediment to the formation of ?-structure. Since residue K280 has a relative preference for nonextended states, deletion of this residue leads to increased sampling of extended states downstream from PHF6*. The relative preference for extended structures downstream from PHF6* in the ?K280 mutant suggests that the ability to propagate ?-structure distal to PHF6* can affect the aggregation potential of tau. These observations therefore explain how the deletion of a single residue can change the aggregation potential of tau.<br>
We also find that in both WT and mutant ensembles residue S285 can adopt ?, ? angles consistent with an ?-helical/turn structure. Recent data on the WT sequence are also consistent with these observations as RDC values and molecular dynamics simulations suggest that S285 adopts an ?-helical/turn structure. Since those experiments were performed in polyacrylamide gel, our data suggest that this structure also occurs with relatively high frequency in solution. It is also worthwhile to note that although we find that a six-residue region including K280 can adopt a similar loop/turn conformation, the associated RDCs for this region are not associated with a change in sign, like that observed at S285 [27]. Nonetheless, unlike RDC measurements for folded proteins, RDC values for unfolded proteins can be difficult to interpret [49]. This is due, in part, to the fact that prior to the measurement of RDC values, the protein of interest must first be embedded in an alignment medium [26]. This induced steric alignment of unfolded proteins may lead to results that do not fully capture the range of structures that an unfolded protein can adopt in solution. Hence the absence of particular RDC values in polyacrylamide gel (or any other alignment media) does not necessarily imply that a given conformation is not present in solutions containing the unfolded protein of interest.<br>
The formation of tau aggregates is likely a complex process as a number of factors have been shown to influence the formation of tau aggregates in vitro [1]?[3]. Consequently, there may be additional factors that contribute to the increased ability of the ?K280 mutant to form aggregates; e.g., a ?K280 mutation leads to an overall decrease in the strength of the intermolecular charge-charge repulsion between tau monomers that self-associate [12]. Nonetheless, our data demonstrate that small changes in the sequence of tau can lead to localized structural changes in the unfolded ensemble that may affect tau's ability to form cross-?-structure. Overall, our data suggest that small sequence-specific changes can promote tau aggregation and that interventions that prevent the propagation of ?-structure downstream from aggregation-initiating sequences, may form the basis for therapies that prevent tau aggregation.<br>
<br>
Methods<br>
Energy-Minima Mapping and Weighting<br>
The EMW method constructs ensembles for unfolded proteins that are consistent with a given set of experimental data. Our model for an unfolded ensemble consists of structures corresponding to local energy minima and associated probabilities (weights) that are assigned to the different conformations. For this work, the experimental measurement used to optimize and validate the model ensembles are chemical shifts for the second tau microtubule binding repeat [17]. In principle, EMW can be used with any given set of experimental data. In this application we focus on chemical shifts that were available for both the K18 and K18?K280 constructs.<br>
The EMW method can be decomposed into three steps (i) conformational sampling, (ii) model optimization, and (iii) ensemble validation. Conformational sampling uses high temperature molecular dynamics (MD) followed by minimization of the resulting structures (i.e., quenched dynamics) to create a library of widely varying conformations representing minima on the potential energy surface. Model optimization is performed to select a subset of these structures and optimize weights that represent the relative prevalence of each structure. Validation is performed by computing additional chemical shifts that not used to construct the ensemble and comparing these data to experimentally measured carbonyl carbon shifts. In what follows we outline each step of the EMW method.<br>
Conformational sampling<br>
We used quenched molecular dynamics (QMD) to sample different local energy minima of the R2 peptide. Conformational sampling was performed on a blocked peptide with the sequence corresponding to the second microtubule binding repeat. A polar-hydrogen model of the WT (VQIINKKLDLSNVQSKCGSKDNIKHVPGGGS) and ?K280 (VQIINKLDLSNVQSKCGSKDNIKHVPGGGS) MTBR2 peptides were constructed using <software>CHARMM</software> [50]. The N and C-termini were blocked using ACE and CBX residues defined in the effective-energy function-1 (EEF1) model [51]. This sampling procedure consisted of high temperature molecular dynamics (used to randomize the initial conformation of the protein) followed by quenched dynamics. To ensure that a wide range of conformations was sampled, constraints were imposed on the peptide for the high temperature and quenching steps. Specifically, conformational sampling was performed in a series of molecular dynamics simulations. In each simulation the end-to-end distance of MTBR2 was restrained to a pre-defined value; i.e., 3, 4, 5, ?, 70 ?, where the end-to-end distance was defined as the distance between the C? carbons on residue VAL1 and SER31 of the peptide. End-to-end restraints were used to ensure that both compact and extended states were sampled during the high temperature simulations. For each end-to-end distance, 4 ns of high temperature MD at 1,000 K was performed with the EEF1 implicit model of solvent [51]. All simulations employed a Berendsen thermostat to maintain the system temperature at the desired value [52]. Hydrogen bond lengths were held near their equilibrium values using <software>SHAKE</software> [53] and a 2 fs timestep was used. Coordinates were saved every 10 ps, yielding a total of 400 structures per end-to-end distance. This procedure was applied to both WT and ?K280 sequences, producing a total of 27,200 structures for each sequence.<br>
Each structure was then used to initiate a new MD trajectory which cools the system to 298 K over 40 ps of simulation by coupling the sampled system (including atom coordinates and corresponding velocities) to a Berendsen heat bath at 298 K. At the end of this cooling simulation, structures were minimized for 10,000 steps using the adopted basis Newton?Rhapson algorithm [50]. Restraints were removed for the minimization step to ensure that minima on the unbiased energy surface are sampled. Searching for minima in the vicinity of the randomized conformation by cooling and equilibration followed by minimization rather than simply performing direct minimization allows the structures to escape shallow local energy minima and find more stable states.<br>
As the conformation of PHF6* is of particular importance, additional simulations were performed to ensure that a large range of PHF6* conformations were represented in the ensembles. Each additional simulation constrained the PHF6* radius of gyration to adopt a predefined radius of gyration (4?5.9 ?) while the restricting the end-to-end distance of MTBR2 to be near 9 ?. This was done because our initial data suggested that compact conformations of MTBR2 were relatively undersampled after early QMD simulations. In total 31,200 local energy minima were generated for the native polypeptide and 31,200 structures were generated for the mutant structure. We refer to this set as our structure library.<br>
We note that no single structure in our structure library had calculated backbone chemical shifts that agreed with the corresponding experimental values. For example, amongst the 31,200 structures, we found one conformer that had a 13C? chemical shift error of approximately 1 ppm (compared to the ensemble shift errors of 0.1 ppm). In addition, this structure had a 13CO chemical shift error of 2.3 ppm (compared to the ensemble CO errors which were all below 0.9 ppm).<br>
<br>
Ensemble optimization<br>
The optimization procedure strives to obtain ensembles that have calculated chemical shifts that agree with experiment. The function to be minimized is:(1)where N is the number of structures in the ensemble, Xi is the Cartesian coordinates of the ith structure, ?i is the weight of the ith structure, r is the number of residues in MTBR2,  is the experimentally determined C? chemical shift of residue j, and SC?(j) is the calculated C? chemical shift of residue j. Using the definition of SC?(j) shown in Figure 1 we have:(2)where  is the calculated chemical shift of residue j in structure Xi.  is computed using <software>SHIFTX</software> [20]. We note that reported errors for the experimentally determined chemical backbone shifts are all approximately 0.1 ppm [17]. Therefore, the experimental errors of individual shifts are not explicitly included in Equation 2. Lastly, errors reported in the text represent  and are therefore in units of parts-per-million (ppm), i.e., the same units used for chemical shift data.<br>
We used a simulated annealing algorithm to minimize f in Equation 2. To implement a simulated annealing protocol we first need an initial ensemble. The candidate ensemble was constructed by dividing the structure library into n different sets based on the radius of gyration of the different conformers (n was allowed to vary between 1 and &gt;100, see below). One structure was randomly chosen from each set to form the initial ensemble. This ensures that our simulated annealing protocol begins with a set of structures that span many different radii of gyration for the molecule. The weights for structures in this ensemble were calculated from the relative energy of each conformation as follows:(3)where the energy associated with each conformation, Ei, is the EEF1 potential energy, Si is the vibrational entropy, and T?=?298 K [21]. This initial model (structures and weights) was the starting point of our simulated annealing protocol.<br>
In our simulated annealing protocol, one performs a number of Monte Carlo steps at a given value of a control parameter (also referred to as the temperature). As the control parameter is gradually decreased, the system approaches its global minimum [54]. Central to any simulated annealing method is the protocol for decreasing the control parameter; i.e., the cooling schedule. We use a cooling scheduled based on the work of Nulton et al. and described in reference [55],[56].<br>
Each Monte Carlo step consisted of several stages:<br>
The simulated annealing algorithm was implemented <software>MATLAB</software> (Mathworks). The number of Monte Carlo steps for a given value of the control parameter is as described in a previous work [55].<br>
To determine the appropriate number of conformers in each ensemble, we performed the optimization procedure described above assuming that the ensemble had n structures, where n ranged from 1 to &gt;100. These calculations found that a minimum of approximately 15 conformers were needed to fit the C? chemical shifts to within 0.1 ppm, which is approximately equal to the experimental error associated with these chemical shift measurements [17] and well-below the error associated with <software>SHIFTX</software> chemical shift predictions [20]. Including additional structures did not significantly improve the error.<br>
<br>
Ensemble validation<br>
Validation consists of computing chemical shifts, using the final optimized model from, and comparing these data to experimentally measured values that were not used in step (ii). As described in the text, 13C?-chemical shifts were used to construct the model and 13CO and 1HN shifts were used for validation purposes. The error between calculated and measured shifts is computed using Equation 2, with 13CO atoms substituted for 13C? atoms. Models were ranked by their error and the 30 models with the best agreement with the 13CO shifts were selected for more detailed analysis as described in the text. To further test whether these models could be used to calculated quantities not used in model construction we computed 1HN chemical shifts from these thirty ensembles and compared these data to the corresponding experimental values.<br>
<br>
<br>
Identifying Locally Preserved Conformations<br>
We searched for conformations of six-residue subsequences that are present in every ensemble. Six residues was a natural characteristic size for a local region of interest, as it is the length of PHF6*. To this end, all structures in each ensemble of either WT or ?K280 MTBR2 were clustered using a matrix consisting of the pairwise RMSD backbone deviation of the each contiguous six-residue segment. Structures were clustered using <software>MATLAB</software> (Mathworks) such that the maximum RMSD between two structures in a cluster was 2.5 ?. A range of maximum RMSD values (1?6 ?) were examined empirically, and it was found that a cutoff of 2.5 ? was sufficient to prevent similar conformations from being divided into separate clusters, while also ensuring that clusters included a relatively homogeneous set of conformations. The probability that a given cluster occurs in an ensemble is equal to the sum of the weights of all structures that contain that motif. Preserved local structural motifs were found by identifying clusters where the total weight of its structures was non-zero across all ensembles.<br>
Structures for each cluster were visualized in <software>VMD</software>. To facilitate visualization of the overall conformation associated with a cluster, an average structure for each cluster was generated after 5,000 steps of steepest descent minimization to remove bad contacts (only the 6 residues were minimized). Visual inspection verified that the energy minimized structures did not differ significantly from their un-minimized counterparts. All molecular structures were made with <software>VMD</software> [59].<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2615076</b><br>
Efficient Network Reconstruction from Dynamical Cascades Identifies<br>
                    Small-World Topology of Neuronal Avalanches<br>
Cascading activity is commonly found in complex systems with directed<br>
                    interactions such as metabolic networks, neuronal networks, or disease spreading<br>
                    in social networks. Substantial insight into a system's organization<br>
                    can be obtained by reconstructing the underlying functional network architecture<br>
                    from the observed activity cascades. Here we focus on Bayesian approaches and<br>
                    reduce their computational demands by introducing the Iterative Bayesian (IB)<br>
                    and Posterior Weighted Averaging (PWA) methods. We introduce a special case of<br>
                    PWA, cast in nonparametric form, which we call the normalized count (NC)<br>
                    algorithm. NC efficiently reconstructs random and small-world functional network<br>
                    topologies and architectures from subcritical, critical, and supercritical<br>
                    cascading dynamics and yields significant improvements over commonly used<br>
                    correlation methods. With experimental data, NC identified a functional and<br>
                    structural small-world topology and its corresponding traffic in cortical<br>
                    networks with neuronal avalanche dynamics.<br>
<br>
Introduction<br>
Cascade-like dynamics is characterized by the succession of events, or processes,<br>
                that are causally related, and is frequently encountered in many complex systems<br>
                (networks) across disciplines. For example, single cells in living organisms<br>
                maintain metabolic, protein and gene-interaction networks with mostly unidirectional<br>
                signaling cascades in which nodes represent metabolites, proteins and genes<br>
                respectively [1]?[3]. At the next higher<br>
                level of cell to cell interactions such as the brain, pyramidal neurons in the<br>
                cortex connect with thousands of other neurons [4] thereby supporting<br>
                cascades of neuronal activity in the form of waves [5], neuronal avalanches<br>
                    [6] and<br>
                synfire chains [7],[8]. Cascade-like dynamics also occurs in many social<br>
                networks such as the spread of epidemics [9] and gossip [10] in human<br>
                networks as well as human travel itself [11]. This cascading<br>
                dynamics carries the signature of the underlying statistical interdependencies<br>
                between the interacting nodes, which are summarized by the functional network<br>
                    topology, represented by adjacency matrix indicating whether<br>
                two nodes interact or not, and architecture<br>
                [12],<br>
                represented by a weighted graph which additionally indicates the magnitude of each<br>
                interaction. The relationship between the cascading dynamics and the functional<br>
                network is often poorly understood, even though reconstructing the network from the<br>
                observed dynamics can provide crucial insights into the causal interactions between<br>
                the nodes as well as the overall functioning of a complex system [13]. Of<br>
                similar challenge remains the problem of how the functional architecture relates<br>
                back to the structural organization of a network, that is to its physical nodes and<br>
                physical connections between nodes [14]. While very similar<br>
                dynamics can arise from fundamentally different network structures, e.g. for small<br>
                neuronal networks with diverse elements [15], for large networks such<br>
                as the human cortex the global brain dynamics has been shown to reflect fairly<br>
                accurately the underlying structural connectivity, i.e. cortex anatomy [16],[17]. It is<br>
                therefore critical to identify new approaches that provide insight into the<br>
                functional and structural organization of a network based on the observed dynamics.<br>
Correlations in the dynamics between nodes have been successfully used to identify<br>
                functional links in relatively large networks such as obtained from MEG or fMRI<br>
                recordings of brain activity (e.g. [18]?[20]). A pure correlation<br>
                approach, however, is prone to induce false connectivities. For example, it will<br>
                introduce a link between two un-connected nodes, if their activities are driven by<br>
                common inputs [21],[22]. More elaborate<br>
                approaches such as Granger Causality [23], partial Granger<br>
                Causality [24], partial directed coherence (for a review see [25]),<br>
                and transfer entropy [26] partially cope with the problem of common input,<br>
                however, these methods require extensive data manipulations and data transformations<br>
                and have been mainly employed for small networks [27],[28].<br>
Here, we propose a new method that efficiently reconstructs the functional<br>
                architecture of a network from the dynamics. In the theoretical part of the<br>
                manuscript, we first introduce two different Bayesian approaches to reconstruct the<br>
                network topology from the observed cascades: (1) the Iterative Bayesian (IB), and<br>
                (2) the Posterior Weighted Averaging (PWA) with equal link priors. We then use PWA<br>
                to derive the Normalized Count (NC) approach, a simple and efficient nonparametric<br>
                algorithm that requires very little knowledge about the dynamical rules underlying<br>
                activity cascades. We show that the NC, which is a hybrid between a Bayesian<br>
                approach and a correlation method, performs almost as well as the IB when the exact<br>
                probabilistic rules of the dynamics known. Using simulations, we demonstrate the<br>
                utility of these algorithms for reconstructing random, small-world and scale-free<br>
                network architectures from activity cascades modeled by subcritical, critical, and<br>
                supercritical branching processes.<br>
We apply our approach to neuronal avalanches, which are the activity cascades in the<br>
                brain. It has been shown [6],[29],[30] that they spontaneously emerge in superficial<br>
                layers of cortex, both in vitro (acute slices and slice cultures)<br>
                    [29]?[32] and in vivo<br>
                [33].<br>
                They have also been demonstrated recently in the spike activity of dissociated<br>
                cortex cultures [34],[35]. The network<br>
                architecture that gives rise to neuronal avalanches is currently not known, although<br>
                neuronal avalanches have been simulated in networks with scale-free [36],[37], fully connected<br>
                    [38],<br>
                random [39], and nearest-neighbors [37],[40]<br>
                topologies. Here we demonstrate a small-world functional topology of neuronal group<br>
                formation in neuronal avalanches.<br>
<br>
Methods<br>
Theory<br>
Bayesian network reconstruction from cascade dynamics<br>
The cascade dynamics on a network can be described as a sequence of events , indicating the node, the time and the amplitude of an<br>
                        event respectively. We assume here that the observed sequence, , can be described by some underlying network structure, , which we are trying to reconstruct, and certain<br>
                        probabilistic rules, . If  is known, then the most accurate reconstruction of  from the observed dynamics is obtained using the Bayesian<br>
                        approach [41]?[43], which relies<br>
                        on the Bayes rule(1)where the index  indicates a particular instance of network topology<br>
                        (adjacency matrix),  is the posterior probability of having , given the observation  is the a priori (prior)<br>
                        probability, and  is the term that incorporates the above mentioned<br>
                        knowledge about the dynamics. The sum in the denominator is over all<br>
                        possible network configurations.<br>
Exploring all possible topological configurations for a complete network with  nodes is a daunting task, since that number is on the<br>
                        order of , making this approach computationally intractable. To<br>
                        reduce the problem, we assume that the activation of a given target node  (descendant; see Figure 1A) can be caused only by a finite<br>
                        set of events , occurring on source nodes  (ancestors) at prior times. The index  now enumerates all link configurations (topologies) by<br>
                        which the  active source nodes can connect to the target node . This reduces the number of configurations to be explored<br>
                        to , where  is the number of active source nodes considered. Thus,<br>
                        when exploring a particular topological configuration , the activation depends only on  active nodes that connect to the target (see example in<br>
                            Figure 1A with ). The number  is not fixed and changes in time as different target nodes<br>
                        are explored. The  relevant ancestors are usually obtained using a cut-off<br>
                        time difference beyond which the activation of the target is impossible or<br>
                        unlikely. When the cascade dynamics is recorded in the form of a raster (see<br>
                            Figure 1B), the<br>
                        event times  are discretized and events are placed into bins of fixed<br>
                        duration , which allows a fixed number of preceding bins to be used<br>
                        in order to determine the  relevant source nodes. A single observation  then reduces to a statement that an event  occurred on the target node , given the set of ancestor events , or, for binned data,  can also state that a node  was not active within a given time bin . Thus, the reconstruction of the whole network,  is subdivided into many simple Bayesian estimation steps<br>
                        focusing on a single target node and its corresponding subset of source<br>
                        nodes. We call this a single target estimation step (STES). To obtain  we combine these simple STES using two approaches that<br>
                        differ mainly in the way of handling the priors: (1) the Iterative<br>
                            Bayesian (IB), which starts with equal priors and builds them<br>
                        iteratively, and (2) the Posterior Weighted Averaging<br>
                        (PWA), for which the case of equal prior probabilities for link existence is<br>
                        explored. Both approaches are described in detail following the description<br>
                        of the dynamical model that we use in this work as an example, thus<br>
                        obtaining the dynamics term  in Equation 1. Finally, we derive a nonparametric method<br>
                        based on PWA, that we call normalized count (NC) approach,<br>
                        to be used for reconstructing networks from point process dynamics.<br>
<br>
Cascade dynamics: branching point process on a network<br>
For many dynamical processes on a network it is reasonable to assume that the<br>
                        activation of a target node, , depends only on a finite set of prior events . Then,  can be written as a general function of the event times, , amplitudes , as well as  and other parameters needed to describe the dynamics.<br>
In this work we focus on a specific type of cascade dynamics, i.e., a<br>
                        branching point process in which the probability  that a given network node  will activate node  is fixed (Figure 1C). The branching process is specified in the form of a<br>
                        directed weighted graph with weights equal to  thus forming the network architecture [12]. The network topology is defined by the links<br>
                        which have non-zero probabilities of activation . Depending on the values of , subcritical, critical and supercritical regimes can be<br>
                        observed for many network topologies.<br>
Given heterogeneous probabilities  for each source node  to activate the target node , and the net configuration  in which  links from the  active nodes exist , the probability that the node  is active at time  is(2)where  indicates the node index for the  active source node, and  represents the probability that the activation occurred<br>
                        through some external means outside the chain of cause and effect within the<br>
                        cascade, or simply noise.<br>
Equation 2 also allows for the reconstruction of networks in situations when<br>
                        the cascades are recorded in continuous time and when the magnitude of the<br>
                        individual node activities are different, in which case  are adjusted using some function  to account for differences in times and amplitudes, i.e.,(3)where  are the amplitudes of events occurring at time  respectively. In the current work we treat the cascades as<br>
                        a pure point process, and ignore the effect of the amplitudes.<br>
Often, neither all of the  nor the precise function  are known and the branching dynamics might be replaced<br>
                        with its mean-field approximation, . The term ?mean-field? used here<br>
                        should not invoke the mean field theory (or self-consistent field theory) in<br>
                        statistical mechanics, but rather its more general meaning, designating any<br>
                        approach in which the actual probability density function  is replaced by the delta function located at its mean<br>
                        value, . In such case, calculations are much easier and the<br>
                        probability of observing the target node  being active at time  is simply given by(4)which now depends only on .<br>
<br>
Iterative Bayesian (IB)<br>
Using the IB approach, we attempt to reconstruct a network represented by a<br>
                        set of  probabilities, , with  being the probability that a given link  exists. From these individual link priors, the prior<br>
                        probability for a particular network configuration  is obtained by(5)where the product on the left contains  terms and the product on the right  terms. Knowing both the priors and the dynamics terms, the<br>
                        posterior probabilities, , for each configuration  can be calculated using Equation 1. The posterior<br>
                        probability for a particular link , is obtained by summing  over those configurations  that contain the link  (see Figure<br>
                            1D and 1E),(6)The subset of links in  that participated in the current STES will be updated with<br>
                        their posterior values, i.e. , essentially modifying the priors used in the next STES.<br>
                        Initially, the link priors,  are assigned some small value for all links, and then this<br>
                        iterative procedure is continued until all target nodes are exhausted.<br>
By examining the Eqs. 5 and 6, one can see that the links that acquire a<br>
                        probability  of 0 or 1 will remain at these probabilities. To avoid<br>
                        this, link probabilities  smaller than some prescribed threshold value  are set to . The minimal threshold value is usually chosen to be equal<br>
                        to the initial small prior probability assigned to each possible link and is<br>
                        on the order of . In the presence of noise, the upper boundary  cannot be reached. If the final posterior probability for<br>
                        the existence of a link is higher than some threshold , the link is significant, otherwise it doesn't<br>
                        exist. A natural choice for the threshold is .<br>
<br>
Posterior weighted averaging (with equal link priors)<br>
A shortcoming of the IB is that it weighs heavily recent events, while early<br>
                        data are likely to be ignored. This can lead to reconstruction errors if<br>
                        sudden bursts of noise, in particular towards the end of an experimental<br>
                        observation, are encountered. As an alternative to the IB, we developed the<br>
                        PWA approach. Here, at each STES, we start with the same, pre-assigned, set<br>
                        of prior link probabilities, . For convenience, we assume that no a<br>
                        priori knowledge about the network  exists and hence make all link priors equal to some fixed<br>
                        value . Then, we obtain the posterior probabilities  at each individual Bayesian STES for  links according to Equation 6. We then derive a weighting<br>
                        factor which is used to combine the individual STES in order to obtain a<br>
                        global measure of connectivity between any two nodes. In order to find the<br>
                        proper weighting factor, we note that when the posterior probability for a<br>
                        link is equal to the prior probability, i.e. , no information is gained and we assign zero weight to<br>
                        such a case. When the posterior probability is 1 we set the desired weight<br>
                        to 1. The suggested weighting factor can then be written as(7)where the index  now enumerates different STES, or, in the case of binned<br>
                        data, different time bins.<br>
When , the prior for a given network configuration , can be written as a function of the number of existing<br>
                        links, , and the number of active source nodes, ,(8)Based on Equation 1, the posterior probability of a<br>
                        particular network configuration , that has  existing links out of  possible links, , can be written as(9)where  is the dynamics term for the configuration  (Equation 2), and  is the normalization term(10)The  in Equation 9 indicates that  depends on the individual activation probabilities  between  active nodes and the target node .<br>
The posterior link probability for a given link , is then the sum of all  for configurations  that contain . This can be written as(11)where the second sum goes over all network configurations  for which  and which contain the link , that is, over all possible configurations of the<br>
                        remaining  remaining existing links and  active source nodes. Since links can have different<br>
                        activation probabilities, the second sum in Equation 11 cannot be simply<br>
                        enumerated and full and tedious evaluation of the expression is needed.<br>
                        However, in the simpler case of equal activation probabilities (Equation 4)<br>
                        the second sum in Equation 11 contains  equal terms, i.e.,(12)where the dynamic and the prior terms are compounded into . Using Eqs. 4, 8, and 12, a closed form expression for the<br>
                        weights in Equation 7 becomes(13)where . We will use this expression to develop a nonparametric<br>
                        network reconstruction algorithm in the next section.<br>
<br>
A nonparametric normalized count approach<br>
While the Bayesian approaches allow for the best possible estimate at each<br>
                        step, it requires that the prior probabilities as well as the probabilistic<br>
                        rules of the dynamics are known. Unfortunately, these assumptions are often<br>
                        too strong in real-world situations and we therefore aim to develop a<br>
                        network reconstruction algorithm in conjunction with the Bayesian approach<br>
                        that (a) relies on little or none a priori knowledge about<br>
                        the system, that is, it is potentially nonparametric, (b) is efficient, i.e.<br>
                        simple and easy to implement yet robust in the reconstruction, and (c) is<br>
                        not prohibitive for large networks. We will apply this nonparametric method<br>
                        directly to the time binned neuronal activity cascades in which optimal<br>
                        binning width  is used [30]?[32], so that the  source nodes for a target node within bin  are identified by the active nodes in the preceding time<br>
                        bin,  (see the Discussion<br>
                        section for an extension to the continuous time dynamics).<br>
The structure and the dynamics are related as is the case for a critical<br>
                        cascade dynamics on a network. Using this we develop a nonparametric<br>
                        approach for network reconstruction. Assuming that a critical branching<br>
                        process is observed for , the average node degree,  are related by . Furthermore, the edge density, or sparsity of the<br>
                        network,  provides a natural choice as the best guess for the<br>
                        uniform link prior, , suggesting that  in Equation 13 can be written as . In order to extend the use of this algorithm to<br>
                        subcritical and supercritical regimes, we use the branching parameter, , which determines the dynamical regime, with  for critical dynamics, and  for subcritical and supercritical dynamics respectively.<br>
                        Thus, . We set  since the actual level of noise is typically not known and<br>
                        to account for noise we will use pairwise shuffling (see Methods). Since the number of nodes, , will be known, the weighting factor (Equation 13) now<br>
                        becomes a function of  only(14)<br>
This expression can be simplified further, if the dynamics is<br>
                        ?sparse? so that the cascade activity at any time bin<br>
                        does not consume a large portion of the whole network . This is a reasonable assumption for a branching process<br>
                        in which  is not much larger than 1. We approximate Equation 14 in<br>
                        two ways. The first one keeps the parameter ,(15)and the second one is nonparametric,(16)<br>
Generally,  can take on negative values, however, in Equation 15, the<br>
                        negative values are an artefact of the approximation and we set  whenever . A rough estimate of  can often be obtained from the observed data, which then<br>
                        renders even Eqs. 14 and 15 nonparametric, but with a caveat that the<br>
                        measured branching parameter might also be influenced by the network<br>
                        topology and thus differ from the purely dynamical . In networks in which the distribution of the node degrees<br>
                        is rather narrow, however, the two values will agree well.<br>
In principle, the PWA method can be applied to any local measure (at the<br>
                        level of a single STES). In the present study, we use the coincidence count<br>
                        between successive time steps, , for each link , where , if an event occurred on node  during the time bin , or 0 otherwise. While the coincidence count is a measure<br>
                        of correlation, it does not measure the actual traffic in a network. In<br>
                        order to estimate the topology , as well as the traffic , we apply PWA that is we take the average of all one-step<br>
                        estimates weighted by  for a given link . This yields the general expression for the weighted count ,(17)<br>
Here, we are not concerned with the overall scaling factor for , except for making it independent of , and we use . To make  less dependent on particular weighting scheme one can use .<br>
By replacing the weighting factor  in Equation 17 with those in Eqs. 14 through 16 we obtain<br>
                        three efficient and simple nonparametric measures, which we label  respectively. The expression for  is particularly simple and is the basis of the NC approach:(18)Thus, situations with a large number of potential source<br>
                        nodes are weighted less in the reconstruction process. The only parameter<br>
                        needed is the bin size, for which an optimal value can be found<br>
                        independently as described in [30]?[32],<br>
                        hence it is a nonparametric approach.<br>
<br>
Other nonparametric approaches<br>
A naive nonparametric approach that one can take to identify directed<br>
                        influence is simply to wait for the instances where exactly one source node<br>
                        is active assuming that active nodes in the near future are causally related<br>
                        to this ancestor. This single source (SS) approach,<br>
                        although simple, is useful to establish a reference for computationally more<br>
                        elaborate methods. This approach employs only a subset of all observations,<br>
                        thereby increasing the likelihood of missed links in the network, which<br>
                        reduces the efficiency of the network reconstruction. As will be shown, the<br>
                        SS approach is also prone to large errors in the presence of noise.<br>
Alternatively, the correlation in activity between nodes is commonly used to<br>
                        reconstruct networks from the observed dynamics. It requires a significance<br>
                        threshold, i.e., the expected correlation produced only by chance obtained<br>
                        from either theoretical predictions or by applying nonparametric<br>
                        randomization techniques in order to make a decision whether a particular<br>
                        correlation between two nodes is significant, thus establishing the<br>
                        existence of a link. Here we use the frequency count (FC)<br>
                        approach for which all occurrences of successive node activations are counted,(19)The FC is directly related to correlation, or conditional<br>
                        probability, depending on the normalization factor used. Here we use . Like all correlation based techniques, the FC approach<br>
                        suffers from the problem of assigning the correct causal structure when<br>
                        multiple source nodes are encountered.<br>
<br>
<br>
Shuffling<br>
For each pair of nodes, we can determine some scalar measure of connectivity. For<br>
                    example, these can be node to node correlations, or the FC approach (Equation<br>
                    19), or the NC approach using  Ultimately, we are trying to use these estimates as a measure<br>
                    of directed influence or causal traffic for each link in the underlying network.<br>
                    However, these measures will also include a contribution from non-causal<br>
                    correlations arising when pairs of nodes are active close in time but had a<br>
                    common ancestor at some prior time during the cascade, or share common inputs<br>
                    directly. We thus have to determine the statistical significance for each of the<br>
                    scalar connectivity estimates. The null-model is obtained by randomizing the<br>
                    recorded activity cascades using constrained pairwise<br>
                    shuffling. In this randomization procedure, the times of two randomly<br>
                    selected, active nodes  will be switched, such that the node  active at time , will be assigned time  and vice versa.<br>
This shuffling method is straightforward to implement for continuous time events,<br>
                    in which case the time interval distribution will be preserved. For binned data,<br>
                    one will encounter situations where the time bin  already has node  active, and vice versa, in which case the shuffle is aborted<br>
                    and a new pair of nodes is sought. Shuffling in this way preserves the average<br>
                    activity at each node as well as the occupation of time bins with active nodes<br>
                    and thus the dynamical regime of the underlying branching process (see Results). To obtain the resampled dataset,<br>
                    the pairwise switching is repeated  times,  being comparable to the total number of active nodes in the<br>
                    dataset.<br>
By repeating this procedure,  resampled datasets are obtained, each with its corresponding  estimate. We use the distribution of the  to determine the threshold value  for the given significance level . The number of shuffled replicates used to obtain the<br>
                    connectivity estimate at a significance level , where  is the ?over-shuffling? factor, usually 5<br>
                    or 10. We obtain the topology, i.e., the adjacency matrix of the estimated<br>
                    network at the significance level  as(20)and the architecture as(21)Hence the reconstructed network is a weighted, directed graph, , which depends on the prescribed level of confidence, and is<br>
                    supposed to be a measure of causal traffic in the network. Note that by using<br>
                    shuffling, we can determine a separate threshold for each link, thus reducing<br>
                    the bias towards more active nodes and reducing the contribution from<br>
                    correlations in the absence of interactions. When comparing reconstruction<br>
                    results using shuffling and individually derived thresholds with results based<br>
                    on a single common threshold in order to determine the significance of links, we<br>
                    always used the best possible (oracular) single threshold, since in our<br>
                    simulations the original network was known. We also investigated in our<br>
                    simulations if the threshold  in the IB approach is indeed optimal and it turns out that<br>
                    choosing  anywhere in the range between 0.1 and 0.9 yields very similar<br>
                    estimates.<br>
<br>
Simulation of Network Topology and Architecture<br>
We simulated the branching process dynamics on 4 different network topologies<br>
                    ranging from a random connectivity with low clustering to a small-world<br>
                    connectivity with high clustering [44]. For the<br>
                    Erd?s-R?nyi (ER) network,  nodes were connected randomly with fixed probability  resulting in an average node degree  and randomly assigned link directionality. In the Watts-Newman<br>
                    (WN) network [45], each node had  outgoing links to its  nearest neighbors, after which new links were added randomly<br>
                    with probability  to introduce long-range connections. This algorithm produces a<br>
                    small-world topology with a high clustering coefficient and an average degree  similar to the topology described by Watts and Strogatz [44]. In<br>
                    our simulations we used . Neither the ER nor the WN topology take into account that<br>
                    many networks self-organize and expand through growth, e.g. cortical neuronal<br>
                    networks. We therefore also tested two growth models that achieve a small-world<br>
                    topology with high clustering coefficients. The Barabasi-Alberts (BA) [46]<br>
                    model uses a preferential attachment rule in which the probability of attachment<br>
                    from a new node is proportional to the node degree of the existing nodes. Each<br>
                    new node establishes  new outgoing links starting initially with  disconnected or fully connected nodes. The resulting topology<br>
                    is scale-free in which the degree distribution decays according to a power law<br>
                    with a slope of ?3. Here we use  and an all-to-all connectivity for the initial network seed.<br>
                    The BA model requires a new node to attain some knowledge about the degree<br>
                    distribution in the network, which might pose a problem for large networks. In<br>
                    contrast, spatial growth networks [47] do not require<br>
                    global information about the existing network during development. We used the<br>
                    Ozik-Hunt-Ott (OHO) network [48], which is initialized with  nodes on a circle and all-to-all connectivity. In this<br>
                    network, a new node, whose location is chosen randomly on the circle, attaches<br>
                    preferentially to its  nearest neighbors with outgoing links, hence its growth rule<br>
                    is named geographical preferential attachment. The OHO network is not<br>
                    scale-free, but has a clear small-world property with a high clustering<br>
                    coefficient  that is independent of the number of nodes. Its average node<br>
                    degree is simply given by  for large networks. In our simulations, we used . The initial seed for the OHO network is the  network with an all-to-all connectivity. We note that for both<br>
                    growth models the number of outgoing links was  for each node and that both models incorporate a subnetwork<br>
                    (the initial seed) with maximal clustering that is particularly difficult to<br>
                    reconstruct in the supercritical dynamical regime.<br>
For each topology, we created specific network architectures by using constant<br>
                    individual link activation probabilities , or alternatively, by drawing from a uniform distribution, or<br>
                    truncated Normal distributions (e.g.  truncated within the range [0,1] and then<br>
                    scaled to ).<br>
Different dynamical regimes for each topology were explored on networks with  nodes and an average node degree of . The quality of network reconstruction as a function of<br>
                    reconstruction algorithm, network topology, and network architecture was studied<br>
                    using  nodes and , which approximates the number of electrodes from planar<br>
                    integrated micro-electrode array recordings for neuronal avalanches and the<br>
                    corresponding node degree. For the BA and OHO network, the average degree is<br>
                    discretized since it directly depends on the integer parameter , ( for undirected case). Here we used .<br>
<br>
Simulation of Network Dynamics<br>
The branching process dynamics was simulated as follows. A source node  was selected randomly according to some initiation probability<br>
                    distribution (see below) and activated. In the next time step, all outgoing<br>
                    links emanating from  will have a chance to activate its neighbors  (targets) with the corresponding link activation probability . Each activated target now becomes a source for the next<br>
                    generation of active nodes, and this is repeated for successive time steps until<br>
                    no active nodes are found. Heterogeneity in node initiation was simulated by<br>
                    assigning the node initiation probability from a truncated Gaussian profile, , where  is the normalized set of ordered node indices so that all<br>
                    nodes span the profile from  is the heterogeneity parameter. Thus, the probability of<br>
                    choosing the center node (the most active one) was a factor of  times larger than the probability of choosing the two edge<br>
                    nodes (the least active ones). We used , hence the ratios were ?1.65, 7.4,<br>
                        2.7?105 respectively.<br>
We evaluated three different dynamical regimes of the branching process. In the<br>
                    critical regime, one active node at time  on average will lead to exactly one active node in the next<br>
                    time step  and the distribution of avalanche sizes obeys a power law with<br>
                    a slope of ?1.5 [49]. In the ER network, the critical regime is<br>
                    reached if the average link probability , for  and for WN networks, . Conversely, sub- and supercritical regimes of the branching<br>
                    process were simulated at , respectively. For the BA and OHO networks, a power law<br>
                    spanning a large range of avalanche sizes was difficult to identify, although<br>
                    their sub- and supercritical regimes were similar to those in ER and WN<br>
                    networks. We therefore used for those simulations a value for  that yielded the closest fit to a power law size distribution<br>
                    between the sub- and supercritical regimes (see also Figure 2). A refractory period ensured that<br>
                    an avalanche ended once, or before, all nodes in the network were activated, a<br>
                    constraint that assured termination of the process particularly when simulating<br>
                    supercritical dynamics.<br>
Random node activation independent from the ongoing dynamics, i.e. due to noise<br>
                    or external inputs, was implemented such that any node on the network could be<br>
                    activated with probability  per time step, expressed as . We used a level of 20% for all simulations with<br>
                    noise, which translated on average into the random activation of one node every<br>
                    five time steps, independent from the ongoing dynamics. Note that randomly<br>
                    activated nodes did not initiate new cascades, otherwise they would increase<br>
                    reconstruction efficiency since the patterns of activity in the<br>
                    ?noise-induced? cascades would also be influenced in the<br>
                    same manner by the underlying network that we are trying to reconstruct. While<br>
                    noise was used universally, in some instances we also tested the robustness of<br>
                    the algorithms to time jitter, implemented such that every active node at time  was displaced into time bin  with 20% chance.<br>
<br>
Reconstructing  from Cascade Dynamics<br>
We applied the NC, FC, IB, and SS algorithms to different instances of the<br>
                    simulated cascade dynamics on all four network topologies and different<br>
                    architectures. Because the algorithms were described in detail in the Theory<br>
                    section, here, we focus on additional, practical issues.<br>
When reconstructing a network using IB, we used a cut-off value for the number of<br>
                    active nodes considered, , above which the IB iteration is skipped. Those iterations<br>
                    would take a significant portion of the evaluation time and yield only a slight<br>
                    gain in the posterior probability. While this diminished somewhat the<br>
                    performance of the IB particularly in the supercritical regimes, larger values<br>
                    of  would have resulted in impractically long reconstruction<br>
                    times.<br>
In order to establish significance for various network parameters, we used two<br>
                    randomization techniques, the Erd?s-R?nyi randomization (ER)<br>
                    and the degree sequence preserving randomization (DSPR) [50],[51].<br>
                    In ER randomization, links were completely randomized in order to obtain an ER<br>
                    network with an equivalent number of nodes, links, and weight distribution as in<br>
                    the original network. This randomization destroys any correlations and changes<br>
                    the node degree distribution. In the DSPR, two directed links were chosen<br>
                    randomly between four different nodes, and then the target nodes of the two<br>
                    links were switched preserving the degree distribution. This is repeated many<br>
                    times, and in our implementation the number of such switches is equal twice the<br>
                    number of the links in the network (number of links that have not been switched<br>
                    even once is less than 2%).<br>
Finally, for each of the network reconstructions, the total error, , was expressed as the number of links that differed between<br>
                    the reconstructed network  and the original network  relative to the total number of links in ,(22)This error counts both false positives, i.e. an estimated link<br>
                    does not exist, as well as false negatives, i.e. an existing link was not<br>
                    identified, and because  is usually sparse, the error can far exceed 100% of<br>
                    the true number of links. The error was averaged over 10 different realizations<br>
                    for each topology and expressed as mean?standard deviation, if not<br>
                    stated otherwise. When comparing two networks, neither of which represents the<br>
                    ?gold standard?, we use the following two measures for<br>
                    comparison. One is, , the percent difference in topology, similar to , but now expressed as the total number of the differences<br>
                    relative to the number of the links that exist in either of the two networks.<br>
                    This is a less stringent measure than the , and the maximal error is limited to 100%. The<br>
                    second is the Pearson correlation coefficient between the link weights among the<br>
                    common links in the two networks, , or alternatively, among the links that are in either of the<br>
                    two, .<br>
In order to reduce a potential bias in reconstruction efficiency from arbitrarily<br>
                    selecting a particular significance level, we chose the best reconstruction<br>
                    obtained from the significance levels . Using an over-shuffling factor of 10, best reconstructions<br>
                    for NC and FC were generally obtained at . In our simulations, we can also measure the traffic of causal<br>
                    activations through any given link by summing all the activations that actually<br>
                    occurred between its source and target nodes. The resulting traffic for each<br>
                    link was compared with the reconstructed link weights (see Equation 21) to study<br>
                    traffic estimates using FC and NC.<br>
<br>
MEA Recording and Neural Avalanches<br>
Coronal slices from rat dorsolateral cortex (postnatal day 0?2;  thick) were attached to a poly-D-lysine coated 8?8<br>
                    multi-electrode-array (MEA; Multichannelsystems, Germany) and grown at  in normal atmosphere in standard culture medium without<br>
                    antibiotics for 4?6 weeks before recording (for details see [29]?[32]). In short,<br>
                    spontaneous avalanche activity was recorded outside the incubator in normal<br>
                    artificial cerebrospinal fluid (aCSF) under stationary conditions (laminar flow<br>
                    of 1?2 ml/min) for up to 10 hrs. For long-term, pharmacological<br>
                    experiments a second set of cultures was recorded inside the incubator (for<br>
                    details on long-term recording conditions see [29]). In short, MEAs<br>
                    with cultures were placed onto storage trays inside the incubator, which were<br>
                    gently rocked (?200 s cycle time). For recording, single cultures grown<br>
                    on the MEAs for 5?6 weeks were placed into a head stage<br>
                    (MultiChannelSystems, Inc.), which was affixed to a second tray within the<br>
                    incubator and which had the exact same motion as the primary storage tray. This<br>
                    allowed recording from cultures inside the incubator in culture medium under<br>
                    conditions identical to growth conditions. Bath application of the AMPA<br>
                    glutamate-receptor antagonist 6,7-dinitro-quinoxaline-2,3(1H,4H)-dione (DNQX,  Sigma) was used to reduce synaptic excitability in the<br>
                    cortical network. DNQX was directly added to the culture chamber. For wash, the  medium was replaced with normal pre-conditioned culture<br>
                    medium. Analysis was based on the following time periods of spontaneous<br>
                    activity: 2?5 hr before, 15?20 hr during DNQX and<br>
                    2?5 hr after 19 hr of washing of the drug.<br>
Spontaneous local field potentials (LFP) were low-pass filtered at 50 Hz and<br>
                    sampled continuously at 1 kHz at each electrode. Negative deflections in the LFP<br>
                    (nLFP) were detected by crossing a noise threshold of ?3 SD followed<br>
                    by negative peak detection within 20 ms and nLFP peak times and nLFP amplitudes<br>
                    were extracted. Neuronal avalanches were defined as spatiotemporal clusters of<br>
                    nLFPs on the MEA. In short, a neuronal avalanche consisted of a consecutive<br>
                    series of time bins with width  that contained at least one nLFP on any of the electrodes.<br>
                    Each avalanche was preceded and ended by at least one time bin with no activity.<br>
                    Without loss of generality, the present analysis was done with bin width , estimated individually [30].  ranged between  for different sets of cultures. Avalanche size was defined as<br>
                    (1) the number of active electrodes that constitute an avalanche, i.e. the<br>
                    number of nLFPs, and (2) as the sum of absolute nLFP amplitudes on active<br>
                    electrodes. In the former case, size ranged from 1 to 60 (corner electrodes were<br>
                    missing on the array), whereas in the latter case size ranged from  (lowest detection level of an nLFP) up to several thousands of .<br>
<br>
<br>
Results<br>
Dynamical Regimes and Cascade Size Distributions<br>
During activity cascades, an active node on average can activate less than 1,<br>
                    exactly 1, or more than 1 node in the next time step in correspondence to the<br>
                    subcritical, critical, and supercritical dynamical regime of a branching<br>
                    process. We therefore identified these three dynamical regimes for each of the 4<br>
                    topologies by calculating the corresponding cascade size distributions on<br>
                    networks with N?=?5000 nodes,  and a constant activation probability  for all links. For both the WN and ER networks, the critical<br>
                    probability, , was characterized by a cascade size distribution that<br>
                    followed a power law with a slope of ?1.5 as predicted by theory [49]<br>
                        (Figure 2;  for ER;  for WN). Conversely, an exponential distribution characterized<br>
                    the subcritical regime in which most cascades engaged only few nodes, whereas in<br>
                    the supercritical regime, a bimodal size distribution revealed that cascades<br>
                    stayed either relatively small or engaged most of the network. For the BA<br>
                    network, the distribution of cascades sizes in the subcritical regime followed a<br>
                    power law with a slope of ??3 for sizes &lt;10, suggesting<br>
                    that cascades in that regime were dominated by the degree distribution (slope<br>
                    ?3). In contrast, the supercritical regime was identified by a bimodal<br>
                    size distribution. At the transition to the supercritical regime, the BA network<br>
                    revealed a power law slope close to ?1.5 for a small range of<br>
                    avalanche sizes (10 to 100 at ), which we used to identify the critical dynamics. For the OHO<br>
                    network, a critical regime was indicated at  (mean field prediction was 0.085) at which the cascade size<br>
                    distribution revealed a corresponding power law with slope of ?1.5<br>
                        (Figure 2), from which<br>
                    it deviates for large cascade sizes. Thus, given the constraints of a constant , the critical regime in the current simulations represented an<br>
                    approximation of a true critical dynamics for both the BA and OHO network (Figure 2).<br>
The characteristic size distributions for each dynamical regime suggest a varying<br>
                    efficiency in reconstructing networks based on the observed activity cascades.<br>
                    For the subcritical regime, we expect fewer ambiguous situations with multiple<br>
                    source nodes (Figure 1C) and<br>
                    thus better accuracy in network reconstruction. These smaller cascades, however,<br>
                    contain fewer links that can be estimated per unit time, which should slow the<br>
                    reconstruction progress. The opposite holds for the supercritical regime where<br>
                    large cascades allow for a larger percentage of links to be estimated per unit<br>
                    time, while the reconstruction accuracy might decrease due to an increase in<br>
                    ambiguous situations. Consequently, we expect the critical dynamical regime to<br>
                    achieve a balance between these opposing tendencies in network reconstruction.<br>
                    Additionally, in subcritical regime much greater number of initial events will<br>
                    not propagate at all, in which case a reconstruction step cannot be performed.<br>
                    Thus, it takes much longer time to collect the same number of STES in the<br>
                    subcritical regime than it does in critical or supercritical regimes.<br>
<br>
NC Robustly Reconstructs ER Networks for All Dynamical Regimes<br>
We quantified the relationship between the dynamical regime and the<br>
                    reconstruction efficacy by plotting the total reconstruction error  as a function of number of propagation steps, , which is the total number of successive time bins that both<br>
                    contain at least one active node. This was done for all three regimes and all<br>
                    four algorithms (Figure 3;<br>
                    ER topology, , uniform link activation probability  for avalanche initiation; see also Figure 4B). For both FC and NC, the<br>
                    significance of a link was based on 1000 shuffles. For the IB algorithm, the<br>
                    correct value of  was used in the dynamic term (Equation 4).<br>
In our initial evaluation without noise, the IB algorithm was superior in<br>
                    reconstructing the network in all three dynamical regimes. As predicted from the<br>
                    cascade size distributions, its reconstruction efficiency was higher in the<br>
                    critical regime compared to the subcritical regime (Figure 3A, left, open arrows). Importantly,<br>
                    the IB algorithm further improved in the supercritical regime demonstrating its<br>
                    robust handling of situations with common inputs, where it achieved a high<br>
                    efficiency that is  possible links were estimated in approximately the same number<br>
                    of propagation steps in order to reach a reconstruction accuracy of<br>
                    1%. Similarly, the correlation algorithm FC, while being less<br>
                    efficient than the IB algorithm, faired better in the critical regime when<br>
                    compared to the subcritical regime. However, it failed in the supercritical<br>
                    regime to achieve 1% accuracy even for up to 106<br>
                    propagation steps demonstrating its sensitivity to correlations due to common<br>
                    inputs (Figure 3A, left, red<br>
                    arrow). Importantly, our newly developed NC algorithm clearly overcame<br>
                    the weakness of the FC algorithm and demonstrated its efficiency in all three<br>
                    regimes (Figure 3A, left, black<br>
                        filled arrow). We note that the error reported is calculated with<br>
                    respect to the number of existing links in the network, i.e. ?600 links<br>
                    for  out of 3,600 possible links. Hence a reported error of<br>
                    1% is equivalent to about<br>
                    1/6?=?0.167% overall error in<br>
                    deciding whether a link existed or not.<br>
The simple SS algorithm, by avoiding ambiguous situations, performed surprisingly<br>
                    well for all regimes and was comparable to the performances of the IB and NC<br>
                    algorithm. However, the SS algorithm was highly sensitive to noise and relied on<br>
                    the assumption that the observed activations completely arose from the intrinsic<br>
                    dynamics. In fact, when we repeated our simulations in the presence of<br>
                    20% noise (Figure 3A,<br>
                        right), SS failed entirely in all regimes resulting in errors<br>
                    significantly larger than 100%. Equally important, the IB algorithm<br>
                    now required 4?5 times more propagation steps to reach an accuracy of<br>
                    1% in the supercritical regime; a sensitivity to noise that<br>
                    originated from the iterative development of the priors over time (Figure 3A, right, open arrow).<br>
                    In the presence of noise, only the NC algorithm robustly reconstructed networks<br>
                    with similar efficiency in the critical and supercritical regime thereby<br>
                    performing even better than the IB in the supercritical regime (Figure 3A, right). In<br>
                    comparison to the standard correlation approach, the NC algorithm provided about<br>
                    50% improvement in the critical regime and more than a 10-fold<br>
                    improvement to achieve 3% accuracy in the supercritical regime.<br>
These results demonstrate that NC performed best given (1) its simplicity,<br>
                    requiring no assumptions about the network connectivity or network dynamics, (2)<br>
                    its high accuracy for all three regimes, and (3) good reconstruction efficiency<br>
                    of about 2.7 propagation steps per potential link (total  links) for the critical and supercritical regime at<br>
                    1% reconstruction error.<br>
<br>
Improvement in Network Reconstruction Using Pairwise Shuffling<br>
Correlation methods in network reconstruction commonly utilize a single, global<br>
                    threshold to identify links, e.g. links are assumed to exist for all pairwise<br>
                    node correlations that are above a minimal correlation value (e.g. [18],<br>
                        [20], [52]?[54]). However,<br>
                    heterogeneous node activation frequencies, as well as other conditions, might<br>
                    require different significance thresholds for each link. For the networks in<br>
                        Figure 3, we compared<br>
                    the efficiency in network reconstruction when establishing link significance<br>
                    using either shuffling or, alternatively, a fixed, best possible threshold for<br>
                    both the FC and NC algorithm in the presence of 20% noise. While<br>
                    shuffling performed slightly worse in the subcritical regime, it significantly<br>
                    improved reconstruction accuracy in the critical and supercritical regime (Figure 3B). For the FC<br>
                    algorithm, shuffling was necessary for an accurate estimation in the critical<br>
                    regime, but it was insufficient in the supercritical regime where the error  remained high above 1%, even for large numbers of<br>
                    propagation steps (Figure 3B, red<br>
                        arrow). For the NC algorithm, shuffling was required to accurately<br>
                    reconstruct a network with supercritical dynamics (Figure 3B, black arrow). The results, here<br>
                    plotted for , were similar for  (data not shown). This analysis clearly demonstrates that<br>
                    correlation based methods benefit from using shuffling estimates for thresholds<br>
                    in the critical regime. On the other hand, the NC algorithm in combination with<br>
                    shuffling is required for network reconstructions in the supercritical regime.<br>
The reconstruction results were obtained on a relatively small network with , and a question arises on how well it performs for larger<br>
                    networks. Since the network model we are trying to reconstruct has  binary parameters, it is natural to expect that the number of<br>
                    needed samples, i.e. propagation steps, for the same reconstruction error should<br>
                    at least increase proportionally to . Using NC to reconstruct an ER topology from the cascades in<br>
                    the critical dynamical regime, we demonstrate (Figure 4A) that the number of propagation<br>
                    steps required for 1% reconstruction accuracy scales approximately<br>
                    linearly with the total number of potential links in the network, i.e. it scaled<br>
                    as , making it a potentially useful algorithm for reconstructing<br>
                    larger networks.<br>
Of particular concern for network reconstruction are situations in which nodes<br>
                    rarely participate in cascade initiations. For example, initiation sites of<br>
                    neuronal avalanches differ up to an order of magnitude in avalanche initiation<br>
                    rate [29],[32]. Such heterogeneity<br>
                    should make it more difficult to reconstruct the topological neighborhood of<br>
                    less active nodes. Nevertheless, as shown in Figure 3B, the NC algorithm accurately<br>
                    reconstructed networks with heterogeneities in node initiation frequency up to a<br>
                    factor of 268,000?1 for all three dynamical regimes and with only a<br>
                    slight increase in computation for critical and supercritical regimes.<br>
Finally, we tested the robustness of the IB, FC and NC algorithms in<br>
                    reconstructing networks with heterogeneous activation probabilities  even though the reconstruction algorithms assume a fixed  In addition, we introduced a temporal jitter of 20%<br>
                    when binning activity cascades as to account for temporal imprecision in cascade<br>
                    measurements. As before, the noise level was 20% and the node<br>
                    initiation heterogeneity was set to . Under these conditions, the IB failed (Figure 4C) to reconstruct the networks to<br>
                    1% accuracy for all dynamical regimes. Similarly, FC was robust in<br>
                    subcritical and critical regimes, but it failed to reach below a 10%<br>
                    error in the supercritical regime. In contrast, NC always reached below<br>
                    1% reconstruction accuracy, and performed the best in all regimes.<br>
                    The performance of NC can be further improved in supercritical regimes when the<br>
                    knowledge of the branching parameter,  is taken into account, as in  (Figure<br>
                    4C).<br>
<br>
Efficiency of NC To Reconstruct Different Network Topologies<br>
The NC algorithm also allowed for a robust and accurate reconstruction of network<br>
                    topologies that differed from random connectivity. We tested its performance for<br>
                    4 different topologies and all three dynamical regimes in comparison to the FC<br>
                    algorithm (Figure 5;  and reconstructed with  and 1000 shuffles). While the FC algorithm failed for the OHO<br>
                    topology in the critical regime, the NC algorithm reconstructed all topologies<br>
                    in the subcritical as well as critical regime (Figure 5B). Significantly, the FC algorithm<br>
                    failed to reconstruct any of the small-world topologies in the supercritical<br>
                    regime, while the NC algorithm reconstructed the WN as well as the BA network,<br>
                    demonstrated here up to an accuracy of 0.1%. Only the OHO network<br>
                    provided a limit above 1% in the efficacy in network reconstruction<br>
                        (Figure 5B). This limit<br>
                    most likely arises because a supercritical dynamics will engage all nodes most<br>
                    of the time in a highly clustered manner at which pairwise shuffling becomes too<br>
                    constrained (i.e. shuffling two active nodes between two different time points).<br>
                    The errors due to reconstruction will most likely be false positives and random<br>
                    in nature. Hence the overall network parameters (average clustering coefficient,<br>
                    mean path length, average degree) might or might not be affected significantly<br>
                    by the errors of this order of magnitude. Accordingly, we plotted the<br>
                    reconstructed network parameters as a function of propagation steps for the OHO<br>
                    network in the supercritical regime. As can be seen from Figure 5B, even seemingly high error rates of<br>
                    10% did not significantly affect the clustering coefficient, while<br>
                    the average degrees are biased to larger values, indicating that most of the<br>
                    errors are false positives.<br>
<br>
The Reconstruction of Network Traffic Using NC<br>
The traffic on a network, i.e. the network flow, is one of the most important<br>
                    aspects that characterizes network functionality [55]. It was reliably<br>
                    estimated by NC for all three dynamical regimes and most topologies. We studied<br>
                    the correlation between the known link activation probabilities  and the estimated link weights  on an ER network for which link activation probabilities were<br>
                    drawn either from a uniform distribution or a truncated normal distribution<br>
                    between [0,1] with  (, and 20% noise). In Figure 6A it is shown that for both uniform<br>
                    and normal distributed activation probabilities, NC did significantly better<br>
                    than FC in relating the reconstructed weights  to the original weights prescribed as , particularly in the supercritical dynamics. Furthermore, when<br>
                    correlating the estimated  with the actual traffic in the network, calculated during the<br>
                    simulation, we found that NC provided a very good measure of the traffic between<br>
                    two nodes (slope close to 1; Figure<br>
                        6B and 6C). In contrast, FC significantly underestimated the traffic<br>
                    for increasingly higher traffic values (slope ?1). These results,<br>
                    obtained on an ER network topology, were also confirmed for small-world<br>
                    topologies, where NC reliably estimated the traffic on the WN and BA network for<br>
                    all three dynamical regimes. Only for the supercritical regime on the OHO<br>
                    network did the NC algorithm estimate the traffic poorly (Figure 6D, black dots). However using  further improved the reconstruction in traffic similar to that<br>
                    of an equivalent ER network (R?=?0.72; data not<br>
                    shown).<br>
<br>
The Small-World Topology of Neuronal Avalanches<br>
Given that the avalanche dynamics can be realized on different topologies (see<br>
                        Figure 2), we used the<br>
                    robust performance of the NC algorithm for different dynamical regimes and<br>
                    widely varying network topologies in order to reconstruct the functional<br>
                    topology and architecture of real neuronal networks that display neuronal<br>
                    avalanches recorded with integrated planar micro-electrode arrays (MEA) from<br>
                    neuronal cortex cultures. Spontaneous activity in these cultures is<br>
                    characterized by negative deflections in the local field potential (nLFP)<br>
                    indicative of a local synchronization within a subgroup of neurons near the<br>
                    electrode (Figure<br>
                    7A?D; [30]). The organization of nLFPs in the neuronal<br>
                    network takes on the form of complex spatiotemporal patterns that evolve over<br>
                    successive time bins (Figure 7E and<br>
                        7F). These patterns, when interpreted as successive node activations<br>
                    (see Figure 1B), were used<br>
                    to reconstruct the functional network topology and network architecture. Under<br>
                    normal conditions, the dynamics that emerges in this system [29] is<br>
                    characterized by neuronal avalanches whose sizes obey a power law with a slope<br>
                    of ?1.5 for avalanche sizes measured in terms of integrated nLFP<br>
                    amplitude or number of nLFPs indicative of a critical state (Figure 7G, [6],[56],[57]).<br>
                    Importantly, the power law in avalanche sizes correlates with a sequential<br>
                    activation of local neuronal groups that is analog to a critical branching<br>
                    process [29]?[32]. In the absence of<br>
                    any knowledge of the real underlying network organization, we reasoned that the<br>
                    reconstructed network architecture might be reliable if its features converged<br>
                    with increasing number of propagation steps in the reconstruction process, e.g.<br>
                    as shown for the simulated OHO network in Figure 5B. Indeed, the network parameters<br>
                    such as the clustering coefficient, , and average node degree, , remained largely constant beyond 30,000 propagation steps.<br>
                    This was in agreement with our simulation results, where NC achieved a smaller<br>
                    than 1% error estimate for all topologies in the critical regime<br>
                    within a similar range of propagation steps (Figure 5). Importantly, despite the<br>
                    relatively small network size of  and an average degree of , the clustering coefficient of  was significantly higher than what would be expected for<br>
                    corresponding randomized versions of the network . Similarly, we also plot the excess clustering , a network parameter (not a reconstruction error in ) that measures the clustering coefficient in the network that<br>
                    is beyond the one of an equivalent randomized version of the network. Results<br>
                    for  indicate that the high clustering coefficient was not simply<br>
                    due to saturation by adding more and more links into a small network (Figure 8A and 8B). These<br>
                    networks have nearly a linear relationship between the node degree and its<br>
                    strength, i.e. the summed weights of all links at a node, , with  (Figure<br>
                    8D) while Figure 8E<br>
                    shows the node in- and out-degree distributions . The weight distribution of the links revealed an<br>
                    exponentially decaying tail demonstrating the presence of a few links with large<br>
                    traffic (Figure 8F).<br>
Given that the relatively high clustering was achieved with a small network<br>
                    diameter of  (Figure 8A and<br>
                    8B), which was similar to those of the equivalent randomized networks , our findings demonstrate that the neuronal cultures with<br>
                    neuronal avalanche dynamics establish a small-world topology as previously<br>
                    reported in abstract form [58],[59]. The functional<br>
                    network topology of the cortex in vitro cultures (and acute<br>
                    slices [31]) derived from neuronal avalanches is compared to<br>
                    the results reported for various neural systems in Table 1. The networks range from full brain<br>
                    and cortical networks among different anatomical and functional areas of the<br>
                    brain [16], [44], [60]?[63] to<br>
                    cortical slices and cultures, as well as the neural network of the nematode<br>
                    C-elegans [44]. The table also shows the results for 21 cortical<br>
                    networks binned at  (14 were acquired in the course of the previous studies, and<br>
                    combined with the current set of 7, also re-binned to the same ). The networks and the sources of this data are listed in the<br>
                    caption. One should note that these networks, with exception of the C-elegans<br>
                    are not very sparse, in which case the clustering coefficient will depend on the<br>
                    size of the network, as the table roughly indicates. A better comparison between<br>
                    these different systems can be achieved by using the excess clustering , found in the range between 0.13 and 0.32, and which shows no<br>
                    obvious dependence on network size or sparsity.<br>
<br>
Correspondence between Functional and Structural Small-World Topology<br>
Functional connectivities are dynamically modulated even on a millisecond time<br>
                    scale [21],[22]. For example, the<br>
                    functional connection of a single synapse, i.e. its efficacy to elicit a spike<br>
                    in a post-synaptic neuron, depends on the depolarization of the post-synaptic<br>
                    neuron, which itself is linked to the neuron's inputs from within the<br>
                    network, i.e. level of network activity. This suggests that the functional<br>
                    small-world topology reconstructed from the dynamical cascades, which captures<br>
                    the spatiotemporal organization of spiking activity [33], might change with<br>
                    a change in network activity. On the other hand, local synaptic plasticity<br>
                    mechanisms such as spike-timing dependent plasticity [64] are expected to<br>
                    translate successive neuronal activations as reflected in the spontaneous<br>
                    dynamical cascades into a corresponding increase in synaptic strength thereby<br>
                    establishing a structural correlate of the observed dynamics. In that case, the<br>
                    network organization might be expected to be relatively robust to a decrease in<br>
                    overall activity levels.<br>
By taking advantage of the NC algorithm to reconstruct network architectures in<br>
                    subcritical and critical regimes, we tested the robustness of the functional<br>
                    small-world topology to acute changes in network activity. We acutely reduced<br>
                    the efficacy of excitatory glutamatergic fast synaptic transmission in the<br>
                    cultured networks by bath application of the AMPA receptor antagonist DNQX<br>
                    (n?=?3 networks). As expected,  of DNQX significantly reduced the rate of spontaneous cascades<br>
                    by . Thus, in order to compensate for the reduced number of<br>
                    propagation steps per time, networks were reconstructed from ?20 hr of<br>
                    activity in the presence of DNQX compared to 2?5 hrs of the control<br>
                    and wash condition. DNQX also reduced the formation of large avalanches leading<br>
                    to size distributions more similar to that of a subcritical state, which clearly<br>
                    deviated from the power law with a slope of ?1.5 for the pre and wash<br>
                    condition (Figure 9A). DNQX<br>
                    significantly reduced the traffic on the network, which under normal conditions<br>
                    revealed an exponential distribution (Figures 8 and 9B). Despite these significant reductions in<br>
                    cascade rate and size as well as link traffic, the small-world topology of the<br>
                    critical network obtained before and after DNQX, nevertheless, was reliably<br>
                    reconstructed during DNQX as indicated by the similarity in the clustering<br>
                    coefficient  with increasing number of propagation steps (Figure 9C). On average, , as well as  was not different between controls and DNQX . A detailed link-by-link comparison using , between the<br>
                    ?pre???wash? showed an error of  and correlations, . Similarly, a comparison between<br>
                    ?pre???DNQX?, and<br>
                    ?DNQX???wash? yielded , respectively. When the comparison were made between the<br>
                    randomized versions of each network (ER randomization), the results were<br>
                    virtually the same for all three cases, . These results show that while these networks are far from<br>
                    identical, their overlap is significantly larger than expected by chance.<br>
<br>
<br>
Discussion<br>
In the present study, we developed a method that derives a weighted directed graph<br>
                based on the observed cascade dynamics, which successfully overcomes ambiguous<br>
                source and target node correlations in all dynamical regimes of a branching point<br>
                process. Several methods have been previously employed to cope with the issue of<br>
                common inputs when using a correlative approach. For example, using delayed<br>
                correlations, Cecci et al. [20] demonstrated power law scaling in human fMRI data<br>
                even when links with zero delays indicative of common input were removed. A<br>
                three-node motif approach using mutual information allowed to remove potential links<br>
                arising from common input resulting in undirected small-world graphs reconstructed<br>
                from spontaneous spiking activity in dissociated cultures [52]. Assuming an<br>
                Ising-model underlying pairwise node correlations, non-directed functional<br>
                connections have been estimated for networks of up to 10 nodes from spontaneous<br>
                neuronal activity in vitro<br>
                [65],[66] and genetic interactions [67]. Although, the last<br>
                approach is able to identify common input situations, it results in non-directed<br>
                graphs, in contrast to our approach which also reconstructs directed network<br>
                traffic.<br>
Bayesian Approaches to Network Reconstruction<br>
The Bayesian approaches described here differ from the so-called Bayesian<br>
                    networks, or belief networks [68]?[70], which specialize<br>
                    in the reconstruction of directed, acyclic graphs with a smaller number of<br>
                    configurations to be explored. In order to reconstruct cyclic graphs,<br>
                    ?loopy? Bayesian network approaches [71] can be used,<br>
                    however, they are, even in their approximate form, NP-hard [72]. Bayesian networks<br>
                    are particularly useful in small networks when precise Bayesian inference is<br>
                    required for each link. In contrast, the IB or PWA approaches in the present<br>
                    study are meant for the reconstruction of large networks from large datasets.<br>
                    For that purpose we derived and tested new methods for reconstructing the<br>
                    functional network topology and traffic from dynamical network cascades. We made<br>
                    the Bayesian methodology feasible by dividing the observations and the network<br>
                    into individual target activations with the corresponding active subnetworks<br>
                    (STES). The essential computational reduction was achieved by using the<br>
                    assumptions of (a) only the events in the near past (the source nodes) are a<br>
                    potential cause for an activation event in the cascade and (b) the activation<br>
                    events of two different target nodes that have common source nodes are<br>
                    independent. Both assumptions make sense in neuronal networks such as the<br>
                    cortex, in which events in the near past predominantly influence the present<br>
                    state of a neuron and where the synaptic transmission of a neuron at different<br>
                    postsynaptic sites is independent. All these methods rely on the assumption that<br>
                    the underlying dynamics is stochastic. A fully deterministic dynamics would not<br>
                    allow to discriminate direct from indirect influences.<br>
To combine individual STES and to obtain the reconstructed network, , we used the IB and PWA approach. They enable one to improve<br>
                    the reconstruction reliability whenever additional knowledge about the dynamics<br>
                    (or priors in the case of PWA) becomes available. They are computationally<br>
                    feasible, since their computational complexity is simply the number of STES, , times the complexity of the individual STES. We will assume<br>
                    that the  needed in an observation for a given reconstruction accuracy<br>
                    is  (as was found for NC, see Figure 4A). Hence, the complexity of the IB<br>
                    is , where  is the average number of  over all STES. It will be likely that  is a function of  in the critical and supercritical regimes, but less so in the<br>
                    subcritical regime. When , the exponential complexity  of IB can be managed to some degree by introducing a cut-off<br>
                    value, , thus reducing the complexity to , but keeping a large pre-factor . The computational complexity of individual STES in PWA will<br>
                    in most cases be equal or less than . For NC, the individual STES have complexity , hence, the NC has the same low complexity as FC and other<br>
                    correlation methods, , but it produces much better estimates of causal traffic and<br>
                    connectivity, making it a candidate algorithm for the reconstruction of large<br>
                    networks. Note, that most of the computational demand in NC comes from<br>
                    shuffling, whose complexity also is . Technical considerations of this algorithm are discussed in<br>
                    the next paragraph (see also Text S1 for the implementation summary).<br>
The PWA approach can also be extended to include situations when the cascade<br>
                    propagation speed is highly heterogeneous, i.e. the continuous time approach is<br>
                    necessary, and/or when the amplitudes of the events need to be considered. This<br>
                    will require some knowledge, or experimental estimate, on how temporal<br>
                    differences and event amplitudes will affect the activation probabilities (see<br>
                    Equation 3). In these cases, the equivalent of the expression in Equation 16 becomes(23)where  is the link activation probability for the link connecting the  active source node  and the target node . This expression is obtained in the limit of . A simple inclusion of the weights can also be obtained by<br>
                    treating , in which case  is not the number of active nodes but the total strength of<br>
                    the sources . This more general framework, requiring the simulation of<br>
                    continuous time dynamics and varying amplitudes was beyond the scope of this<br>
                    manuscript.<br>
Although PWA was derived from Bayesian considerations, strictly speaking it is<br>
                    not a Bayesian method, particularly not the NC algorithm. When PWA uses uniform<br>
                    priors, one can argue that it is essentially a maximum likelihood method. The<br>
                    difference, however, with the maximum likelihood approach is that we use uniform<br>
                    priors on the links, but not the configurations themselves, which are the<br>
                    elements of our sample space. Thus, different configurations will get assigned<br>
                    different prior probabilities. When the prior probabilities for the existence of<br>
                    any link , are small, or are assigned based on the sparsity  of a network, the existence of a link can be established using<br>
                    a nonparametric measure similar to correlation. Historically, arguments have<br>
                    been made that, in situations where prior knowledge is not available, a precise<br>
                    choice of the prior probability is not crucial [73] as long as the<br>
                    choice is smooth in the region of high likelihood. Thus, a uniform and<br>
                    sufficiently small probability will lead to essentially the same final estimate<br>
                        [74].<br>
<br>
Technical Considerations of the NC Algorithm<br>
The general methodology of PWA and IB was derived in our Theory section. We then<br>
                    tested a particular nonparametric instance of PWA, the NC algorithm, with the<br>
                    goal of reconstructing large networks from large records of a point process<br>
                    dynamics. The NC is essentially a weighted correlation measure, with the weight<br>
                    inversely proportional to the number of potential source nodes. This weighting<br>
                    is not arbitrary, and if one uses a different weighting factor, e.g. , it does not perform as well as NC (data not shown). If one<br>
                    assumes small prior probabilities for each link, this result becomes intuitive,<br>
                    since the posterior probability for the existence of simultaneous links is<br>
                    negligible, hence each link's probability is inversely proportional to<br>
                    the number of possibilities, i.e. active source nodes . Importantly, we did not assume that  is small, but only that it is equal to the sparsity of the<br>
                    network and that the dynamics is near the critical point. This indicates that<br>
                    the validity of the NC algorithm does not rely on the precise choice of . The more elaborate IB approach with fully known dynamics<br>
                    established a benchmark that was closely met by the NC algorithm. The NC<br>
                    algorithm returns the link weights that are an approximate measure of the causal<br>
                    traffic across each link. In this paper we tested, using the simulations of a<br>
                    branching point process on a network, the case when the activation probabilities<br>
                    do not depend on the magnitude of the events and the event times are discrete.<br>
                    More general cases can be addressed using an appropriate activation function in<br>
                    equation 3, and using a different weighting factor for PWA (see Equation 23).<br>
The advantages and limitations of NC<br>
Advantages: (i) The NC algorithm is nonparametric and<br>
                        requires no prior knowledge of the dynamics, but performs close to the IB<br>
                        approach when the latter fully utilizes that knowledge; (ii) It is<br>
                        computationally as simple as FC and other correlation methods, but produces<br>
                        much better estimates of causal traffic and connectivity, particularly for<br>
                        small-world networks; (iii) the NC algorithm is robust, not only to changes<br>
                        in the dynamical regime, but also to deviations from the dynamical<br>
                        assumptions. The NC algorithm performed well when applied to the branching<br>
                        point process dynamics with large heterogeneities in initiation rate,<br>
                        heterogeneities in activation probabilities, , and uncertainty to temporal binning, in contrast to IB<br>
                        and FC (Figure 4C).<br>
Limitations: (i) the NC is not as specific, or reliable, as<br>
                        the Bayesian Networks when the existence of a particular link in a network<br>
                        is to be established; (ii) The selection of  prior events as the potential sources requires some<br>
                        knowledge about the dynamics. The fixed time cut-off, of time-binned events<br>
                        that we use in this work, might fail when more complex temporal dependences<br>
                        between the nodes are encountered; (iii) The NC relies on shuffling to<br>
                        obtain the null-model nonparametrically. However, shuffling can be<br>
                        constrained in certain dynamical conditions, for example, in the subcritical<br>
                        and supercritical dynamical regimes (see the subsection on Shuffling below).<br>
In general, we envision this algorithm to be used for a general network<br>
                        topology when the dynamics of the network is moderately sparse, i.e. when<br>
                        the number of active nodes at any time is not very large, as opposed to<br>
                        correlation based methods which work only when the dynamics is extremely<br>
                        sparse. Note that even a dense network can be reconstructed, provided the<br>
                        observed dynamics is sparse. We expect NC to work well in situations where<br>
                        the correlation methods are commonly used, but with an added advantage that<br>
                        it will be less sensitive to changes in the dynamical regime. The algorithm<br>
                        summary is given in Text S1.<br>
<br>
The influence of network size and the length of observation on network<br>
                        reconstruction<br>
We note that when increasing the network size while keeping the average<br>
                        degree constant, a network reconstruction error of 1%, which is<br>
                        relative to the existing links in the network, becomes more and more<br>
                        stringent as  increases and the sparsity  of the network drops . For example, while the error rate per potential link is , this changes to . Furthermore, as  grows the minimal achievable error rate, , becomes finite and grows as  increases. For , while for . For very large and very sparse networks a different<br>
                        shuffling scheme with additional constraints might improve this accuracy<br>
                        limit. For example, one could consider to partially shuffle the record of<br>
                        dynamical cascades, e.g. where the number of pairwise shuffles is one<br>
                        quarter of the total number of active sites in the dataset i.e. using eight<br>
                        times less pairwise shuffles than the default that we use throughout the<br>
                        paper. In this case, the minimal achievable error for , but requires a 20% longer data record (data<br>
                        not shown).<br>
As our results show, the number of needed propagation steps  is on the order of few multiples of . Since the shuffling is not guaranteed to provide an<br>
                        accurate null model, having too many observations will tend to introduce<br>
                        false positives. Thus, of particular concern is the stability of the<br>
                        reconstructed architecture as a function of observation length. Often,<br>
                        reconstructions are done based on the whole, a priori defined length of<br>
                        recording. Robustness can be demonstrated by repeat analysis of subdivisions<br>
                        of the record [75] or devising records of different lengths<br>
                            [10], including the calculation of a cut-off<br>
                        parameter [75]. In the present study, robustness of the<br>
                        reconstructed network was demonstrated by the convergence of a set of<br>
                        network parameters towards a reasonable constant value with increasing<br>
                        number of propagation steps. Naturally, this convergence was particularly<br>
                        robust for the clustering coefficient , which in contrast to the average degree, is less affected<br>
                        by the erroneous addition of random links. Importantly, this convergence<br>
                        occurred for the neuronal networks at around the same number of propagation<br>
                        steps as was expected from our network simulations and was robust to changes<br>
                        in the dynamical regime. In general, any quantity that is not sensitive to<br>
                        the addition of random links will be robust to the existence of the false<br>
                        positives in the reconstruction.<br>
<br>
<br>
Shuffling To Increase Reconstruction Reliability<br>
Shuffling of the original time series is commonly used to establish a priori<br>
                    statistical distributions for the null-hypothesis. Our results clearly<br>
                    demonstrate that pairwise shuffling significantly improves the reconstruction<br>
                    accuracy in the critical and supercritical regime. On the other hand, this<br>
                    method imposes strong limitations resulting in a conservative model that not<br>
                    only maintains the average activity rate of each node, which prevents the<br>
                    introduction of correlations due to rate modulation [22], but also the<br>
                    exact lifetime and size distribution of cascades, thus ensuring that the<br>
                    shuffled raster remains in the same dynamical regime. This shuffling method<br>
                    reaches its limits in the supercritical regime with highly synchronized<br>
                    cascades, e.g. when almost all nodes become active within 1 time step for most<br>
                    cascades, in which the constraints of the pairwise shuffling limit its<br>
                    statistical power. Similarly, pairwise shuffling becomes constrained in the<br>
                    subcritical regime because of the limited number of nodes participating in<br>
                    cascades. Alternative methods combined with pairwise shuffling, such as temporal<br>
                    jittering, using a smaller portion of the raster to determine thresholds, or<br>
                    limiting total number of shuffles, might improve reconstruction efforts further<br>
                    in these cases.<br>
The ad hoc use of a global threshold in order to extract a<br>
                    functional connectivity from correlation matrices is often justified by<br>
                    providing a range of thresholds for which the obtained results are robust [18],<br>
                        [20], [52]?[54]. In the present<br>
                    study, we obtained thresholds for each potential link, which significantly<br>
                    outperformed the global threshold approach in the critical and supercritical<br>
                    regimes. The calculation of a probability value using a conservative model, i.e.<br>
                    maintained firing rate and cascade sizes and durations also naturally allows<br>
                    these thresholds to be interpreted in terms of significance for individual link<br>
                    existence. As shown in Figure<br>
                    8C, topological features were shown to be robust for different<br>
                    significance thresholds.<br>
<br>
Branching Process Dynamics<br>
Our simulation of the branching process incorporated a refractory period during<br>
                    which a node remained inactive before being able to participate in a cascade<br>
                    again. Thus, the simulated dynamics represents a branching process only in the<br>
                    limit of large number of nodes . Notably, refractory periods for nodes are common in many real<br>
                    systems, where they arise from energy limitations such as transport capacities<br>
                    and where they serve several major purposes, such as limiting the rate with<br>
                    which each node engages in the network dynamics and terminating cascades in the<br>
                    supercritical regime. In the temporal domain, refractory periods support the<br>
                    formation of non-recurrent dynamics in an otherwise recurrent network. For<br>
                    example, in neuronal networks, each neuron after its action potential is not<br>
                    responsive to the near future neuronal feedback [76], or in epidemics<br>
                        [9] typically studied in Susceptible-Infected-Removed<br>
                    models [77], in which infected individuals acquire immunity<br>
                    against re-infection supporting the view of epidemic spread as an essential<br>
                    forward cascade with little recurrence. While we have addressed the existence of<br>
                    different dynamical regimes on different topologies, we have not studied<br>
                    comprehensively all possible issues that might affect the dynamics of the<br>
                    network, e.g. network modularity [78]. Despite the<br>
                    dynamic feed-forward aspects of most cascades, the resulting functional<br>
                    architecture is not limited to acyclic graphs because potentially recurrent<br>
                    links between nodes that do not engage in one cascade can be active during other<br>
                    times.<br>
<br>
Small-World Functional Topology of Cortical Microcircuits<br>
In the present study, we derived the directed, weighted functional architecture<br>
                    of superficial cortical layers [29],[31] grown on planar<br>
                    integrated micro-electrode arrays. We demonstrated that a small-world functional<br>
                    topology of neuronal avalanches is robust to an acute reduction in network<br>
                    traffic, suggesting that it potentially arises from a corresponding structural<br>
                    small-world topology of cortical micro-circuits.<br>
The neuronal avalanche dynamics that arises in these layers in<br>
                    vitro parallels layer formation in the intact animal [33].<br>
                    The reconstruction of the architecture was based on neuronal avalanches,<br>
                    dynamical cascades that form in analogy to a critical branching process [29],[30] for which our<br>
                    simulations show robust and accurate network reconstruction using the NC<br>
                    algorithm. The estimated clustering coefficient stabilized as predicted from our<br>
                    network simulations. Importantly, a similar topology was recovered from acute,<br>
                    subcritical network dynamics in the presence of DNQX. This suggests that the<br>
                    subgraph described by a cascade does not depend on the overall state of the<br>
                    network, but might underlie structural components of the network as formed by<br>
                    the number and strengths of neuronal connections. A small-world topology<br>
                    combines short distances between network sites with high clustering that allows<br>
                    for diverse functionality of subgraphs, as shown recently for sensory activities<br>
                    in the visual cortex of the cat [79].<br>
Previous studies in dissociated neuronal cultures have quantified dynamical<br>
                    cascades during spontaneous neuronal activity using a variety of measures such<br>
                    as conditional probability [80], pairwise delayed-correlation indices [81], and<br>
                    sequential ordering [82]. Additionally, functional topologies were<br>
                    derived using correlation methods with global correlation thresholds [83]?[85]. As shown in the<br>
                    present study, the correlation approach might not adequately address functional<br>
                    connectivity, particular for dissociated cultures which have been shown to<br>
                    display supercritical dynamical cascades [82]. Despite these<br>
                    potential limitations, correlation and mutual information based methods derived<br>
                    non-directed functional small-world topologies from spontaneous activity in<br>
                    dissociated cortical cultures [52],[86],<br>
                    in line with our topological findings for the neuronal avalanche dynamics in<br>
                    layered cultures. Our study further quantified the network traffic, which was<br>
                    characterized by an exponential tail distribution similar to what has been found<br>
                    for the weight distribution in dissociated neuronal cultures [52] and airport traffic networks [55].<br>
                    These characteristics of the small-world architecture formed by neuronal<br>
                    avalanches provide important constraints for future simulations of this type of<br>
                    cortical dynamics.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2705677</b><br>
Power Efficiency of Outer Hair Cell Somatic Electromotility<br>
Cochlear outer hair cells (OHCs) are fast biological motors that serve to enhance the vibration of the organ of Corti and increase the sensitivity of the inner ear to sound. Exactly how OHCs produce useful mechanical power at auditory frequencies, given their intrinsic biophysical properties, has been a subject of considerable debate. To address this we formulated a mathematical model of the OHC based on first principles and analyzed the power conversion efficiency in the frequency domain. The model includes a mixture-composite constitutive model of the active lateral wall and spatially distributed electro-mechanical fields. The analysis predicts that: 1) the peak power efficiency is likely to be tuned to a specific frequency, dependent upon OHC length, and this tuning may contribute to the place principle and frequency selectivity in the cochlea; 2) the OHC power output can be detuned and attenuated by increasing the basal conductance of the cell, a parameter likely controlled by the brain via the efferent system; and 3) power output efficiency is limited by mechanical properties of the load, thus suggesting that impedance of the organ of Corti may be matched regionally to the OHC. The high power efficiency, tuning, and efferent control of outer hair cells are the direct result of biophysical properties of the cells, thus providing the physical basis for the remarkable sensitivity and selectivity of hearing.<br>
<br>
Introduction<br>
Outer hair cells (OHC) in the mammalian cochlea are essential to the remarkable sensitivity of hearing. These highly specialized cells actively feed mechanical power into the organ of Corti and amplify its mechanical vibrations in response to sound [1]?[5]. How this is achieved at auditory frequencies is a subject of considerable debate. Five biological motor mechanisms have been described in outer hair cells that may contribute [2],[3],[5],[6]. Motors localized to the hair bundles include: actin-myosin motors associated with slow bundle movements and adaptation mechano-electrical transduction (MET) currents [7],[8]; Ca2+ sensitive reclosure or conformational change of the MET molecular apparatus associated with fast bundle movements and adaptation [9]; and electrically-driven bundle displacement that act independent of MET function [10]. Motors localized to the soma include: cytoskeletal remodeling mechanisms [11],[12] and electrically-driven changes in length [13]?[15]. The ability of each of these mechanisms to feed mechanical power into cochlea is limited by their intrinsic thermodynamic properties. As such, some of these motors can be ruled out as key to amplification of mechanical motions in the cochlea simply because they are too slow. The mammalian cochlear amplifier is extremely fast and capable of cycle-by-cycle action, in some species at frequencies exceeding 50 kHz [16],[17]. This rules out mechanisms that require cyclic phosphorylation, transport and/or protein synthesis. In non-mammalian species, that do not have OHCs or the protein prestin, bundle-based motors underlie the active amplification process [18],[19]. In mammals, the evidence indicates OHC somatic motility is a key contributor [20]?[24], and this is the motor we focus on here.<br>
OHC somatic electromotility is driven by the MET current entering the cell and likely draws thermodynamic power from the electo-chemical potential between fluid compartments in the cochlea. The apical surfaces of OHCs are bathed in high-potassium endolymph, biased to approximately +50 to +80 mV, and their basal poles bathed in high-sodium perilymph at 0 mV reference. This endocochlear potential is maintained by the stria vacularis and associated cells [25]?[27]. When the hair bundle is displaced and MET channels open at the tips the stereocilia, ionic currents (primarily K+ and Ca2+) are driven into the OHC. A fraction of this MET current enters the apical face of the soma at the base of the stereocilia. In the absence of phosphorylation, it is likely that this current carries the thermodynamic electrical power input that drives the OHC mechanical power output. Here, we analyze how this electrochemical energy is converted into useful mechanical work by somatic electromotility using the model illustrated in Fig. 1. The current model is fundamentally piezoelectric in nature and extends concepts developed by Iwasa [28],[29] to address frequency-dependent power conversion efficiency.<br>
New results include the force vs. velocity, and power vs. velocity curves for OHCs (c.f. skeletal muscle cells [30]), and the frequency-dependent power efficiency that arises from intrinsic limitations on impedance matching between the cell and the load. Results indicate that OHCs are broadly tuned to have maximum power efficiency at a best frequency, thus contributing to tuning and the place principle in the cochlea. Furthermore, results provide an interpretation of how efferent activation may directly attenuate and de-tune the power output of OHCs and thereby providing a means for the brain to command exquisite control over the cochlear amplifier in a frequency dependent manner.<br>
<br>
Methods<br>
Experimental procedures and animal care were designed to advance animal welfare and were approved by the Baylor College of Medicine animal care and use committee.<br>
Our primary objective was to estimate what fraction of the electrical power entering the soma is converted into useful mechanical power output, and to estimate how this conversion efficiency would vary with frequency and biophysical parameters. It has not yet been technically possible to directly measure the electrical to mechanical power conversion efficiency of the OHC. The primary challenge is that one must measure the MET current, membrane potential, mechanical force generated and mechanical strain and velocity, all simultaneously and under physiologically relevant mechanical loading conditions. Therefore, we applied first principles of physics to formulate a relatively simple mathematical model of the OHC that reproduces all key published experimental data using a single set of physical parameters. The same model was then applied to compute the power conversion efficiency.<br>
1. Model Derivation<br>
Constitutive model for the lateral wall<br>
The OHC lateral wall, where the motor elements are located [24],[31], was modeled as a series mixture of passive ?elastic? and active piezoelectric ?motor? elements [29] to arrive at a composite constitutive model for the lateral wall. We assume here that the passive portion of the lateral wall is associated with fraction, , of the total lateral wall strain and the motor element is associated with fraction, . The fraction due to the motor was modeled as a leaky piezoelectric material. Following the notation of Tiersten [32], for the motor elements the stress tensor, , is related to the strain tensor, , and the electric field, , in the material according to(1)where the superscript ?M? denotes the motor element and the subscript ?p? denotes the component of the stress tensor (). The tensor  contains the elastic coefficients and the tensor  contains the piezoelectric coefficients. Einstein's summation convention applies for repeated indices. The electrical displacement current  in the motor portion of the lateral wall is related to the strain and the electric field according to the constitutive model(2)where  is the electrical permittivity of free space,  is the electrical relative permittivity, and  is the electrical conductivity of the motor portion of the lateral wall. The appearance of electrical conductivity is necessary to account for membrane electrical conductance makes Eq. 2 distinct from classical ideal piezoelectricity. The passive elastic component was modeled using the same approach, but with no piezoelectricity. In this case the stress tensor is(3)and the displacement current is(4)The constitutive behavior of OHCs is nonlinear and included here by allowing the piezoelectric and elasticity tensors to depend upon the electric field and strain.<br>
We further simplified the model by treating the OHC lateral wall as a thin shell undergoing axisymmetric deformations (; Fig. 1A?C), and assumed the axial strain is related to circumferential strain by a negative 2?1 ratio (; Fig. 1C). This strain ratio is consistent with experimental data [24] and, for small deformations, automatically enforces incompressibility of the intracellular volume (;  volume, a?=?cell radius, dx?=?differential length) for each differential slice of the OHC (i.e. ). To derive the series-composite model we assumed the axial stress to be identical in the motor and passive elastic components, , and that the total axial strain is found by series addition: . For the case when the elasticity tensors for the two materials are the same, the mixture parameter  () would be the fraction of the membrane surface area occupied by the passive elastic component, and the complement, , the fraction of the membrane area occupied by the motor. The electric field is dropped almost entirely across the plasma membrane and therefore varies through the thickness of the composite. After algebra, Eqs. 1?4 simplify to give the axial stress, T, in the composite(5)where upper case  is the composite material stiffness, S is the overall axial strain,  is the composite piezoelectric coefficient, h is the reference thickness of the composite lateral wall, and v(x,t) is the perturbation in membrane potential from the resting potential. The electrical current per unit membrane area, im, is related to the electric charge displacement by  and given by:(6)where lower case  is the capacitance per unit area under zero strain that arises from the membrane permittivity, and  the membrane conductance per unit area. The effective permittivity of the composite, , is referenced in the present analysis to the composite thickness, h. Since the passive capacitance arises almost exclusively from the plasma membrane,  of the composite is related to the permittivity of the plasma membrane,  by  (under the special case  as discussed below). We note that the composite thickness, h, always appears as a product with physical parameters, and therefore can be selected as the membrane thickness or a larger value incorporating the cortical lattice with equivalent results (providing the related parameters are scaled appropriately as discussed below). Eq. 5?6 are shown in their linearized form where the stress T, strain S, electric field E, and the charge displacement D are small perturbations from the resting state (, ,  and , the superscript ?0? refers to values in the resting state). In accordance, the physical parameters in Eq. 5?6 are the linearized values about the resting state (composite stiffness , composite piezoelectric coefficient , composite capacitance , composite conductance , all evaluated at the resting state:  and ). We also note that perturbations in the stain are related to axial displacement from rest u(x,t) by , perturbations in stress are related to changes in axial force f(x,t) by , and perturbations in the electric field are related to changes in voltage v(x,t) by . Accordingly, the axial force generated by the cell is related to the displacement and voltage by  and the axial current is related to the gradient of the voltage by .<br>
This composite material is distinct from ideal piezoelectric materials due to the presence of a membrane conductance, and because the motor itself only occupies a fraction of the membrane surface area while the remainder is occupied by passive elastic material [32],[33]. It is important to note that the composite properties are related to the mixture fractions and properties of the motor and elastic constituents. The composite elasticity of the series-composite lateral wall in the axial direction is , and the composite piezoelectric coefficient is . Maxwell piezoelectric reciprocity applies for each of the constituent materials and for the series-composite (Eq. 5?6) ? consistent with OHC data [34],[35]. The capacitance per unit area of the series composite under conditions of zero strain is , where  is the electrical permittivity of the plasma membrane. Note that the piezoelectric coefficient  contributes to the passive capacitance even in the zero strain case. This is because series expansion of the motor element can be offset by contraction of the passive element thus resulting in zero composite strain but non-zero piezoelectric charge displacement. This is distinct from ideal piezoelectric materials, but necessary when modeling OHCs to account for voltage-dependent capacitance in model cell lines observed even under zero overall strain (e.g. prestin transfected HEK cells [36]). We also found that standard piezoelectric materials were unable to simultaneously match the capacitance, displacement, and force observed in OHC, while the composite model was capable of matching all of the data with a single model parameter set. Since the piezoelectric coefficient  is voltage-dependent, the area-specific capacitance  is also voltage-dependent and cells exhibit nonlinear capacitance even when the strain is zero. Present simulations assume the membrane permittivity, , is not voltage dependent and is spatially uniform. Although it is not difficult to include, present results therefore do not address voltage dependence of the linear capacitance or any influence prestin configuration might have on this [36].<br>
In piezoelectric materials occurring in nature, the coefficient  is a function of strain and saturates for large strains. This occurs because of kinematic constraints on the molecular configuration within the material that limits the strain range of the piezoelectric effect. The strain-dependent saturating effect in OHCs follows this rule in that OHCs simultaneously exhibit length changes and charge movements upon varying the holding potential [37] and/or intracellular turgor pressure [38]. Piezoelectric saturation is more easily observed in OHCs experimentally using command voltages [36],[39],[40] due to the difficulty of strain controlled experiments. Because of this, we modeled the piezoelectric coefficient as dependent upon the holding potential using a Boltzmann function of the form , where  is the variance associated with thermal motion,  is the peak piezoelectric coefficient,  is the voltage at which the peak piezoelectric coefficient occurs and  is the membrane potential about which the OHC model equations were linearized. This is the same form that has been routinely applied to describe OHC voltage-dependent capacitance (e.g.  and ) [36],[39]. We note that a voltage-dependence can be converted to a strain-dependence, under conditions of zero change in stress, using the piezoelectric constitutive Eqs 5?6. Hence, an intrinsic strain-dependent piezoelectric coefficient associated with a change in molecular configuration can be observed experimentally as voltage-dependence.<br>
<br>
Conservation of momentum (Newton's 2nd law)<br>
Conservation of momentum in the axial direction can be written [41],[42](7)where  is the density of the composite membrane material and  is the fluid drag shear stress per unit length acting on the membrane from the extra- and intra-cellular fluids. Substituting the axial stress from Eq. 5 and the fluid drag from Section 2 into Eq. 7 provides(8)where u(x,t) is the local axial displacement of the membrane, v(x,t) is the perturbation in the local membrane potential, x is the axial position along the cell, t is time,  is the passive mechanical wave speed in a vacuum,  (??80 m3/volt-sec2) is a piezoelectric coefficient, and  (see below) is a damping coefficient resulting from immersion of the cell in fluid. If the voltage is uniform in space or the piezoelectric coefficient is zero, Eq. 8 reduces to the classical mechanical wave equation [41]. For OHCs the mechanical equations are overdamped such that propagating waves decay and sharp mechanical resonance is not expected [43]. If energetically favorable frequencies do exist, they would be electromechanical in nature and not strictly mechanical [44]. Also, at auditory frequencies, the first term () is small and could be ignored relative to other terms (retained in the present simulations).<br>
<br>
Conservation of charge (Kirchhoff's current law)<br>
Electrically, the OHC was modeled as a cylinder filled with conducting cytoplasm and immersed in a conducting fluid media (Fig. 1). The extracellular media was assumed to be space clamped and grounded (zero voltage), but the intracellular fluid voltage v(x,t) was allowed to vary with axial distance ?x? from the apex and with time ?t?. Current entering the MET channels was assumed to travel in the axial direction from the apex to the base. This configuration creates a current divider, with one fraction of the current directed out the base of the cell while the other fraction drives the lateral-wall motor. For simplicity, we consider the idealized case where the motor is modeled as homogeneously distributed along the lateral wall. We modeled the intracellular voltage using the same approach used for passive axons, reviewed by Weiss [45], but we replaced the classical plasma membrane electrical impedance with the series-composite model (Eq. 6) to obtain:(9)where  (;  intracellular axial resistance per unit length in Ohm/m,  cell radius and  membrane conductance per unit area in S/m2) is the DC electrical space constant analogous to that in the standard cable equation,  () is the composite membrane time constant under zero deformation, and  (?0.075 volt-s;  composite piezoelectric constant) is a piezoelectric coefficient coupling the piezoelectric charge movement to strain. Eq. 9 reduces to the standard cable equation used for passive axons in the absence of strain and/or piezoelectricity [45].<br>
<br>
Experimental conditions simulated<br>
Four types of stimuli and three loading conditions were considered. For stimuli, we considered voltage clamp (VC) of the basal region of the cell (Vb specified in Fig. 1), current injection at the apex of the cell simulating a constant amplitude sinusoidal MET currents (IT specified in Fig. 1), micro-chamber (MC) control of the extracellular voltage surrounding the basal pole of the cell (Vp specified in Fig. 1) [24],[46], and hair bundle displacement leading to an adapting MET current (IT from Eq. 10 below) [47]. For boundary conditions, we considered isometric loading (zero displacement at ), zero-load displacement (force zero at ), and the ideal intermediate case where the OHC was loaded in a way to achieve maximum mechanical power output.<br>
<br>
MET current adaptation<br>
In a subset of simulations we estimated the velocity and force for physiological hair bundle movements. The OHC transduction current appears to adapt very rapidly to step hair bundle displacements (time constant on the order of 100 micro-seconds), and the adaptation may be nearly 100% complete in some cells [47]. Although adaptive responses of OHC transduction currents are nonlinear, a simple first-order linear adaptation model captures some of the major features:(10)where the transduction current is  (Amp), the adaptation time constant is , the transduction current gain is ; ,  is the transduction current electrochemical potential, and the hair bundle displacement is  (m). The transduction current was set equal to axial current at the apical end of the cell and related to the voltage gradient along the cell at its apex using  where  is the intracellular axial resistance. Adaptation causes the current to increase as the frequency is increased, at least for stimuli below  (rad/sec). This counters the capacitance of OHCs and thus would be expected to flatten the frequency response of the intracellular voltage relative to responses to sinusoidal current injection.<br>
<br>
Analytical solution<br>
Equations 8?10 define the model and were solved in the frequency domain using an eigenvector expansion. The model equations were solved by first considering the a solution in the form:  and , where ,  (rad/sec) is the stimulus frequency, and  is an eigenvalue. Substitution into Eqs. 8?9 provides a 4th order eigenproblem(11)which yields, at each frequency, four eigenvalues  and corresponding eigenvectors . The frequency-dependent AC space constant under which each piezoelectric eigenwave propagates is , and the phase velocity is . Having these eigenvalues, we write the general solution for a finite length OHC in the form of an eigenvector expansion(12)where the frequency-domain voltage and displacement are components of . The four independent eigenvectors are , with corresponding eigenvalues .<br>
The coefficients  are found from four boundary conditions. To model the isometric condition, we require the displacement at the two ends of the OHC lateral wall to be zero (at ). From Eqs. 5 and 11 this gives two equations(13)For the zero-force condition, we require the stress to vanish at the ends of the lateral wall (at ) to find(14)To close the problem, we need two additional boundary conditions. In most simulations we drive the OHC via a sinusoidal current injection at the apical end of the cell, x?=?0. Under this condition the intracellular voltage gradient is related to the current injection  at the apex and the axial resistance per unit length according to . Substitution into Eq. 11 gives(15)At the other end of the cell, current exits the region adjacent to the lateral wall motor and enters the basal compartment ? a compartment we model using a lumped impedance Zb at the base of the cell. For this case(16)For a voltage clamp simulations, we specify the intracellular voltage Vb at the base of the cell(17)Stimulation of the OHC by modulating the voltage Vp in a pipette microchamber enveloping the base of the cell [48] requires us to account for current through the membrane and gives(18)After selecting isometric or zero-load conditions, and the stimulus type, the equations above provide 4 equations that are easily solved for the 4 unknown constants Bn. Since the equations are linear, we use superposition to consider mixed boundary conditions as described below.<br>
<br>
Power and Efficiency<br>
The power efficiency was defined as the mechanical power output divided by the electrical power input. The mechanical power output is computed in the frequency domain using , where  is force and  is the complex conjugate of the velocity, both evaluated at the apical end of the cell. The real part of the power output provides the time-averaged power transferred from the OHC to the external dissipative load. This real part of the power is the component that would be needed to overpower viscosity, for example. Similarly, the electrical power input via the MET channels is , where V is the voltage drop across the MET and  is the complex conjugate of the MET current. Since the model was linearized about the resting state, superposition of the isometric case and the zero-force case could be used to simulate any loading condition. By superposition, the force output by the cell is , where m () is a complex-valued parameter controlling the load,  is the force under isometric conditions, and  is the unloaded zero force condition (subscript 0 denotes the isometric case and 1 denotes the zero-force case). Similarly, the velocity is . The corresponding MET voltage and current are  and , respectively. Combining these expressions gives the power conversion efficiency, , of the OHC as(19)Note that the efficiency is zero under isometric conditions () and is zero if no load is applied (). There is a unique load, magnitude and phase, that maximizes the efficiency. This ?impedance-matched? load is frequency dependent and was found by solving for the complex-valued parameter m that maximized .<br>
The present model has some features similar to previous piezoelectric-like models of the OHC [29], [49]?[54], but the formulation differs by including a series elastic-piezoelectric composite constitutive model of the lateral wall and axial conductance of the intracellular space, and differs in considering power conversion from electrical power entering the transduction channels to mechanical power output to do useful work.<br>
<br>
<br>
2. Visco-Elastic Fluid Drag<br>
Dissipative drag from the cytoplasm and the extracellular space are unavoidable. As a first approximation we modeled the axial component of the drag acting on the plasma membrane using a version of the Navier-Stokes equations. Assuming small displacements from the resting configuration, and ignoring the convective nonlinearity, the Navier-Stokes equations reduce to(20)where  is the density of the fluid, r is radial coordinate,  is the effective viscosity, and  is the axial velocity. To approximate the visco-elastic properties of the materials, we used a complex-valued viscosity of the form , where ,  is the frequency,  is a material constant, and the  is a parameter that determines the relative contributions of viscosity vs. elasticity of the material. When  this model reduces to the standard Newtonian viscous fluid and when  this reduces to the standard shear elastic solid. For biological materials ? falls between these two extremes ? e.g.  for the tectorial membrane [55]. These equations account for both the visco-elastic drag and entrained fluid mass. We solved the equations to obtain the velocity field  resulting when a cylinder oscillates in the axial direction with displacement . Having the velocity field, we computed the axial shear stress  acting on the cylinder wall per unit axial displacement(21)where  are Hankel functions,  is the non-dimensional Womersley number (complex-valued), and a is the cylinder radius. With this, the damping parameter appearing in the momentum equation (Eq. 8) is . This model is approximate, but matches the viscous analysis of Tolomeo and Steel [43] if the length of the cell is much longer than the diameter, motions are axial, and the viscosity is strictly real valued, i.e. .<br>
<br>
3. Model Parameters<br>
Model parameters were estimated from known dimensions and physical constants combined with voltage clamp and mechanical data shown in Figs. 2?3 as well as microchamber data in Fig. 4. All other results (Fig. 5?8) and voltage clamp data in Fig. 4 are model predictions and the associated data were not used to estimate parameters. The model uses a reference thickness  to describe the multi-component composite lateral wall and it is important to note that some parameters cannot be independently separated from this reference thickness (e.g. , ,  appear as groups). Coefficients appearing in the cable equation were computed from the physical parameters listed below using: , , and . Coefficients appearing in the wave equation were computed using ,  and . Dimensions were based on OHCs from the guinea pig cochlea. Data in Fig. 2?3 were used to find the effective stiffness, piezoelectric coefficient, electrical permittivity and conductance of the membrane. These data are for relatively low stimulus frequencies where the intracellular axial resistance has negligible effect on the results. To estimate the axial resistance we used the corner frequency where the capacitance measured at the basal pole of the cell begins to roll off (Fig. 3). The fraction of the membrane occupied by the motor was set to 80% () and the passive component to 20% (). The overall cell compliance was estimated from the slope of the compliance vs. cell length reported by Frank et al. [48], reproduced in Fig. 2C, using  as well as the gain reproduced in Fig. 4 (solid, microchamber curve). An iterative optimization routine was run to refine the initial estimates of ,  and  to simultaneously fit data in Fig. 2?4. Specific optimized numerical parameters include: OHC radius a?=?4.5e-6 m; composite mechanical stiffness C*?=?1.4e6 N/m2 (based on ,  and ); plasma membrane conductance ; apical face membrane conductance ; basal membrane conductance ; transduction current gain ; composite reference thickness ; OHC length ; length of the active lateral wall was , and  was set by requiring passive basal pole to have a passive capacitance of 7 pF; intracellular axial resistance ri?=?5.76e10 Ohm/m; composite piezoelectric coefficient at rest  (C/m2) at rest; plasma membrane area specific capacitance ; density ; transduction current adaptation time constant ; fluid viscosity ; and fractional viscosity coefficient . We note that the mixture fraction  is not uniquely determined by currently available data and it is possible to find alternative mixture fractions and stiffness parameters that result in the same composite stiffness C*. Nevertheless, it was necessary to use a value of  to simultaneously fit all of the data and explain the magnitude of voltage dependent capacitance under unloaded and zero strain conditions. Additional experiments, perhaps involving voltage-dependent capacitance measurements under controlled mechanical loads, have the potential to resolve this ambiguity and reveal more about the lateral wall motor, but are not necessary for the purpose of the present power analysis since the composite parameters would not change.<br>
<br>
4. Experimental Methods<br>
Experimental procedures and animal care were designed to advance animal welfare and were approved by the Baylor College of Medicine animal care and use committee. All physical parameters were deduced from the published literature, with the exception of the intracellular electrical resistance, . To estimate , we isolated OHCs from the guinea pig cochlea [56] and examined the frequency dependence of the input electrical impedance under whole-cell voltage clamp (Axopatch 200 B, Molecular Devices, Sunnyvale, CA). OHCs were harvested from euthanized guinea pigs. Cells were patch-clamped at the base with quartz pipettes covered with Sylgard, and hyperpolarized to minimize the voltage-dependent nonlinear capacitance. K+ and Ca2+ ion channels were blocked with the addition of (C2H5)4N(Cl), CsCl and CoCl to the bathing and/or pipette solutions [57]. The input admittance was determined with a single sinusoidal voltage (0.015 V peak to peak, 90?3200 Hz) superimposed on top of a ?0.13 V holding potential after correcting for the inherent phase shifts of the amplifier [58]. 210 measurements were averaged at each frequency. The resistance and capacitance were calculated from the input admittance [57] accounting for the series resistance (?6 Mohm, remained constant throughout experiment). Experiments were conducted at room temperature.<br>
<br>
<br>
Results<br>
1. Theory vs. Experiment<br>
Voltage-dependent capacitance<br>
When the lateral wall deforms there is a compensatory electrical charge movement due to deformation in the motor portion of the membrane ? behavior that is fundamentally piezoelectric in nature [35]. Fig. 2A compares the voltage dependent input capacitance measured at the basal pole to model predictions (solid curves) for a 50 ?m long cell tested using a small ?100 Hz interrogation signal. Changing the piezoelectric parameter  causes the nonlinear capacitance to increase or decrease (dashed curves). The area-specific capacitance has the form , where the motor-independent term, , arises from membrane permittivity and historically is termed the ?linear capacitance?. The motor-dependent term arises from piezoelectric charge movement and is termed the ?non-linear capacitance?. The non-linear capacitance depends on motor stiffness  and the mixture fraction . Under conditions of zero load  (Fig. 2A), and under zero-displacement  (note, ). Unlike classic piezoelectrics, the composite admits nonlinear capacitance under zero-displacement because active extension of the piezoelectric element is absorbed by contraction of the series elastic element. This is particularly relevant to understanding capacitance measurements in prestin transfected HEK cells where the strain is small [36],[59]. In both cases, the magnitude of  is proportional to  and therefore is also directly related to the isometric force generated by the cell under zero-displacement conditions (Fig. 2B).<br>
<br>
Frequency-dependent input impedance<br>
The input capacitance of OHCs measured at the base is nearly constant below 1 kHz, but begins to roll-off as the interrogation frequency is increased (Fig. 3 for two ?55 ?m long cells). The roll-off is captured in the model by a loss of space clamp at high frequencies. When cells are deeply hyperpolarized, the voltage dependent component of the capacitance approaches zero, , and the model reduces to the cable equation with AC space constant  (where  is the standard DC space constant and  is the passive membrane time constant) [60]. We selected the intracellular resistance to fit the capacitance corner frequency. The resistance value implies that the axial ionic current flows along 21% of the intracellular cross-sectional area (based on electrical conductivity of ?1.3 S/m). This area is orders of magnitude larger than the annular extra-cisternal space and therefore it is unlikely that all current is channeled strictly along this narrow space as hypothesized previously [61].<br>
<br>
Zero-load displacement<br>
Fig. 4 compares model predictions to OHC displacement gains in the microchamber [48] and under voltage clamp conditions [15],[62] (cell length range roughly estimated by horizontal error bar). The magnitude of displacement gain reported in Fig. 4 is controlled primarily by the piezoelectric coefficient and the cell mechanical stiffness, while the curved shape is geometrical and arises from the fact that the base of the cell is not electromotile. The microchamber commands the extracellular voltage, Vp, around the basal pole of the cell and, therefore, the intracellular voltage, Vb, is less than present during voltage clamp. This is why the displacements (and gains) in the microchamber configuration (lower curve) are less than in the voltage clamp configuration (upper curve). Voltage clamp data in Fig. 4 was not used to estimate model parameters, yet the simulations correspond well with the experimental observations. Simulations also show the effect of increasing the basal membrane conductance in the microchamber configuration (dashed curve) ? a prediction that agrees with previous data collected after application of the efferent transmitter ACh [63] thus further showing the predictive capability of the model. We note that the fast electrical effect of efferent activation occurs even in the absence of additional efferent mediated changes in cell stiffness.<br>
<br>
Velocity and force vs. frequency<br>
The predictive capability of the model is further illustrated in Fig. 5 comparing velocity predictions for an 80 ?m long cell to data collected by Frank et al. in the microchamber (symbols, data for a similar length cell [48]). Model predictions used parameters determined from Figs. 1?4, yet were remarkably similar to these independent experimental observations. OHC velocities increased decade-by-decade over a wide bandwidth, and exhibited a corner frequency, (A, *), above which the velocity flattened and the phase began to roll off. The initial roll-off begins in the model when the piezoelectric force can no longer overpower the viscous drag. The model also predicts a small delay time associated with a dispersive traveling wave along the OHC (?4 ?s for 80 ?m cell, not shown). Fig. 5C compares the isometric force predicted by the model for the same cell to experimental data [48]. As expected, the dominant corner frequency observed under isometric force conditions (5C) is higher than observed under zero-load conditions (5A,B) simply because the cell is not moving as much and therefore experiences less electrical and mechanical losses. The same model was used to predict frequency dependent velocity responses for sinusoidal current injection at the apex of the cell (dashed curves) and under physiological hair bundle deflections (dotted curves) leading to adapting apical transduction currents (Eq. 10). It is notable that MET adaptation is predicted to shift the phase of the OHC force and displacement relative to non-adapting current entering the apex of the cell, at least in the mid-frequency band [64]?[66]. Models of the cochlea suggest that a 90? phase shift may be beneficial in the cochlea to align the time of maximum OHCs force generation with that required to increase the vibration of the organ of Corti [67], thus indicating a potential advantage of MET adaptation.<br>
<br>
<br>
2. Electromotility Efficiency<br>
Power output vs. velocity<br>
Most experimental data addressing OHC electromotility are collected under conditions of isometric length (Fig. 5C), or zero load (Figs. 2A, 3, 4, 5A?B). In both cases, the mechanical work done by the OHC is zero, and the efficiency is zero. Fig. 6 shows that the peak mechanical efficiency (*) occurs a specific impedance-matched load falling approximately half way between the isometric and zero-load conditions. Specific results shown in Fig. 6 for a 28 ?m long OHC at 1 kHz. Although details vary slightly with frequency and cell length, the concept is universal and analogous to the well-known power vs. velocity curves for skeletal muscle cells [30]. Subsequent results (Figs. 7?8) assume that the cochlea efficiently extracts power from OHCs and therefore that the load and the OHC are impedance matched. This implies operation at the peak efficiency, *, in Fig. 6. If true, it is technically feasible, but beyond the present scope, to imply the local impedance within the organ of Corti based on that necessary to match that of the OHCs.<br>
<br>
Power conversion efficiency vs. frequency<br>
Classical piezoelectricity is thermodynamically conservative [32] and has the potential for 100% efficiency (). In practice piezoelectric coupling limits efficiency [68], in OHCs to less than 60% [29], due to interplay between the piezoelectric coefficient, stiffness and the electrical permittivity [51],[53],[54],[69]). This loss is shown as ?series piezoelectric coupling? at the top of Fig. 7A. Present simulations predict that OHC efficiency is frequency dependent and may reach ?40% at the best frequency, . At this optimum frequency, power is lost to fluid viscosity and piezoelectric coupling. We assumed in these simulations that the power delivered by the OHC to fluid viscosity was not of use to the cochlea and therefore causes a reduction in efficiency. It is likely that some of the viscous pumping by OHCs is not lost, but instead may used in cochlea to further amplify vibrations. Therefore, the efficiencies reported here are likely to be a lower bound. In addition to viscous losses, there are two additional intrinsic properties of OHCs that limit efficiency and, in fact, are predicted to be responsible for frequency tuning of the cells. Below , OHC stiffness limits the efficiency. Above , the axial electrical resistance inside the OHC limits the efficiency. Fig. 7B provides efficiency predictions for three different cell lengths, with all other parameters held constant. It is important to note that shorter cells are predicted to be more efficient at high frequencies. This occurs primarily because the space constant of the intracellular electric field shortens with increasing frequency such that  in Eq. 8 becomes nonzero. This couples the electro-mechanical equations and leads to dissipation of electrical power along the length of the cell. Interestingly, simulations for long OHCs exhibited a second peak in efficiency at ultrasonic frequencies (Fig. 7B, near 100 kHz for the 80 ?m cell, blue curves), reminiscent of electrical admittances observed previously in isolated OHCs [44].<br>
<br>
Membrane conductance and efferent control<br>
Activation of the medial olivocochlear efferent bundle reduces mechanical amplification by outer hair cells [4], [70]?[72]. Efferent mediated changes in OHC stiffness [12],[73] likely contribute, but present result also highlight the importance of changes in ionic conductances. Geisler (1974) proposed previously that efferent activation by the brain might alter basal conductance of hair cells and thereby reduce their response [74]. Additional evidence for the conductance proposal comes from the vestibular system and lateral line in vivo where activation of the efferent system greatly decreases hair cell receptor gain due to a marked increase in electrical conductance [75],[76], and from the turtle cochlea where efferent activation decreases tuning and receptor gain [77]. These findings are consistent with responses of OHCs to application of the putative efferent transmitter ACh in the dish, where cells increase their displacements evoked in the microchamber configuration [63] ? as would be expected with an increase in basolateral membrane conductance noted above (see Fig. 4) due to the additional current that would flow from the microchamber pipette into the cell through the reduced basolateral impedance Zb.<br>
The present model also addresses how the brain likely controls mechanical power output of OHCs through efferent mediated ionic conductances at the base of the cell. Electrical current entering the MET channels is divided into two parts. The first part drives charge displacement in the lateral wall and is responsible for somatic electromotility through the piezoelectric effect (Eq. 2). The second part of the current exits the base of the cell through conductive ionic channels. If the ion channels are closed (high Zb case), the part of the MET current driving the somatic motor is maximized. If the ion channels are opened by efferent neurotransmitter (low Zb case), current will be shunted out the base of the cell and therefore not available to power the motor. This is the reason why opening of basolateral ion channels reduces the efficiency of OHC electrical to mechanical power conversion. This is shown in Fig. 7A as the efficiency drops from the solid curve to the dashed curve. At the same time, the peak efficiency drops ( to ) while the best frequency shifts higher ( to ). OHCs in the mammalian cochlea do not really experience stimulus frequencies above  because the traveling wave along the basilar membrane becomes cut off. Hence, the efferent system could reduce power output of OHCs at  by almost two orders of magnitude simply by shunting the MET power out the basolateral membrane. Solid curves vs. dashed curves in Fig. 7B illustrate that similar effects are present cells of various lengths. Thus, it seems likely that efferent synaptic action upon OHCs sharply attenuates the mechanical power output at best frequency by shunting the electrical power input via the MET current to ground.<br>
<br>
OHC length vs. best frequency<br>
Shorter cells exhibited their best efficiency at high frequencies while longer cells exhibited their best efficiency at low frequencies. Fig. 8A shows OHC length vs. maximum efficiency frequency () along with data correlating the lengths of OHCs in the cochlea to the place principle describing the best frequency of sound sensation. Above 1 kHz, the morphological relationship between OHC length and physiological best frequency in the cochlea [78] is bracketed by the best efficiency predicted here. These results are consistent with the hypothesis that OHC lengths are matched to the frequency requirements at their location in the cochlea. Interestingly, if we consider the peak efficiencies over all hair cells studied, the analysis predicts that hair cells tuned to ?3?4 kHz are the most efficient (Fig. 8B). This might augment the high efficiency of the middle ear in this frequency band [79],[80] and further accentuate sensitivity to damage by acoustic overexposure.<br>
<br>
<br>
<br>
Discussion<br>
There are four major observations that can be drawn from the present work. The first addresses how OHCs operate at high frequencies given their electrical capacitance [9], [62], [81]?[83]. Capacitance is thermodynamically conservative and present results confirm that the ability of OHCs to supply mechanical power to the cochlea is not limited by electrical capacitance [84], even at frequencies much higher than the membrane time constant (e.g. Fig. 7). This is true because capacitance is not dissipative. Instead, present results suggest the most serious factor that may limit power output by OHCs is how well the ?impedance? of the hair cell is matched to that of the cochlear partition (e.g. Fig. 6). OHCs driving against an excessively stiff cochlear partition, for example, would be inefficient.<br>
The second observation is that OHCs may be tuned to maximize their power output at a best frequency, albeit broadly tuned. Although OHC displacement and force are quite flat over a broad range of frequencies when driven by voltage (e.g. Fig. 5, present model and published data [48]), OHC power output is tuned when one considers the mechanical power output relative to the electrical power input. The predicted tuning is dependent upon cell length and correlates with the cochlear place principle [78], thus indicating that tuning of OHCs may contribute to the sharp mechanical and afferent neural tuning in the living cochlea.<br>
The third observation addresses how the MET channels would be expected to further tune output of the somatic motor. MET adaptation generates high-pass filtered MET currents [47],[66],[85],[86]. Since the filtering is upstream of the somatic motor it would further sharpen tuning of OHC somatic motor output by attenuating low-frequency amplification. In the context of the organ of Corti, MET adaptation would also be expected to alter the phase of the OHC force possibly to maximize power input to the cochlea near the best frequency [67] and, additionally, might introduce a non-optimal phase that would sharply attenuate cochlear gain at both low and high frequencies. Because of these factors, the influence of tuning in isolated OHCs on tuning curves in the cochlea would be expected to be even more significant than implied by the OHC motor efficiency alone (Fig. 7).<br>
The fourth observation is that OHC somatic power output may be controlled by the brain via efferent activated ionic conductance(s). The model predicts that increasing the conductance of the basal pole would reduce OHC power output and tuning, thus providing a plausible explanation for a fast mechanism that may be used by the brain to control both sensitivity and frequency selectivity of hearing (e.g. Fig. 7).<br>
Finally, it is important to note that the OHC somatic motor is not present in non-mammals, yet these animals also exhibit many of the properties of the mammalian cochlear amplifier [87],[88]. The MET apparatus itself is clearly a key contributor to hair bundle motility and amplification [9],[89]. In addition, there is an MET-independent component of hair bundle motility driven by voltage [10]. This voltage-dependent component has analogy to the somatic motility addressed here, and may be involved in tuning and the power stroke of hair bundle motility with potential relevance to active bundle amplification in high frequency hearing organs [90]. These hair-bundle features occur upstream of the somatic motor and the two clearly interact with each other via micromechanical environment and electrical fields [91].<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2766072</b><br>
Evolution of Resistance to Targeted Anti-Cancer Therapies during Continuous and Pulsed Administration Strategies<br>
The discovery of small molecules targeted to specific oncogenic pathways has revolutionized anti-cancer therapy. However, such therapy often fails due to the evolution of acquired resistance. One long-standing question in clinical cancer research is the identification of optimum therapeutic administration strategies so that the risk of resistance is minimized. In this paper, we investigate optimal drug dosing schedules to prevent, or at least delay, the emergence of resistance. We design and analyze a stochastic mathematical model describing the evolutionary dynamics of a tumor cell population during therapy. We consider drug resistance emerging due to a single (epi)genetic alteration and calculate the probability of resistance arising during specific dosing strategies. We then optimize treatment protocols such that the risk of resistance is minimal while considering drug toxicity and side effects as constraints. Our methodology can be used to identify optimum drug administration schedules to avoid resistance conferred by one (epi)genetic alteration for any cancer and treatment type.<br>
<br>
Introduction<br>
Alteration of the normal regulation of cell-cycle progression, division and death lies at the heart of the processes driving tumorigenesis. A detailed molecular understanding of these processes provides an opportunity to design targeted anti-cancer agents. The term ?targeted therapy? refers to drugs with a focused mechanism that specifically act on well-defined protein targets or biological pathways that, when altered by therapy, impair the abnormal proliferation of cancer cells. Examples of this type of therapy include hormonal-based therapies in breast and prostate cancer; small-molecule inhibitors of the EGFR pathway in lung, breast, and colorectal cancers ? such as erlotinib (Tarceva), gefitinib (Iressa), and cetuximab (Erbitux); inhibitors of the JAK2, FLT3 and BCR-ABL tyrosine kinases in leukemias ? such as imatinib (Gleevec), dasatinib (Sprycel), and nilotinib (Tasigna); blockers of invasion and metastasis; anti-angiogenesis agents like bevacizumab (Avastin); proapoptotic drugs; and proteasome inhibitors such as bortezomib (Velcade) [1],[2]. The target-driven approach to drug development contrasts with the conventional, more empirical approach used to develop cytotoxic chemotherapeutics, and the successes of the past few years illustrate the power of this concept. The absence of prolonged clinical responses in many cases, however, stresses the importance of continued basic studies into the mechanisms of targeted drugs and their failure in the clinic.<br>
Acquired drug resistance is an important reason for the failure of targeted therapies. Resistance emerges due to drug metabolism, drug export, and alteration of the drug target by mutation, deletion, or overexpression. Depending on the cancer type and its stage, the therapy administered, and the genetic background of the patient, one or several (epi)genetic alterations may be necessary to confer drug resistance to cells. In this paper, we investigate drug resistance emerging due to a single alteration. For example, treatment of chronic myeloid leukemia (CML) with the targeted agent imatinib fails due to acquired point mutations in the BCR-ABL kinase domain [3]. To date, ninety different point mutations have been identified, each of which is sufficient to confer resistance to imatinib [4]. The second-generation BCR-ABL inhibitors, dasatinib and nilotinib, can circumvent most mutations that confer resistance to imatinib; the T315I mutation, however, causes resistance to all BCR-ABL kinase inhibitors developed so far. Similarly, the T790M point mutation in the epidermal growth factor receptor (EGFR) confers resistance to the EGFR tyrosine kinase inhibitors gefitinib and erlotinib [5], which are used to treat non-small cell lung cancer. Other mechanisms of resistance include gene amplification or overexpression of the P-glycoprotein family of membrane transporters (e.g., MDR1, MRP, LRP) which decreases intracellular drug accumulation, changes in cellular proteins involved in detoxification or activation of the drug, changes in molecules involved in DNA repair, activation of oncogenes such as Her-2/Neu, c-Myc, and Ras as well as inactivation of tumor suppressor genes like p53 [6]?[11].<br>
The design of optimal drug administration schedules to minimize the risk of resistance represents an important issue in clinical cancer research. Currently, many targeted drugs are administered continuously at sufficiently low doses so that no drug holidays are necessary to limit the side effects. Alternatively, the drug may be administered at higher doses in short pulses followed by rest periods to allow for recovery from toxicity. Clinical studies evaluating the advantages of different approaches have been ambivalent. Some investigations found that a low-dose continuous strategy is more effective [12], while others have advocated more concentrated dosages [13]. The effectiveness of a low-dose continuous approach is often attributed to its targeted effect on tumor endothelial cells and the prevention of angiogenesis rather than low rates of resistance [14]. The continuous dosing strategy is often implemented as combination therapy, sometimes including a second drug administered at a higher dose in a pulsed fashion.<br>
A significant amount of research effort has been devoted to developing mathematical models of tumor growth and response to chemotherapy. In a seminal paper, Norton and Simon proposed a model of kinetic (not mutation-driven) resistance to cell-cycle specific therapy in which tumor growth followed a Gompertzian law [15]. The authors used a differential equation model in which the rate of cell kill was proportional to the rate of growth for an unperturbed tumor of a given size. They suggested that one way of combating the slowing rate of tumor regression was to increase the intensity of treatment as the tumor became smaller, thus increasing the chance of cure. The authors also published a review of clinical trials employing dosing schedules related to their proposed dose-intensification strategy, and concluded that the concept of intensification was clinically feasible, and possibly efficacious [16]. Later, predictions of an extension of this model were validated in a clinical trial evaluating the effects of a dose-dense strategy and a conventional regimen for chemotherapy [17]. Their model and its predictions have become known as the Norton-Simon hypothesis and have generated substantial interest in mathematical modeling of chemotherapy and kinetic resistance. For example, Dibrov and colleagues formulated a kinetic cell-cycle model to describe cell synchronization by cycle phase-specific blockers [18]; this model was then used for optimizing treatment schedules to increase the degree of synchronization and thus the effectiveness of a cycle-specific drug. Agur introduced another model describing cell-cycle dynamics of tumor and host cells to investigate the effect of drug scheduling on responsiveness to chemotherapy [19]; this model was then used to optimize scheduling of chemotherapeutics to maximize efficacy while controlling host toxicity. Other theoretical studies include a mathematical model of tumor recurrence and metastasis during periodically pulsed chemotherapy [20], a control theoretic approach to optimal dosing strategies [21], and an evaluation of chemotherapeutic strategies in light of their anti-angiogenic effects [14]. For a more comprehensive survey of kinetic models of tumor response to chemotherapy, we refer the reader to reviews [22]?[24] and references therein.<br>
There have also been substantial research efforts devoted to developing mathematical models of genetic resistance, i.e. resistance driven by genetic alterations in cancer cells. Since mutations conferring resistance can arise as random events during the DNA replication phase of cell division, the dynamics of resistant populations are well-suited to description with stochastic mathematical models. Coldman and co-authors pioneered this field by introducing stochastic models of resistance against chemotherapy to guide treatment schedules (see e.g., [25],[26] and references therein). In 1986, Coldman and Goldie studied the emergence of resistance to one or two functionally equivalent chemotherapeutic drugs using a branching process model of tumor growth with a differentiation hierarchy [25]. In this model, the birth and death rates of cells were time-independent constants and each sensitive cell division gave rise to a resistant cell with a certain probability. The effect of drug was modeled as an additional probabilistic cell kill law on the existing population, and the drug could be administered in a fixed dose at a series of fixed time points. The goal of the model was to schedule the sequential administration of both drugs in order to maximize the probability of cure. Later, the assumption of equivalence or ?symmetry? between the two drugs was relaxed [27]. These models were also extended to include the toxic effects of chemotherapy on normal tissue, and an optimal control problem was formulated to maximize the probability of tumor cure without toxicity [26]. More recently, Iwasa and colleagues used a multi-type birth-death process model to study the probability of resistance emerging due to one or multiple mutations in populations under selection pressure [28]. The authors considered pre-existing resistance mutations and determined the optimum intervention strategy utilizing multiple drugs. Multi-drug resistance was also investigated using a multi-type birth-death process model in work by Komarova and Wodarz [29],[30]. In their models, the resistance to each drug was conferred by genetic alterations within a mutational network. The birth and death rates of each cell type were time-independent constants and cells had an additional drug-induced death rate if they were sensitive to one or more of the drugs. The authors studied the evolution of resistant cells both before and after the start of treatment, and calculated the probability of treatment success under continuous treatment scenarios with a variable number of drugs. Recently, the dynamics of resistance emerging due to one or two genetic alterations in a clonally expanding population of sensitive cells prior to the start of therapy were studied using a time-homogenous multi-type birth-death process [31],[32].<br>
One common feature of these models of genetic resistance is that the treatment effect is formulated as an additional probabilistic cell death rate on sensitive cells, separate from the underlying birth and death process model with constant birth and death rates. Under these model assumptions, the drug cannot alter the proliferation rate of either sensitive or resistant cells; however, a main effect of many targeted therapies (e.g. imatinib, erlotinib, gefinitib) is the inhibition of proliferation of cancer cells. Inhibited proliferation in turn leads to a reduced probability of resistance since resistant cells are generated during sensitive cell divisions. In this paper, we utilize a non-homogenous multi-type birth-death process model wherein the birth and death rates of both sensitive and resistant cells are dependent on a temporally varying drug concentration profile. This study represents a significant departure from existing models of resistance since we incorporate the effect of inhibition of sensitive cell proliferation as well as drug-induced death, obtaining a more accurate description of the evolutionary dynamics of the system. In addition, we generalize our model to incorporate partial resistance, so that the drug may also have an effect on the birth and death rates of resistant cells. The goals of our analysis also differ from those of previous work. Coldman and Murray were interested in finding the optimal administration strategy for multiple chemotherapeutic drugs in combination or sequential administration [26]; they aimed to maximize the probability of cure while limiting toxicity. Komarova was interested in studying the effect of multiple drugs administered continuously on the probability of eventual cure [30]. In contrast, in this paper we derive estimates for the expected size of the resistant cell population as well as the probability of resistance during a full spectrum of continuous and pulsed treatment schedules with one targeted drug. We then propose a methodology for selecting the optimal strategy from this spectrum to minimize the probability of resistance as well as to maximally delay the progression of disease by controlling the expected size of the resistant population, while incorporating toxicity constraints. In many clinical scenarios, the probability of resistance is high regardless of dosing strategy, and thus the maximal delay of disease progression is a more realistic objective than tumor cure. The methodology developed in this paper can be applied to study acquired resistance in any cancer and treatment type.<br>
<br>
Methods<br>
Consider a population of  drug-sensitive cancer cells at the time of diagnosis. These cells may represent the total bulk of tumor cells or, alternatively, cancer stem cells only; if only the latter cells are capable of self-renewal to produce and maintain a resistant cell clone, then the effective population size  equals the number of cancer stem cells in the tumor. We assume for now that there are no resistant cells present at the time of diagnosis; this assumption will be relaxed in later sections. The evolutionary dynamics of the sensitive and resistant cancer cell populations are modeled as a multi-type branching process [33].<br>
Let us first consider this process during a continuous dosing schedule. Cells are chosen at random to divide or die in accordance with a specified growth and death rate. During therapy, the growth and death rates of sensitive cancer cells are denoted by  and , respectively. During each sensitive cancer cell division, a resistant cell arises with probability ; such a cell has growth and death rates  and  during treatment. The growth and death rates during therapy are determined by the dose intensity of treatment.<br>
Let  denote the random process representing the number of sensitive cancer cells at time , and let  denote the number of resistant cancer cells at time . The initial conditions at the start of treatment, , are defined as  and . Denote the state of the process after the  event (represented by a cell birth or death) as . The transition probabilities of the stochastic process are given byHere  is the sum of the rates, normalizing the total probability to 1. The arrival times between events are exponentially distributed: if  denotes the time at the beginning of the  step, then  is an exponential random variable with parameter .<br>
Let us now consider the evolution of these cell populations during general pulsed treatment schedules. To investigate dosing strategies consisting of drug pulses and treatment holidays, we implement time-varying growth and death rates in the stochastic process model. During treatment pulses, the growth rates of sensitive and resistant cells are again denoted by  and , respectively, while their death rates are given by  and . During drug holidays, the growth rates of sensitive and resistant cells are given by  and , respectively, and their death rates by  and  (see Table 1 for a summary of notation). Here we assume that the drug reaches its maximum concentration immediately after administration and remains at that level until the beginning of the treatment break; additionally, we assume that the intensity of the dose does not vary from pulse to pulse. The length of each treatment pulse is denoted as  and the length of each treatment break is denoted as . Each treatment cycle consists of one pulse and one break, and every treatment cycle has the same length: . Thus, a strategy in which  corresponds to the continuous dosing regimen. Both sensitive and resistant cells expand in the absence of therapy,  and . We consider only drugs at concentrations that lead to a reduction of the sensitive cancer cell population (). The (epi)genetic alteration conferring resistance may be neutral (), advantageous (), or disadvantageous () in the absence of therapy. If  and , then the drug has no effect on resistant cancer cells and the mutation confers complete resistance; it is this case that we consider first. We investigate only viable treatment options (i.e. dosing strategies that are capable of depleting the sensitive cancer cell population). Specifically, we define , , and , and require that the relation  must hold. This criterion ensures that the sensitive cell population decreases on average over time. If this criterion is not met, the question of resistance becomes less important as the treatment schedule does not prevent the expansion of the sensitive cancer cell population. Figure 1 shows an example of the dynamics of sensitive cancer cells during continuous and pulsed drug administration.<br>
The probability of resistance<br>
Let us now calculate the probability of resistance during a given dosing schedule. Under the assumption of complete resistance, the probability of extinction of a resistant cell clone starting from one resistant cell is , regardless of which dosing strategy is used [33]. The number of resistant cells produced from the sensitive cell population is on average proportional to the number of sensitive cell divisions; these two quantities are related through the mutation rate .<br>
As a preliminary calculation, consider the behavior of the sensitive cell population, , with constant growth rate  and death rate . Under the assumption that the mutation rate  is small enough such that the stochastic emergence of resistant cells from sensitive cells has negligible effects on the sensitive cell number, we approximate  as a simple birth-death process. Recall that the initial size of the sensitive cell population is ; then the mean abundance of the sensitive cell population at time  is approximated as . The number of sensitive cell divisions in the time interval  is approximately given by(1)The number of surviving resistant cell clones arising from the sensitive cell population in the time interval  is then binomially distributed as .<br>
Let us now study the probability of resistance under a general pulsed treatment regimen. Define  as the expected number of sensitive cancer cells at the beginning of the  treatment cycle, and  as the expected number of sensitive cancer cells at the beginning of the  treatment holiday. Then we have , , , etc. We obtain the general formulae for the number of sensitive cancer cells at the beginning and end of the  treatment cycle as(2)where . The number of cycles  before extinction of the sensitive cell population is approximated as . To estimate the total number of sensitive cancer cell births before extinction, we sum the number of births during the on- and off-treatment phases over all cycles. Let  be the number of sensitive cancer cell births during the  on-treatment phase and  the number of births during the  off-treatment phase. The expected number of births, , during the entire treatment regime is then approximated as(3)Here  is the estimated number of sensitive cancer cell divisions in the treatment interval, evaluated as in equation (1). Next, let us define the functions  and . We can express each sum as the geometric series(4)<br>
Then we obtain the expected number of sensitive cancer cell births during the entire duration of therapy as . We can approximate  with . Substituting in the correct expressions for  and , we obtain a final estimate for the number of sensitive cell divisions before the extinction of sensitive cells as<br>
The number of surviving resistant cell clones produced from the sensitive cell population is a random variable with distribution . We can thus make a Poisson approximation to estimate the probability that at least one surviving resistant cell clone is produced before the extinction of sensitive cells as(5)<br>
In the special case of continuous dosing, , the number of sensitive cell divisions is approximated by(6)As a consistency check, this formula can also be arrived at via equation (1) with the initial size of the sensitive cancer cell population, , and the amount of time until the extinction of the sensitive cells, . Then the probability of resistance emerging during continuous dosing is again calculated using formula (5) with equation (6).<br>
When the (epi)genetic alteration confers partial resistance to the cell (i.e. when  and/or ), then the probability of resistance emerging during continuous dosing is given by(7)where  is again calculated as in equation (6). To accommodate this modification in pulsed schedules, we introduce ?effective? growth and death rates for resistant cells. The effective growth rate of resistant cells is given by , while the effective death rate of resistant cells is . For general pulsed schedules, the probability of resistance is then approximated by(8)where  is calculated as in equation (5).<br>
<br>
The expected number of resistant cells<br>
We next approximate the expected number of resistant cancer cells at time . To calculate this quantity, we estimate the number of surviving resistant cell clones produced during each small time interval and then calculate the growth of each resistant cell clone until time . More precisely, we take the convolution of the rate of production of resistant cell clones from the sensitive cell process with the average rate of clonal expansion of resistant cells.<br>
Let us first consider general pulsed treatment schedules. Using methods from the previous section, we find that the expected number of sensitive cancer cell divisions until time  is given byHere  denotes the fractional number of treatment cycles until time . After making the approximation , we have(9)Since the number of resistant cells produced directly from the sensitive cell population until time  is binomially distributed, , the expected number of such cells is given by . Thus we estimate the average number of resistant cells at time  as(10)<br>
In the special case of continuous dosing strategies, , the average number of resistant cells at time  is given by(11)Once again we can check for consistency by deriving this formula via equation (1). Recall that the expected number of sensitive cancer cell births starting from a population of size  until time  is given by  (equation (1)). Then the expected number of resistant cells produced is , and the expected number of resistant cells at time  is given by<br>
We are also interested in calculating the expected number of resistant cells averaged only over those patients who develop resistance. This quantity is clinically relevant since many treatment choices may inevitably lead to resistance; in those cases, the drug should be dosed in such a way that the number of resistant cells is minimum, thereby maximizing the time until detection of resistance and disease progression. Mathematically, this amounts to estimating the expected size of the resistant cell population, conditioned on the event that at least one surviving resistant clone is produced prior to the extinction of sensitive cancer cells. We make the approximation that the expected resistant cell number, conditioned on the complementary event of no surviving resistant cell clones, is negligible. Then the expected number of resistant cells averaged over the cohort of patients who develop resistance is estimated as(12)<br>
<br>
Pre-existing resistance<br>
Suppose that at the start of therapy there exists a small population of resistant cells. We may then adapt the theory to calculate the probability of resistance and expected size of the resistant clone under various dosing schedules. Let us consider the initial population as two separate populations:  sensitive cells and  resistant cells, where  is the initial fraction of resistant cells (assume for simplicity that  is an integer). Then the probability of avoiding resistance is given by the probability that the pre-existing resistant cell clones become extinct times the probability that the initial sensitive cell population does not give rise to any surviving resistant clones during treatment. Let  denote the probability, calculated as in equation (5), of de novo resistance arising from the initial sensitive population of size . The probability of extinction of the pre-existing clone is given by  if  and  otherwise. (Note that  and  may be replaced by  and  in the case of pulsed schedules with partial resistance). Then the total probability of resistance is given by(13)Let  represent the expected number of resistant cells arising from the initial sensitive cell population of size , calculated as in equation (10). The expected number of resistant cells at time  is given by  plus the expected current size of the initially resistant population. Thus we have(14)where once again the rates  and  may be replaced by their effective values in the case of pulsed therapy with partial resistance.<br>
<br>
<br>
Results<br>
Exact stochastic simulations of the process  are performed using the standard Monte Carlo technique; each time an event occurs, a cell is chosen to divide or die based on the current cell growth and death rates and the population size of each cell type. When the drug concentration changes, the cell growth and death rates are modified accordingly.<br>
Let us now investigate the fit between the analytic approximations and exact stochastic computer simulations, as well as the parameter dependence of these approximations. We first study the dependence of the probability of resistance emerging during a continuous dosing schedule on the growth rate of sensitive cancer cells during therapy,  (Fig. 2A), and their initial number,  (Fig. 2B). The simulations exhibit a good fit with the analytical approximations. As the growth rate of sensitive cells during therapy () increases, the risk of developing resistance increases as well. Similarly, as the initial size of the sensitive cancer cell population increases, the number of sensitive cell divisions until extinction becomes larger, thus enhancing the likelihood of producing a successful resistant cell clone. Hence the probability of developing resistance also increases with .<br>
We next investigate the expected number of resistant cells as a function of time for varying growth rates of sensitive and resistant cells during continuous treatment. When the sensitive cell growth rate during treatment, , increases and the growth rate of resistant cells, , is kept constant, then the expected number of resistant cells increases (Fig. 2C). This behavior is apparent from equation (11); during therapy the denominator  is always negative, thus making  the dominant term in the numerator. Therefore the growth rate of the expected population size is dominated by the growth rate of resistant cells at later times as the other time-dependent term in the numerator approaches zero. Figure 2D confirms that as  increases, the expected number of the resistant cell population also increases.<br>
Let us now consider treatment schedules incorporating pulsed doses and treatment holidays. We first investigate the dependence of the probability of resistance on the growth rates of sensitive and resistant cancer cells during treatment phases,  and , the duration of each treatment pulse, , and the initial number of sensitive cancer cells,  (Figs. 3A?D). As in the case of continuous dosing, the probability of resistance increases as  and  increase. An increase in the duration of each treatment pulse, , decreases the probability of resistance while an increase in  linearly enhances the risk of resistance. The parameter dependence of the expected number of resistant cells as a function of time is shown in Figs. 3E and 3F. The expected number of resistant cells increases with increasing growth rates of sensitive and resistant cancer cells during therapy ( and ).<br>
Optimizing dosing strategies<br>
Using the estimates derived above, we now propose a method for optimizing dosing strategies to minimize the probability of resistance. In cases where the emergence of resistance is certain, this method will predict a dosing strategy that maximally delays the detection of resistance by minimizing the number of resistant cancer cells. The optimal dosing strategy is selected from a range of tolerated treatment schedules specified by toxicity constraints. In practice, these toxicity constraints, in addition to the growth and death rates of sensitive and resistant cells at varying dose levels, must be determined experimentally for each drug and cancer type. In the following we will construct example toxicity constraints to demonstrate the methodology and test for sensitivity to the constraint profile.<br>
A modification of treatment schedules can change the duration of each treatment pulse (affecting  and ), the intensity of the dose (affecting growth and death rates of sensitive cancer cells,  and ), or both. When considering complete resistance, the growth and death rates of resistant cells are unaltered by changing treatment strategies. We assume that all other parameters are unaffected by changes in administration schedules as well. Thus, we consider toxicity constraints to provide a bounded domain in the four-dimensional parameter space spanning , and . We can immediately reduce the dimension of the constraint domain to two, since  specifies  explicitly through the fixed length of the treatment-and-break cycle, , and  and  are both dependent on the concentration of the drug and thus cannot vary independently. Therefore, we consider toxicity constraints in the form of a function specifying the maximum amount of time, , that a drug can be administered to a patient at a particular concentration before causing dose-limiting toxicities. In the following, we make the simplifying assumption that this drug concentration specifies the death rate of sensitive cancer cells, , and does not alter the growth rate, ; alternatively, we can also investigate treatment strategies that modulate the growth rate rather than the death rate of sensitive cancer cells, or both. We assume that such relationships between  and  are monotonically decreasing functions of ; see Figure 4A for an example of a toxicity constraint.<br>
From clinical trial data we obtain the maximum amount of time for which a range of drug concentrations are tolerated, leading to a relationship between  and the drug concentration. The effect of particular drug concentrations on  and/or  may then be found experimentally by exposing sensitive cancer cells to drug doses and measuring the growth and death rates . Such investigations identify a toxicity constraint relating  and . We display example constraint functions in Figures 4B and 4C.<br>
We next show some example toxicity data for the targeted drug erlotinib, which is an EGFR inhibitor used in treating solid malignancies such as non-small cell lung cancer. Compiling data from several clinical trials [34]?[36], we obtain a relationship between the drug dose and plasma concentration (measured as the maximum concentration achieved after a single dose). This data is plotted in Figure 5A; here we observe a relatively linear relationship between dose and plasma concentration. We also compiled data points on the number of days each particular dose was tolerated in continuous daily administration. We converted each dose level to concentration using the linear relationship found, and plot these points in Figure 5B. A conservative toxicity constraint in terms of  vs. concentration is plotted, where we assume that any concentration or length of pulse increased beyond what was tolerated in the trials would not be admissible. This toxicity constraint, in conjunction with further experimental data on the growth and death rates of sensitive and resistant cancer cells at various concentrations, would enable us to calculate optimal dosing schedules for this specific system using our model.<br>
For our theoretical investigations, we now introduce several example families of toxicity constraints to test for sensitivity of the probability of resistance to several key aspects of the shape of the curve. All of these example constraints are convex, monotonically decreasing functions of . Thus we have implicitly assumed that as the drug concentration increases and the cell death rate increases, the maximum tolerated length of a treatment pulse decreases. In the first family, we vary the maximum dose that can be tolerated for the full treatment schedule of  days. In the second family we vary the maximum dose that can be tolerated for just one day, and in the third family we vary the degree of convexity of the constraint curve, or the initial rate of decrease in  as the concentration increases.<br>
Consider the first family of toxicity curves in Figure 6A, specified by(15)where  for  and . In our notation, , the subscript  denotes the constraint family and the superscript  indicates a specific function belonging to this family. These constraints serve to vary the endpoint representing the maximum dose that can be tolerated for a full treatment cycle ( days) while fixing the endpoint representing the maximum dose that is tolerated for just one day of a treatment cycle, specified by the death rate . In other words, in this family of constraints we vary the continuous-dose concentration endpoint (represented by black circles in Figure 6A) of the toxicity constraint via the parameter , while keeping the form of the constraint and the high-dose concentration endpoint fixed.<br>
We also test for sensitivity to two other aspects of the toxicity constraints: the high-dose concentration endpoint (i.e. the maximum dose that is tolerated for just one day) and the degree of concavity of the curve. Figure 6B shows a family of constraints varying the high-dose endpoints (shown in black circles). These example constraints are specified by equation(16)where  for  and . Likewise, a family of constraints varying the degree of convexity is exhibited in Figure 6C and specified by the following equations:(17)where, for each function, the  and  are determined by setting the endpoints to be(18)<br>
Once the toxicity constraint is established (e.g. Figure 4), the tolerable range of treatment schedules is specified by the area under the curve on the . We then aim to locate the optimal point within this area that minimizes the probability of resistance. In situations in which the optimum probability of resistance is 1 or close to 1, we aim to locate the optimal point minimizing the expected number of resistant cells conditional on developing resistance, thus maximizing the time until disease progression. We note from the analytical approximations that a change in the mutation rate  does not modify the choice of optimal dosing schedule. We also observe from our analytical approximations that the optimizing points must always lie directly on the toxicity constraint curve itself ? intuitively, any point lying below the toxicity curve represents a weaker than tolerated dosing schedule and hence cannot minimize the risk of resistance. Once a minimizing point is located, the optimal treatment schedule is entirely specified since the duration of treatment pulses are given by , the length of the drug holiday is given as the remainder of the cycle duration , and the intensity of the dose is specified by the death rate of sensitive cells, .<br>
To illustrate this concept, let us consider the toxicity constraint  from equation (15). Recall the constraint  restricting the treatment schedules to viable dosing strategies in which the population of sensitive cells decreases overall in time. This constraint may restrict the domain of the toxicity curve to a limited range of . For the current example, this restriction is shown in Figure 5. We can then calculate the probability of resistance, the expected number of resistant cells, and the conditional expected number of resistant cells over the range of treatment schedules specified by this restricted constraint curve. Note again that in the formula describing the expected number of resistant cells, equation (10), the growth rate of  is dominated by the growth rate of resistant cells, , at later times, since the other time-dependent term in the expression, , approaches zero as  increases. Thus we can neglect the latter term when considering the long-term growth of the resistant cell population. Rewrite equation (10) as . Here  is the time-independent constant comprised of the remaining terms in equation (10) except for . Analogously, the expected number of resistant cells conditional to the emergence of resistance is approximated by .<br>
In Figures 7A?C, we show the probability of resistance, , the time-independent term of the equation describing the number of resistant cells, , and  over the range of treatment schedules specified by the restricted constraint curve from Figure 6D. As the drug concentration and hence the death rate of sensitive cancer cells, , increase, we move along the constraint curve from the continuous-dose endpoint towards the high-dose endpoint. As a particular numerical example, consider an initial number of sensitive cancer cells of , a mutation rate conferring resistance of , and a neutral resistance mutation (). Then the probability of resistance, shown in Figure 7A, is minimized when . This result is subsequently used to identify the corresponding optimal treatment schedule in Figure 6D, which in this case is given by  days,  days, and a drug concentration achieving . When this optimal treatment schedule is used, the probability of resistance is below 10%. However, if a higher dose is chosen, the probability of resistance may increase up to . This example illustrates the importance of locating the optimal dosing regime for the clinical management of patients.<br>
The values proportional to the expected number of resistant cells, , and to the conditional expected number of resistant cells, , are displayed in Figures 7B and 7C. Interestingly, in the event that resistance occurs, the optimal treatment schedule for minimizing the resistant cell population is specified by , which differs from the optimal schedule for minimizing the probability of developing resistance. For a general cohort of patients treated with this dosing schedule, the probability of developing resistance would be close to 1; for the subset of patients who do develop resistance, however, this dosing schedule would delay disease progression by the largest amount of time.<br>
<br>
Parameter dependence of optimal dosing strategies<br>
Let us now examine the dependence of these optimal dosing regimens on variations in parameters and toxicity constraints. Specifically, we investigate the sensitivity of the optimal dosing strategies to several characteristics of the toxicity curves: the maximum dose that can be administered for the whole treatment cycle of  days (the continuous-dose endpoint), the maximum dose that can be administered for one day only (the high-dose endpoint), and the degree of concavity of the toxicity curve. The optimal dosing regimens are identified over a range of parameter values of  and .<br>
First, we consider the family of curves  for  (equations (15) and (14) as shown in Figure 6A). The optimal dosing strategy minimizing the probability of resistance and/or the conditional number of resistant cells is displayed in Figure 8. In column (A), we show the value of  that corresponds to the dosing schedule which minimizes the probability of resistance for a given  and . The corresponding minimal probability of resistance is shown in column (C). Column (B) displays the value of  that specifies the dosing schedule minimizing the conditional expected number of resistant cells, i.e. maximizing the amount of time until disease progression in patients who develop resistance. The rows show the results for constraints , and , respectively. Note that the optimal dosing schedules in the first and second column are not identical, reflecting the fact that the recommended dosing regimens for these two clinical goals are different. In addition, we observe that as the continuous-dose endpoint is varied, the minimal probability of resistance changes (in column (C)) while the optimal dosing schedules remain relatively unchanged. In particular, the minimal probability of resistance decreases as the continuous-dose endpoint shifts to the right.<br>
Next we consider the family of curves , for  (equations (15) and (16), shown in Figure 6B). We plot the results in Figure 9, where the columns show the optimal treatment schedules and the probability of resistance for constraints , and , respectively. For both clinical goals of minimizing the probability of resistance and maximizing the time until detection of resistance, we observe that as the maximum dose tolerated for one day (the high-dose endpoint) is increased, the optimal dosing schedule shifts slightly to a more high-dose pulsed regimen in some regions of the parameter space (particularly when  is small). However, the minimal probability of resistance changes only slightly as this endpoint is increased.<br>
Lastly, we consider the family of curves , for  (equation (17), shown in Figure 6C). We plot the results in Figure 10. The columns again show the optimal schedules and the probability of resistance for constraints , and . The results for the first two constraints,  and , differ markedly from those of . In particular, for functions with a lower degree of convexity, a high-dose pulsed treatment is optimal for both clinical goals. For these cases a minimal probability of resistance near zero can be achieved. However, for  the optimal dosing schedule shifts more towards the continuous end of the dosing spectrum, and in certain parameter ranges the minimal probability of resistance reaches higher values.<br>
<br>
Non-cytoreductive therapies<br>
So far we have only considered treatment strategies during which the total number of sensitive cancer cells declines on average, i.e. when  holds. However, for some therapies and cancer types it is impossible to reduce the number of sensitive cancer cells. Then the goal of therapy becomes to slow or even halt the rate of tumor growth. For these cases, the probability of resistance is always one. However, we can still identify treatment schedules that maximally delay progression of disease by controlling the number of resistant cells. The approximations for the expected number of resistant cells derived above remain valid, except when . In this case, we revisit the calculation of  and estimate the total number of births during on- and off-treatment phases as(19)Once again making the approximation , we obtainAfter taking the convolution of the derivative  with the expected growth rate of resistant cells, we obtain the expected number of resistant cells at time  asNote that for cases when , the formula for the expected number of resistant cells, equation (10), experiences a singularity in the denominator when , i.e. when the net growth rate of the resistant cancer cells equals the net growth rate of the sensitive cancer cells. However, the range of therapies considered should be restricted to those in which the net growth rate of sensitive cancer cells is less than that of resistant cancer cells; otherwise, the problem of resistance is secondary to the problem of controlling the sensitive cell population. In these cases, the singularity does not occur.<br>
<br>
<br>
Discussion<br>
In this paper, we have constructed a simple mathematical model using birth and death processes to describe the evolution of resistance during targeted anti-cancer therapy. We have derived and validated analytical approximations to this model, which provide a useful tool for predicting the risk of resistance and the growth of resistant cell populations under various dosing strategies. We have used our model and estimates to develop a methodology for designing optimal drug administration strategies to minimize the risk of resistance. In cases in which the risk of resistance is high for any treatment schedule, these strategies are modified to maximize the time until the progression of disease.<br>
The probability of resistance is shown to be largely dependent on , the rate of sensitive cell division, which is the product of the current sensitive cell population size and its growth rate. Drugs whose main goal is to increase the death rate of sensitive cells can decrease the sensitive cell population, thus decreasing  and reducing the probability of resistance; however, if the initial tumor size is large, it may take a significant amount of time to deplete the sensitive cell population. During this delay, there is still a high probability of generating resistant mutants since the sensitive cell proliferation rate is unchanged. On the other hand, for drugs that inhibit sensitive cell proliferation and effectively reduce the growth rate of sensitive cells, the quantity  is immediately reduced to zero regardless of the initial size of the tumor. This implies that drugs that inhibit cancer cell proliferation could be promising for the prevention of resistance in the absence of pre-existing resistant cell clones. Combination therapies in which an inhibitor of sensitive cell proliferation is dosed continuously while short, high pulses of a drug that increases the death rate of resistant cells are administered may also be of interest, as are any combination strategies which separately target the sensitive and resistant populations.<br>
We have also extended the theory to incorporate pre-existing resistant cells at the start of therapy. The effect of pre-existing resistant clones on the optimal dosing strategy is highly dependent upon system parameters including the growth and death rates of sensitive and resistant cells, the initial tumor size, and initial number of resistant cells. Consider the probability of resistance in this scenario, given by equation (13). We note that the term , denoting the probability of extinction of the pre-existing clone, consists of the  power of a quantity usually less than one . Thus even a small population of pre-existing resistant cells can cause the total probability of resistance to be effectively equal to one. For example, if the growth rate of resistant cells is twice their death rate, then the probability of extinction for an initial population consisting of only  resistant cells evaluates to . Then the total probability of resistance, given by equation (13), is approximately one. Therefore, the presence of even a small number of resistant cells at the start of therapy can effectively prevent a cure. In these cases, we may instead attempt to delay disease progression by controlling the number of resistant cells. Equation (14) describes the current size of the resistant population as the sum of the average de novo and pre-existing resistant clone sizes. Observe that both terms in this expression grow at the same exponential rate; the term for pre-existing resistance starts at time zero with the value , while the term for de novo resistance starts with value zero at time zero. This fact has implications for treatment schedules in the case of pre-existing resistance: as long as an eventual decline of sensitive cancer cells is achieved, high-dose strategies which slow the effective net growth rate of resistant cells may be more effective than low-dose strategies aimed at maximal continuous inhibition of sensitive cells.<br>
By testing several families of toxicity constraints, we have observed that the optimal dosing strategies are strongly affected by the degree of convexity of the toxicity curve, thus delineating a clear priority in experimental efforts to determine the parameters of this constraint. In our experience of studying published results of Phase I clinical trials of molecularly targeted anti-cancer therapies, patient toxicity reports are usually not detailed enough to accurately determine toxicity curves. In light of our observations, we would like to stress the importance of publishing detailed quantitative data on toxicity in clinical trials, so that statistical analyses can be performed to inform these constraint curves. It is also important to estimate the growth and death rates of sensitive and resistant cancer cells during administration of diverse drug concentrations. These curves can be estimated by studying the growth and death kinetics of cancer cells, either in vivo or in vitro. For example, in vitro net growth rates can be determined by subjecting sensitive and resistant cell populations to drug at varying concentrations and counting viable cells at multiple time points. Then, through fluorescence-activated cell sorting techniques, the amount of cell death at multiple time points can be observed, providing the cell death rate at each drug concentration. If the parameters of the model are also estimated for treatment with conventional cytotoxic chemotherapeutics, then our model can be applied to these treatment choices as well. This methodology, together with key parameters derived experimentally, can aid in the design of optimum administration strategies of treatment options for all cancer types that evolve resistance via a single (epi)genetic alteration.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2855316</b><br>
Global Entrainment of Transcriptional Systems to Periodic Inputs<br>
This paper addresses the problem of providing mathematical conditions that allow one to ensure that biological networks, such as transcriptional systems, can be globally entrained to external periodic inputs. Despite appearing obvious at first, this is by no means a generic property of nonlinear dynamical systems. Through the use of contraction theory, a powerful tool from dynamical systems theory, it is shown that certain systems driven by external periodic signals have the property that all their solutions converge to a fixed limit cycle. General results are proved, and the properties are verified in the specific cases of models of transcriptional systems as well as constructs of interest in synthetic biology. A self-contained exposition of all needed results is given in the paper.<br>
<br>
Introduction<br>
Periodic, clock-like rhythms pervade nature and regulate the function of all living organisms. For instance, circadian rhythms are regulated by an endogenous biological clock entrained by the light signals from the environment that then acts as a pacemaker [1]. Moreover, such an entrainment can be obtained even if daily variations are present, like e.g. temperature and light variations. Another important example of entrainment in biological systems is at the molecular level, where the synchronization of several cellular processes is regulated by the cell cycle [2].<br>
An important question in mathematical and computational biology is that of finding conditions ensuring that entrainment occurs. The objective is to identify classes of biological systems that can be entrained by an exogenous signal. To solve this problem, modelers often resort to simulations in order to show the existence of periodic solutions in the system of interest. Simulations, however, can never prove that solutions will exist for all parameter values, and they are subject to numerical errors. Moreover, robustness of entrained solutions needs to be checked in the presence of noise and uncertainties, which cannot be avoided experimentally.<br>
From a mathematical viewpoint, the problem of formally showing that entrainment takes place is known to be extremely difficult. Indeed, if a stable linear time-invariant model is used to represent the system of interest, then entrainment is usually expected, when the system is driven by an external periodic input, with the system response being a filtered, shifted version of the external driving signal. However, in general, as is often the case in biology, models are nonlinear. The response of nonlinear systems to periodic inputs is the subject of much current systems biology experimentation; for example, in [3], the case of a cell signaling system driven by a periodic square-wave input is considered. From measurements of a periodic output, the authors fit a transfer function to the system, implicitly modeling the system as linear even though (as stated in the Supplemental Materials to [3]) there are saturation effects so the true system is nonlinear. For nonlinear systems, driving the system by an external periodic signal does not guarantee the system response to also be a periodic solution, as nonlinear systems can exhibit harmonic generation or suppression and complex behavior such as chaos or quasi-periodic solutions [4]. This may happen even if the system is well-behaved with respect to constant inputs; for example, there are systems which converge to a fixed steady state no matter what is the input excitation, so long as this input signal is constant, yet respond chaotically to the simplest oscillatory input; we outline such an example in the Materials and Methods Section, see also [5]. Thus, a most interesting open problem is that of finding conditions for the entrainment to external inputs of biological systems modeled by sets of nonlinear differential equations.<br>
One approach to analyzing the convergence behavior of nonlinear dynamical systems is to use Lyapunov functions. However, in biological applications, the appropriate Lyapunov functions are not always easy to find and, moreover, convergence is not guaranteed in general in the presence of noise and/or uncertainties. Also, such an approach can be hard to apply to the case of non-autonomous systems (that is, dynamical systems directly dependent on time), as is the case when dealing with periodically forced systems.<br>
The above limitations can be overcome if the convergence problem is interpreted as a property of all trajectories, asking that all solutions converge towards one another (contraction). This is the viewpoint of contraction theory, [6], [7], and more generally incremental stability methods [8]. Global results are possible, and these are robust to noise, in the sense that, if a system satisfies a contraction property then trajectories remain bounded in the phase space [9]. Contraction theory has a long history. Contractions in metric functional spaces can be traced back to the work of Banach and Caccioppoli [10] and, in the field of dynamical systems, to [11] and even to [12] (see also [13], [8], and e.g. [14] for a more exhaustive list of related references). Contraction theory has been successfully applied to both nonlinear control and observer problems, [7], [15] and, more recently, to synchronization and consensus problems in complex networks [16], [17],[18]. In [19] it was proposed that contraction can be particularly useful when dealing with the analysis and characterization of biological networks. In particular, it was found that using non Euclidean norms, as also suggested in [6] (Sec. 3.7ii), can be particularly effective in this context [19], [20].<br>
One of the objectives of this paper is to give a self-contained exposition, with all proofs included, of results in contraction theory as applied to entrainment of periodic signals, and, moreover, to show their applicability to problems of biological interest. We believe that contraction analysis should be recognized as an important component of the ?toolkit? of systems biology, and this paper should be useful to other researchers contemplating the use of these tools.<br>
For concreteness, we focus mainly on transcriptional systems, as well as related biochemical systems, which are basic building blocks for more complex biochemical systems. However, the results that we obtain are of more generality. To illustrate this generality, and to emphasize the use of our techniques in synthetic biology design, we discuss as well the entrainment of a Repressilator circuit in a parameter regime in which endogenous oscillations do not occur, as well as the synchronization of a network of Repressilators. A surprising fact is that, for these applications, and contrary to many engineering applications, norms other than Euclidean, and associated matrix measures, must be considered.<br>
Mathematical tools<br>
We consider in this paper systems of ordinary differential equations, generally time-dependent:(1)defined for  and , where  is a subset of . It will be assumed that  is differentiable on , and that , as well as the Jacobian of  with respect to , denoted as , are both continuous in . In applications of the theory, it is often the case that  will be a closed set, for example given by non-negativity constraints on variables as well as linear equalities representing mass-conservation laws. For a non-open set , differentiability in  means that the vector field  can be extended as a differentiable function to some open set which includes , and the continuity hypotheses with respect to  hold on this open set.<br>
We denote by  the value of the solution  at time  of the differential equation (1) with initial value . It is implicit in the notation that  (?forward invariance? of the state set ). This solution is in principle defined only on some interval , but we will assume that  is defined for all . Conditions which guarantee such a ?forward-completeness? property are often satisfied in biological applications, for example whenever the set  is closed and bounded, or whenever the vector field  is bounded. (See Appendix C in [21] for more discussion, as well as [22] for a characterization of the forward completeness property.) Under the stated assumptions, the function  is jointly differentiable in all its arguments (this is a standard fact on well-posedness of differential equations, see for example Appendix C in [21]).<br>
We recall (see for instance [23]) that, given a vector norm on Euclidean space (), with its induced matrix norm , the associated matrix measure  is defined as the directional derivative of the matrix norm, that is,For example, if  is the standard Euclidean 2-norm, then  is the maximum eigenvalue of the symmetric part of . As we shall see, however, different norms will be useful for our applications. Matrix measures are also known as ?logarithmic norms?, a concept independently introduced by Germund Dahlquist and Sergei Lozinskii in 1959, [24],[25]. The limit is known to exist, and the convergence is monotonic, see [24], [26].<br>
We will say that system (1) is infinitesimally contracting on a convex set  if there exists some norm in , with associated matrix measure  such that, for some constant ,(2)<br>
Let us discuss informally (rigorous proofs are given later) the motivation for this concept. Since by assumption  is continuously differentiable, the following exact differential relation can be obtained from (1):(3)where, as before,  denotes the Jacobian of the vector field , as a function of  and , and where  denotes a small change in states and ?? means , evaluated along a trajectory. (In mechanics, as in [27],  is called ?virtual displacement?, and formally it may be thought of as a linear tangent differential form, differentiable with respect to time.) Consider now two neighboring trajectories of (1), evolving in , and the virtual displacements between them. Note that (3) can be thought of as a linear time-varying dynamical system of the form:once that  is thought of as a fixed function of time. Hence, an upper bound for the magnitude of its solutions can be obtained by means of the Coppel inequality [28], yielding:(4)where  is the matrix measure of the system Jacobian induced by the norm being considered on the states and . Using (4) and (2), we have thatThus, trajectories starting from infinitesimally close initial conditions converge exponentially towards each other. In what follows we will refer to  as contraction (or convergence) rate.<br>
The key theoretical result about contracting systems links infinitesimal and global contractivity, and is stated below. This result can be traced, under different technical assumptions, to e.g. [6], [13], [12], [11].<br>
Theorem 1. Suppose that  is a convex subset of  and that  is infinitesimally contracting with contraction rate . Then, for every two solutions  and  of (1), it holds that:(5)<br>
In other words, infinitesimal contractivity implies global contractivity. In the Materials and Methods section, we provide a self-contained proof of Theorem 1. In fact, the result is shown there in a generalized form, in which convexity is replaced by a weaker constraint on the geometry of the space.<br>
In actual applications, often one is given a system which depends implicitly on the time, , by means of a continuous function , i.e. systems dynamics are represented by . In this case,  (where  is some subset of ), represents an external input. It is important to observe that the contractivity property does not require any prior information about this external input. In fact, since  does not depend on the system state variables, when checking the property, it may be viewed as a constant parameter, . Thus, if contractivity of  holds uniformly , then it will also hold for .<br>
Given a number , we will say that system (1) is -periodic if it holds thatNotice that the system  is -periodic, if the external input, , is itself a periodic function of period .<br>
The following is the basic theoretical result about periodic orbits that will be used in the paper. A proof may be found in [6], Sec. 3.7.vi.<br>
Theorem 2. Suppose that:<br>
Then, there is a unique periodic solution  of (1) of period  and, for every solution , it holds that  as .<br>
In the Materials and Methods section of this paper, we provide a self-contained proof of Theorem 2, in a generalized form which does not require convexity.<br>
<br>
A simple example<br>
As a first example to illustrate the application of the concepts introduced so far, we choose a simple bimolecular reaction, in which a molecule of  and one of  can reversibly combine to produce a molecule of .<br>
This system can be modeled by the following set of differential equations:(6)where we are using  to denote the concentration of  and so forth. The system evolves in the positive orthant of . Solutions satisfy (stoichiometry) constraints:(7)for some constants  and .<br>
We will assume that one or both of the ?kinetic constants?  are time-varying, with period . Such a situation arises when the 's depend on concentrations of additional enzymes, which are available in large amounts compared to the concentrations of , but whose concentrations are periodically varying. The only assumption will be that  and  for all .<br>
Because of the conservation laws (7), we may restrict our study to the equation for . Once that all solutions of this equation are shown to globally converge to a periodic orbit, the same will follow for  and . We have that:(8)Because  and , this system is studied on the subset of  defined by . The equation can be rewritten as:(9)Differentiation with respect to  of the right-hand side in the above system yields this () Jacobian:(10)Since we know that  and , it follows thatfor . Using any norm (this example is in dimension one) we have that . So (6) is contracting and, by means of Theorem 2, solutions will globally converge to a unique solution of period  (notice that such a solution depends on system parameters).<br>
Figure 1 shows the behavior of the dynamical system (9), using two different values of . Notice that the asymptotic behavior of the system depends on the particular choice of the biochemical parameters being used. Furthermore, it is worth noticing here that the higher the value of , the faster will be the convergence to the attractor.<br>
<br>
<br>
Results<br>
Mathematical model and problem statement<br>
We study a general externally-driven transcriptional module. We assume that the rate of production of a transcription factor  is proportional to the value of a time dependent input function , and  is subject to degradation and/or dilution at a linear rate. (Later, we generalize the model to also allow nonlinear degradation as well.) The signal  might be an external input, or it might represent the concentration of an enzyme or of a second messenger that activates . In turn,  drives a downstream transcriptional module by binding to a promoter (or substrate), denoted by  with concentration . The binding reaction of  with  is reversible and given by:where  is the complex protein-promoter, and the binding and dissociation rates are  and  respectively. As the promoter is not subject to decay, its total concentration, , is conserved, so that the following conservation relation holds:(11)We wish to study the behavior of solutions of the system that couples  and , and specifically to show that, when the input  is periodic with period , this coupled system has the property that all solutions converge to some globally attracting limit cycle whose period is also .<br>
Such transcriptional modules are ubiquitous in biology, natural as well as synthetic, and their behavior was recently studied in [29] in the context of ?retroactivity? (impedance or load) effects. If we think of  as the concentration of a protein  that is a transcription factor for , and we ignore fast mRNA dynamics, such a system can be schematically represented as in Figure 2, which is adapted from [29]. Notice that  here does not need to be the concentration of a transcriptional activator of  for our results to hold. The results will be valid for any mathematical model for the concentrations, , of  and , of  (the concentration of  is conserved) of the form:(12)<br>
An objective in this paper is, thus, to show that, when  is a periodic input, all solutions of system (12) converge to a (unique) limit cycle (Figure 3). The key tool in this analysis is to show that uniform contractivity holds. Since in this example the input appears additively, uniform contractivity is simply the requirement that the unforced system () is contractive. Thus, the main step will be to establish the following technical result, see the Material and Methods:<br>
Proposition 1. The systemwhere(13)for all , and , , , and  are arbitrary positive constants, is contracting.<br>
Appealing to Theorem 2, we then have the following immediate Corollary:<br>
Proposition 2. For any given nonnegative periodic input  of period , all solutions of system (12) converge exponentially to a periodic solution of period .<br>
In the following sections, we introduce a matrix measure that will help establish contractivity, and we prove Proposition 1. We will also discuss several extensions of this result, allowing the consideration of multiple driven subsystems as well as more general nonlinear systems with a similar structure. (A graphical algorithm to prove contraction of generic networks of nonlinear systems can also be found in [18] where this transcriptional module is also studied.)<br>
<br>
Proof of Proposition 1<br>
We will use Theorem 2. The Jacobian matrix to be studied is:(14)As matrix measure, we will use the measure  induced by the vector norm , where  is a suitable nonsingular matrix. More specifically, we will pick  diagonal:(15)where  and  are two positive numbers to be appropriately chosen depending on the parameters defining the system.<br>
It follows from general facts about matrix measures that(16)where  is the measure associated to the  norm and is explicitly given by the following formula:(17)Observe that, if the entries of  are negative, then asking that  amounts to a column diagonal dominance condition. (The above formula is for real matrices. If complex matrices would be considered, then the term  should be replaced by its real part .)<br>
Thus, the first step in computing  is to calculate :(18)Using (17), we obtain:(19)Note that we are not interested in calculating the exact value for the above measure, but just in ensuring that it is negative. To guarantee that , the following two conditions must hold:(20)(21)Thus, the problem becomes that of checking if there exists an appropriate range of values for ,  that satisfy (20) and (21) simultaneously.<br>
The left hand side of (21) can be written as:(22)which is negative if and only if . In particular, in this case we have:The idea is now to ensure negativity of (20) by using appropriate values for  and  which fulfill the above constraint. Recall that the term  because of the choice of the state space (this quantity represents a concentration). Thus, the left hand side of (20) becomes(23)The next step is to choose appropriately  and  (without violating the constraint ). Imposing , , (23) becomes(24)Then, we have to choose an appropriate value for  in order to make the above quantity uniformly negative. In particular, (24) is uniformly negative if and only if(25)We can now choosewith . In this case, (24) becomesThus, choosing  and , with , we have . Furthermore, the contraction rate , is given by:Notice that  depends on both system parameters and on the elements , , i.e. it depends on the particular metric chosen to prove contraction. This completes the proof of the Proposition.<br>
<br>
Generalizations<br>
In this Section, we discuss various generalizations that use the same proof technique.<br>
Assuming  activation by enzyme kinetics<br>
The previous model assumed that  was created in proportion to the amount of external signal . While this may be a natural assumption if  is a transcription factor that controls the expression of , a different model applies if, instead, the ?active? form  is obtained from an ?inactive? form , for example through a phosphorylation reaction which is catalyzed by a kinase whose abundance is represented by . Suppose that  can also be constitutively deactivated. Thus, the complete system of reactions consists oftogether withwhere the forward reaction depends on . Since the concentrations of  must remain constant, let us say at a value , we eliminate  and have:(26)<br>
We will prove that if  is periodic and positive, i.e. , then a globally attracting limit cycle exists. Namely, it will be shown, after having performed a linear coordinate transformation, that there exists a negative matrix measure for the system of interest.<br>
Consider, indeed, the following change of the state variables:(27)The system dynamics then becomes:(28)As matrix measure, we will now use the measure  induced by the vector norm . (Notice that this time, the matrix  is the identity matrix).<br>
Given a real matrix , the matrix measure  is explicitly given by the following formula (see e.g. [23]):(29)(Observe that this is a row-dominance condition, in contrast to the dual column-dominance condition used for .)<br>
Differentiation of (28) yields the Jacobian matrix:Thus, it immediately follow from (29) that  is negative if and only if:(30)(31)The first inequality is clearly satisfied since by hypotheses both system parameters and the periodic input  are positive. In particular, we have:By using (27) (recall that ), the right hand side of the second inequality can be written as:Since all system parameters are positive and , the above quantity is negative and upper bounded by .<br>
Thus, we have that , where:The contraction property for the system is then proved. By means of Theorem 2, we can then conclude that the system can be entrained by any periodic input.<br>
Simulation results are presented in Figure 4, where the presence of a stable limit cycle having the same period as  is shown.<br>
<br>
Multiple driven systems<br>
We may also treat the case in which the species  regulates multiple downstream transcriptional modules which act independently from each other, as shown in Figure 5. The biochemical parameters defining the different downstream modules may be different from each other, representing a situation in which the transcription factor  regulates different species. After proving a general result on oscillations, and assuming that parameters satisfy the retroactivity estimates discussed in [29], one may in this fashion design a single input-multi output module in which e.g. the outputs are periodic functions with different mean values, settling times, and so forth.<br>
We denote by  the various promoters, and use  to denote the concentrations of the respective promoters complexed with . The resulting mathematical model becomes:(32)<br>
We consider the corresponding system with no input first, assuming that the states satisfy  and  for all .<br>
Our generalization can be stated as follows:<br>
Proposition 3. System (32) with no input (i.e. ) is contracting. Hence, if  is a non-zero periodic input, its solutions exponentially converge towards a periodic orbit of the same period as .<br>
Proof. We only outline the proof, since it is similar to the proof of Proposition 2. We employ the following matrix measure:(33)where(34)and the scalars  have to be chosen appropriately ().<br>
In this case,(35)and(36)<br>
Hence, the  inequalities to be satisfied are:(37)and(38)<br>
Clearly, the set of inequalities above admits a solution. Indeed, the left hand side of (38) can be recast aswhich is negative definite if and only if  for all . Specifically, in this case we haveAlso, from (37), as  for all , we have that (37) can be rewritten as:Since , we can impose  (with ) and the above inequality becomesClearly, such inequality is satisfied if we choose  sufficiently small; namely:Following a similar derivation to that of the previous Section, we can choosewith . In this case, we have:Thus, , whereThe second part of the Proposition is then proved by applying Theorem 2.<br>
In Figure 6 the behavior of two-driven downstream transcriptional modules is shown. Notice that both the downstream modules are entrained by the periodic input , but their steady state behavior is different.<br>
Notice that, by the same arguments used above, it can be proven that(39)is contracting.<br>
<br>
Transcriptional cascades<br>
A cascade of (infinitesimally) contracting systems is also (infinitesimally) contracting [6], [30] (see Materials and Methods for an alternative proof). This implies that any transcriptional cascade, will also give rise to a contracting system, and, in particular, will entrain to periodic inputs. By a transcriptional cascade we mean a system as shown in Figure 7. In this figure, we interpret the intermediate variables  as transcription factors, making the simplifying assumption that TF concentration is proportional to active promoter for the corresponding gene. (More complex models, incorporating transcription, translation, and post-translational modifications could themselves, in turn, be modeled as cascades of contracting systems.)<br>
<br>
More abstract systems<br>
We can extend our results even further, to a larger class of nonlinear systems, as long as the same general structure is present. This can be useful for example to design new synthetic transcription modules or to analyze the entrainment properties of general biological systems. We start with a discussion of a two dimensional system of the form:(40)In molecular biology,  would typically represent a nonlinear degradation, for instance in Michaelis-Menten form, while the function  represents the interaction between  and . The aim of this Section is to find conditions on the degradation and interaction terms that allow one to show contractivity of the unforced (no input ) system, and hence existence of globally attracting limit cycles.<br>
We assume that the state space  is compact (closed and bounded) as well as convex. Since the input appears additively, we must prove contractivity of the unforced system.<br>
Theorem 3. System (40), without inputs , evolving on a convex compact subset of phase space is contracting, provided that the following conditions are all satisfied, for each :<br>
Notice that the last condition is automatically satisfied if , because .<br>
As before, we prove contraction by constructing an appropriate negative measure for the Jacobian of the vector field. In this case, the Jacobian matrix is:(41)Once again, as matrix measure we will use:(42)with (43)and  appropriately chosen.<br>
Using (42) we have(44)Following the same steps as the proof of Proposition 1, we have to show that:(45)(46)<br>
Clearly, if  for every  and , the first inequality is satisfied, withTo prove the theorem we need to show that there exists  and  satisfying (46). For such inequality, since  does not change sign in  by hypothesis, we have two possibilities:<br>
In the first case, the right hand side of (46) becomes(47)Choosing , with , we have:Specifically, if we now pickwhere  and , we have that the above quantity is uniformly negative definite, i.e.In the second case, the right hand side of (46) becomes(48)Again, by choosing , with , we have the following upper bound for the expression in (48):(49)Thus, it follows that  provided that the above quantity is uniformly negative definite. Since, by hypotheses,(50)then . The proof of the Theorem is now complete.<br>
From a biological viewpoint, the hardest hypothesis to satisfy in Theorem 3 might be that on the derivatives of . However, it is possible to relax the hypothesis on  if the rate of change of  with respect to , i.e. , is sufficiently larger than . In particular, the following result can be proved.<br>
Theorem 4. System (40), without inputs , evolving on a convex compact set, is contractive provided that:<br>
Proof. The proof is similar to that of Theorem 3. In particular, we can repeat the same derivation to obtain again inequality (46). Thence, as no hypothesis is made on the sign of , choosing  we have(51)Thus, it follows that, if , then   such that , implying contractivity. The above condition is satisfied by hypotheses, hence the theorem is proved.<br>
<br>
Remarks<br>
Theorems 3 and 4 show the possibility of designing with high flexibility the self-degradation and interaction functions for an input-output module.<br>
This flexibility can be further increased, for example in the following ways:<br>
<br>
<br>
Applications to synthetic biology<br>
We introduced above a methodology for checking if a given transcriptional module can be entrained to some periodic input. The aim of this section is to show that our methodology can serve as an effective tool for designing synthetic biological circuits that are entrained to some desired external input.<br>
In particular, we will consider the synthetic biological oscillator known as the Repressilator [31], for which an additional coupling module has been recently proposed in [32]. A numerical investigation of the synchronization of a network of non-identical Repressilators was independently reported in [33].<br>
We will show that our results can be used to isolate a set of biochemical parameters for which one can guarantee the entrainment to any external periodic signal of this synthetic biological circuit. In what follows, we will use the equations presented in [32] to model the Repressilator and the additional coupling model.<br>
Entrainment using an intra-cellular auto-inducer<br>
The Repressilator is a synthetic biological circuit that consists of three genes that inhibit each other in a cyclic way [31]. As shown in Figure 8, gene lacI (associated to the state variable  in the model) expresses protein LacI (), which inhibits the transcription of gene tetR (). This translates into protein TetR (), which inhibits transcription of gene cI (). Finally, the protein CI (), translated from cI, inhibits expression of lacI, completing the cycle.<br>
In Figure 9 a modular addition to the three-genes circuit is presented. The module was first presented in [32] and makes the Repressilator circuit sensitive to the concentration of the auto-inducer (labeled as  in the model) which is a small molecule that can pass through the cell membrane. Specifically, the module makes use of two proteins: (i) LuxI, which synthesizes the auto-inducer; (ii) LuxR, with which the auto-inducer synthesized by LuxI forms a complex that activates the transcription of various genes.<br>
We model the above circuit with the simplified set of differential equations proposed in [32]. Specifically, the dynamics of the mRNAs are(53)<br>
Notice that the above equations are dimensionless. This is done by: (i) measuring time in units of mRNA lifetime (which is assumed equal for the three genes), and (ii) expressing the protein levels in units of their Michaelis constant. The parameter  represents the dimensionless transcription rate in the absence of self-repression, while  denotes the maximum contribution of the auto-inducer to the expression of lacI.<br>
The dynamics of the proteins are described by(54)The parameters , ,  represent the ratios between the mRNAs and the respective proteins' lifetimes and , ,  represent the protein decay rate.<br>
The last differential equation of the model from [32] keeps track of the evolution of the intra-cellular auto-inducer. It is assumed that the proteins TetR and LuxI have equal lifetimes. This in turn implies that the dynamics of such proteins are identical, and hence one uses the same variable to describe both protein concentrations. Thus, the dynamics of the auto-inducer are given by:where  is the rate of degradation of .<br>
We now model the forcing on the intracellular auto-inducer concentration by adding an external input  to the above dynamical equation. The equation for  becomes:(55)where  can be thought as a diffusion rate.<br>
We will now use the analytical methodology developed in the previous sections, to properly tune the biochemical parameters of the Repressilator circuit, whose mathematical model consists of the set of differential equations (53), (54), (55), so that it shows entrainment to the periodic input . That is, the measured output (e.g. ), oscillates asymptotically with a period equal to that of . Of course, the periodic orbit of the output will depend on the particular choice of the parameters.<br>
In what follows, we assume that all the system parameters can be varied except for the self-degradations that we assume to be fixed as, in practice, they are difficult to modify.<br>
In this case, the Jacobian matrix to be studied is(56)<br>
The matrix measure that we will use to prove contraction iswhere  is a  diagonal matrix having on the main diagonal the positive arbitrary scalars . Computation of  yields(57)Thus, from the definition of  given in (29), we have that there exists some  such that ,  if and only if there exists a set of scalars , , such that(58a)(58b)(58c)(58d)(58e)(58f)(58g)<br>
It is easy to check that the nonlinear terms in the above equations satisfy the following inequalities:andfor all . Hence, the system of inequalities (58a)?(58g) are satisfied, if the following set is fulfilled:(59a)(59b)(59c)(59d)(59e)(59f)(59g)<br>
The system can then be proved to be contracting for a given set of biochemical parameters, if there exists a set of scalars ,  satisfying the above inequalities. For example, if the repressilator parameters are chosen so that(60)then it is trivial to prove that, for any constant value , the set of scalars , for , satisfies (59a)?(59g). Indeed, in Figure 10 we provide a set of biochemical parameters for which the circuit is contracting and shows entrainment to the periodic input . (These parameters, except for the maximal transcription rate , are in the same ranges as those used in [31], [32]. These parameters are also close to those used in [33] and [34]. The reason for picking an  much smaller than in [32], is that we need to slow down transcription so as to eliminate intrinsic oscillations; otherwise the entrainment effect cannot be shown. This lowering of  by two orders of magnitude is also found in other works, for example in [35], where the same model is studied, with  somewhat larger but of the same order of magnitude as here.)<br>
Note that using the set of inequalities (59a)?(59g) as a guideline, it is possible to find other parameter regions where the system is still contracting but exhibit some other desired properties. For instance, to tune (e.g. increase) the amplitude of the output oscillations shown in Figure 10, a possible approach can be that of increasing the biochemical parameter  so as to make stronger the effect of the auto-inducer on the dynamics of the gene  (variable  in the model).<br>
Again we can prove that the set of inequalities (59a)?(59g) is satisfied for  arbitrarily large, if we set , for  and choose  such thatandNow, due to biochemical constraints the parameter  is considerably smaller than  and  (in our simulations the ratio is of about two orders of magnitude). Therefore, whatever the value of , it suffices to set  and , with  being a positive arbitrary constant, to get a solution to (59a)?(59g) and hence guarantee the system to be contracting.<br>
Figure 11 shows the behavior of the system output with the modified parameters confirming that with this choice of parameters the oscillation amplitude is indeed larger as expected.<br>
Observe the nonlinear character of the oscillation depicted in Figure 11, which is reflected in the lack of symmetry in the behavior at minima and maxima of cI. Our theory predicts the existence (and uniqueness) of such a nonlinear oscillations. None of the usual techniques, based on linear analysis, can explain such behavior.<br>
<br>
Entrainment using an extra-cellular auto-inducer<br>
We now consider the case in which the extracellular auto-inducer can change due to an external signal as well as diffusion from intracellular auto-inducer, as represented in Figure 9. A new variable must be introduced, to keep track of the extracellular auto-inducer concentration. The only difference in the new model with respect to the previous one is that the differential equation for  becomes:(61)Notice that the parameter  measures the diffusion rate of the auto-inducer across the cell membrane, i.e. , with  representing the membrane permeability,  its surface area and  the cell volume. In the above equation,  denotes the concentration of the extra-cellular auto-inducer, whose dynamics are given by:(62)where , with  denoting the total extracellular volume, while  stands for the decay rate.<br>
In analogy with the previous section, we will ensure entrainment of the dynamical system consisting of (53), (54), (61), (62), by tuning the biochemical parameters of this new circuit. Again, the guidelines for engineering the parameters will be provided by the tools developed in the previous sections.<br>
Following the schematic of the previous section, we will prove that there exists  and a  constant diagonal matrix , such that , where  is the system Jacobian.<br>
If we denote with ,  the diagonal elements of , we obtain the following block-structure for the matrix :(63)where  is given in (57) and:(64)<br>
Thus, we have that  if and only if there exist some ,  such that inequalities (58a)?(58f) are all satisfied and additionally:(65a)(65b)<br>
Again, we can find sets of biochemical parameters in order to satisfy the above inequalities and hence ensure global entrainment of the circuit to some external input. For example, if we set(66)then, as in the previous section, it is trivial to show that setting all  to the same identical value satisfies the set of inequality required to prove contraction and hence guarantees entrainment. Notice that the last constraint in (66) is automatically satisfied by the physical (i.e. positivity) constraints on the system parameters.<br>
In Figure 12, the behavior of the circuit is shown with the parameters chosen so as to satisfy the constraints given in (66).<br>
<br>
Entraining a population of Repressilators<br>
Consider, now, a population of  Repressilator circuits, which are coupled by means of an auto-inducer molecule. We can think of such a network as having an all-to-all topology, with the coupling given by the concentration of the extracellular auto-inducer, . The aim of this section is to show that the methodology proposed here can also be used as an effective tool to guarantee the synchronization of an entire population of biochemical oscillators onto some entraining external periodic input.<br>
We denote with the subscript  the state variables of the -th circuit in the network, which is modelled using the equations reported in [32] as:(67)<br>
Figure 13 shows a simulation of a population of Repressilators modeled as in (67), with biochemical parameters tuned as in the previous Section: all the circuits composing the network evolve asymptotically towards the same synchronous evolution, which has period equal to that of the input signal . The interested reader is referred to the Materials and Methods for the proof.<br>
<br>
<br>
<br>
Materials and Methods<br>
All simulations are performed in <software>MATLAB</software> (<software>Simulink</software>), Version 7.4, with variable step ODE solver ODE23t. <software>Simulink</software> models are available upon request. The proofs of the results are as follows.<br>
-reachable sets<br>
We will make use of the following definition:<br>
Definition 1. Let  be any positive real number. A subset  is -reachable if, for any two points  and  in  there is some continuously differentiable curve  such that:<br>
For convex sets , we may pick , so  and we can take . Thus, convex sets are -reachable, and it is easy to show that the converse holds as well.<br>
Notice that a set  is -reachable for some  if and only if the length of the geodesic (smooth) path (parametrized by arc length), connecting any two points  and  in , is bounded by some multiple  of the Euclidean norm, . Indeed, re-parametrizing to a path  defined on , we have:Since in finite dimensional spaces all the norms are equivalent, then it is possible to obtain a suitable  for Definition 1.<br>
Remark 1. The notion of -reachable set is weaker than that of convex set. Nonetheless, in Theorem 5, we will prove that trajectories of a smooth system, evolving on a -reachable set, converge towards each other, even if  is not convex. This additional generality allows one to establish contracting behavior for systems evolving on phase spaces exhibiting ?obstacles?, as are frequently encountered in path-planning problems, for example. A mathematical example of a set with obstacles follows.<br>
Example 1. Consider the two dimensional set, , defined by the following constraints:Clearly,  is a non-convex subset of . We claim that  is -reachable, for any positive real number . Indeed, given any two points  and  in , there are two possibilities: either the segment connecting  and  is in , or it intersects the unit circle. In the first case, we can simply pick the segment as a curve (). In the second case, one can consider a straight segment that is modified by taking the shortest perimeter route around the circle; the length of the perimeter path is at most  times the length of the omitted segment. (In order to obtain a differentiable, instead of merely a piecewise-differentiable, path, an arbitrarily small increase in  is needed.)<br>
<br>
Proof of Theorem 1<br>
We now prove the main result on contracting systems, i.e. Theorem 1, under the hypotheses that the set , i.e. the set on which the system evolves, is -reachable.<br>
Theorem 5. Suppose that  is a -reachable subset of  and that  is infinitesimally contracting with contraction rate . Then, for every two solutions  and  it holds that:(68)<br>
Proof. Given any two points  and  in , pick a smooth curve , such that  and . Let , that is, the solution of system (1) rooted in , . Since  and  are continuously differentiable, also  is continuously differentiable in both arguments. We defineIt follows thatNow,so, we have:(69)where . Using Coppel's inequality [28], yields(70), , and . Notice the Fundamental Theorem of Calculus, we can writeHence, we obtainNow, using (70), the above inequality becomes:The Theorem is then proved.<br>
Proof of Theorem 1. The proof follows trivially from Theorem 5, after having noticed that in the convex case, we may assume .<br>
<br>
Proof of Theorem 2<br>
In this Section we assume that the vector field  is -periodic and prove Theorem 2.<br>
Before starting with the proof of Theorem 2 we make the following:<br>
Remark 2. Periodicity implies that the initial time is only relevant modulo . More precisely:(71)Indeed, let , , and consider the function , for . So, where the last equality follows by -periodicity of . Since , it follows by uniqueness of solutions that , which is (71). As a corollary, we also have that(72)where the first equality follows from the semigroup property of solutions (see e.g. [21]), and the second one from (71) applied to  instead of .<br>
Define nowwhere . The following Lemma will be useful in what follows.<br>
Lemma 1.  for all  and .<br>
Proof. We will prove the Lemma by recursion. In particular, the statement is true by definition when . Inductively, assuming it true for , we have:as wanted.<br>
Theorem 6. Suppose that:<br>
Then, there is an unique periodic solution  of (1) having period . Furthermore, every solution , such that , converges to , i.e.  as .<br>
Proof. Observe that  is a contraction with factor :  for all , as a consequence of Theorem 5. The set  is a closed subset of  and hence complete as a metric space with respect to the distance induced by the norm being considered. Thus, by the contraction mapping theorem, there is a (unique) fixed point  of . Let . Since ,  is a periodic orbit of period . Moreover, again by Theorem 5, we have that . Uniqueness is clear, since two different periodic orbits would be disjoint compact subsets, and hence at positive distance from each other, contradicting convergence. This completes the proof.<br>
Proof of Theorem 2. It will suffice to note that the assumption  in Theorem 6 is automatically satisfied when the set  is convex (i.e. ) and the system is infinitesimally contracting.<br>
Notice that, even in the non-convex case, the assumption  can be ignored, if we are willing to assert only the existence (and global convergence to) a unique periodic orbit, with some period  for some integer . Indeed, the vector field is also -periodic for any integer . Picking  large enough so that , we have the conclusion that such an orbit exists, applying Theorem 6.<br>
<br>
Cascades<br>
In order to show that cascades of contracting systems remain contracting, it is enough to show this, inductively, for a cascade of two systems.<br>
Consider a system of the following form:where  and  for all  ( and  are two -reachable sets). We write the Jacobian of  with respect to  as , the Jacobian of  with respect to  as , and the Jacobian of  with respect to  as ,<br>
We assume the following:<br>
We claim that, under these assumptions, the complete system is infinitesimally contracting. More precisely, pick any two positive numbers  and  such thatand letWe will show that , where  is the full Jacobian:(73)with respect to the matrix measure  induced by the following norm in :Sincefor all  and , we have that, for all  and :where from now on we drop subscripts for norms. Pick now any  and a unit vector  (which depends on ) such that . Such a vector  exists by the definition of induced matrix norm, and we note that , by the definition of the norm in the product space. Therefore:where the last inequality is a consequence of the fact that  for any nonnegative numbers with  (convex combination of the 's). Now taking limits as , we conclude thatas desired.<br>
<br>
Entraining a population of Repressilators: proof<br>
The general principle that we apply to prove entrainment of a population of Repressilators is as follows.<br>
Assume that the cascade system(74)with  being an exogenous input, satisfies the contractivity assumptions of the above Section. Then, consider the interconnection of  identical systems which interact through the variable  as follows:(75)Suppose that  is a solution of (75) defined for all , for some input . Then, we have the synchronization condition: , as .<br>
Indeed, we only need to observe that every pair  is a solution of (74) with the same input Furthermore, if  is a -periodic function, the  interconnected dynamical systems synchronize onto a -periodic trajectory.<br>
The above principle can be immediately applied to prove that synchronization onto a -periodic orbit is attained for the Repressilator circuits composing network (67) (see also [19]).<br>
Specifically, let  and ; we have that  is a solution of (67). We notice that any pair  is a solution of the following cascade system(76)Thus, as shown above, contraction of (76) implies synchronization of (67). Differentiation of (76) yields the Jacobian matrix(77)where  and  denote the partial derivatives of decreasing and increasing Hill functions with respect to the state variable of interest and , .<br>
Note that the Jacobian matrix  has the structure of a cascade, i.e.with:, . Thus, to prove contraction of the virtual system (76) it suffices to prove that there exist two matrix measures,  and  such that:<br>
where . Clearly, since  is a positive real parameter, the second condition above is satisfied (with  being any matrix measure). Now, notice that matrix  has the same form as the Jacobian matrix of the Repressilator circuit (56). Hence, if the parameters of the Repressilator are chosen so that they satisfy (66), then there exist a set of positive real parameters , , such that  (that is, the first condition above is also satisfied with ).<br>
Thus, we can conclude that (76) is contracting. Furthermore, all the trajectories of the virtual system converge towards a -periodic solution (see Theorem 6). This in turn implies that all the trajectories of network (67) converge towards the same -periodic solution. That is, all the nodes of (67) synchronize onto a periodic orbit of period .<br>
<br>
A counterexample to entrainment<br>
In [5] there is given an example of a system with the following property: when the external signal  is constant, all solutions converge to a steady state; however, when , solutions become chaotic. (Obviously, this system is not contracting.) The equations are as follows:where  and . Figure 14 shows typical solutions of this system with a periodic and constant input respectively. The function ?rand? was used in <software>MATLAB</software> to produce random values in the range .<br>
<br>
<br>
Discussion<br>
We have presented a systematic methodology to derive conditions for various types of biochemical systems to be globally entrained to periodic inputs. For concreteness, we focused mainly on transcriptional systems, which constitute basic building blocks for more complex biochemical systems. However, the results that we obtained are of more generality. To illustrate this generality, and to emphasize the use of our techniques in synthetic biology design, we discussed as well the entrainment of a Repressilator circuit in a parameter regime in which endogenous oscillations to not occur, as well as the synchronization of a network of Repressilators. These latter examples serve to illustrate the power of the tools even when a large amount of feedback is present.<br>
Our key tool is the use of contraction theory, which we believe should be recognized as an important component of the ?toolkit? of systems biology. In all cases conditions are derived by proving that the module of interest is contracting under appropriate generic assumptions on its parameters. A surprising fact is that, for these applications, and contrary to many engineering applications, norms other than Euclidean, and associated matrix measures, must be considered. Of course, more than one norm may be appropriate for a given problem: for example we can pick different 's in our weighted norms, and each such choice gives rise to a different estimate of convergence rates. This is entirely analogous to the use of Lyapunov functions in classical stability analysis: different Lyapunov functions provide different estimates.<br>
Ultimately, and as with any other method for the analysis of nonlinear systems, such as the classical tool of Lyapunov functions, finding the ?right? norm is more of an art than a science. A substantial amount of trial and error, intuition, and numerical experimentation may be needed in order to come up with an appropriate norm, and experience with a set of already-studied systems (such as the ones studied here) should prove invaluable in guiding the search.<br>
<br>
<br>
<br>
<p><hr><p>

</body></html>
